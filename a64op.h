/*°′″  «»  ≤≥  ≠≈  —¦  ÷×  !¡  ©®  £€  $¢  №⋕  λμ  πφ  ∑∏  ¶§  †‡  ±∞  √∆  ∫∳ 

This file defines the armv8 ops. It applies to all systems 
since support for Windows armv8 will not be implemented until
Microsoft implements the ACLE.

ARMv8 CONDITION FLAGS ("NZCV"):
    union nzcv {
        struct {
            uint64_t 
                :       28,
                V:      1, // signed overflow
                C:      1, // unsigned overflow
                Z:      1, // zero
                N:      1, // negative
                :       32
        };
        struct {
            uint64_t 
                :28, 
                All: 4,
                :32,
                :0;
        };
    };    
    0b0000 (v=0,c=0,z=0,n=0) ne,lo
    0b0001
    0b0010
    0b0011
    0b0100 eq
    0b0101
    0b0110
    0b0111
    0b1000
    0b1001
    0b1010
    0b1011
    0b1100
    0b1101
    0b1110
    0b1111
    
    
ARM CONDITION CODES:
    cc  c=0 ("C flag Clear")
    cs  c=1 ("C flag Set")
    vc  v=0 ("V flag Clear")
    vs  v=1 ("V flag Set")
    pl  n=0 ("PLus")
    mi  n=1 ("MInus)

    eq  z=1        1 (ra = rb, "EQual")
    ne  z=0        0 (ra ≠ rh, "Not Equal")

    hs  z=1 | c=1  5 (ua ≥ ub, "Higher or Same")
    ge  n=v        b (ia ≥ ib, "signed Greater or Equal")

    hi  z=0 & c=1  4 (ua > ub, "HIgher")
    gt  z=0 & n=v  a (ia > ib, "signed Greater Than")

    lo  c=0        0 (ua < ub, "unsigned LOwer"
    ls  z=1 & c=0  1 (ua ≤ ub, "Lower or Same")

    lt  n≠v        2 (ia < ib, "signed Less Than")
    le  z=1 | n≠v   (ia ≤ ib, "signed Less or Equal")
    
        vcnz
    0   0000 ne,lo
    1   0001 eq,ls
    2   0010 lt
    3   0011 le
    4   0100 hi
    5   0101 hs
    6   0110
    7   0111
    8   1000
    9   1001
    a   1010 gt
    b   1011 ge
    c   1100
    d   1101
    e   1110
    f   1111
 
 
    eq  0000 
    ne  0001
    cs  0010
    hs  0010
    cc  0011
    lo  0011
    mi  0100
    pl  0101
    vs  0110
    vc  0111
    hi  1000
    ls  1001
    ge  1010
    lt  1011
    gt  1100
    le  1101
    al  1110
    nv  1111
    
HISTORY:

2024-04-20
* implemented dupww

2024-04-21
* several changes to get1 and dupw

2024-04-23
* added packed bool load ops
* fixed cltryu
* implemented add2

2024-04-28
* (re)implemented sirr
* implemented sillw
* implemented silrw (hurriedly)
* implemented sprlw

2024-04-29
* (re)implemented lunl, lunr, sunl, and sunr
* completed most (all) missing 32 bit vector ops

2024-04-30
* replaced all occurrences of Vx_Ky with y
* updated newl 
* updated unol (after renaming unos to unol)
* implemented unor

2024-05-01
* fixed tstyw and tstsw
* simplified neglu
* deleted extraneous neglf defs

2024-05-02
* fixed various shl2 bugs 

2024-05-30
* fixed add2 (wasn't using vaddl)
* implemented sub2

2024-06-07
*   reimplemented bfsl. TODO: optimize 128 bit version

*/

/*  

https://developer.arm.com/documentation/ddi0602/2022-06/Shared-Pseudocode/Shared-Functions?lang=en

Note:
    s0 = x;
    s1 = l;
    s2 = r;
fcmp    s0, s2
    Compares s0 with s2 and sets PSTATE.{N,Z,C,V} as follows:

        if isnan(s0) or isnan(s2)
            //                  0b0011
            PSTATE = {.N=0, .Z=0, .C=1, .V=1} 
        elif s0 == s2:
            //                  0b0110
            PSTATE = {.N=0, .Z=1, .C=1, .V=0}
        elif s0 < s2:
            //                  0b1000
            PSTATE = {.N=1, .Z=0, .C=0, .V=0}
        else: // s0 > s2
            //                  0b0010
            PSTATE = {.N=0, .Z=0, .C=1, .V=0}

    if   s0 == s2: N=0
    elif s0 <  s2: N=1
    else         : N=0
    
movi    d2, #0000000000000000
    set d2 to 0 

fccmp   s0, s1, #8, pl
    pl means execute fcmp(s0,s1) only if PSTATE.N == 0
    otherwise, set nzcv=0b1000 (N=1). N=1 if s0 < s2

fmov    s0, #1.00000000
    set s0=1.0f

fcsel   s0, s2, s0, pl
    pl means set s0=s2 if N=0 otherwise set s0=s0

ret

fcmp    s0, s2
movi    d2, #0000000000000000
fccmp   s0, s1, #8, pl
fmov    s0, #1.00000000
fcsel   s0, s2, s0, pl


fcmp    x, r
    if   x == r: N=0 // nzcv=eq
    elif x <  r: N=1 // nzcv=lt
    else:        N=0 // nzcv=gt
    
movi    d2, #0000000000000000
    r = 0

fccmp   s0, s1, #8, pl
    if  PSTATE.N == 0: // fcmp(x, r) => eq|gt
        PSTATE.N = 1
    else:
        if   x == l: PSTATE = {.N=0, .Z=1, .C=1, .V=0}
        elif x <  l: PSTATE = {.N=1, .Z=0, .C=0, .N=0}
        else:        PSTATE = {.N=0, .Z=0, .C=1, .V=0}

fmov    s0, #1.00000000
    x = 1.0f
    
fcsel   s0, s2, s0, pl
    if PSTATE.N == 0: 
        x = r // aka 0
    else:
        x = x // aka 1.0f
float 
cnbywf(float x, float l, float r)
{
    if (vclts_f32(x, l)) // x < l
        return 1.0f;
    if (vclts_f32(r, x)) // r < x
        return 1.0f;
    return 0.0f;

    
}
*/


#include "allop.h"

union MY_A64_VQ {
    Vqyu Yu;
    Vqbu Bu; Vqbi Bi; Vqbc Bc;
    Vqhu Hu; Vqhi Hi; Vqhf Hf;
    Vqwu Wu; Vqwi Wi; Vqwf Wf;
    Vqdu Du; Vqdi Di; Vqdf Df;
    Vqqu Qu; Vqqi Qi; Vqqf Qf;
    QUAD_TYPE V;
};

#undef Vw
#undef Vd
#undef Vq

typedef union WORD_VTYPE
{
    float V0;
    uint32_t U;
    int32_t  I;
    float   F;
    struct {WORD_TYPE MY_NV1(W, );};
    struct {HALF_TYPE MY_NV2(H, );};
    struct {BYTE_TYPE MY_NV4(B, );};
    union {
        union {Vwyu U;}                 Y;
        union {Vwbu U; Vwbi I; Vwbc C;} B;
        union {Vwhu U; Vwhi I; Vwhf F;} H;
        union {Vwwu U; Vwwi I; Vwwf F;} W;
    };
} WORD_VTYPE;

#define Vw union WORD_VTYPE

typedef union DWRD_VTYPE
{
    double V0;
    double F;
    uint64_t U;
    int64_t I;
    struct {DWRD_TYPE MY_NV1(D,);};
    struct {WORD_TYPE MY_NV2(W,);};
    struct {HALF_TYPE MY_NV4(H,);};
    struct {BYTE_TYPE MY_NV8(B,);};
    struct {
        union {
            struct {WORD_TYPE MY_NV1(W,);};
            struct {HALF_TYPE MY_NV2(H,);};
            struct {BYTE_TYPE MY_NV4(B,);};
            union {Vwyu U;}                 Y;
            union {Vwbu U; Vwbi I; Vwbc C;} B;
            union {Vwhu U; Vwhi I; Vwhf F;} H;
            union {Vwwu U; Vwwi I; Vwwf F;} W;
            Vw V;
        } MY_NAT_PAIR(L, R, );
    };
    union {
        union {Vdyu U;}                 Y;
        union {Vdbu U; Vdbi I; Vdbc C;} B;
        union {Vdhu U; Vdhi I; Vdhf F;} H;
        union {Vdwu U; Vdwi I; Vdwf F;} W;
        union {Vddu U; Vddi I; Vddf F;} D;
    };
} DWRD_VTYPE;
_Static_assert( (8==sizeof(DWRD_VTYPE)), "aint 8");

#define Vd union DWRD_VTYPE

typedef union QUAD_VTYPE
{
    QUAD_FTYPE F;
    QUAD_UTYPE U;
    QUAD_ITYPE I;
    struct {QUAD_TYPE MY_NV1(Q, );};
    struct {DWRD_TYPE MY_NV2(D, );};
    struct {WORD_TYPE MY_NV4(W, );};
    struct {HALF_TYPE MY_NV8(H, );};
    struct {BYTE_TYPE MY_NV16(B, );};
    struct {
        union {
            struct {DWRD_TYPE MY_NV1(D, );};
            struct {WORD_TYPE MY_NV2(W, );};
            struct {HALF_TYPE MY_NV4(H, );};
            struct {BYTE_TYPE MY_NV8(B, );};
            union {Vdyu U;}                 Y;
            union {Vdbu U; Vdbi I; Vdbc C;} B;
            union {Vdhu U; Vdhi I; Vdhf F;} H;
            union {Vdwu U; Vdwi I; Vdwf F;} W;
            union {Vddu U; Vddi I; Vddf F;} D;
            Vd                              V;
        } MY_NAT_PAIR(L, R, );
    };
    union {
        union {Vqyu U;}                 Y;
        union {Vqbu U; Vqbi I; Vqbc C;} B;
        union {Vqhu U; Vqhi I; Vqhf F;} H;
        union {Vqwu U; Vqwi I; Vqwf F;} W;
        union {Vqdu U; Vqdi I; Vqdf F;} D;
        union {Vqqu U; Vqqi I; Vqqf F;} Q;
    };
} QUAD_VTYPE;

#define Vq union QUAD_VTYPE
_Static_assert((4==sizeof(Vw)), "Vw not 4 bytes");
_Static_assert((8==sizeof(Vd)), "Vd not 8 bytes");

#if 0 // _ENTER_ARM__
{
#endif

#define MY_VSET2(V, F, v, k0, k1) \
F(k1, F(k0, v, V##_K0), V##_K1)

#define MY_VSET4(V, F, v, k0, k1, k2, k3) \
F(k3, F(k2, MY_VSET2(V, F, v, k0, k1), V##_K2), V##_K3)

#define MY_VSET8(V, F, v, k0, k1, k2, k3, k4, k5, k6, k7) \
F(k7, F(k6, F(k5, F(k4, MY_VSET4(V, F, v, k0, k1, k2, k3), \
V##_K4), V##_K5), V##_K6), V##_K7)

/*  Putting these here to avoid 24+ if / else/endif blocks */

#if CHAR_MIN
#   define  DBC_ASYU        vreinterpret_u64_s8
#   define  DBC_ASBU        vreinterpret_u8_s8
#   define  DBC_ASBI(V)     (V)
#   define  DBC_ASHU        vreinterpret_u16_s8
#   define  DBC_ASHI        vreinterpret_s16_s8
#   define  DBC_ASHF        vreinterpret_f16_s8
#   define  DBC_ASWU        vreinterpret_u32_s8
#   define  DBC_ASWI        vreinterpret_s32_s8
#   define  DBC_ASWF        vreinterpret_f32_s8
#   define  DBC_ASDU        vreinterpret_u64_s8
#   define  DBC_ASDI        vreinterpret_s64_s8
#   define  DBC_ASDF        vreinterpret_f64_s8

#   define  QBC_ASYU        vreinterpretq_u64_s8
#   define  QBC_ASBU        vreinterpretq_u8_s8
#   define  QBC_ASBI(V)     (V)
#   define  QBC_ASHU        vreinterpretq_u16_s8
#   define  QBC_ASHI        vreinterpretq_s16_s8
#   define  QBC_ASHF        vreinterpretq_f16_s8
#   define  QBC_ASWU        vreinterpretq_u32_s8
#   define  QBC_ASWI        vreinterpretq_s32_s8
#   define  QBC_ASWF        vreinterpretq_f32_s8
#   define  QBC_ASDU        vreinterpretq_u64_s8
#   define  QBC_ASDI        vreinterpretq_s64_s8
#   define  QBC_ASDF        vreinterpretq_f64_s8

#else

#   define  DBC_ASYU        vreinterpret_u64_u8
#   define  DBC_ASBU(V)     (V)
#   define  DBC_ASBI        vreinterpret_s8_u8
#   define  DBC_ASHU        vreinterpret_u16_u8
#   define  DBC_ASHI        vreinterpret_s16_u8
#   define  DBC_ASHF        vreinterpret_f16_u8
#   define  DBC_ASWU        vreinterpret_u32_u8
#   define  DBC_ASWI        vreinterpret_s32_u8
#   define  DBC_ASWF        vreinterpret_f32_u8
#   define  DBC_ASDU        vreinterpret_u64_u8
#   define  DBC_ASDI        vreinterpret_s64_u8
#   define  DBC_ASDF        vreinterpret_f64_u8

#   define  QBC_ASYU        vreinterpretq_u64_u8
#   define  QBC_ASBU(V)     (V)
#   define  QBC_ASBI        vreinterpretq_s8_u8
#   define  QBC_ASHU        vreinterpretq_u16_u8
#   define  QBC_ASHI        vreinterpretq_s16_u8
#   define  QBC_ASHF        vreinterpretq_f16_u8
#   define  QBC_ASWU        vreinterpretq_u32_u8
#   define  QBC_ASWI        vreinterpretq_s32_u8
#   define  QBC_ASWF        vreinterpretq_f32_u8
#   define  QBC_ASDU        vreinterpretq_u64_u8
#   define  QBC_ASDI        vreinterpretq_s64_u8
#   define  QBC_ASDF        vreinterpretq_f64_u8
#endif

#if 0 // _ENTER_ARM_VOID
{
#endif

#define     FLT16_VOID  ((flt16_t) 0)
#define     FLT16_VOIDA ((flt16_t *) 0)
#define     FLT16_VOIDAC ((flt16_t const *) 0)

#define        FLT_VOID          (0.0F)
#define        FLT_VOIDA  ((float *) 0)
#define        FLT_VOIDAC  ((float const *) 0)

#define        DBL_VOID          (0.0)
#define        DBL_VOIDA ((double *) 0)
#define        DBL_VOIDAC ((double const *) 0)


#define     WYU_VOID    (0.0F)

#define     WBU_VOID    (0.0F)
#define     WBI_VOID    (0.0F)
#define     WBC_VOID    (0.0F)

#define     WHU_VOID    (0.0F)
#define     WHI_VOID    (0.0F)
#define     WHF_VOID    (0.0F)

#define     WWU_VOID    (0.0F)
#define     WWI_VOID    (0.0F)
#define     WWF_VOID    (0.0F)


#define     DYU_VOID vcreate_u64(0)
#define     DBU_VOID vcreate_u8(0)
#define     DBI_VOID vcreate_s8(0)
#if CHAR_MIN
#   define  DBC_VOID DBI_VOID
#else
#   define  DBC_VOID DBU_VOID
#endif

#define     DHU_VOID vcreate_u16(0)
#define     DHI_VOID vcreate_s16(0)
#define     DHF_VOID vcreate_f16(0)

#define     DWU_VOID vcreate_u32(0)
#define     DWI_VOID vcreate_s32(0)
#define     DWF_VOID vcreate_f32(0)

#define     DDU_VOID vcreate_u64(0)
#define     DDI_VOID vcreate_s64(0)
#define     DDF_VOID vcreate_f64(0)


#define     QYU_VOID vdupq_n_u64(0)
#define     QBU_VOID vdupq_n_u8(0)
#define     QBI_VOID vdupq_n_s8(0)

#if CHAR_MIN
#   define  QBC_VOID QBI_VOID
#else
#   define  QBC_VOID QBU_VOID
#endif

#define     QHU_VOID vdupq_n_u16(0)
#define     QHI_VOID vdupq_n_s16(0)
#define     QHF_VOID vreinterpretq_f16_u8(QBU_VOID)

#define     QWU_VOID vdupq_n_u32(0)
#define     QWI_VOID vdupq_n_s32(0)
#define     QWF_VOID vreinterpretq_f32_u8(QBU_VOID)

#define     QDU_VOID vdupq_n_u64(0)
#define     QDI_VOID vdupq_n_s64(0)
#define     QDF_VOID vreinterpretq_f64_u8(QBU_VOID)

#define     QQU_VOID vdupq_n_u64(0)
#define     QQI_VOID vdupq_n_s64(0)
#define     QQF_VOID ((QUAD_TYPE){0}).F

#define     VWYU_VOID   ((Vwyu){0})
#define     VWBU_VOID   ((Vwbu){0})
#define     VWBI_VOID   ((Vwbi){0})
#define     VWBC_VOID   ((Vwbc){0})
#define     VWHU_VOID   ((Vwhu){0})
#define     VWHI_VOID   ((Vwhi){0})
#define     VWHF_VOID   ((Vwhf){0})
#define     VWWU_VOID   ((Vwwu){0})
#define     VWWI_VOID   ((Vwwi){0})
#define     VWWF_VOID   ((Vwwf){0})

#define     VDYU_VOID   ((Vdyu){0})

#define     VDBU_VOID   ((Vdbu){0})
#define     VDBI_VOID   ((Vdbi){0})
#define     VDBC_VOID   ((Vdbc){0})

#define     VDHU_VOID   ((Vdhu){0})
#define     VDHI_VOID   ((Vdhi){0})
#define     VDHF_VOID   ((Vdhf){0})

#define     VDWU_VOID   ((Vdwu){0})
#define     VDWI_VOID   ((Vdwi){0})
#define     VDWF_VOID   ((Vdwf){0})

#define     VDDU_VOID   ((Vddu){0})
#define     VDDI_VOID   ((Vddi){0})
#define     VDDF_VOID   ((Vddf){0})

#define     VQYU_VOID   ((Vqyu){0})
#define     VQBU_VOID   ((Vqbu){0})
#define     VQBI_VOID   ((Vqbi){0})
#define     VQBC_VOID   ((Vqbc){0})

#define     VQHU_VOID   ((Vqhu){0})
#define     VQHI_VOID   ((Vqhi){0})
#define     VQHF_VOID   ((Vqhf){0})

#define     VQWU_VOID   ((Vqwu){0})
#define     VQWI_VOID   ((Vqwi){0})
#define     VQWF_VOID   ((Vqwf){0})

#define     VQDU_VOID   ((Vqdu){0})
#define     VQDI_VOID   ((Vqdi){0})
#define     VQDF_VOID   ((Vqdf){0})

#define     VQQU_VOID   ((Vqqu){0})
#define     VQQI_VOID   ((Vqqi){0})
#define     VQQF_VOID   ((Vqqf){0})

#if 0 // _LEAVE_ARM_VOID
}
#endif

#if 0 // _ENTER_ARM_ASTM
{
#endif

#define     VWYU_ASTM(V)    _Generic((V),VWYU_TYPE:(V).V0)
#define     VWBU_ASTM(V)    _Generic((V),VWBU_TYPE:(V).V0)
#define     VWBI_ASTM(V)    _Generic((V),VWBI_TYPE:(V).V0)
#define     VWBC_ASTM(V)    _Generic((V),VWBC_TYPE:(V).V0)
#define     VWHU_ASTM(V)    _Generic((V),VWHU_TYPE:(V).V0)
#define     VWHI_ASTM(V)    _Generic((V),VWHI_TYPE:(V).V0)
#define     VWHF_ASTM(V)    _Generic((V),VWHF_TYPE:(V).V0)
#define     VWWU_ASTM(V)    _Generic((V),VWWU_TYPE:(V).V0)
#define     VWWI_ASTM(V)    _Generic((V),VWWI_TYPE:(V).V0)
#define     VWWF_ASTM(V)    _Generic((V),VWWF_TYPE:(V).V0)

#define     VDYU_ASTM(V)    _Generic((V),VDYU_TYPE:(V).V0)
#define     VDBU_ASTM       VDBU_REQS
#define     VDBI_ASTM       VDBI_REQS
#define     VDBC_ASTM(V)    _Generic((V),VDBC_TYPE:(V).V0)
#define     VDHU_ASTM       VDHU_REQS
#define     VDHI_ASTM       VDHI_REQS
#define     VDHF_ASTM       VDHF_REQS
#define     VDWU_ASTM       VDWU_REQS
#define     VDWI_ASTM       VDWI_REQS
#define     VDWF_ASTM       VDWF_REQS
#define     VDDU_ASTM       VDDU_REQS
#define     VDDI_ASTM       VDDI_REQS
#define     VDDF_ASTM       VDDF_REQS

#define     VQYU_ASTM(V)    _Generic((V),VQYU_TYPE:(V).V0)
#define     VQBU_ASTM       VQBU_REQS
#define     VQBI_ASTM       VQBI_REQS
#define     VQBC_ASTM(V)    _Generic((V),VQBC_TYPE:(V).V0)
#define     VQHU_ASTM       VQHU_REQS
#define     VQHI_ASTM       VQHI_REQS
#define     VQHF_ASTM       VQHF_REQS
#define     VQWU_ASTM       VQWU_REQS
#define     VQWI_ASTM       VQWI_REQS
#define     VQWF_ASTM       VQWF_REQS
#define     VQDU_ASTM       VQDU_REQS
#define     VQDI_ASTM       VQDI_REQS
#define     VQDF_ASTM       VQDF_REQS
#if 0 // _LEAVE_ARM_ASTM
}
#endif

#if 0 // _ENTER_ARM_ASTV
{
#endif

#define     WYU_ASTV(M)     ((VWYU_TYPE){M})
#define     WBU_ASTV(M)     ((VWBU_TYPE){M})
#define     WBI_ASTV(M)     ((VWBI_TYPE){M})
#define     WBC_ASTV(M)     ((VWBC_TYPE){M})
#define     WHU_ASTV(M)     ((VWHU_TYPE){M})
#define     WHI_ASTV(M)     ((VWHI_TYPE){M})
#define     WHF_ASTV(M)     ((VWHF_TYPE){M})
#define     WWU_ASTV(M)     ((VWWU_TYPE){M})
#define     WWI_ASTV(M)     ((VWWI_TYPE){M})
#define     WWF_ASTV(M)     ((VWWF_TYPE){M})

#define     DYU_ASTV(M)     ((VDYU_TYPE){_Generic((M),VDYU_MTYPE:(M))})
#define     DBU_ASTV        VDBU_REQS
#define     DBI_ASTV        VDBI_REQS
#define     DBC_ASTV(M)     ((VDBC_TYPE){_Generic((M),VDBC_MTYPE:(M))})
#define     DHU_ASTV        VDHU_REQS
#define     DHI_ASTV        VDHI_REQS
#define     DHF_ASTV        VDHF_REQS
#define     DWU_ASTV        VDWU_REQS
#define     DWI_ASTV        VDWI_REQS
#define     DWF_ASTV        VDWF_REQS
#define     DDU_ASTV        VDDU_REQS
#define     DDI_ASTV        VDDI_REQS
#define     DDF_ASTV        VDDF_REQS

#define     QYU_ASTV(M)     ((VQYU_TYPE){M})
#define     QBU_ASTV        VQBU_REQS
#define     QBI_ASTV        VQBI_REQS
#define     QBC_ASTV(M)     ((VQBC_TYPE){M})
#define     QHU_ASTV        VQHU_REQS
#define     QHI_ASTV        VQHI_REQS
#define     QHF_ASTV        VQHF_REQS
#define     QWU_ASTV        VQWU_REQS
#define     QWI_ASTV        VQWI_REQS
#define     QWF_ASTV        VQWF_REQS
#define     QDU_ASTV        VQDU_REQS
#define     QDI_ASTV        VQDI_REQS
#define     QDF_ASTV        VQDF_REQS

INLINE(Vwwu,  UINT_ASTV)   (uint m)
{
#define     UINT_ASTV(M)     WWU_ASTV(((WORD_TYPE){.U=M}).F)
    return  UINT_ASTV(m);
}

INLINE(Vwwi,   INT_ASTV)    (int m)
{
#define     INT_ASTV(M)     WWI_ASTV(((WORD_TYPE){.I=M}).F)
    return  INT_ASTV(m);
}

INLINE(Vwwf,   FLT_ASTV)  (float m)
{
#define     FLT_ASTV(M)     WWF_ASTV(M)
    return  FLT_ASTV(m);
}


INLINE(Vddf,   DBL_ASTV) (double m) 
{
#define     DBL_ASTV    vdup_n_f64
    return  DBL_ASTV(m);
}

#if DWRD_NLONG == 2

INLINE(Vwwu, ULONG_ASTV)  (ulong m)
{
#define     ULONG_ASTV(M)   ((VWWU_TYPE){((WORD_TYPE){.U=M}).F})

    return  ULONG_ASTV(m);
}

INLINE(Vwwi,  LONG_ASTV)   (long m)
{
#define     LONG_ASTV(M)   ((VWWI_TYPE){((WORD_TYPE){.I=M}).F})
    return  LONG_ASTV(m);
}

#else

INLINE(Vddu, ULONG_ASTV)  (ulong m)
{
#define     ULONG_ASTV(M)   vdup_n_u64(M)
    return  ULONG_ASTV(m);
}

INLINE(Vddi,  LONG_ASTV)   (long m)
{
#define     LONG_ASTV(M)    vdup_n_s64(M)
    return  LONG_ASTV(m);
}

#endif

#if QUAD_NLLONG == 2
INLINE(Vddu,ULLONG_ASTV) (ullong m)
{
#define     ULLONG_ASTV(M)   vdup_n_u64(M)
    return  ULLONG_ASTV(m);
}

INLINE(Vddi, LLONG_ASTV)  (llong m)
{
#define     LLONG_ASTV(M)    vdup_n_s64(M)
    return  LLONG_ASTV(m);
}

INLINE(Vqqu,astvqu) (QUAD_UTYPE m)
{
    QUAD_TYPE   c = {.U=m};
    uint64x1_t  l = vdup_n_u64(c.Lo.U);
    uint64x1_t  r = vdup_n_u64(c.Hi.U);
    Vqqu        v = {vcombine_u64(l, r)};
    return      v;
}

INLINE(Vqqi,astvqi) (QUAD_ITYPE m)
{
    QUAD_TYPE   c = {.I=m};
    int64x1_t   l = vdup_n_s64(c.Lo.U);
    int64x1_t   r = vdup_n_s64(c.Hi.U);
    Vqqi        v = {vcombine_s64(l, r)};
    return      v;
}

#endif

INLINE(Vqqf,astvqf) (QUAD_FTYPE m)
{
    return ((Vqqf){m});
}


INLINE(uint32_t,VWWU_ASTV) (Vwwu v)
{
#   define  VWWU_ASTV(V)    (((WORD_TYPE){.F=VWWU_ASTM(V)}).U)
    return  VWWU_ASTV(v);
}

INLINE( int32_t,VWWI_ASTV) (Vwwi v)
{
#   define  VWWI_ASTV(V)    (((WORD_TYPE){.F=VWWI_ASTM(V)}).I)
    return  VWWI_ASTV(v);
}

INLINE(   float,VWWF_ASTV) (Vwwf v)
{
#   define  VWWF_ASTV(V)    VWWF_ASTM(V)
    return  VWWF_ASTV(v);
}


INLINE(uint64_t,VDDU_ASTV) (Vddu v)
{
#define     VDDU_ASTV(V)    vget_lane_u64(V,0)
    return  VDDU_ASTV(v);
}

INLINE( int64_t,VDDI_ASTV) (Vddi v)
{
#define     VDDI_ASTV(V)    vget_lane_s64(V,0)
    return  VDDI_ASTV(v);
}

INLINE(  double,VDDF_ASTV) (Vddf v)
{
#define     VDDF_ASTV(V)    vget_lane_f64(V,0)
    return  VDDF_ASTV(v);
}

INLINE(QUAD_UTYPE,VQQU_ASTV) (Vqqu x) 
{
#define     VQQU_ASTV(X) (((union MY_A64_VQ){.Qu=X}).V.U)
    return  VQQU_ASTV(x);
}

INLINE(QUAD_ITYPE,VQQI_ASTV) (Vqqi x) 
{
#define     VQQI_ASTV(X) (((union MY_A64_VQ){.Qi=X}).V.I)
    return  VQQI_ASTV(x);
}

INLINE(QUAD_FTYPE,VQQF_ASTV) (Vqqf v) {return v.V0;}


#if 0 // _LEAVE_ARM_ASTV
}
#endif

#if 0 // _ENTER_ARM_ASYU
{
#endif

#define     DBU_ASYU        vreinterpret_u64_u8
#define     DBI_ASYU        vreinterpret_u64_s8
#define     DHU_ASYU        vreinterpret_u64_u16
#define     DHI_ASYU        vreinterpret_u64_s16
#define     DHF_ASYU        vreinterpret_u64_f16
#define     DWU_ASYU        vreinterpret_u64_u32
#define     DWI_ASYU        vreinterpret_u64_s32
#define     DWF_ASYU        vreinterpret_u64_f32
#define     DDU_ASYU        VDDU_REQS
#define     DDI_ASYU        vreinterpret_u64_s64
#define     DDF_ASYU        vreinterpret_u64_f64

#define     QBU_ASYU        vreinterpretq_u64_u8
#define     QBI_ASYU        vreinterpretq_u64_s8
#define     QHU_ASYU        vreinterpretq_u64_u16
#define     QHI_ASYU        vreinterpretq_u64_s16
#define     QHF_ASYU        vreinterpretq_u64_f16
#define     QWU_ASYU        vreinterpretq_u64_u32
#define     QWI_ASYU        vreinterpretq_u64_s32
#define     QWF_ASYU        vreinterpretq_u64_f32
#define     QDU_ASYU        VQDU_REQS
#define     QDI_ASYU        vreinterpretq_u64_s64
#define     QDF_ASYU        vreinterpretq_u64_f64

INLINE(Vwyu,VWBU_ASYU) (Vwbu v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWBI_ASYU) (Vwbi v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWBC_ASYU) (Vwbc v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWHU_ASYU) (Vwhu v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWHI_ASYU) (Vwhi v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWHF_ASYU) (Vwhf v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWWU_ASYU) (Vwwu v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWWI_ASYU) (Vwwi v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWWF_ASYU) (Vwwf v) {return (VWYU_TYPE){v.V0};}

#define     VWBU_ASYU(V)    WYU_ASTV(VWBU_ASTM(V))
#define     VWBI_ASYU(V)    WYU_ASTV(VWBI_ASTM(V))
#define     VWBC_ASYU(V)    WYU_ASTV(VWBC_ASTM(V))
#define     VWHU_ASYU(V)    WYU_ASTV(VWHU_ASTM(V))
#define     VWHI_ASYU(V)    WYU_ASTV(VWHI_ASTM(V))
#define     VWHF_ASYU(V)    WYU_ASTV(VWHF_ASTM(V))
#define     VWWU_ASYU(V)    WYU_ASTV(VWWU_ASTM(V))
#define     VWWI_ASYU(V)    WYU_ASTV(VWWI_ASTM(V))
#define     VWWF_ASYU(V)    WYU_ASTV(VWWF_ASTM(V))

INLINE(Vdyu,VDBU_ASYU) (Vdbu v)
{
#define     VDBU_ASYU(V)    DYU_ASTV(DBU_ASYU(VDBU_ASTM(V)))
    return  VDBU_ASYU(v);
}

INLINE(Vdyu,VDBI_ASYU) (Vdbi v)
{
#define     VDBI_ASYU(V)    DYU_ASTV(DBI_ASYU(VDBI_ASTM(V)))
    return  VDBI_ASYU(v);
}

INLINE(Vdyu,VDBC_ASYU) (Vdbc v)
{
#define     VDBC_ASYU(V)    DYU_ASTV(DBC_ASYU(VDBC_ASTM(V)))
    return  VDBC_ASYU(v);
}


INLINE(Vdyu,VDHU_ASYU) (Vdhu v)
{
#define     VDHU_ASYU(V)    DYU_ASTV(DHU_ASYU(VDHU_ASTM(V)))
    return  VDHU_ASYU(v);
}

INLINE(Vdyu,VDHI_ASYU) (Vdhi v)
{
#define     VDHI_ASYU(V)    DYU_ASTV(DHI_ASYU(VDHI_ASTM(V)))
    return  VDHI_ASYU(v);
}

INLINE(Vdyu,VDHF_ASYU) (Vdhf v)
{
#define     VDHF_ASYU(V)    DYU_ASTV(DHF_ASYU(VDHF_ASTM(V)))
    return  VDHF_ASYU(v);
}


INLINE(Vdyu,VDWU_ASYU) (Vdwu v)
{
#define     VDWU_ASYU(V)    DYU_ASTV(DWU_ASYU(VDWU_ASTM(V)))
    return  VDWU_ASYU(v);
}

INLINE(Vdyu,VDWI_ASYU) (Vdwi v)
{
#define     VDWI_ASYU(V)    DYU_ASTV(DWI_ASYU(VDWI_ASTM(V)))
    return  VDWI_ASYU(v);
}

INLINE(Vdyu,VDWF_ASYU) (Vdwf v)
{
#define     VDWF_ASYU(V)    DYU_ASTV(DWF_ASYU(VDWF_ASTM(V)))
    return  VDWF_ASYU(v);
}


INLINE(Vdyu,VDDU_ASYU) (Vddu v)
{
#define     VDDU_ASYU(V)    DYU_ASTV(V)
    return  VDDU_ASYU(v);
}

INLINE(Vdyu,VDDI_ASYU) (Vddi v)
{
#define     VDDI_ASYU(V)    DYU_ASTV(DDI_ASYU(V))
    return  VDDI_ASYU(v);
}

INLINE(Vdyu,VDDF_ASYU) (Vddf v)
{
#define     VDDF_ASYU(V)    DYU_ASTV(DDF_ASYU(V))
    return  VDDF_ASYU(v);
}


INLINE(Vqyu,VQBU_ASYU) (Vqbu v)
{
#define     VQBU_ASYU(V)    QYU_ASTV(QBU_ASYU(V))
    return  VQBU_ASYU(v);
}

INLINE(Vqyu,VQBI_ASYU) (Vqbi v)
{
#define     VQBI_ASYU(V)    QYU_ASTV(QBI_ASYU(V))
    return  VQBI_ASYU(v);
}

INLINE(Vqyu,VQBC_ASYU) (Vqbc v)
{
#define     VQBC_ASYU(V)    QYU_ASTV(QBC_ASYU(VQBC_ASTM(V)))
    return  VQBC_ASYU(v);
}


INLINE(Vqyu,VQHU_ASYU) (Vqhu v)
{
#define     VQHU_ASYU(V)    QYU_ASTV(QHU_ASYU(V))
    return  VQHU_ASYU(v);
}

INLINE(Vqyu,VQHI_ASYU) (Vqhi v)
{
#define     VQHI_ASYU(V)    QYU_ASTV(QHI_ASYU(V))
    return  VQHI_ASYU(v);
}

INLINE(Vqyu,VQHF_ASYU) (Vqhf v)
{
#define     VQHF_ASYU(V)    QYU_ASTV(QHF_ASYU(V))
    return  VQHF_ASYU(v);
}


INLINE(Vqyu,VQWU_ASYU) (Vqwu v)
{
#define     VQWU_ASYU(V)    QYU_ASTV(QWU_ASYU(V))
    return  VQWU_ASYU(v);
}

INLINE(Vqyu,VQWI_ASYU) (Vqwi v)
{
#define     VQWI_ASYU(V)    QYU_ASTV(QWI_ASYU(V))
    return  VQWI_ASYU(v);
}

INLINE(Vqyu,VQWF_ASYU) (Vqwf v)
{
#define     VQWF_ASYU(V)    QYU_ASTV(QWF_ASTV(V))
    return  VQWF_ASYU(v);
}


INLINE(Vqyu,VQDU_ASYU) (Vqdu v)
{
#define     VQDU_ASYU(V)    QYU_ASTV(V)
    return  VQDU_ASYU(v);
}

INLINE(Vqyu,VQDI_ASYU) (Vqdi v)
{
#define     VQDI_ASYU(V)    QYU_ASTV(QDI_ASYU(V))
    return  VQDI_ASYU(v);
}

INLINE(Vqyu,VQDF_ASYU) (Vqdf v)
{
#define     VQDF_ASYU(V)    QYU_ASTV(QDF_ASYU(V))
    return  VQDF_ASYU(v);
}

INLINE(Vqyu,VQQU_ASYU) (Vqqu v) 
{
    return  ((Vqyu){v.V0});
}

INLINE(Vqyu,VQQI_ASYU) (Vqqi v) 
{
    return  ((Vqyu){vreinterpretq_u64_s64(v.V0)});
}

INLINE(Vqyu,VQQF_ASYU) (Vqqf v) 
{
    return  ((union MY_A64_VQ){.Qf=v}).Yu;
}

#if 0 // _LEAVE_ARM_ASYU
}
#endif

#if 0 // _ENTER_ARM_ASBU
{
#endif

#define     DYU_ASBU        vreinterpret_u8_u64
//efine     DBU_ASBU
#define     DBI_ASBU        vreinterpret_u8_s8
//efine     DBC_ASBU
#define     DHU_ASBU        vreinterpret_u8_u16
#define     DHI_ASBU        vreinterpret_u8_s16
#define     DHF_ASBU        vreinterpret_u8_f16
#define     DWU_ASBU        vreinterpret_u8_u32
#define     DWI_ASBU        vreinterpret_u8_s32
#define     DWF_ASBU        vreinterpret_u8_f32
#define     DDU_ASBU        vreinterpret_u8_u64
#define     DDI_ASBU        vreinterpret_u8_s64
#define     DDF_ASBU        vreinterpret_u8_f64

#define     QYU_ASBU        vreinterpretq_u8_u64
//efine     QBU_ASBU
#define     QBI_ASBU        vreinterpretq_u8_s8
//efine     QBC_ASBU
#define     QHU_ASBU        vreinterpretq_u8_u16
#define     QHU_ASBU        vreinterpretq_u8_u16
#define     QHI_ASBU        vreinterpretq_u8_s16
#define     QHF_ASBU        vreinterpretq_u8_f16
#define     QWU_ASBU        vreinterpretq_u8_u32
#define     QWI_ASBU        vreinterpretq_u8_s32
#define     QWF_ASBU        vreinterpretq_u8_f32
#define     QDU_ASBU        vreinterpretq_u8_u64
#define     QDI_ASBU        vreinterpretq_u8_s64
#define     QDF_ASBU        vreinterpretq_u8_f64

INLINE(Vwbu,VWYU_ASBU) (Vwyu v)
{
#define     VWYU_ASBU(V)    WBU_ASTV(VWYU_ASTM(V))
    return  (Vwbu){VWYU_ASTM(v)};
}

INLINE(Vwbu,VWBI_ASBU) (Vwbi v)
{
#   define  VWBI_ASBU(V)    WBU_ASTV(VWBI_ASTM(V))
    return  VWBI_ASBU(v);
}

INLINE(Vwbu,VWBC_ASBU) (Vwbc v)
{
#   define  VWBC_ASBU(V)    WBU_ASTV(VWBC_ASTM(V))
    return  VWBC_ASBU(v);
}


INLINE(Vwbu,VWHU_ASBU) (Vwhu v)
{
#define     VWHU_ASBU(V)    WBU_ASTV(VWHU_ASTM(V))
    return  VWHU_ASBU(v);
}

INLINE(Vwbu,VWHI_ASBU) (Vwhi v)
{
#define     VWHI_ASBU(V)    WBU_ASTV(VWHI_ASTM(V))
    return  VWHI_ASBU(v);
}

INLINE(Vwbu,VWHF_ASBU) (Vwhf v)
{
#define     VWHF_ASBU(V)    WBU_ASTV(VWHF_ASTM(V))
    return  VWHF_ASBU(v);
}


INLINE(Vwbu,VWWU_ASBU) (Vwwu v)
{
#define     VWWU_ASBU(V)    WBU_ASTV(VWWU_ASTM(V))
    return  VWWU_ASBU(v);
}

INLINE(Vwbu,VWWI_ASBU) (Vwwi v)
{
#define     VWWI_ASBU(V)    WBU_ASTV(VWWI_ASTM(V))
    return  VWWI_ASBU(v);
}

INLINE(Vwbu,VWWF_ASBU) (Vwwf v)
{
#define     VWWF_ASBU(V)    WBU_ASTV(VWWF_ASTM(V))
    return  VWWF_ASBU(v);
}


INLINE(Vdbu,VDYU_ASBU) (Vdyu v)
{
#define     VDYU_ASBU(V)    DYU_ASBU(VDYU_ASTM(V))
    return  VDYU_ASBU(v);
}

INLINE(Vdbu,VDBI_ASBU) (Vdbi v)
{
    return  vreinterpret_u8_s8(v);
}

INLINE(Vdbu,VDBC_ASBU) (Vdbc v)
{
#   define  VDBC_ASBU(V)    DBC_ASBU(VDBC_ASTM(V))
    return  VDBC_ASBU(v);
}

INLINE(Vdbu,VDHU_ASBU) (Vdhu v) {return vreinterpret_u8_u16(v);}
INLINE(Vdbu,VDHI_ASBU) (Vdhi v) {return vreinterpret_u8_s16(v);}
INLINE(Vdbu,VDHF_ASBU) (Vdhf v) {return vreinterpret_u8_f16(v);}
INLINE(Vdbu,VDWU_ASBU) (Vdwu v) {return vreinterpret_u8_u32(v);}
INLINE(Vdbu,VDWI_ASBU) (Vdwi v) {return vreinterpret_u8_s32(v);}
INLINE(Vdbu,VDWF_ASBU) (Vdwf v) {return vreinterpret_u8_f32(v);}
INLINE(Vdbu,VDDU_ASBU) (Vddu v) {return vreinterpret_u8_u64(v);}
INLINE(Vdbu,VDDI_ASBU) (Vddi v) {return vreinterpret_u8_s64(v);}
INLINE(Vdbu,VDDF_ASBU) (Vddf v) {return vreinterpret_u8_f64(v);}


INLINE(Vqbu,VQYU_ASBU) (Vqyu v)
{
#define     VQYU_ASBU(V)    QYU_ASBU(VQYU_ASTM(V))
    return  VQYU_ASBU(v);
}

INLINE(Vqbu,VQBI_ASBU) (Vqbi v) {return vreinterpretq_u8_s8(v);}
INLINE(Vqbu,VQBC_ASBU) (Vqbc v)
{
#   define  VQBC_ASBU(V)    QBC_ASBU(VQBC_ASTM(V))
    return  VQBC_ASBU(v);
}

INLINE(Vqbu,VQHU_ASBU) (Vqhu v) {return vreinterpretq_u8_u16(v);}
INLINE(Vqbu,VQHI_ASBU) (Vqhi v) {return vreinterpretq_u8_s16(v);}
INLINE(Vqbu,VQHF_ASBU) (Vqhf v) {return vreinterpretq_u8_f16(v);}
INLINE(Vqbu,VQWU_ASBU) (Vqwu v) {return vreinterpretq_u8_u32(v);}
INLINE(Vqbu,VQWI_ASBU) (Vqwi v) {return vreinterpretq_u8_s32(v);}
INLINE(Vqbu,VQWF_ASBU) (Vqwf v) {return vreinterpretq_u8_f32(v);}
INLINE(Vqbu,VQDU_ASBU) (Vqdu v) {return vreinterpretq_u8_u64(v);}
INLINE(Vqbu,VQDI_ASBU) (Vqdi v) {return vreinterpretq_u8_s64(v);}
INLINE(Vqbu,VQDF_ASBU) (Vqdf v) {return vreinterpretq_u8_f64(v);}
INLINE(Vqbu,VQQU_ASBU) (Vqqu v) {return vreinterpretq_u8_u64(v.V0);}
INLINE(Vqbu,VQQI_ASBU) (Vqqi v) {return vreinterpretq_u8_s64(v.V0);}
INLINE(Vqbu,VQQF_ASBU) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Bu;}

#if 0 // _LEAVE_ARM_ASBU
}
#endif

#if 0 // _ENTER_ARM_ASBI
{
#endif

#define     DYU_ASBI        vreinterpret_s8_u64
#define     DBU_ASBI        vreinterpret_s8_u8
//          DBC_ASBI
#define     DHU_ASBI        vreinterpret_s8_u16
#define     DHI_ASBI        vreinterpret_s8_s16
#define     DHF_ASBI        vreinterpret_s8_f16
#define     DWU_ASBI        vreinterpret_s8_u32
#define     DWI_ASBI        vreinterpret_s8_s32
#define     DWF_ASBI        vreinterpret_s8_f32
#define     DDU_ASBI        vreinterpret_s8_u64
#define     DDI_ASBI        vreinterpret_s8_s64
#define     DDF_ASBI        vreinterpret_s8_f64

#define     QYU_ASBI        vreinterpretq_s8_u64
#define     QBU_ASBI        vreinterpretq_s8_u8
#define     QHU_ASBI        vreinterpretq_s8_u16
#define     QHI_ASBI        vreinterpretq_s8_s16
#define     QHF_ASBI        vreinterpretq_s8_f16
#define     QWU_ASBI        vreinterpretq_s8_u32
#define     QWI_ASBI        vreinterpretq_s8_s32
#define     QWF_ASBI        vreinterpretq_s8_f32
#define     QDU_ASBI        vreinterpretq_s8_u64
#define     QDI_ASBI        vreinterpretq_s8_s64
#define     QDF_ASBI        vreinterpretq_s8_f64

INLINE(Vwbi,VWYU_ASBI) (Vwyu v)
{
#define     VWYU_ASBI(V)    WBI_ASTV(VWYU_ASTM(V))
    return  VWYU_ASBI(v);
}

INLINE(Vwbi,VWBU_ASBI) (Vwbu v)
{
#   define  VWBU_ASBI(V)    WBI_ASTV(VWBU_ASTM(V))
    return  VWBU_ASBI(v);
}

INLINE(Vwbi,VWBC_ASBI) (Vwbc v)
{
#   define  VWBC_ASBI(V)    WBI_ASTV(VWBC_ASTM(V))
    return  VWBC_ASBI(v);
}


INLINE(Vwbi,VWHU_ASBI) (Vwhu v)
{
#define     VWHU_ASBI(V)    WBI_ASTV(VWHU_ASTM(V))
    return  VWHU_ASBI(v);
}

INLINE(Vwbi,VWHI_ASBI) (Vwhi v)
{
#define     VWHI_ASBI(V)    WBI_ASTV(VWHI_ASTM(V))
    return  VWHI_ASBI(v);
}

INLINE(Vwbi,VWHF_ASBI) (Vwhf v)
{
#define     VWHF_ASBI(V)    WBI_ASTV(VWHF_ASTM(V))
    return  VWHF_ASBI(v);
}


INLINE(Vwbi,VWWU_ASBI) (Vwwu v)
{
#define     VWWU_ASBI(V)    WBI_ASTV(VWWU_ASTM(V))
    return  VWWU_ASBI(v);
}

INLINE(Vwbi,VWWI_ASBI) (Vwwi v)
{
#define     VWWI_ASBI(V)    WBI_ASTV(VWWI_ASTM(V))
    return  VWWI_ASBI(v);
}

INLINE(Vwbi,VWWF_ASBI) (Vwwf v)
{
#define     VWWF_ASBI(V)    WBI_ASTV(VWWF_ASTM(V))
    return  VWWF_ASBI(v);
}


INLINE(Vdbi,VDYU_ASBI) (Vdyu v)
{
#define     VDYU_ASBI(V)    DYU_ASBI(VDYU_ASTM(V))
    return  VDYU_ASBI(v);
}

INLINE(Vdbi,VDBU_ASBI) (Vdbu v) {return vreinterpret_s8_u8(v);}

INLINE(Vdbi,VDBC_ASBI) (Vdbc v)
{
#define     VDBC_ASBI(V)    DBC_ASBI(VDBC_ASTM(V))
    return  VDBC_ASBI(v);
}

INLINE(Vdbi,VDHU_ASBI) (Vdhu v) {return vreinterpret_s8_u16(v);}
INLINE(Vdbi,VDHI_ASBI) (Vdhi v) {return vreinterpret_s8_s16(v);}
INLINE(Vdbi,VDHF_ASBI) (Vdhf v) {return vreinterpret_s8_f16(v);}
INLINE(Vdbi,VDWU_ASBI) (Vdwu v) {return vreinterpret_s8_u32(v);}
INLINE(Vdbi,VDWI_ASBI) (Vdwi v) {return vreinterpret_s8_s32(v);}
INLINE(Vdbi,VDWF_ASBI) (Vdwf v) {return vreinterpret_s8_f32(v);}
INLINE(Vdbi,VDDU_ASBI) (Vddu v) {return vreinterpret_s8_u64(v);}
INLINE(Vdbi,VDDI_ASBI) (Vddi v) {return vreinterpret_s8_s64(v);}
INLINE(Vdbi,VDDF_ASBI) (Vddf v) {return vreinterpret_s8_f64(v);}

INLINE(Vqbi,VQYU_ASBI) (Vqyu v)
{
#define     VQYU_ASBI(V)    QYU_ASBI(VQYU_ASTM(V))
    return  VQYU_ASBI(v);
}

INLINE(Vqbi,VQBU_ASBI) (Vqbu v) {return vreinterpretq_s8_u8(v);}

INLINE(Vqbi,VQBC_ASBI) (Vqbc v)
{
#   define  VQBC_ASBI(V)    QBC_ASBI(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqbi,VQHU_ASBI) (Vqhu v) {return vreinterpretq_s8_u16(v);}
INLINE(Vqbi,VQHI_ASBI) (Vqhi v) {return vreinterpretq_s8_s16(v);}
INLINE(Vqbi,VQHF_ASBI) (Vqhf v) {return vreinterpretq_s8_f16(v);}
INLINE(Vqbi,VQWU_ASBI) (Vqwu v) {return vreinterpretq_s8_u32(v);}
INLINE(Vqbi,VQWI_ASBI) (Vqwi v) {return vreinterpretq_s8_s32(v);}
INLINE(Vqbi,VQWF_ASBI) (Vqwf v) {return vreinterpretq_s8_f32(v);}
INLINE(Vqbi,VQDU_ASBI) (Vqdu v) {return vreinterpretq_s8_u64(v);}
INLINE(Vqbi,VQDI_ASBI) (Vqdi v) {return vreinterpretq_s8_s64(v);}
INLINE(Vqbi,VQDF_ASBI) (Vqdf v) {return vreinterpretq_s8_f64(v);}
INLINE(Vqbi,VQQU_ASBI) (Vqqu v) {return vreinterpretq_s8_u64(v.V0);}
INLINE(Vqbi,VQQI_ASBI) (Vqqi v) {return vreinterpretq_s8_s64(v.V0);}
INLINE(Vqbi,VQQF_ASBI) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Bi;}

#if 0 // _LEAVE_ARM_ASBI
}
#endif

#if 0 // _ENTER_ARM_ASBC
{
#endif

#if CHAR_MIN

#   define  DYU_ASBC        vreinterpret_s8_u64
#   define  DBU_ASBC        vreinterpret_s8_u8
#   define  DBI_ASBC(M)     (M)
//  define  DBC_ASBC
#   define  DHU_ASBC        vreinterpret_s8_u16
#   define  DHI_ASBC        vreinterpret_s8_s16
#   define  DHF_ASBC        vreinterpret_s8_f16
#   define  DWU_ASBC        vreinterpret_s8_u32
#   define  DWI_ASBC        vreinterpret_s8_s32
#   define  DWF_ASBC        vreinterpret_s8_f32
#   define  DDU_ASBC        vreinterpret_s8_u64
#   define  DDI_ASBC        vreinterpret_s8_s64
#   define  DDF_ASBC        vreinterpret_s8_f64

#   define  QYU_ASBC        vreinterpretq_s8_u64
#   define  QBU_ASBC        vreinterpretq_s8_u8
#   define  QBI_ASBC(M)     (M)
//  define  QBC_ASBC
#   define  QHU_ASBC        vreinterpretq_s8_u16
#   define  QHI_ASBC        vreinterpretq_s8_s16
#   define  QHF_ASBC        vreinterpretq_s8_f16
#   define  QWU_ASBC        vreinterpretq_s8_u32
#   define  QWI_ASBC        vreinterpretq_s8_s32
#   define  QWF_ASBC        vreinterpretq_s8_f32
#   define  QDU_ASBC        vreinterpretq_s8_u64
#   define  QDI_ASBC        vreinterpretq_s8_s64
#   define  QDF_ASBC        vreinterpretq_s8_f64

#else

#   define  DYU_ASBC        vreinterpret_u8_u64
#   define  DBU_ASBC(M)     (M)
#   define  DBI_ASBC        vreinterpret_u8_s8
//  define  DBC_ASBC
#   define  DHU_ASBC        vreinterpret_u8_u16
#   define  DHI_ASBC        vreinterpret_u8_s16
#   define  DHF_ASBC        vreinterpret_u8_f16
#   define  DWU_ASBC        vreinterpret_u8_u32
#   define  DWI_ASBC        vreinterpret_u8_s32
#   define  DWF_ASBC        vreinterpret_u8_f32
#   define  DDU_ASBC        vreinterpret_u8_u64
#   define  DDI_ASBC        vreinterpret_u8_s64
#   define  DDF_ASBC        vreinterpret_u8_f64

#   define  QYU_ASBC        vreinterpretq_u8_u64
#   define  QBU_ASBC(M)     (M)
#   define  QBI_ASBC        vreinterpretq_u8_s8
//  define  QBC_ASBC
#   define  QHU_ASBC        vreinterpretq_u8_u16
#   define  QHI_ASBC        vreinterpretq_u8_s16
#   define  QHF_ASBC        vreinterpretq_u8_f16
#   define  QWU_ASBC        vreinterpretq_u8_u32
#   define  QWI_ASBC        vreinterpretq_u8_s32
#   define  QWF_ASBC        vreinterpretq_u8_f32
#   define  QDU_ASBC        vreinterpretq_u8_u64
#   define  QDI_ASBC        vreinterpretq_u8_s64
#   define  QDF_ASBC        vreinterpretq_u8_f64

#endif

INLINE(Vwbc,VWYU_ASBC) (Vwyu v)
{
#define     VWYU_ASBC(V)    WBC_ASTV(VWYU_ASTM(V))
    return  VWYU_ASBC(v);
}

INLINE(Vwbc,VWBU_ASBC) (Vwbu v)
{
#define     VWBU_ASBC(V)    WBC_ASTV(VWBU_ASTM(V))
    return  VWBU_ASBC(v);
}

INLINE(Vwbc,VWBI_ASBC) (Vwbi v)
{
#define     VWBI_ASBC(V)    WBC_ASTV(VWBI_ASTM(V))
    return                  WBC_ASTV(VWBI_ASTM(v));
}


INLINE(Vwbc,VWHU_ASBC) (Vwhu v)
{
#define     VWHU_ASBC(V)    WBC_ASTV(VWHU_ASTM(V))
    return                  WBC_ASTV(VWHU_ASTM(v));
}

INLINE(Vwbc,VWHI_ASBC) (Vwhi v)
{
#define     VWHI_ASBC(V)    WBC_ASTV(VWHI_ASTM(V))
    return                  WBC_ASTV(VWHI_ASTM(v));
}

INLINE(Vwbc,VWHF_ASBC) (Vwhf v)
{
#define     VWHF_ASBC(V)    WBC_ASTV(VWHF_ASTM(V))
    return                  WBC_ASTV(VWHF_ASTM(v));
}


INLINE(Vwbc,VWWU_ASBC) (Vwwu v)
{
#define     VWWU_ASBC(V)    WBC_ASTV(VWWU_ASTM(V))
    return                  WBC_ASTV(VWWU_ASTM(v));
}

INLINE(Vwbc,VWWI_ASBC) (Vwwi v)
{
#define     VWWI_ASBC(V)    WBC_ASTV(VWWI_ASTM(V))
    return                  WBC_ASTV(VWWI_ASTM(v));
}

INLINE(Vwbc,VWWF_ASBC) (Vwwf v)
{
#define     VWWF_ASBC(V)    WBC_ASTV(VWWF_ASTM(V))
    return                  WBC_ASTV(VWWF_ASTM(v));
}


INLINE(Vdbc,VDYU_ASBC) (Vdyu v)
{
#define     VDYU_ASBC(V)    DBC_ASTV(DYU_ASBC(VDYU_ASTM(V)))
    return  VDYU_ASBC(v);
}

INLINE(Vdbc,VDBU_ASBC) (Vdbu v)
{
#define     VDBU_ASBC(V)    DBC_ASTV(DBU_ASBC(V))
    return  VDBU_ASBC(v);
}

INLINE(Vdbc,VDBI_ASBC) (Vdbi v)
{
#define     VDBI_ASBC(V)    DBC_ASTV(DBI_ASBC(V))
    return  VDBI_ASBC(v);
}


INLINE(Vdbc,VDHU_ASBC) (Vdhu v)
{
#define     VDHU_ASBC(V)    DBC_ASTV(DHU_ASBC(V))
    return  VDHU_ASBC(v);
}

INLINE(Vdbc,VDHI_ASBC) (Vdhi v)
{
#define     VDHI_ASBC(V)    DBC_ASTV(DHI_ASBC(V))
    return  VDHI_ASBC(v);
}

INLINE(Vdbc,VDHF_ASBC) (Vdhf v)
{
#define     VDHF_ASBC(V)    DBC_ASTV(DHF_ASBC(V))
    return  VDHF_ASBC(v);
}


INLINE(Vdbc,VDWU_ASBC) (Vdwu v)
{
#define     VDWU_ASBC(V)    DBC_ASTV(DWU_ASBC(V))
    return  VDWU_ASBC(v);
}

INLINE(Vdbc,VDWI_ASBC) (Vdwi v)
{
#define     VDWI_ASBC(V)    DBC_ASTV(DWI_ASBC(V))
    return  VDWI_ASBC(v);
}

INLINE(Vdbc,VDWF_ASBC) (Vdwf v)
{
#define     VDWF_ASBC(V)    DBC_ASTV(DWF_ASBC(V))
    return  VDWF_ASBC(v);
}


INLINE(Vdbc,VDDU_ASBC) (Vddu v)
{
#define     VDDU_ASBC(V)    DBC_ASTV(DDU_ASBC(V))
    return  VDDU_ASBC(v);
}

INLINE(Vdbc,VDDI_ASBC) (Vddi v)
{
#define     VDDI_ASBC(V)    DBC_ASTV(DDI_ASBC(V))
    return  VDDI_ASBC(v);
}

INLINE(Vdbc,VDDF_ASBC) (Vddf v)
{
#define     VDDF_ASBC(V)    DBC_ASTV(DDF_ASBC(VDDF_ASTM(V)))
    return  VDDF_ASBC(v);
}


INLINE(Vqbc,VQYU_ASBC) (Vqyu v)
{
#define     VQYU_ASBC(V)    QBC_ASTV(QYU_ASBC(VQYU_ASTM(V)))
    return  VQYU_ASBC(v);
}

INLINE(Vqbc,VQBU_ASBC) (Vqbu v)
{
#define     VQBU_ASBC(V)    QBC_ASTV(QBU_ASBC(V))
    return  VQBU_ASBC(v);
}

INLINE(Vqbc,VQBI_ASBC) (Vqbi v)
{
#define     VQBI_ASBC(V)    QBC_ASTV(QBI_ASBC(V))
    return  VQBI_ASBC(v);
}


INLINE(Vqbc,VQHU_ASBC) (Vqhu v)
{
#define     VQHU_ASBC(V)    QBC_ASTV(QHU_ASBC(V))
    return  VQHU_ASBC(v);
}

INLINE(Vqbc,VQHI_ASBC) (Vqhi v)
{
#define     VQHI_ASBC(V)    QBC_ASTV(QHI_ASBC(V))
    return  VQHI_ASBC(v);
}

INLINE(Vqbc,VQHF_ASBC) (Vqhf v)
{
#define     VQHF_ASBC(V)    QBC_ASTV(QHF_ASBC(V))
    return  VQHF_ASBC(v);
}


INLINE(Vqbc,VQWU_ASBC) (Vqwu v)
{
#define     VQWU_ASBC(V)    QBC_ASTV(QWU_ASBC(V))
    return  VQWU_ASBC(v);
}

INLINE(Vqbc,VQWI_ASBC) (Vqwi v)
{
#define     VQWI_ASBC(V)    QBC_ASTV(QWI_ASBC(V))
    return  VQWI_ASBC(v);
}

INLINE(Vqbc,VQWF_ASBC) (Vqwf v)
{
#define     VQWF_ASBC(V)    QBC_ASTV(QWF_ASBC(V))
    return  VQWF_ASBC(v);
}


INLINE(Vqbc,VQDU_ASBC) (Vqdu v)
{
#define     VQDU_ASBC(V)    QBC_ASTV(QDU_ASBC(V))
    return  VQDU_ASBC(v);
}

INLINE(Vqbc,VQDI_ASBC) (Vqdi v)
{
#define     VQDI_ASBC(V)    QBC_ASTV(QDI_ASBC(V))
    return  VQDI_ASBC(v);
}

INLINE(Vqbc,VQDF_ASBC) (Vqdf v)
{
#define     VQDF_ASBC(V)    QBC_ASTV(QDF_ASBC(V))
    return  VQDF_ASBC(v);
}

INLINE(Vqbc,VQQU_ASBC) (Vqqu v) {return ((union MY_A64_VQ){.Qu=v}).Bc;}
INLINE(Vqbc,VQQI_ASBC) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Bc;}
INLINE(Vqbc,VQQF_ASBC) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Bc;}

#if 0 // _LEAVE_ARM_ASBC
}
#endif


#if 0 // _ENTER_ARM_ASHU
{
#endif

#define     DYU_ASHU        vreinterpret_u16_u64
#define     DBU_ASHU        vreinterpret_u16_u8
#define     DBI_ASHU        vreinterpret_u16_s8
//efine     DBC_ASHU
//efine     DHU_ASHU
#define     DHI_ASHU        vreinterpret_u16_s16
#define     DHF_ASHU        vreinterpret_u16_f16
#define     DWU_ASHU        vreinterpret_u16_u32
#define     DWI_ASHU        vreinterpret_u16_s32
#define     DWF_ASHU        vreinterpret_u16_f32
#define     DDU_ASHU        vreinterpret_u16_u64
#define     DDI_ASHU        vreinterpret_u16_s64
#define     DDF_ASHU        vreinterpret_u16_f64

#define     QYU_ASHU        vreinterpretq_u16_u64
#define     QBU_ASHU        vreinterpretq_u16_u8
#define     QBI_ASHU        vreinterpretq_u16_s8
//efine     QBC_ASHU
//efine     QHU_ASHU
#define     QHI_ASHU        vreinterpretq_u16_s16
#define     QHF_ASHU        vreinterpretq_u16_f16
#define     QWU_ASHU        vreinterpretq_u16_u32
#define     QWI_ASHU        vreinterpretq_u16_s32
#define     QWF_ASHU        vreinterpretq_u16_f32
#define     QDU_ASHU        vreinterpretq_u16_u64
#define     QDI_ASHU        vreinterpretq_u16_s64
#define     QDF_ASHU        vreinterpretq_u16_f64


INLINE(Vwhu,VWYU_ASHU) (Vwyu v)
{
#define     VWYU_ASHU(V)    WHU_ASTV(VWYU_ASTM(V))
    return  VWYU_ASHU(v);
}

INLINE(Vwhu,VWBU_ASHU) (Vwbu v)
{
#   define  VWBU_ASHU(V)    WHU_ASTV(VWBU_ASTM(V))
    return  VWBU_ASHU(v);
}

INLINE(Vwhu,VWBI_ASHU) (Vwbi v)
{
#define     VWBI_ASHU(V)    WHU_ASTV(VWBI_ASTM(V))
    return  VWBI_ASHU(v);
}

INLINE(Vwhu,VWBC_ASHU) (Vwbc v)
{
#   define  VWBC_ASHU(V)    WHU_ASTV(VWBC_ASTM(V))
    return  VWBC_ASHU(v);
}


INLINE(Vwhu,VWHI_ASHU) (Vwhi v)
{
#define     VWHI_ASHU(V)    WHU_ASTV(VWHI_ASTM(V))
    return  VWHI_ASHU(v);
}

INLINE(Vwhu,VWHF_ASHU) (Vwhf v)
{
#define     VWHF_ASHU(V)    WHU_ASTV(VWHF_ASTM(V))
    return  VWHF_ASHU(v);
}


INLINE(Vwhu,VWWU_ASHU) (Vwwu v)
{
#define     VWWU_ASHU(V)    WHU_ASTV(VWWU_ASTM(V))
    return  VWWU_ASHU(v);
}

INLINE(Vwhu,VWWI_ASHU) (Vwwi v)
{
#define     VWWI_ASHU(V)    WHU_ASTV(VWWI_ASTM(V))
    return  VWWI_ASHU(v);
}

INLINE(Vwhu,VWWF_ASHU) (Vwwf v)
{
#define     VWWF_ASHU(V)    WHU_ASTV(VWWF_ASTM(V))
    return  VWWF_ASHU(v);
}


INLINE(Vdhu,VDYU_ASHU) (Vdyu v)
{
#define     VDYU_ASHU(V)    DYU_ASHU(VDYU_ASTM(V))
    return                  DYU_ASHU(VDYU_ASTM(v));
}

INLINE(Vdhu,VDBU_ASHU) (Vdbu v) {return vreinterpret_u16_u8(v);}
INLINE(Vdhu,VDBI_ASHU) (Vdbi v) {return vreinterpret_u16_s8(v);}
INLINE(Vdhu,VDBC_ASHU) (Vdbc v)
{
#define     VDBC_ASHU(V)    DBC_ASHU(VDBC_ASTM(V))
    return  VDBC_ASHU(v);
}

INLINE(Vdhu,VDHI_ASHU) (Vdhi v) {return vreinterpret_u16_s16(v);}
INLINE(Vdhu,VDHF_ASHU) (Vdhf v) {return vreinterpret_u16_f16(v);}
INLINE(Vdhu,VDWU_ASHU) (Vdwu v) {return vreinterpret_u16_u32(v);}
INLINE(Vdhu,VDWI_ASHU) (Vdwi v) {return vreinterpret_u16_s32(v);}
INLINE(Vdhu,VDWF_ASHU) (Vdwf v) {return vreinterpret_u16_f32(v);}
INLINE(Vdhu,VDDU_ASHU) (Vddu v) {return vreinterpret_u16_u64(v);}
INLINE(Vdhu,VDDI_ASHU) (Vddi v) {return vreinterpret_u16_s64(v);}
INLINE(Vdhu,VDDF_ASHU) (Vddf v) {return vreinterpret_u16_f64(v);}


INLINE(Vqhu,VQYU_ASHU) (Vqyu v)
{
#define     VQYU_ASHU(V)    QYU_ASHU(VQYU_ASTM(V))
    return  VQYU_ASHU(v);
}

INLINE(Vqhu,VQBU_ASHU) (Vqbu v) {return vreinterpretq_u16_u8(v);}
INLINE(Vqhu,VQBI_ASHU) (Vqbi v) {return vreinterpretq_u16_s8(v);}
INLINE(Vqhu,VQBC_ASHU) (Vqbc v)
{
#   define  VQBC_ASHU(V)    QBC_ASHU(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqhu,VQHI_ASHU) (Vqhi v) {return vreinterpretq_u16_s16(v);}
INLINE(Vqhu,VQHF_ASHU) (Vqhf v) {return vreinterpretq_u16_f16(v);}
INLINE(Vqhu,VQWU_ASHU) (Vqwu v) {return vreinterpretq_u16_u32(v);}
INLINE(Vqhu,VQWI_ASHU) (Vqwi v) {return vreinterpretq_u16_s32(v);}
INLINE(Vqhu,VQWF_ASHU) (Vqwf v) {return vreinterpretq_u16_f32(v);}
INLINE(Vqhu,VQDU_ASHU) (Vqdu v) {return vreinterpretq_u16_u64(v);}
INLINE(Vqhu,VQDI_ASHU) (Vqdi v) {return vreinterpretq_u16_s64(v);}
INLINE(Vqhu,VQDF_ASHU) (Vqdf v) {return vreinterpretq_u16_f64(v);}
INLINE(Vqhu,VQQU_ASHU) (Vqqu v) {return ((union MY_A64_VQ){.Qu=v}).Hu;}
INLINE(Vqhu,VQQI_ASHU) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Hu;}
INLINE(Vqhu,VQQF_ASHU) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Hu;}

#if 0 // _LEAVE_ARM_ASHU
}
#endif

#if 0 // _ENTER_ARM_ASHI
{
#endif

#define     DYU_ASHI        vreinterpret_s16_u64
#define     DBU_ASHI        vreinterpret_s16_u8
#define     DBI_ASHI        vreinterpret_s16_s8
#define     DHU_ASHI        vreinterpret_s16_u16
//efine     DHI_ASHI
#define     DHF_ASHI        vreinterpret_s16_f16
#define     DWU_ASHI        vreinterpret_s16_u32
#define     DWI_ASHI        vreinterpret_s16_s32
#define     DWF_ASHI        vreinterpret_s16_f32
#define     DDU_ASHI        vreinterpret_s16_u64
#define     DDI_ASHI        vreinterpret_s16_s64
#define     DDF_ASHI        vreinterpret_s16_f64


#define     QYU_ASHI        vreinterpretq_s16_u64
#define     QBU_ASHI        vreinterpretq_s16_u8
#define     QBI_ASHI        vreinterpretq_s16_s8
#define     QHU_ASHI        vreinterpretq_s16_u16
//efine     QHI_ASHI
#define     QHF_ASHI        vreinterpretq_s16_f16
#define     QWU_ASHI        vreinterpretq_s16_u32
#define     QWI_ASHI        vreinterpretq_s16_s32
#define     QWF_ASHI        vreinterpretq_s16_f32
#define     QDU_ASHI        vreinterpretq_s16_u64
#define     QDI_ASHI        vreinterpretq_s16_s64
#define     QDF_ASHI        vreinterpretq_s16_f64

INLINE(Vwhi,VWYU_ASHI) (Vwyu v)
{
#define     VWYU_ASHI(V)    WHI_ASTV(VWYU_ASTM(V))
    return  VWYU_ASHI(v);
}

INLINE(Vwhi,VWBU_ASHI) (Vwbu v)
{
#   define  VWBU_ASHI(V)    WHI_ASTV(VWBU_ASTM(V))
    return  VWBU_ASHI(v);
}

INLINE(Vwhi,VWBI_ASHI) (Vwbi v)
{
#define     VWBI_ASHI(V)    WHI_ASTV(VWBI_ASTM(V))
    return  VWBI_ASHI(v);
}

INLINE(Vwhi,VWBC_ASHI) (Vwbc v)
{
#   define  VWBC_ASHI(V)    WHI_ASTV(VWBC_ASTM(V))
    return  VWBC_ASHI(v);
}


INLINE(Vwhi,VWHU_ASHI) (Vwhu v)
{
#define     VWHU_ASHI(V)    WHI_ASTV(VWHU_ASTM(V))
    return  VWHU_ASHI(v);
}

INLINE(Vwhi,VWHF_ASHI) (Vwhf v)
{
#define     VWHF_ASHI(V)    WHI_ASTV(VWHF_ASTM(V))
    return  VWHF_ASHI(v);
}


INLINE(Vwhi,VWWU_ASHI) (Vwwu v)
{
#define     VWWU_ASHI(V)    WHI_ASTV(VWWU_ASTM(V))
    return  VWWU_ASHI(v);
}

INLINE(Vwhi,VWWI_ASHI) (Vwwi v)
{
#define     VWWI_ASHI(V)    WHI_ASTV(VWWI_ASTM(V))
    return  VWWI_ASHI(v);
}

INLINE(Vwhi,VWWF_ASHI) (Vwwf v)
{
#define     VWWF_ASHI(V)    WHI_ASTV(VWWF_ASTM(V))
    return  VWWF_ASHI(v);
}


INLINE(Vdhi,VDYU_ASHI) (Vdyu v)
{
#define     VDYU_ASHI(V)    DYU_ASHI(VDYU_ASTM(V))
    return  VDYU_ASHI(v);
}

INLINE(Vdhi,VDBU_ASHI) (Vdbu v) {return vreinterpret_s16_u8(v);}
INLINE(Vdhi,VDBI_ASHI) (Vdbi v) {return vreinterpret_s16_s8(v);}
INLINE(Vdhi,VDBC_ASHI) (Vdbc v)
{
#define     VDBC_ASHI(V)    DBC_ASHI(VDBC_ASTM(V))
    return  VDBC_ASHI(v);
}

INLINE(Vdhi,VDHU_ASHI) (Vdhi v) {return vreinterpret_s16_u16(v);}
INLINE(Vdhi,VDHF_ASHI) (Vdhf v) {return vreinterpret_s16_f16(v);}
INLINE(Vdhi,VDWU_ASHI) (Vdwu v) {return vreinterpret_s16_u32(v);}
INLINE(Vdhi,VDWI_ASHI) (Vdwi v) {return vreinterpret_s16_s32(v);}
INLINE(Vdhi,VDWF_ASHI) (Vdwf v) {return vreinterpret_s16_f32(v);}
INLINE(Vdhi,VDDU_ASHI) (Vddu v) {return vreinterpret_s16_u64(v);}
INLINE(Vdhi,VDDI_ASHI) (Vddi v) {return vreinterpret_s16_s64(v);}
INLINE(Vdhi,VDDF_ASHI) (Vddf v) {return vreinterpret_s16_f64(v);}


INLINE(Vqhi,VQYU_ASHI) (Vqyu v)
{
#define     VQYU_ASHI(V)    QYU_ASHI(VQYU_ASTM(V))
    return  VQYU_ASHI(v);
}

INLINE(Vqhi,VQBU_ASHI) (Vqbu v) {return vreinterpretq_s16_u8(v);}
INLINE(Vqhi,VQBI_ASHI) (Vqbi v) {return vreinterpretq_s16_s8(v);}
INLINE(Vqhi,VQBC_ASHI) (Vqbc v)
{
#   define  VQBC_ASHI(V)    QBC_ASHI(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqhi,VQHU_ASHI) (Vqhu v) {return vreinterpretq_s16_u16(v);}
INLINE(Vqhi,VQHF_ASHI) (Vqhf v) {return vreinterpretq_s16_f16(v);}
INLINE(Vqhi,VQWU_ASHI) (Vqwu v) {return vreinterpretq_s16_u32(v);}
INLINE(Vqhi,VQWI_ASHI) (Vqwi v) {return vreinterpretq_s16_s32(v);}
INLINE(Vqhi,VQWF_ASHI) (Vqwf v) {return vreinterpretq_s16_f32(v);}
INLINE(Vqhi,VQDU_ASHI) (Vqdu v) {return vreinterpretq_s16_u64(v);}
INLINE(Vqhi,VQDI_ASHI) (Vqdi v) {return vreinterpretq_s16_s64(v);}
INLINE(Vqhi,VQDF_ASHI) (Vqdf v) {return vreinterpretq_s16_f64(v);}
INLINE(Vqhi,VQQU_ASHI) (Vqqu v) {return ((union MY_A64_VQ){.Qu=v}).Hi;}
INLINE(Vqhi,VQQI_ASHI) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Hi;}
INLINE(Vqhi,VQQF_ASHI) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Hi;}

#if 0 // _LEAVE_ARM_ASHI
}
#endif

#if 0 // _ENTER_ARM_ASHF
{
#endif

#define     DYU_ASHF        vreinterpret_f16_u64
#define     DBU_ASHF        vreinterpret_f16_u8
#define     DBI_ASHF        vreinterpret_f16_s8
#define     DHU_ASHF        vreinterpret_f16_u16
#define     DHI_ASHF        vreinterpret_f16_s16
//efine     DHF_ASHF
#define     DWU_ASHF        vreinterpret_f16_u32
#define     DWI_ASHF        vreinterpret_f16_s32
#define     DWF_ASHF        vreinterpret_f16_f32
#define     DDU_ASHF        vreinterpret_f16_u64
#define     DDI_ASHF        vreinterpret_f16_s64
#define     DDF_ASHF        vreinterpret_f16_f64


#define     QYU_ASHF        vreinterpretq_f16_u64
#define     QBU_ASHF        vreinterpretq_f16_u8
#define     QBI_ASHF        vreinterpretq_f16_s8
#define     QHU_ASHF        vreinterpretq_f16_u16
#define     QHI_ASHF        vreinterpretq_f16_s16
//efine     QHF_ASHF
#define     QWU_ASHF        vreinterpretq_f16_u32
#define     QWI_ASHF        vreinterpretq_f16_s32
#define     QWF_ASHF        vreinterpretq_f16_f32
#define     QDU_ASHF        vreinterpretq_f16_u64
#define     QDI_ASHF        vreinterpretq_f16_s64
#define     QDF_ASHF        vreinterpretq_f16_f64


INLINE(Vwhf,VWYU_ASHF) (Vwyu v)
{
#define     VWYU_ASHF(V)    WHF_ASTV(VWYU_ASTM(V))
    return  VWYU_ASHF(v);
}

INLINE(Vwhf,VWBU_ASHF) (Vwbu v)
{
#   define  VWBU_ASHF(V)    WHF_ASTV(VWBU_ASTM(V))
    return  VWBU_ASHF(v);
}

INLINE(Vwhf,VWBI_ASHF) (Vwbi v)
{
#define     VWBI_ASHF(V)    WHF_ASTV(VWBI_ASTM(V))
    return  VWBI_ASHF(v);
}

INLINE(Vwhf,VWBC_ASHF) (Vwbc v)
{
#   define  VWBC_ASHF(V)    WHF_ASTV(VWBC_ASTM(V))
    return  VWBC_ASHF(v);
}


INLINE(Vwhf,VWHU_ASHF) (Vwhu v)
{
#define     VWHU_ASHF(V)    WHF_ASTV(VWHU_ASTM(V))
    return  VWHU_ASHF(v);
}

INLINE(Vwhf,VWHI_ASHF) (Vwhi v)
{
#define     VWHI_ASHF(V)    WHF_ASTV(VWHI_ASTM(V))
    return  VWHI_ASHF(v);
}


INLINE(Vwhf,VWWU_ASHF) (Vwwu v)
{
#define     VWWU_ASHF(V)    WHF_ASTV(VWWU_ASTM(V))
    return  VWWU_ASHF(v);
}

INLINE(Vwhf,VWWI_ASHF) (Vwwi v)
{
#define     VWWI_ASHF(V)    WHF_ASTV(VWWI_ASTM(V))
    return  VWWI_ASHF(v);
}

INLINE(Vwhf,VWWF_ASHF) (Vwwf v)
{
#define     VWWF_ASHF(V)    WHF_ASTV(VWWF_ASTM(V))
    return  VWWF_ASHF(v);
}


INLINE(Vdhf,VDYU_ASHF) (Vdyu v)
{
#define     VDYU_ASHF(V)    DYU_ASHF(VDYU_ASTM(V))
    return  VDYU_ASHF(v);
}

INLINE(Vdhf,VDBU_ASHF) (Vdbu v) {return vreinterpret_f16_u8(v);}
INLINE(Vdhf,VDBI_ASHF) (Vdbi v) {return vreinterpret_f16_s8(v);}
INLINE(Vdhf,VDBC_ASHF) (Vdbc v)
{
#define     VDBC_ASHF(V)    DBC_ASHF(VDBC_ASTM(V))
    return  VDBC_ASHF(v);
}

INLINE(Vdhf,VDHU_ASHF) (Vdhu v) {return vreinterpret_f16_u16(v);}
INLINE(Vdhf,VDHI_ASHF) (Vdhi v) {return vreinterpret_f16_s16(v);}
INLINE(Vdhf,VDWU_ASHF) (Vdwu v) {return vreinterpret_f16_u32(v);}
INLINE(Vdhf,VDWI_ASHF) (Vdwi v) {return vreinterpret_f16_s32(v);}
INLINE(Vdhf,VDWF_ASHF) (Vdwf v) {return vreinterpret_f16_f32(v);}
INLINE(Vdhf,VDDU_ASHF) (Vddu v) {return vreinterpret_f16_u64(v);}
INLINE(Vdhf,VDDI_ASHF) (Vddi v) {return vreinterpret_f16_s64(v);}
INLINE(Vdhf,VDDF_ASHF) (Vddf v) {return vreinterpret_f16_f64(v);}


INLINE(Vqhf,VQYU_ASHF) (Vqyu v)
{
#define     VQYU_ASHF(V)    QYU_ASHF(VQYU_ASTM(V))
    return  VQYU_ASHF(v);
}

INLINE(Vqhf,VQBU_ASHF) (Vqbu v) {return vreinterpretq_f16_u8(v);}
INLINE(Vqhf,VQBI_ASHF) (Vqbi v) {return vreinterpretq_f16_s8(v);}
INLINE(Vqhf,VQBC_ASHF) (Vqbc v)
{
#   define  VQBC_ASHF(V)    QBC_ASHF(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqhf,VQHU_ASHF) (Vqhu v) {return vreinterpretq_f16_u16(v);}
INLINE(Vqhf,VQHI_ASHF) (Vqhi v) {return vreinterpretq_f16_s16(v);}
INLINE(Vqhf,VQWU_ASHF) (Vqwu v) {return vreinterpretq_f16_u32(v);}
INLINE(Vqhf,VQWI_ASHF) (Vqwi v) {return vreinterpretq_f16_s32(v);}
INLINE(Vqhf,VQWF_ASHF) (Vqwf v) {return vreinterpretq_f16_f32(v);}
INLINE(Vqhf,VQDU_ASHF) (Vqdu v) {return vreinterpretq_f16_u64(v);}
INLINE(Vqhf,VQDI_ASHF) (Vqdi v) {return vreinterpretq_f16_s64(v);}
INLINE(Vqhf,VQDF_ASHF) (Vqdf v) {return vreinterpretq_f16_f64(v);}
INLINE(Vqhf,VQQU_ASHF) (Vqqu v) {return ((union MY_A64_VQ){.Qu=v}).Hf;}
INLINE(Vqhf,VQQI_ASHF) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Hf;}
INLINE(Vqhf,VQQF_ASHF) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Hf;}

#if 0 // _LEAVE_ARM_ASHF
}
#endif

#if 0 // _ENTER_ARM_ASWU
{
#endif

#define     DYU_ASWU        vreinterpret_u32_u64
#define     DBU_ASWU        vreinterpret_u32_u8
#define     DBI_ASWU        vreinterpret_u32_s8
//efine     DBC_ASWU
#define     DHU_ASWU        vreinterpret_u32_u16
#define     DHI_ASWU        vreinterpret_u32_s16
#define     DHF_ASWU        vreinterpret_u32_f16
//efine     DWU_ASWU
#define     DWI_ASWU        vreinterpret_u32_s32
#define     DWF_ASWU        vreinterpret_u32_f32
#define     DDU_ASWU        vreinterpret_u32_u64
#define     DDI_ASWU        vreinterpret_u32_s64
#define     DDF_ASWU        vreinterpret_u32_f64


#define     QYU_ASWU        vreinterpretq_u32_u64
#define     QBU_ASWU        vreinterpretq_u32_u8
#define     QBI_ASWU        vreinterpretq_u32_s8
//efine     QBC_ASWU
#define     QHU_ASWU        vreinterpretq_u32_u16
#define     QHI_ASWU        vreinterpretq_u32_s16
#define     QHF_ASWU        vreinterpretq_u32_f16
//efine     QWU_ASWU
#define     QWI_ASWU        vreinterpretq_u32_s32
#define     QWF_ASWU        vreinterpretq_u32_f32
#define     QDU_ASWU        vreinterpretq_u32_u64
#define     QDI_ASWU        vreinterpretq_u32_s64
#define     QDF_ASWU        vreinterpretq_u32_f64


INLINE(Vwwu,VWYU_ASWU) (Vwyu v)
{
#define     VWYU_ASWU(V)    WWU_ASTV(VWYU_ASTM(V))
    return  VWYU_ASWU(v);
}

INLINE(Vwwu,VWBU_ASWU) (Vwbu v)
{
#   define  VWBU_ASWU(V)    WWU_ASTV(VWBU_ASTM(V))
    return  VWBU_ASWU(v);
}

INLINE(Vwwu,VWBI_ASWU) (Vwbi v)
{
#define     VWBI_ASWU(V)    WWU_ASTV(VWBI_ASTM(V))
    return  VWBI_ASWU(v);
}

INLINE(Vwwu,VWBC_ASWU) (Vwbc v)
{
#   define  VWBC_ASWU(V)    WWU_ASTV(VWBC_ASTM(V))
    return  VWBC_ASWU(v);
}


INLINE(Vwwu,VWHU_ASWU) (Vwhu v)
{
#define     VWHU_ASWU(V)    WWU_ASTV(VWHU_ASTM(V))
    return  VWHU_ASWU(v);
}

INLINE(Vwwu,VWHI_ASWU) (Vwhi v)
{
#define     VWHI_ASWU(V)    WWU_ASTV(VWHI_ASTM(V))
    return  VWHI_ASWU(v);
}

INLINE(Vwwu,VWHF_ASWU) (Vwhf v)
{
#define     VWHF_ASWU(V)    WWU_ASTV(VWHF_ASTM(V))
    return  VWHF_ASWU(v);
}


INLINE(Vwwu,VWWI_ASWU) (Vwwi v)
{
#define     VWWI_ASWU(V)    WWU_ASTV(VWWI_ASTM(V))
    return  VWWI_ASWU(v);
}

INLINE(Vwwu,VWWF_ASWU) (Vwwf v)
{
#define     VWWF_ASWU(V)    WWU_ASTV(VWWF_ASTM(V))
    return  VWWF_ASWU(v);
}


INLINE(Vdwu,VDYU_ASWU) (Vdyu v)
{
#define     VDYU_ASWU(V)    DYU_ASWU(VDYU_ASTM(V))
    return  VDYU_ASWU(v);
}

INLINE(Vdwu,VDBU_ASWU) (Vdbu v) {return vreinterpret_u32_u8(v);}
INLINE(Vdwu,VDBI_ASWU) (Vdbi v) {return vreinterpret_u32_s8(v);}
INLINE(Vdwu,VDBC_ASWU) (Vdbc v)
{
#define     VDBC_ASWU(V)    DBC_ASWU(VDBC_ASTM(V))
    return  VDBC_ASWU(v);
}

INLINE(Vdwu,VDHU_ASWU) (Vdhu v) {return vreinterpret_u32_u16(v);}
INLINE(Vdwu,VDHI_ASWU) (Vdhi v) {return vreinterpret_u32_s16(v);}
INLINE(Vdwu,VDHF_ASWU) (Vdhf v) {return vreinterpret_u32_f16(v);}
INLINE(Vdwu,VDWI_ASWU) (Vdwi v) {return vreinterpret_u32_s32(v);}
INLINE(Vdwu,VDWF_ASWU) (Vdwf v) {return vreinterpret_u32_f32(v);}
INLINE(Vdwu,VDDU_ASWU) (Vddu v) {return vreinterpret_u32_u64(v);}
INLINE(Vdwu,VDDI_ASWU) (Vddi v) {return vreinterpret_u32_s64(v);}
INLINE(Vdwu,VDDF_ASWU) (Vddf v) {return vreinterpret_u32_f64(v);}


INLINE(Vqwu,VQYU_ASWU) (Vqyu v)
{
#define     VQYU_ASWU(V)    QYU_ASWU(VQYU_ASTM(V))
    return  VQYU_ASWU(v);
}

INLINE(Vqwu,VQBU_ASWU) (Vqbu v) {return vreinterpretq_u32_u8(v);}
INLINE(Vqwu,VQBI_ASWU) (Vqbi v) {return vreinterpretq_u32_s8(v);}
INLINE(Vqwu,VQBC_ASWU) (Vqbc v)
{
#   define  VQBC_ASWU(V)    QBC_ASWU(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqwu,VQHU_ASWU) (Vqhu v) {return vreinterpretq_u32_u16(v);}
INLINE(Vqwu,VQHI_ASWU) (Vqhi v) {return vreinterpretq_u32_s16(v);}
INLINE(Vqwu,VQHF_ASWU) (Vqhf v) {return vreinterpretq_u32_f16(v);}
INLINE(Vqwu,VQWI_ASWU) (Vqwi v) {return vreinterpretq_u32_s32(v);}
INLINE(Vqwu,VQWF_ASWU) (Vqwf v) {return vreinterpretq_u32_f32(v);}
INLINE(Vqwu,VQDU_ASWU) (Vqdu v) {return vreinterpretq_u32_u64(v);}
INLINE(Vqwu,VQDI_ASWU) (Vqdi v) {return vreinterpretq_u32_s64(v);}
INLINE(Vqwu,VQDF_ASWU) (Vqdf v) {return vreinterpretq_u32_f64(v);}
INLINE(Vqwu,VQQU_ASWU) (Vqqu v) {return ((union MY_A64_VQ){.Qu=v}).Wu;}
INLINE(Vqwu,VQQI_ASWU) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Wu;}
INLINE(Vqwu,VQQF_ASWU) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Wu;}

#if 0 // _LEAVE_ARM_ASWU
}
#endif

#if 0 // _ENTER_ARM_ASWI
{
#endif

#define     DYU_ASWI        vreinterpret_s32_u64
#define     DBU_ASWI        vreinterpret_s32_u8
#define     DBI_ASWI        vreinterpret_s32_s8
//efine     DBC_ASWI
#define     DHU_ASWI        vreinterpret_s32_u16
#define     DHI_ASWI        vreinterpret_s32_s16
#define     DHF_ASWI        vreinterpret_s32_f16
#define     DWU_ASWI        vreinterpret_s32_u32
//efine     DWI_ASWI
#define     DWF_ASWI        vreinterpret_s32_f32
#define     DDU_ASWI        vreinterpret_s32_u64
#define     DDI_ASWI        vreinterpret_s32_s64
#define     DDF_ASWI        vreinterpret_s32_f64


#define     QYU_ASWI        vreinterpretq_s32_u64
#define     QBU_ASWI        vreinterpretq_s32_u8
#define     QBI_ASWI        vreinterpretq_s32_s8
//efine     QBC_ASWI
#define     QHU_ASWI        vreinterpretq_s32_u16
#define     QHI_ASWI        vreinterpretq_s32_s16
#define     QHF_ASWI        vreinterpretq_s32_f16
#define     QWU_ASWI        vreinterpretq_s32_u32
//efine     QWI_ASWI
#define     QWF_ASWI        vreinterpretq_s32_f32
#define     QDU_ASWI        vreinterpretq_s32_u64
#define     QDI_ASWI        vreinterpretq_s32_s64
#define     QDF_ASWI        vreinterpretq_s32_f64


INLINE(Vwwi,VWYU_ASWI) (Vwyu v)
{
#define     VWYU_ASWI(V)    WWI_ASTV(VWYU_ASTM(V))
    return  VWYU_ASWI(v);
}

INLINE(Vwwi,VWBU_ASWI) (Vwbu v)
{
#   define  VWBU_ASWI(V)    WWI_ASTV(VWBU_ASTM(V))
    return  VWBU_ASWI(v);
}

INLINE(Vwwi,VWBI_ASWI) (Vwbi v)
{
#define     VWBI_ASWI(V)    WWI_ASTV(VWBI_ASTM(V))
    return  VWBI_ASWI(v);
}

INLINE(Vwwi,VWBC_ASWI) (Vwbc v)
{
#   define  VWBC_ASWI(V)    WWI_ASTV(VWBC_ASTM(V))
    return  VWBC_ASWI(v);
}


INLINE(Vwwi,VWHU_ASWI) (Vwhu v)
{
#define     VWHU_ASWI(V)    WWI_ASTV(VWHU_ASTM(V))
    return  VWHU_ASWI(v);
}

INLINE(Vwwi,VWHI_ASWI) (Vwhi v)
{
#define     VWHI_ASWI(V)    WWI_ASTV(VWHI_ASTM(V))
    return  VWHI_ASWI(v);
}

INLINE(Vwwi,VWHF_ASWI) (Vwhf v)
{
#define     VWHF_ASWI(V)    WWI_ASTV(VWHF_ASTM(V))
    return  VWHF_ASWI(v);
}


INLINE(Vwwi,VWWU_ASWI) (Vwwu v)
{
#define     VWWU_ASWI(V)    WWI_ASTV(VWWU_ASTM(V))
    return  VWWU_ASWI(v);
}

INLINE(Vwwi,VWWF_ASWI) (Vwwf v)
{
#define     VWWF_ASWI(V)    WWI_ASTV(VWWF_ASTM(V))
    return  VWWF_ASWI(v);
}


INLINE(Vdwi,VDYU_ASWI) (Vdyu v)
{
#define     VDYU_ASWI(V)    DYU_ASWI(VDYU_ASTM(V))
    return  VDYU_ASWI(v);
}

INLINE(Vdwi,VDBU_ASWI) (Vdbu v) {return vreinterpret_s32_u8(v);}
INLINE(Vdwi,VDBI_ASWI) (Vdbi v) {return vreinterpret_s32_s8(v);}
INLINE(Vdwi,VDBC_ASWI) (Vdbc v)
{
#define     VDBC_ASWI(V)    DBC_ASWI(VDBC_ASTM(V))
    return  VDBC_ASWI(v);
}

INLINE(Vdwi,VDHU_ASWI) (Vdhu v) {return vreinterpret_s32_u16(v);}
INLINE(Vdwi,VDHI_ASWI) (Vdhi v) {return vreinterpret_s32_s16(v);}
INLINE(Vdwi,VDHF_ASWI) (Vdhf v) {return vreinterpret_s32_f16(v);}
INLINE(Vdwi,VDWU_ASWI) (Vdwu v) {return vreinterpret_s32_u32(v);}
INLINE(Vdwi,VDWF_ASWI) (Vdwf v) {return vreinterpret_s32_f32(v);}
INLINE(Vdwi,VDDU_ASWI) (Vddu v) {return vreinterpret_s32_u64(v);}
INLINE(Vdwi,VDDI_ASWI) (Vddi v) {return vreinterpret_s32_s64(v);}
INLINE(Vdwi,VDDF_ASWI) (Vddf v) {return vreinterpret_s32_f64(v);}

INLINE(Vqwi,VQYU_ASWI) (Vqyu v)
{
#define     VQYU_ASWI(V)    QYU_ASWI(VQYU_ASTM(V))
    return  VQYU_ASWI(v);
}

INLINE(Vqwi,VQBU_ASWI) (Vqbu v) {return vreinterpretq_s32_u8(v);}
INLINE(Vqwi,VQBI_ASWI) (Vqbi v) {return vreinterpretq_s32_s8(v);}
INLINE(Vqwi,VQBC_ASWI) (Vqbc v)
{
#   define  VQBC_ASWI(V)    QBC_ASWI(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqwi,VQHU_ASWI) (Vqhu v) {return vreinterpretq_s32_u16(v);}
INLINE(Vqwi,VQHI_ASWI) (Vqhi v) {return vreinterpretq_s32_s16(v);}
INLINE(Vqwi,VQHF_ASWI) (Vqhf v) {return vreinterpretq_s32_f16(v);}
INLINE(Vqwi,VQWU_ASWI) (Vqwu v) {return vreinterpretq_s32_u32(v);}
INLINE(Vqwi,VQWF_ASWI) (Vqwf v) {return vreinterpretq_s32_f32(v);}
INLINE(Vqwi,VQDU_ASWI) (Vqdu v) {return vreinterpretq_s32_u64(v);}
INLINE(Vqwi,VQDI_ASWI) (Vqdi v) {return vreinterpretq_s32_s64(v);}
INLINE(Vqwi,VQDF_ASWI) (Vqdf v) {return vreinterpretq_s32_f64(v);}
INLINE(Vqwi,VQQU_ASWI) (Vqqu v) {return ((union MY_A64_VQ){.Qu=v}).Wi;}
INLINE(Vqwi,VQQI_ASWI) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Wi;}
INLINE(Vqwi,VQQF_ASWI) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Wi;}

#if 0 // _LEAVE_ARM_ASWI
}
#endif

#if 0 // _ENTER_ARM_ASWF
{
#endif

#define     DYU_ASWF        vreinterpret_f32_u64
#define     DBU_ASWF        vreinterpret_f32_u8
#define     DBI_ASWF        vreinterpret_f32_s8
//efine     DBC_ASWF
#define     DHU_ASWF        vreinterpret_f32_u16
#define     DHI_ASWF        vreinterpret_f32_s16
#define     DHF_ASWF        vreinterpret_f32_f16
#define     DWU_ASWF        vreinterpret_f32_u32
#define     DWI_ASWF        vreinterpret_f32_s32
//efine     DWF_ASWF
#define     DDU_ASWF        vreinterpret_f32_u64
#define     DDI_ASWF        vreinterpret_f32_s64
#define     DDF_ASWF        vreinterpret_f32_f64

#define     QYU_ASWF        vreinterpretq_f32_u64
#define     QBU_ASWF        vreinterpretq_f32_u8
#define     QBI_ASWF        vreinterpretq_f32_s8
//efine     QBC_ASWF
#define     QHU_ASWF        vreinterpretq_f32_u16
#define     QHI_ASWF        vreinterpretq_f32_s16
#define     QHF_ASWF        vreinterpretq_f32_f16
#define     QWU_ASWF        vreinterpretq_f32_u32
#define     QWI_ASWF        vreinterpretq_f32_s32
//efine     QWF_ASWF
#define     QDU_ASWF        vreinterpretq_f32_u64
#define     QDI_ASWF        vreinterpretq_f32_s64
#define     QDF_ASWF        vreinterpretq_f32_f64


INLINE(Vwwf,VWYU_ASWF) (Vwyu v)
{
#define     VWYU_ASWF(V)    WWF_ASTV(VWYU_ASTM(V))
    return  VWYU_ASWF(v);
}

INLINE(Vwwf,VWBU_ASWF) (Vwbu v)
{
#   define  VWBU_ASWF(V)    WWF_ASTV(VWBU_ASTM(V))
    return  VWBU_ASWF(v);
}

INLINE(Vwwf,VWBI_ASWF) (Vwbi v)
{
#define     VWBI_ASWF(V)    WWF_ASTV(VWBI_ASTM(V))
    return  VWBI_ASWF(v);
}

INLINE(Vwwf,VWBC_ASWF) (Vwbc v)
{
#   define  VWBC_ASWF(V)    WWF_ASTV(VWBC_ASTM(V))
    return  VWBC_ASWF(v);
}


INLINE(Vwwf,VWHU_ASWF) (Vwhu v)
{
#define     VWHU_ASWF(V)    WWF_ASTV(VWHU_ASTM(V))
    return  VWHU_ASWF(v);
}

INLINE(Vwwf,VWHI_ASWF) (Vwhi v)
{
#define     VWHI_ASWF(V)    WWF_ASTV(VWHI_ASTM(V))
    return  VWHI_ASWF(v);
}

INLINE(Vwwf,VWHF_ASWF) (Vwhf v)
{
#define     VWHF_ASWF(V)    WWF_ASTV(VWHF_ASTM(V))
    return  VWHF_ASWF(v);
}


INLINE(Vwwf,VWWU_ASWF) (Vwwu v)
{
#define     VWWU_ASWF(V)    WWF_ASTV(VWWU_ASTM(V))
    return  VWWU_ASWF(v);
}

INLINE(Vwwf,VWWI_ASWF) (Vwwi v)
{
#define     VWWI_ASWF(V)    WWF_ASTV(VWWI_ASTM(V))
    return  VWWI_ASWF(v);
}


INLINE(Vdwf,VDYU_ASWF) (Vdyu v)
{
#define     VDYU_ASWF(V)    DYU_ASWF(VDYU_ASTM(V))
    return  VDYU_ASWF(v);
}

INLINE(Vdwf,VDBU_ASWF) (Vdbu v) {return vreinterpret_f32_u8(v);}
INLINE(Vdwf,VDBI_ASWF) (Vdbi v) {return vreinterpret_f32_s8(v);}
INLINE(Vdwf,VDBC_ASWF) (Vdbc v)
{
#define     VDBC_ASWF(V)    DBC_ASWF(VDBC_ASTM(V))
    return  VDBC_ASWF(v);
}

INLINE(Vdwf,VDHU_ASWF) (Vdhu v) {return vreinterpret_f32_u16(v);}
INLINE(Vdwf,VDHI_ASWF) (Vdhi v) {return vreinterpret_f32_s16(v);}
INLINE(Vdwf,VDHF_ASWF) (Vdhf v) {return vreinterpret_f32_f16(v);}
INLINE(Vdwf,VDWU_ASWF) (Vdwu v) {return vreinterpret_f32_u32(v);}
INLINE(Vdwf,VDWI_ASWF) (Vdwi v) {return vreinterpret_f32_s32(v);}
INLINE(Vdwf,VDDU_ASWF) (Vddu v) {return vreinterpret_f32_u64(v);}
INLINE(Vdwf,VDDI_ASWF) (Vddi v) {return vreinterpret_f32_s64(v);}
INLINE(Vdwf,VDDF_ASWF) (Vddf v) {return vreinterpret_f32_f64(v);}


INLINE(Vqwf,VQYU_ASWF) (Vqyu v)
{
#define     VQYU_ASWF(V)    QYU_ASWF(VQYU_ASTM(V))
    return  VQYU_ASWF(v);
}

INLINE(Vqwf,VQBU_ASWF) (Vqbu v) {return vreinterpretq_f32_u8(v);}
INLINE(Vqwf,VQBI_ASWF) (Vqbi v) {return vreinterpretq_f32_s8(v);}
INLINE(Vqwf,VQBC_ASWF) (Vqbc v)
{
#   define  VQBC_ASWF(V)    QBC_ASWF(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqwf,VQHU_ASWF) (Vqhu v) {return vreinterpretq_f32_u16(v);}
INLINE(Vqwf,VQHI_ASWF) (Vqhi v) {return vreinterpretq_f32_s16(v);}
INLINE(Vqwf,VQHF_ASWF) (Vqhf v) {return vreinterpretq_f32_f16(v);}
INLINE(Vqwf,VQWU_ASWF) (Vqwu v) {return vreinterpretq_f32_u32(v);}
INLINE(Vqwf,VQWI_ASWF) (Vqwi v) {return vreinterpretq_f32_s32(v);}
INLINE(Vqwf,VQDU_ASWF) (Vqdu v) {return vreinterpretq_f32_u64(v);}
INLINE(Vqwf,VQDI_ASWF) (Vqdi v) {return vreinterpretq_f32_s64(v);}
INLINE(Vqwf,VQDF_ASWF) (Vqdf v) {return vreinterpretq_f32_f64(v);}
INLINE(Vqwf,VQQU_ASWF) (Vqqu v) {return ((union MY_A64_VQ){.Qu=v}).Wf;}
INLINE(Vqwf,VQQI_ASWF) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Wf;}
INLINE(Vqwf,VQQF_ASWF) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Wf;}

#if 0 // _LEAVE_ARM_ASWF
}
#endif

#if 0 // _ENTER_ARM_ASDU
{
#endif

#define     DYU_ASDU        VDDU_REQS
#define     DBU_ASDU        vreinterpret_u64_u8
#define     DBI_ASDU        vreinterpret_u64_s8
//efine     DBC_ASDU
#define     DHU_ASDU        vreinterpret_u64_u16
#define     DHI_ASDU        vreinterpret_u64_s16
#define     DHF_ASDU        vreinterpret_u64_f16
#define     DWU_ASDU        vreinterpret_u64_u32
#define     DWI_ASDU        vreinterpret_u64_s32
#define     DWF_ASDU        vreinterpret_u64_f32
//efine     DDU_ASDU
#define     DDI_ASDU        vreinterpret_u64_s64
#define     DDF_ASDU        vreinterpret_u64_f64

#define     QYU_ASDU        VQDU_REQS
#define     QBU_ASDU        vreinterpretq_u64_u8
#define     QBI_ASDU        vreinterpretq_u64_s8
#define     QHU_ASDU        vreinterpretq_u64_u16
#define     QHI_ASDU        vreinterpretq_u64_s16
#define     QHF_ASDU        vreinterpretq_u64_f16
#define     QWU_ASDU        vreinterpretq_u64_u32
#define     QWI_ASDU        vreinterpretq_u64_s32
#define     QWF_ASDU        vreinterpretq_u64_f32
#define     QDI_ASDU        vreinterpretq_u64_s64
#define     QDF_ASDU        vreinterpretq_u64_f64

INLINE(Vddu,VDYU_ASDU) (Vdyu v)
{
#define     VDYU_ASDU(V)    DYU_ASDU(VDYU_ASTM(V))
    return  VDYU_ASDU(v);
}

INLINE(Vddu,VDBU_ASDU) (Vdbu v) {return vreinterpret_u64_u8(v);}
INLINE(Vddu,VDBI_ASDU) (Vdbi v) {return vreinterpret_u64_s8(v);}
INLINE(Vddu,VDBC_ASDU) (Vdbc v)
{
#define     VDBC_ASDU(V)    DBC_ASDU(VDBC_ASTM(V))
    return  VDBC_ASDU(v);
}

INLINE(Vddu,VDHU_ASDU) (Vdhu v) {return vreinterpret_u64_u16(v);}
INLINE(Vddu,VDHI_ASDU) (Vdhi v) {return vreinterpret_u64_s16(v);}
INLINE(Vddu,VDHF_ASDU) (Vdhf v) {return vreinterpret_u64_f16(v);}
INLINE(Vddu,VDWU_ASDU) (Vdwu v) {return vreinterpret_u64_u32(v);}
INLINE(Vddu,VDWI_ASDU) (Vdwi v) {return vreinterpret_u64_s32(v);}
INLINE(Vddu,VDWF_ASDU) (Vdwf v) {return vreinterpret_u64_f32(v);}
INLINE(Vddu,VDDI_ASDU) (Vddi v) {return vreinterpret_u64_s64(v);}
INLINE(Vddu,VDDF_ASDU) (Vddf v) {return vreinterpret_u64_f64(v);}

INLINE(Vqdu,VQYU_ASDU) (Vqyu v)
{
#define     VQYU_ASDU(V)    QYU_ASDU(VQYU_ASTM(V))
    return  VQYU_ASDU(v);
}

INLINE(Vqdu,VQBU_ASDU) (Vqbu v) {return vreinterpretq_u64_u8(v);}
INLINE(Vqdu,VQBI_ASDU) (Vqbi v) {return vreinterpretq_u64_s8(v);}
INLINE(Vqdu,VQBC_ASDU) (Vqbc v)
{
#   define  VQBC_ASDU(V)    QBC_ASDU(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqdu,VQHU_ASDU) (Vqhu v) {return vreinterpretq_u64_u16(v);}
INLINE(Vqdu,VQHI_ASDU) (Vqhi v) {return vreinterpretq_u64_s16(v);}
INLINE(Vqdu,VQHF_ASDU) (Vqhf v) {return vreinterpretq_u64_f16(v);}
INLINE(Vqdu,VQWU_ASDU) (Vqwu v) {return vreinterpretq_u64_u32(v);}
INLINE(Vqdu,VQWI_ASDU) (Vqwi v) {return vreinterpretq_u64_s32(v);}
INLINE(Vqdu,VQWF_ASDU) (Vqwf v) {return vreinterpretq_u64_f32(v);}
INLINE(Vqdu,VQDI_ASDU) (Vqdi v) {return vreinterpretq_u64_s64(v);}
INLINE(Vqdu,VQDF_ASDU) (Vqdf v) {return vreinterpretq_u64_f64(v);}
INLINE(Vqdu,VQQU_ASDU) (Vqqu v) {return v.V0;}
INLINE(Vqdu,VQQI_ASDU) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Du;}
INLINE(Vqdu,VQQF_ASDU) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Du;}

#if 0 // _LEAVE_ARM_ASDU
}
#endif

#if 0 // _ENTER_ARM_ASDI
{
#endif

#define     DYU_ASDI        vreinterpret_s64_u64
#define     DBU_ASDI        vreinterpret_s64_u8
#define     DBI_ASDI        vreinterpret_s64_s8
//          DBC_ASDI
#define     DHU_ASDI        vreinterpret_s64_u16
#define     DHI_ASDI        vreinterpret_s64_s16
#define     DHF_ASDI        vreinterpret_s64_f16
#define     DWU_ASDI        vreinterpret_s64_u32
#define     DWI_ASDI        vreinterpret_s64_s32
#define     DWF_ASDI        vreinterpret_s64_f32
#define     DDU_ASDI        vreinterpret_s64_u64
#define     DDF_ASDI        vreinterpret_s64_f64

#define     QYU_ASDI        vreinterpretq_s64_u64
#define     QBU_ASDI        vreinterpretq_s64_u8
#define     QBI_ASDI        vreinterpretq_s64_s8
//efine     QBC_ASDI
#define     QHU_ASDI        vreinterpretq_s64_u16
#define     QHI_ASDI        vreinterpretq_s64_s16
#define     QHF_ASDI        vreinterpretq_s64_f16
#define     QWU_ASDI        vreinterpretq_s64_u32
#define     QWI_ASDI        vreinterpretq_s64_s32
#define     QWF_ASDI        vreinterpretq_s64_f32
#define     QDU_ASDI        vreinterpretq_s64_u64
#define     QDF_ASDI        vreinterpretq_s64_f64

INLINE(Vddi,VDYU_ASDI) (Vdyu v)
{
#define     VDYU_ASDI(V)    DYU_ASDI(VDYU_ASTM(V))
    return  VDYU_ASDI(v);
}

INLINE(Vddi,VDBU_ASDI) (Vdbu v) {return vreinterpret_s64_u8(v);}
INLINE(Vddi,VDBI_ASDI) (Vdbi v) {return vreinterpret_s64_s8(v);}
INLINE(Vddi,VDBC_ASDI) (Vdbc v)
{
#define     VDBC_ASDI(V)    DBC_ASDI(VDBC_ASTM(V))
    return  VDBC_ASDI(v);
}

INLINE(Vddi,VDHU_ASDI) (Vdhu v) {return vreinterpret_s64_u16(v);}
INLINE(Vddi,VDHI_ASDI) (Vdhi v) {return vreinterpret_s64_s16(v);}
INLINE(Vddi,VDHF_ASDI) (Vdhf v) {return vreinterpret_s64_f16(v);}
INLINE(Vddi,VDWU_ASDI) (Vdwu v) {return vreinterpret_s64_u32(v);}
INLINE(Vddi,VDWI_ASDI) (Vdwi v) {return vreinterpret_s64_s32(v);}
INLINE(Vddi,VDWF_ASDI) (Vdwf v) {return vreinterpret_s64_f32(v);}
INLINE(Vddi,VDDU_ASDI) (Vddi v) {return vreinterpret_s64_u64(v);}
INLINE(Vddi,VDDF_ASDI) (Vddf v) {return vreinterpret_s64_f64(v);}

INLINE(Vqdi,VQYU_ASDI) (Vqyu v)
{
#define     VQYU_ASDI(V)    QYU_ASDI(VQYU_ASTM(V))
    return  VQYU_ASDI(v);
}

INLINE(Vqdi,VQBU_ASDI) (Vqbu v) {return vreinterpretq_s64_u8(v);}
INLINE(Vqdi,VQBI_ASDI) (Vqbi v) {return vreinterpretq_s64_s8(v);}
INLINE(Vqdi,VQBC_ASDI) (Vqbc v)
{
#   define  VQBC_ASDI(V)    QBC_ASDI(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqdi,VQHU_ASDI) (Vqhu v) {return vreinterpretq_s64_u16(v);}
INLINE(Vqdi,VQHI_ASDI) (Vqhi v) {return vreinterpretq_s64_s16(v);}
INLINE(Vqdi,VQHF_ASDI) (Vqhf v) {return vreinterpretq_s64_f16(v);}
INLINE(Vqdi,VQWU_ASDI) (Vqwu v) {return vreinterpretq_s64_u32(v);}
INLINE(Vqdi,VQWI_ASDI) (Vqwi v) {return vreinterpretq_s64_s32(v);}
INLINE(Vqdi,VQWF_ASDI) (Vqwf v) {return vreinterpretq_s64_f32(v);}
INLINE(Vqdi,VQDU_ASDI) (Vqdi v) {return vreinterpretq_s64_u64(v);}
INLINE(Vqdi,VQDF_ASDI) (Vqdf v) {return vreinterpretq_s64_f64(v);}
INLINE(Vqdi,VQQU_ASDI) (Vqqu v) {return ((union MY_A64_VQ){.Qu=v}).Di;}
INLINE(Vqdi,VQQI_ASDI) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Di;}
INLINE(Vqdi,VQQF_ASDI) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Di;}

#if 0 // _LEAVE_ARM_ASDI
}
#endif

#if 0 // _ENTER_ARM_ASDF
{
#endif

#define     DYU_ASDF        vreinterpret_f64_u64
#define     DBU_ASDF        vreinterpret_f64_u8
#define     DBI_ASDF        vreinterpret_f64_s8
#define     DHU_ASDF        vreinterpret_f64_u16
#define     DHI_ASDF        vreinterpret_f64_s16
#define     DHF_ASDF        vreinterpret_f64_f16
#define     DWU_ASDF        vreinterpret_f64_u32
#define     DWI_ASDF        vreinterpret_f64_s32
#define     DWF_ASDF        vreinterpret_f64_f32
#define     DDU_ASDF        vreinterpret_f64_u64
#define     DDI_ASDF        vreinterpret_f64_u64
//efine     DDF_ASDF

#define     QYU_ASDF        vreinterpretq_f64_u64
#define     QBU_ASDF        vreinterpretq_f64_u8
#define     QBI_ASDF        vreinterpretq_f64_s8
#define     QHU_ASDF        vreinterpretq_f64_u16
#define     QHI_ASDF        vreinterpretq_f64_s16
#define     QHF_ASDF        vreinterpretq_f64_f16
#define     QWU_ASDF        vreinterpretq_f64_u32
#define     QWI_ASDF        vreinterpretq_f64_s32
#define     QWF_ASDF        vreinterpretq_f64_f32
#define     QDU_ASDF        vreinterpretq_f64_u64
#define     QDI_ASDF        vreinterpretq_f64_s64
//efine     QDF_ASDF

INLINE(Vddf,VDYU_ASDF) (Vdyu v)
{
#define     VDYU_ASDF(V)    DYU_ASDF(VDYU_ASTM(V))
    return  VDYU_ASDF(v);
}

INLINE(Vddf,VDBU_ASDF) (Vdbu v) {return vreinterpret_f64_u8(v);}
INLINE(Vddf,VDBI_ASDF) (Vdbi v) {return vreinterpret_f64_s8(v);}
INLINE(Vddf,VDBC_ASDF) (Vdbc v)
{
#define     VDBC_ASDF(V)    DBC_ASDF(VDBC_ASTM(V))
    return  VDBC_ASDF(v);
}

INLINE(Vddf,VDHU_ASDF) (Vdhu v) {return vreinterpret_f64_u16(v);}
INLINE(Vddf,VDHI_ASDF) (Vdhi v) {return vreinterpret_f64_s16(v);}
INLINE(Vddf,VDHF_ASDF) (Vdhf v) {return vreinterpret_f64_f16(v);}
INLINE(Vddf,VDWU_ASDF) (Vdwu v) {return vreinterpret_f64_u32(v);}
INLINE(Vddf,VDWI_ASDF) (Vdwi v) {return vreinterpret_f64_s32(v);}
INLINE(Vddf,VDWF_ASDF) (Vdwf v) {return vreinterpret_f64_f32(v);}
INLINE(Vddf,VDDU_ASDF) (Vddu v) {return vreinterpret_f64_u64(v);}
INLINE(Vddf,VDDI_ASDF) (Vddi v) {return vreinterpret_f64_s64(v);}

INLINE(Vqdf,VQYU_ASDF) (Vqyu v)
{
#define     VQYU_ASDF(V)    QYU_ASDF(VQYU_ASTM(V))
    return  VQYU_ASDF(v);
}

INLINE(Vqdf,VQBU_ASDF) (Vqbu v) {return vreinterpretq_f64_u8(v);}
INLINE(Vqdf,VQBI_ASDF) (Vqbi v) {return vreinterpretq_f64_s8(v);}
INLINE(Vqdf,VQBC_ASDF) (Vqbc v)
{
#   define  VQBC_ASDF(V)    QBC_ASDF(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqdf,VQHU_ASDF) (Vqhu v) {return vreinterpretq_f64_u16(v);}
INLINE(Vqdf,VQHI_ASDF) (Vqhi v) {return vreinterpretq_f64_s16(v);}
INLINE(Vqdf,VQHF_ASDF) (Vqhf v) {return vreinterpretq_f64_f16(v);}
INLINE(Vqdf,VQWU_ASDF) (Vqwu v) {return vreinterpretq_f64_u32(v);}
INLINE(Vqdf,VQWI_ASDF) (Vqwi v) {return vreinterpretq_f64_s32(v);}
INLINE(Vqdf,VQWF_ASDF) (Vqwf v) {return vreinterpretq_f64_f32(v);}
INLINE(Vqdf,VQDU_ASDF) (Vqdu v) {return vreinterpretq_f64_u64(v);}
INLINE(Vqdf,VQDI_ASDF) (Vqdf v) {return vreinterpretq_f64_s64(v);}
INLINE(Vqdf,VQQU_ASDF) (Vqqu v) {return ((union MY_A64_VQ){.Qu=v}).Df;}
INLINE(Vqdf,VQQI_ASDF) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Df;}
INLINE(Vqdf,VQQF_ASDF) (Vqqf v) {return ((union MY_A64_VQ){.Qf=v}).Df;}

#if 0 // _LEAVE_ARM_ASDF
}
#endif

#if 0 // _ENTER_ARM_ASQU
{
#endif

INLINE(Vqqu,VQYU_ASQU) (Vqyu v) {return ((Vqqu){v.V0});}

INLINE(Vqqu,VQBU_ASQU) (Vqbu v) {return ((Vqqu){vreinterpretq_u64_u8(v)});}
INLINE(Vqqu,VQBI_ASQU) (Vqbi v) {return ((Vqqu){vreinterpretq_u64_s8(v)});}
INLINE(Vqqu,VQBC_ASQU) (Vqbc v) {return ((Vqqu){VQBC_ASDU(v)});}

INLINE(Vqqu,VQHU_ASQU) (Vqhu v) {return ((Vqqu){vreinterpretq_u64_u16(v)});}
INLINE(Vqqu,VQHI_ASQU) (Vqhi v) {return ((Vqqu){vreinterpretq_u64_s16(v)});}
INLINE(Vqqu,VQHF_ASQU) (Vqhf v) {return ((Vqqu){vreinterpretq_u64_f16(v)});}

INLINE(Vqqu,VQWU_ASQU) (Vqwu v) {return ((Vqqu){vreinterpretq_u64_u32(v)});}
INLINE(Vqqu,VQWI_ASQU) (Vqwi v) {return ((Vqqu){vreinterpretq_u64_s32(v)});}
INLINE(Vqqu,VQWF_ASQU) (Vqwf v) {return ((Vqqu){vreinterpretq_u64_f32(v)});}

INLINE(Vqqu,VQDU_ASQU) (Vqdu v) {return ((Vqqu){v});}
INLINE(Vqqu,VQDI_ASQU) (Vqdi v) {return ((Vqqu){vreinterpretq_u64_s64(v)});}
INLINE(Vqqu,VQDF_ASQU) (Vqdf v) {return ((Vqqu){vreinterpretq_u64_f64(v)});}

INLINE(Vqqu,VQQI_ASQU) (Vqqi v) {return ((Vqqu){vreinterpretq_u64_s64(v.V0)});}
INLINE(Vqqu,VQQF_ASQU) (Vqqf v) 
{
    QUAD_TYPE   r = {.F=v.V0};
    return ((Vqqu){vcombine_u64(vdup_n_u64(r.Lo.U), vdup_n_u64(r.Hi.U))});
}

#if 0 // _LEAVE_ARM_ASQU
}
#endif

#if 0 // _ENTER_ARM_ASQI
{
#endif

INLINE(Vqqi,VQYU_ASQI) (Vqyu v) {return ((Vqqi){vreinterpretq_s64_u64(v.V0)});}

INLINE(Vqqi,VQBU_ASQI) (Vqbu v) {return ((Vqqi){vreinterpretq_s64_u8(v)});}
INLINE(Vqqi,VQBI_ASQI) (Vqbi v) {return ((Vqqi){vreinterpretq_s64_s8(v)});}
INLINE(Vqqi,VQBC_ASQI) (Vqbc v) {return ((Vqqi){VQBC_ASDI(v)});}

INLINE(Vqqi,VQHU_ASQI) (Vqhu v) {return ((Vqqi){vreinterpretq_s64_u16(v)});}
INLINE(Vqqi,VQHI_ASQI) (Vqhi v) {return ((Vqqi){vreinterpretq_s64_s16(v)});}
INLINE(Vqqi,VQHF_ASQI) (Vqhf v) {return ((Vqqi){vreinterpretq_s64_f16(v)});}

INLINE(Vqqi,VQWU_ASQI) (Vqwu v) {return ((Vqqi){vreinterpretq_s64_u32(v)});}
INLINE(Vqqi,VQWI_ASQI) (Vqwi v) {return ((Vqqi){vreinterpretq_s64_s32(v)});}
INLINE(Vqqi,VQWF_ASQI) (Vqwf v) {return ((Vqqi){vreinterpretq_s64_f32(v)});}

INLINE(Vqqi,VQDU_ASQI) (Vqdu v) {return ((Vqqi){vreinterpretq_s64_u64(v)});}
INLINE(Vqqi,VQDI_ASQI) (Vqdi v) {return ((Vqqi){v});}
INLINE(Vqqi,VQDF_ASQI) (Vqdf v) {return ((Vqqi){vreinterpretq_s64_f64(v)});}

INLINE(Vqqi,VQQU_ASQI) (Vqqu v) {return ((Vqqi){vreinterpretq_s64_u64(v.V0)});}

INLINE(Vqqi,VQQF_ASQI) (Vqqf v) 
{
    QUAD_TYPE   r = {.F=v.V0};
    return  ((Vqqi){vcombine_s64(vdup_n_s64(r.Lo.I), vdup_n_s64(r.Hi.I))});
}

#if 0 // _LEAVE_ARM_ASQI
}
#endif

#if 0 // _ENTER_ARM_ASQF
{
#endif

INLINE(Vqqf,VQYU_ASQF) (Vqyu v) {return ((union MY_A64_VQ){.Yu=v}).Qf;}
INLINE(Vqqf,VQBU_ASQF) (Vqbu v) {return ((union MY_A64_VQ){.Bu=v}).Qf;}
INLINE(Vqqf,VQBI_ASQF) (Vqbi v) {return ((union MY_A64_VQ){.Bi=v}).Qf;}
INLINE(Vqqf,VQBC_ASQF) (Vqbc v) {return ((union MY_A64_VQ){.Bc=v}).Qf;}
INLINE(Vqqf,VQHU_ASQF) (Vqhu v) {return ((union MY_A64_VQ){.Hu=v}).Qf;}
INLINE(Vqqf,VQHI_ASQF) (Vqhi v) {return ((union MY_A64_VQ){.Hi=v}).Qf;}
INLINE(Vqqf,VQHF_ASQF) (Vqhf v) {return ((union MY_A64_VQ){.Hf=v}).Qf;}
INLINE(Vqqf,VQWU_ASQF) (Vqwu v) {return ((union MY_A64_VQ){.Wu=v}).Qf;}
INLINE(Vqqf,VQWI_ASQF) (Vqwi v) {return ((union MY_A64_VQ){.Wi=v}).Qf;}
INLINE(Vqqf,VQWF_ASQF) (Vqwf v) {return ((union MY_A64_VQ){.Wf=v}).Qf;}
INLINE(Vqqf,VQDU_ASQF) (Vqdu v) {return ((union MY_A64_VQ){.Du=v}).Qf;}
INLINE(Vqqf,VQDI_ASQF) (Vqdi v) {return ((union MY_A64_VQ){.Di=v}).Qf;}
INLINE(Vqqf,VQDF_ASQF) (Vqdf v) {return ((union MY_A64_VQ){.Df=v}).Qf;}
INLINE(Vqqf,VQQU_ASQF) (Vqqu v) {return ((union MY_A64_VQ){.Qu=v}).Qf;}
INLINE(Vqqf,VQQI_ASQF) (Vqqi v) {return ((union MY_A64_VQ){.Qi=v}).Qf;}

#if 0 // _LEAVE_ARM_ASQF
}
#endif


#if 0 // _ENTER_ARM_REVS
{
#endif

INLINE(float,WYU_REVS) (float x)
{
    DWRD_VTYPE v = {.W.F={x}};
    v.B.U = vrbit_u8(v.B.U);
    v.B.U = vrev32_u8(v.B.U);
    return  vget_lane_f32(v.W.F, 0);
}

INLINE(float,WBR_REVS) (float x)
{
    DWRD_VTYPE v = {.W.F={x}};
    v.B.U = vrev32_u8(v.B.U);
    return  vget_lane_f32(v.W.F, 0);
}

INLINE(float,WHR_REVS) (float x)
{
    DWRD_VTYPE v = {.W.F={x}};
    v.H.U = vrev32_u16(v.H.U);
    return  vget_lane_f32(v.W.F, 0);
}

INLINE(Vwhu,VWHU_REVS) (Vwhu x) {return ((Vwhu){WHR_REVS(x.V0)});}
INLINE(Vwhi,VWHI_REVS) (Vwhi x) {return ((Vwhi){WHR_REVS(x.V0)});}
INLINE(Vwhf,VWHF_REVS) (Vwhf x) {return ((Vwhf){WHR_REVS(x.V0)});}
INLINE(Vwbu,VWBU_REVS) (Vwbu x) {return ((Vwbu){WBR_REVS(x.V0)});}
INLINE(Vwbi,VWBI_REVS) (Vwbi x) {return ((Vwbi){WBR_REVS(x.V0)});}
INLINE(Vwbc,VWBC_REVS) (Vwbc x) {return ((Vwbc){WBR_REVS(x.V0)});}
INLINE(Vwyu,VWYU_REVS) (Vwyu x) {return ((Vwyu){WYU_REVS(x.V0)});}

INLINE(Vdwu,VDWU_REVS) (Vdwu x) {return vrev64_u32(x);}
INLINE(Vdwi,VDWI_REVS) (Vdwi x) {return vrev64_s32(x);}
INLINE(Vdwf,VDWF_REVS) (Vdwf x) {return vrev64_f32(x);}
INLINE(Vdhu,VDHU_REVS) (Vdhu x) {return vrev64_u16(x);}
INLINE(Vdhi,VDHI_REVS) (Vdhi x) {return vrev64_s16(x);}
INLINE(Vdhf,VDHF_REVS) (Vdhf x) 
{
    DWRD_VTYPE d = {.H.F=x};
    d.H.U = VDHU_REVS(d.H.U);
    return  d.H.F;
}
INLINE(Vdbu,VDBU_REVS) (Vdbu x) {return vrev64_u8(x);}
INLINE(Vdbi,VDBI_REVS) (Vdbi x) {return vrev64_s8(x);}
INLINE(Vdbc,VDBC_REVS) (Vdbc x) 
{
    DWRD_VTYPE d = {.B.C=x};
    d.B.U = VDBU_REVS(d.B.U);
    return  d.B.C;
}

INLINE(uint64x1_t,DYU_REVS) (uint64x1_t x)
{
    uint8x8_t b = vreinterpret_u8_u64(x);
    b = vrbit_u8(b);
    b = vrev64_u8(b);
    return  vreinterpret_u64_u8(b);
}

INLINE(Vdyu,VDYU_REVS) (Vdyu x) 
{
    x.V0 = DYU_REVS(x.V0);
    return  x;
}

INLINE(Vqdu,VQDU_REVS) (Vqdu x)
{
    uint64x1_t l = vget_high_u64(x);
    uint64x1_t r = vget_low_u64(x);
    return  vcombine_u64(l, r);
}

INLINE(Vqdi,VQDI_REVS) (Vqdi x)
{
    QUAD_VTYPE q = {.D.I=x};
    q.D.U = VQDU_REVS(q.D.U);
    return  q.D.I;
}

INLINE(Vqdf,VQDF_REVS) (Vqdf x)
{
    QUAD_VTYPE q = {.D.F=x};
    q.D.U = VQDU_REVS(q.D.U);
    return  q.D.F;
}

INLINE(Vqwu,VQWU_REVS) (Vqwu x)
{
    QUAD_VTYPE q = {.W.U=x};
    q.W.U = vrev64q_u32(q.W.U);
    q.D.U = VQDU_REVS(q.D.U);
    return  q.W.U;
}

INLINE(Vqwi,VQWI_REVS) (Vqwi x)
{
    QUAD_VTYPE q = {.W.I=x};
    q.W.U = VQWU_REVS(q.W.U);
    return  q.W.I;
}

INLINE(Vqwf,VQWF_REVS) (Vqwf x)
{
    QUAD_VTYPE q = {.W.F=x};
    q.W.U = VQWU_REVS(q.W.U);
    return  q.W.F;
}

INLINE(Vqhu,VQHU_REVS) (Vqhu x)
{
    QUAD_VTYPE q = {.H.U=x};
    q.H.U = vrev64q_u16(q.H.U);
    q.D.U = VQDU_REVS(q.D.U);
    return  q.H.U;
}

INLINE(Vqhi,VQHI_REVS) (Vqhi x)
{
    QUAD_VTYPE q = {.H.I=x};
    q.H.U = VQHU_REVS(q.H.U);
    return  q.H.I;
}

INLINE(Vqhf,VQHF_REVS) (Vqhf x)
{
    QUAD_VTYPE q = {.H.F=x};
    q.H.U = VQHU_REVS(q.H.U);
    return  q.H.F;
}

INLINE(Vqbu,VQBU_REVS) (Vqbu x)
{
    QUAD_VTYPE q = {.B.U=x};
    q.B.U = vrev64q_u8(q.D.U);
    q.D.U = VQDU_REVS(q.D.U);
    return  q.B.U;
}

INLINE(Vqbi,VQBI_REVS) (Vqbi x)
{
    QUAD_VTYPE q = {.B.I=x};
    q.B.U = VQBU_REVS(q.B.U);
    return  q.B.I;
}

INLINE(Vqbc,VQBC_REVS) (Vqbc x)
{
    QUAD_VTYPE q = {.B.C=x};
    q.B.U = VQBU_REVS(q.B.U);
    return  q.B.C;
}

INLINE(uint64x2_t,QYU_REVS) (uint64x2_t x)
{
    QUAD_VTYPE s = {.D.U=x};
    s.B.U = vrbitq_u8(s.B.U);
    s.B.U = vrev64q_u8(s.B.U);
    uint64x1_t l = vget_high_u64(s.D.U);
    uint64x1_t r = vget_low_u64(s.D.U);
    return  vcombine_u64(l, r);
}

INLINE(Vqyu,VQYU_REVS) (Vqyu x)
{
    x.V0 = QYU_REVS(x.V0);
    return x;
}

#if 0 // _LEAVE_ARM_REVS
}
#endif

#if 0 // _ENTER_ARM_REVW
{
#endif

#if DWRD_NLONG == 1
INLINE(  ulong, ULONG_REVW)   (ulong x) 
{
    DWRD_VTYPE d = {.D0.U=x};
    d.W.U = vrev64_u32(d.W.U);
    return  vget_lane_u64(d.D.U, 0);
}

INLINE(   long,  LONG_REVW)    (long x) {return ULONG_REVW(x);}
#endif

#if QUAD_NLLONG == 2

INLINE(ullong,ULLONG_REVW)  (ullong x) 
{
    DWRD_VTYPE d = {.U=x};
    d.W.U = vrev64_u32(d.W.U);
    return  vget_lane_u64(d.D.U, 0);
}

INLINE(QUAD_UTYPE,revwqu)  (QUAD_UTYPE x) 
{
    QUAD_VTYPE q = {.U=x};
    q.W.U = VQWU_REVS(q.W.U);
    return  q.U;
}

INLINE(QUAD_ITYPE,revwqi)  (QUAD_ITYPE x) 
{
    QUAD_TYPE q = {.I=x};
    q.U = revwqu(q.U);
    return  q.I;
}

#else

INLINE(ullong,ULLONG_REVW)  (ullong x) 
{
    QUAD_VTYPE q = {.Q0.U=x};
    q.W.U = VQWU_REVS(q.W.U);
    return  q.U;
}

#endif

INLINE( llong, LLONG_REVW)   (llong x) {return ULLONG_REVW(x);}

INLINE( double,   DBL_REVW)  (double x)
{
    DWRD_VTYPE d = {x};
    d.W.U = VDWU_REVS(d.W.U);
    return  d.F;
}

INLINE(Vddu,VDDU_REVW) (Vddu x)
{
    DWRD_VTYPE d = {.D.U=x};
    d.W.U = vrev64_u32(d.W.U);
    return  d.D.U;
}

INLINE(Vddi,VDDI_REVW) (Vddi x)
{
    DWRD_VTYPE d = {.D.I=x};
    d.D.U = VDDU_REVW(d.D.U);
    return  d.D.I;
}

INLINE(Vddf,VDDF_REVW) (Vddf x)
{
    DWRD_VTYPE d = {.D.F=x};
    d.D.U = VDDU_REVW(d.D.U);
    return  d.D.F;
}


INLINE(Vqdu,VQDU_REVW) (Vqdu x)
{
    QUAD_VTYPE q = {.D.U=x};
    q.W.U = vrev64q_u32(q.W.U);
    return  q.D.U;
}

INLINE(Vqdi,VQDI_REVW) (Vqdi x)
{
    QUAD_VTYPE q = {.D.I=x};
    q.D.U = VQDU_REVW(q.D.U);
    return  q.D.I;
}

INLINE(Vqdf,VQDF_REVW) (Vqdf x)
{
    QUAD_VTYPE q = {.D.F=x};
    q.D.U = VQDU_REVW(q.D.U);
    return  q.D.F;
}

INLINE(Vqqu,VQQU_REVW) (Vqqu x)
{
    QUAD_VTYPE q = {.Q.U=x};
    q.W.U = VQWU_REVS(q.W.U);
    return  q.Q.U;
}

INLINE(Vqqi,VQQI_REVW) (Vqqi x)
{
    QUAD_VTYPE q = {.Q.I=x};
    q.Q.U = VQQU_REVW(q.Q.U);
    return  q.Q.I;
}

INLINE(Vqqf,VQQF_REVW) (Vqqf x)
{
    QUAD_VTYPE q = {.Q.F=x};
    q.Q.U = VQQU_REVW(q.Q.U);
    return  q.Q.F;
}

#if 0 // _LEAVE_ARM_REVW
}
#endif

#if 0 // _ENTER_ARM_REVH
{
#endif

INLINE(   uint,  UINT_REVH)    (uint x) {return (x>>16)|(x<<16);}

INLINE(    int,   INT_REVH)     (int x) {return  UINT_REVH(x);}

INLINE(  ulong, ULONG_REVH)   (ulong x) 
{
#if DWRD_NLONG == 2
    return  (x>>16)|(x<<16);
#else
    DWRD_VTYPE s = {.D.U=vdup_n_u64(x)};
    s.H.U = vrev64_u16(s.H.U);
    return vget_lane_u64(s.D.U, 0);
#endif
}

INLINE(   long,  LONG_REVH)    (long x) 
{
    return  ULONG_REVH(x);
}

#if QUAD_NLLONG == 2

INLINE(ullong,ULLONG_REVH)  (ullong x) 
{
    DWRD_VTYPE s = {.D.U=vdup_n_u64(x)};
    s.H.U = vrev64_u16(s.H.U);
    return vget_lane_u64(s.D.U, 0);
}

INLINE( llong, LLONG_REVH)   (llong x) {return ULLONG_REVH(x);}

INLINE(QUAD_UTYPE,revhqu)  (QUAD_UTYPE x) 
{
    QUAD_VTYPE q = {.U=x};
    q.H.U = vrev64q_u16(q.H.U);
    uint64x1_t l = vget_high_u64(q.D.U);
    uint64x1_t r = vget_low_u64(q.D.U);
    q.D.U = vcombine_u64(l, r);
    return  q.U;
}

INLINE(QUAD_ITYPE,revhqi)  (QUAD_ITYPE x) 
{
    QUAD_TYPE q = {.I=x};
    q.U = revhqu(q.U);
    return  q.I;
}

#else

INLINE(ullong,ULLONG_REVH)  (ullong x) 
{
    QUAD_VTYPE q = {.U=x};
    q.H.U = VQHU_REVS(q.H.U);
    return  q.U;
}

INLINE( llong,LLONG_REVH) (llong x) {return ULLONG_REVH(x);}

#endif

INLINE(  float,   FLT_REVH)   (float x)
{
    return  WHR_REVS(x);
}

INLINE( double,   DBL_REVH)  (double x)
{
    DWRD_VTYPE d = {.F=x};
    d.H.U = vrev64_u16(d.H.U);
    return  d.F;
}


INLINE(Vwwu,VWWU_REVH) (Vwwu x) {return ((Vwwu){WHR_REVS(x.V0)});}
INLINE(Vwwi,VWWI_REVH) (Vwwi x) {return ((Vwwi){WHR_REVS(x.V0)});}
INLINE(Vwwf,VWWF_REVH) (Vwwf x) {return ((Vwwf){WHR_REVS(x.V0)});}


INLINE(Vdwu,VDWU_REVH) (Vdwu x)
{
    DWRD_VTYPE d = {.W.U=x};
    d.H.U = vrev32_u16(d.H.U);
    return  d.W.U;
}

INLINE(Vdwi,VDWI_REVH) (Vdwi x)
{
    DWRD_VTYPE d = {.W.I=x};
    d.W.U = VDWU_REVH(d.W.U);
    return  d.W.U;
}

INLINE(Vdwf,VDWF_REVH) (Vdwf x)
{
    DWRD_VTYPE d = {.W.F=x};
    d.W.U = VDWU_REVH(d.W.U);
    return  d.W.F;
}


INLINE(Vddu,VDDU_REVH) (Vddu x)
{
    DWRD_VTYPE d = {.D.U=x};
    d.H.U = vrev64_u16(d.H.U);
    return  d.D.U;
}

INLINE(Vddi,VDDI_REVH) (Vddi x)
{
    DWRD_VTYPE d = {.D.I=x};
    d.D.U = VDDU_REVH(d.D.U);
    return  d.D.I;
}

INLINE(Vddf,VDDF_REVH) (Vddf x)
{
    DWRD_VTYPE d = {.D.F=x};
    d.D.U = VDDU_REVH(d.D.U);
    return  d.D.F;
}


INLINE(Vqwu,VQWU_REVH) (Vqwu x)
{
    QUAD_VTYPE q = {.W.U=x};
    q.H.U = vrev32q_u16(q.H.U);
    return q.W.U;
}

INLINE(Vqwi,VQWI_REVH) (Vqwi x)
{
    QUAD_VTYPE q = {.W.I=x};
    q.W.U = VQWU_REVH(q.W.U);
    return q.W.I;
}

INLINE(Vqwf,VQWF_REVH) (Vqwf x)
{
    QUAD_VTYPE q = {.W.F=x};
    q.W.U = VQWU_REVH(q.W.U);
    return q.W.I;
}


INLINE(Vqdu,VQDU_REVH) (Vqdu x)
{
    QUAD_VTYPE q = {.D.U=x};
    q.H.U = vrev64q_u16(q.H.U);
    return  q.D.U;
}

INLINE(Vqdi,VQDI_REVH) (Vqdi x)
{
    QUAD_VTYPE q = {.D.I=x};
    q.D.U = VQDU_REVH(q.D.U);
    return  q.D.I;
}

INLINE(Vqdf,VQDF_REVH) (Vqdf x)
{
    QUAD_VTYPE q = {.D.F=x};
    q.D.U = VQDU_REVH(q.D.U);
    return  q.D.F;
}

INLINE(Vqqu,VQQU_REVH) (Vqqu x)
{
    QUAD_VTYPE q = {.Q.U=x};
    q.H.U = VQHU_REVS(q.H.U);
    return  q.Q.U;
}

INLINE(Vqqi,VQQI_REVH) (Vqqi x)
{
    QUAD_VTYPE q = {.Q.I=x};
    q.Q.U = VQQU_REVH(q.Q.U);
    return  q.Q.I;
}

INLINE(Vqqf,VQQF_REVH) (Vqqf x)
{
    QUAD_VTYPE q = {.Q.F=x};
    q.Q.U = VQQU_REVH(q.Q.U);
    return  q.Q.F;
}

#if 0 // _LEAVE_ARM_REVH
}
#endif

#if 0 // _ENTER_ARM_REVB
{
#endif

INLINE( ushort, USHRT_REVB) (unsigned x) {return __revsh(x);}
INLINE(  short,  SHRT_REVB)   (signed x) {return __revsh(x);}
INLINE(   uint,  UINT_REVB)     (uint x) {return __rev(x);}
INLINE(    int,   INT_REVB)      (int x) {return __rev(x);}
INLINE(  ulong, ULONG_REVB)    (ulong x) {return __revl(x);}
INLINE(   long,  LONG_REVB)     (long x) {return __revl(x);}
INLINE( ullong,ULLONG_REVB)   (ullong x) {return __revll(x);}
INLINE(  llong, LLONG_REVB)    (llong x) {return __revll(x);}
INLINE(flt16_t, FLT16_REVB)  (flt16_t x)
{
    HALF_TYPE r = {.F=x};
    HALF_TYPE l = {.Lo.U=r.Hi.U, .Hi.U=r.Lo.U};
    return l.F;
}

INLINE(  float,   FLT_REVB)   (float x)
{
    WORD_TYPE r = {.F=x};
    WORD_TYPE l = {
        .B0.U=r.B3.U, 
        .B1.U=r.B2.U, 
        .B2.U=r.B1.U,
        .B3.U=r.B0.U,
    };
    return l.F;
}

INLINE( double,   DBL_REVB)  (double x)
{
    float64x1_t m = vdup_n_f64(x);
    uint8x8_t   v = vreinterpret_u8_f64(m);
    v = vrev64_u8(v);
    return  vget_lane_f64(v, 0);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,revbqu) (QUAD_UTYPE x)
{
    QUAD_TYPE l = {.U=x};
    QUAD_TYPE r = {
        .Lo.U = __revll(l.Hi.U),
        .Hi.U = __revll(l.Lo.U),
    };
    return  r.U;
}

INLINE(QUAD_ITYPE,revbqi) (QUAD_ITYPE x)
{
    QUAD_TYPE l = {.I=x};
    QUAD_TYPE r = {
        .Lo.U = __revll(l.Hi.U),
        .Hi.U = __revll(l.Lo.U),
    };
    return  r.I;
}

INLINE(QUAD_FTYPE,revbqf) (QUAD_FTYPE x)
{
    QUAD_TYPE l = {.F=x};
    QUAD_TYPE r = {
        .Lo.U = __revll(l.Hi.U),
        .Hi.U = __revll(l.Lo.U),
    };
    return  r.F;
}

#endif

INLINE(float,WHU_REVB) (float v)
{
    float32x2_t m = vdup_n_f32(v);
    uint8x8_t   r = vreinterpret_u8_f32(m);
    r = vrev16_u8(r);
    m = vreinterpret_f32_u8(r);
    return  vget_lane_f32(m, 0);
}

INLINE(float,WWU_REVB) (float v)
{
    float32x2_t m = vdup_n_f32(v);
    uint8x8_t   r = vreinterpret_u8_f32(m);
    r = vrev32_u8(r);
    m = vreinterpret_f32_u8(r);
    return  vget_lane_f32(m, 0);
}

INLINE(Vwhu,VWHU_REVB) (Vwhu v)
{
    return  WHU_ASTV(WHU_REVB(VWHU_ASTM(v)));
}

INLINE(Vwhi,VWHI_REVB) (Vwhi v)
{
    return  WHI_ASTV(WHU_REVB(VWHI_ASTM(v)));
}

INLINE(Vwhf,VWHF_REVB) (Vwhf v)
{
    return  WHF_ASTV(WHU_REVB(VWHF_ASTM(v)));
}


INLINE(Vwwu,VWWU_REVB) (Vwwu v)
{
    return  WWU_ASTV(WWU_REVB(VWWU_ASTM(v)));
}

INLINE(Vwwi,VWWI_REVB) (Vwwi v)
{
    return  WWI_ASTV(WWU_REVB(VWWI_ASTM(v)));
}

INLINE(Vwwf,VWWF_REVB) (Vwwf v)
{
    return  WWF_ASTV(WWU_REVB(VWWF_ASTM(v)));
}


INLINE(Vdhu,VDHU_REVB) (Vdhu v)
{
    return  vreinterpret_u16_u8(vrev16_u8(vreinterpret_u8_u16(v)));
}

INLINE(Vdhi,VDHI_REVB) (Vdhi v)
{
    return  vreinterpret_s16_u8(vrev16_u8(vreinterpret_u8_s16(v)));
}

INLINE(Vdhf,VDHF_REVB) (Vdhf v)
{
    return  vreinterpret_f16_u8(vrev16_u8(vreinterpret_u8_f16(v)));
}


INLINE(Vdwu,VDWU_REVB) (Vdwu v)
{
    return  vreinterpret_u32_u8(vrev32_u8(vreinterpret_u8_u32(v)));
}

INLINE(Vdwi,VDWI_REVB) (Vdwi v)
{
    return  vreinterpret_s32_u8(vrev32_u8(vreinterpret_u8_s32(v)));
}

INLINE(Vdwf,VDWF_REVB) (Vdwf v)
{
    return  vreinterpret_f32_u8(vrev32_u8(vreinterpret_u8_f32(v)));
}


INLINE(Vddu,VDDU_REVB) (Vddu v)
{
    return  vreinterpret_u64_u8(vrev64_u8(vreinterpret_u8_u64(v)));
}

INLINE(Vddi,VDDI_REVB) (Vddi v)
{
    return  vreinterpret_s64_u8(vrev64_u8(vreinterpret_u8_s64(v)));
}

INLINE(Vddf,VDDF_REVB) (Vddf v)
{
    return  vreinterpret_f64_u8(vrev64_u8(vreinterpret_u8_f64(v)));
}


INLINE(Vqhu,VQHU_REVB) (Vqhu v)
{
    return  vreinterpretq_u16_u8(vrev16q_u8(vreinterpretq_u8_u16(v)));
}

INLINE(Vqhi,VQHI_REVB) (Vqhi v)
{
    return  vreinterpretq_s16_u8(vrev16q_u8(vreinterpretq_u8_s16(v)));
}

INLINE(Vqhf,VQHF_REVB) (Vqhf v)
{
    return  vreinterpretq_f16_u8(vrev16q_u8(vreinterpretq_u8_f16(v)));
}


INLINE(Vqwu,VQWU_REVB) (Vqwu v)
{
    return  vreinterpretq_u32_u8(vrev32q_u8(vreinterpretq_u8_u32(v)));
}

INLINE(Vqwi,VQWI_REVB) (Vqwi v)
{
    return  vreinterpretq_s32_u8(vrev32q_u8(vreinterpretq_u8_s32(v)));
}

INLINE(Vqwf,VQWF_REVB) (Vqwf v)
{
    return  vreinterpretq_f32_u8(vrev32q_u8(vreinterpretq_u8_f32(v)));
}


INLINE(Vqdu,VQDU_REVB) (Vqdu v)
{
    return  vreinterpretq_u64_u8(vrev64q_u8(vreinterpretq_u8_u64(v)));
}

INLINE(Vqdi,VQDI_REVB) (Vqdi v)
{
    return  vreinterpretq_s64_u8(vrev64q_u8(vreinterpretq_u8_s64(v)));
}

INLINE(Vqdf,VQDF_REVB) (Vqdf v)
{
    return  vreinterpretq_f64_u8(vrev64q_u8(vreinterpretq_u8_f64(v)));
}

INLINE(uint8x16_t,QQR_REVB) (uint8x16_t b)
{
    b = vrev64q_u8(b);
    return  vcombine_u8(
        vget_high_u8(b), 
        vget_low_u8(b)
    );
}

INLINE(Vqqu,VQQU_REVB) (Vqqu v)
{
    uint8x16_t b = vreinterpretq_u8_u64(v.V0);
    b = QQR_REVB(b);
    v.V0 = vreinterpretq_u64_u8(b);
    return v;
}

INLINE(Vqqi,VQQI_REVB) (Vqqi v)
{
    uint8x16_t b = vreinterpretq_u8_u64(v.V0);
    b = QQR_REVB(b);
    v.V0 = vreinterpretq_u64_u8(b);
    return v;
}

INLINE(Vqqf,VQQF_REVB) (Vqqf v)
{
    uint8x16_t u = VQQF_ASBU(v);
    u = QQR_REVB(u);
    return VQBU_ASQF(u);
}

#if 0 // _LEAVE_ARM_REVB
}
#endif

#if 0 // _ENTER_ARM_REVY
{
#endif

INLINE( uchar, UCHAR_REVY) (unsigned x) 
{
    return  __rbit(x)>>(UINT_WIDTH-UCHAR_WIDTH);
}

INLINE( schar, SCHAR_REVY)   (signed x) {return UCHAR_REVY(x);}

INLINE(  char,  CHAR_REVY)      (int x) {return UCHAR_REVY(x);}

INLINE(ushort, USHRT_REVY) (unsigned x) 
{
    return  __rbit(x)>>(UINT_WIDTH-USHRT_WIDTH);
}

INLINE( short,  SHRT_REVY)   (signed x) {return USHRT_REVY(x);}
INLINE(  uint,  UINT_REVY)     (uint x) {return __rbit(x);}
INLINE(   int,   INT_REVY)      (int x) {return __rbit(x);}
INLINE( ulong, ULONG_REVY)    (ulong x) {return __rbitl(x);}
INLINE(  long,  LONG_REVY)     (long x) {return __rbitl(x);}
INLINE(ullong,ULLONG_REVY)   (ullong x) {return __rbitll(x);}
INLINE( llong, LLONG_REVY)    (llong x) {return __rbitll(x);}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE, revyqu) (QUAD_UTYPE x)
{
    QUAD_TYPE s={.U=x}, d;
    d.Lo.U = UINT64_REVY(s.Hi.U);
    d.Hi.U = UINT64_REVY(s.Lo.U);
    return  d.U;
}
INLINE(QUAD_ITYPE, revyqi) (QUAD_ITYPE x)
{
    QUAD_TYPE s={.I=x}, d;
    d.Lo.U = UINT64_REVY(s.Hi.U);
    d.Hi.U = UINT64_REVY(s.Lo.U);
    return  d.U;
}
#endif

INLINE(flt16_t, FLT16_REVY) (flt16_t x)
{
    float16x4_t m = vdup_n_f16(x);
    uint8x8_t   b = vreinterpret_u8_f16(m);
    b = vrev16_u8(b);
    b = vrbit_u8(b);
    m = vreinterpret_f16_u8(b);
    return vget_lane_f16(m, 0);
}

INLINE(float, FLT_REVY) (float x)
{
    float32x2_t m = vdup_n_f32(x);
    uint8x8_t   b = vreinterpret_u8_f32(m);
    b = vrev32_u8(b);
    b = vrbit_u8(b);
    m = vreinterpret_f32_u8(b);
    return vget_lane_f32(m, 0);
}

INLINE(double, DBL_REVY) (double x)
{
    float64x1_t m = vdup_n_f64(x);
    uint8x8_t   b = vreinterpret_u8_f64(m);
    b = vrev64_u8(b);
    b = vrbit_u8(b);
    m = vreinterpret_f64_u8(b);
    return vget_lane_f64(m, 0);
}

INLINE(float,WBU_REVY) (float v)
{
    float32x2_t m = vdup_n_f32(v);
    uint8x8_t   r = vreinterpret_u8_f32(m);
    r = vrbit_u8(r);
    m = vreinterpret_f32_u8(r);
    return vget_lane_f32(m, 0);
}

INLINE(float,WHU_REVY) (float v)
{
    float32x2_t m = vdup_n_f32(v);
    uint8x8_t   r = vreinterpret_u8_f32(m);
    r = vrbit_u8(r);
    r = vrev16_u8(r);
    m = vreinterpret_f32_u8(r);
    return vget_lane_f32(m, 0);
}

INLINE(float,WWU_REVY) (float v)
{
    float32x2_t m = vdup_n_f32(v);
    uint8x8_t   r = vreinterpret_u8_f32(m);
    r = vrbit_u8(r);
    r = vrev32_u8(r);
    m = vreinterpret_f32_u8(r);
    return vget_lane_f32(m, 0);
}

INLINE(Vwbu,VWBU_REVY) (Vwbu v)
{
#define     VWBU_REVY(V) WBU_ASTV(WBU_REVY(VWBU_ASTM(v)))
    return  VWBU_REVY(v);
}

INLINE(Vwbi,VWBI_REVY) (Vwbi v)
{
#define     VWBI_REVY(V) WBI_ASTV(WBU_REVY(VWBI_ASTM(v)))
    return  VWBI_REVY(v);
}

INLINE(Vwbc,VWBC_REVY) (Vwbc v)
{
#define     VWBC_REVY(V) WBC_ASTV(WBU_REVY(VWBC_ASTM(v)))
    return  VWBC_REVY(v);
}


INLINE(Vwhu,VWHU_REVY) (Vwhu v)
{
#define     VWHU_REVY(V) WHU_ASTV(WHU_REVY(VWHU_ASTM(v)))
    return  VWHU_REVY(v);
}

INLINE(Vwhi,VWHI_REVY) (Vwhi v)
{
#define     VWHI_REVY(V) WHI_ASTV(WHU_REVY(VWHI_ASTM(v)))
    return  VWHI_REVY(v);
}


INLINE(Vwwu,VWWU_REVY) (Vwwu v)
{
#define     VWWU_REVY(V) WWU_ASTV(WWU_REVY(VWWU_ASTM(v)))
    return  VWWU_REVY(v);
}

INLINE(Vwwi,VWWI_REVY) (Vwwi v)
{
#define     VWWI_REVY(V) WWI_ASTV(WWU_REVY(VWWI_ASTM(v)))
    return  VWWI_REVY(v);
}


INLINE(Vdbu,VDBU_REVY) (Vdbu v) {return vrbit_u8(v);}
INLINE(Vdbi,VDBI_REVY) (Vdbi v) {return vrbit_u8(v);}
INLINE(Vdbc,VDBC_REVY) (Vdbc v)
{
    return  VDBU_ASBC(VDBU_REVY(VDBC_ASBU(v)));
}


INLINE(Vdhu,VDHU_REVY) (Vdhu x)
{
#define     VDHU_REVY(X) \
vreinterpret_u16_u8(vrev16_u8(vrbit_u8(vreinterpret_u8_u16(X))))

    return  VDHU_REVY(x);
}

INLINE(Vdhi,VDHI_REVY) (Vdhi x)
{
#define     VDHI_REVY(X) \
vreinterpret_s16_u8(vrev16_u8(vrbit_u8(vreinterpret_u8_s16(X))))

    return  VDHI_REVY(x);
}


INLINE(Vdwu,VDWU_REVY) (Vdwu v)
{
    return  vreinterpret_u32_u8(
        vrev32_u8(
            vrbit_u8(
                vreinterpret_u8_u32(v)
            )
        )
    );
}

INLINE(Vdwi,VDWI_REVY) (Vdwi v)
{
    return  VDWU_ASWI(VDWU_REVY(VDWI_ASWU(v)));
}

INLINE(Vddu,VDDU_REVY) (Vddu v)
{
    return  vreinterpret_u64_u8(
        vrev64_u8(
            vrbit_u8(
                vreinterpret_u8_u64(v)
            )
        )
    );
}


INLINE(Vddi,VDDI_REVY) (Vddi v)
{
    return  vreinterpret_s64_u8(
        vrev64_u8(
            vrbit_u8(
                vreinterpret_u8_s64(v)
            )
        )
    );
}


INLINE(Vqbu,VQBU_REVY) (Vqbu v) {return  vrbitq_u8(v);}
INLINE(Vqbi,VQBI_REVY) (Vqbi v) {return  vrbitq_s8(v);}
INLINE(Vqbc,VQBC_REVY) (Vqbc v) {return  VQBU_ASBC(vrbitq_u8(VQBC_ASBU(v)));}


INLINE(Vqhu,VQHU_REVY) (Vqhu v)
{
    return  VQBU_ASHU(vrev16q_u8(vrbitq_u8(VQHU_ASBU(v))));
}

INLINE(Vqhi,VQHI_REVY) (Vqhi v)
{
    return  VQBU_ASHI(vrev16q_u8(vrbitq_u8(VQHI_ASBU(v))));
}


INLINE(Vqwu,VQWU_REVY) (Vqwu v)
{
    return  VQBU_ASWU(vrev32q_u8(vrbitq_u8(VQWU_ASBU(v))));
}

INLINE(Vqwi,VQWI_REVY) (Vqwi v)
{
    return  VQBU_ASWU(vrev32q_u8(vrbitq_u8(VQWI_ASBU(v))));
}

INLINE(Vqdu,VQDU_REVY) (Vqdu v)
{
    return  VQBU_ASDU(vrev64q_u8(vrbitq_u8(VQDU_ASBU(v))));
}

INLINE(Vqdi,VQDI_REVY) (Vqdi v)
{
    return  VQBU_ASDI(vrev64q_u8(vrbitq_u8(VQDI_ASBU(v))));
}

INLINE(Vqqu,VQQU_REVY) (Vqqu v)
{
    uint8x16_t  b = vreinterpretq_u8_u64(v.V0);
    b = vrbitq_u8(b);
    b = vrev64q_u8(b);
    uint64x2_t  m = vreinterpretq_u64_u8(b);
    v.V0 = vcombine_u64(vget_high_u64(m), vget_low_u64(m));
    return v;
}

INLINE(Vqqi,VQQI_REVY) (Vqqi v)
{
    uint8x16_t  b = vreinterpretq_u8_s64(v.V0);
    b = vrbitq_u8(b);
    b = vrev64q_u8(b);
    int64x2_t  m = vreinterpretq_s64_u8(b);
    v.V0 = vcombine_s64(vget_high_s64(m), vget_low_s64(m));
    return v;
}

#if 0 // _LEAVE_ARM_REVY
}
#endif


#if 0 // _ENTER_ARM_UNOL
{
#endif

INLINE(Vwyu,VWYU_UNOL) (Rc(0, 1) n)
{
#define     VWYU_UNOL(N) VWWU_ASYU(UINT_ASTV((N?UINT_MAX:0)))
    return  VWYU_UNOL(n);
}


INLINE(Vwbu,VWBU_UNOL) (Rc(0, 8) n)
{
#define     WBU_UNOL(N)                     \
(                                           \
    (8 == N)                                \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u8(            \
                vdup_n_u8(UINT8_MAX)        \
            ),                              \
            0                               \
        )                                   \
    :   vget_lane_f32(                      \
            vreinterpret_f32_u8(            \
                vshr_n_u8(                  \
                    vdup_n_u8(UINT8_MAX),   \
                    (8-(N&7))               \
                )                           \
            ),                              \
            0                               \
        )                                   \
)

#define     VWBU_UNOL(N)    WBU_ASTV(WBU_UNOL(N))
    uint8x8_t   d = vdup_n_u8((UINT8_MAX>>(8-n)));
    float32x2_t m = vreinterpret_f32_u8(d);
    float       f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_UNOL) (Rc(0, 8) n)
{
#define     VWBI_UNOL(N)    WBI_ASTV(WBU_UNOL(N))
    uint8x8_t   d = vdup_n_u8((UINT8_MAX>>(8-n)));
    float32x2_t m = vreinterpret_f32_u8(d);
    float       f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_UNOL) (Rc(0, 8) n)
{
#define     VWBC_UNOL(N)    WBC_ASTV(WBU_UNOL(N))
    uint8x8_t   d = vdup_n_u8((UINT8_MAX>>(8-n)));
    float32x2_t m = vreinterpret_f32_u8(d);
    float       f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
}


INLINE(Vwhu,VWHU_UNOL) (Rc(0, 16) n)
{
#define     WHU_UNOL(N)                     \
(                                           \
    (16 == N)                               \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u16(           \
                vdup_n_u16(UINT16_MAX)      \
            ),                              \
            0                               \
        )                                   \
    :   vget_lane_f32(                      \
            vreinterpret_f32_u16(           \
                vshr_n_u16(                 \
                    vdup_n_u16(UINT16_MAX), \
                    (16-(N&15))             \
                )                           \
            ),                              \
            0                               \
        )                                   \
)

#define     VWHU_UNOL(N)    WHU_ASTV(WHU_UNOL(N))
    uint16x4_t  d = vdup_n_u16((UINT16_MAX>>(16-n)));
    float32x2_t m = vreinterpret_f32_u16(d);
    float       f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_UNOL) (Rc(0, 16) n)
{
#define     VWHI_UNOL(N)    WHI_ASTV(WHU_UNOL(N))
    uint16x4_t  d = vdup_n_u16((UINT16_MAX>>(16-n)));
    float32x2_t m = vreinterpret_f32_u16(d);
    float       f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_UNOL) (Rc(0, 32) n)
{
#define     WWU_UNOL(N)                     \
(                                           \
    (32 == N)                               \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u32(           \
                vdup_n_u32(UINT32_MAX)      \
            ),                              \
            0                               \
        )                                   \
    :   vget_lane_f32(                      \
            vreinterpret_f32_u32(           \
                vshr_n_u32(                 \
                    vdup_n_u32(UINT32_MAX), \
                    (32-(N&31))             \
                )                           \
            ),                              \
            0                               \
        )                                   \
)

#define     VWWU_UNOL(N)    WWU_ASTV(WWU_UNOL(N))
    uint32x2_t  d = vdup_n_u32((UINT32_MAX>>(32-n)));
    float32x2_t m = vreinterpret_f32_u32(d);
    float       f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_UNOL) (Rc(0, 32) n)
{
#define     VWWI_UNOL(N)    WWI_ASTV(WWU_UNOL(N))
    uint32x2_t  d = vdup_n_u32((UINT32_MAX>>(32-n)));
    float32x2_t m = vreinterpret_f32_u32(d);
    float       f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}


INLINE(Vdyu,VDYU_UNOL) (Rc(0, 1) n)
{
#define     VDYU_UNOL(N)                    \
(                                           \
    (N == 1)                                \
    ?   VDDU_ASYU(vdup_n_u64(UINT64_MAX))   \
    :   ((Vdyu){0})                         \
)

    uint64x1_t  l = vdup_n_u64(n);
    uint64x1_t  r = vdup_n_u64(1);
    l = vceq_u64(l, r);
    return  VDDU_ASYU(l);
}


INLINE(Vdbu,VDBU_UNOL) (Rc(0, 8) n)
{
#define     VDBU_UNOL(N)   vdup_n_u8((UINT8_MAX>>(8-N)))
    int8x8_t r = vdup_n_s8(8-n);
    return  vshl_u8(vdup_n_u8(UINT8_MAX), vneg_s8(r));
}

INLINE(Vdbi,VDBI_UNOL) (Rc(0, 8) n)
{
#define     VDBI_UNOL(N)    vdup_n_s8((UINT8_MAX>>(8-N)))
    return  vreinterpret_s8_u8( (VDBU_UNOL)(n) );
}

INLINE(Vdbc,VDBC_UNOL) (Rc(0, 8) n)
{
#define     VDBC_UNOL(N)    VDBU_ASBC(vdup_n_u8((UINT8_MAX>>(8-N))))
    return  VDBC_UNOL(n);
}


INLINE(Vdhu,VDHU_UNOL) (Rc(0, 16) n)
{
#define     VDHU_UNOL(N)    vdup_n_u16((UINT16_MAX>>(16-N)))
    return  VDHU_UNOL(n);
}

INLINE(Vdhi,VDHI_UNOL) (Rc(0, 16) n)
{
#define     VDHI_UNOL(N)    VDHU_ASHI(VDHU_UNOL(N))
    return  VDHI_UNOL(n);
}


INLINE(Vdwu,VDWU_UNOL) (Rc(0, 32) n)
{
#define     VDWU_UNOL(N)    vdup_n_u32((UINT32_MAX>>(32-N)))
    return  VDWU_UNOL(n);
}

INLINE(Vdwi,VDWI_UNOL) (Rc(0, 32) n)
{
#define     VDWI_UNOL(N)    vreinterpret_s32_u32(VDWU_UNOL(N))
    return  VDWI_UNOL(n);
}


INLINE(Vddu,VDDU_UNOL) (Rc(0, 64) n)
{
#define     VDDU_UNOL(N)    vdup_n_u64((UINT64_MAX>>(64-N)))
    return  VDDU_UNOL(n);
}

INLINE(Vddi,VDDI_UNOL) (Rc(0, 64) n)
{
#define     VDDI_UNOL(N)    vreinterpret_s64_u64(VDDU_UNOL(N))
    return  VDDU_UNOL(n);
}


INLINE(Vqyu,VQYU_UNOL) (Rc(0, 1) n)
{
#define     VQYU_UNOL(N)                    \
(                                           \
    (N == 1)                                \
    ?   VQDU_ASYU(vdupq_n_u64(UINT64_MAX))  \
    :   ((Vqyu){0})                         \
)

    uint64x2_t  l = vdupq_n_u64(n);
    uint64x2_t  r = vdupq_n_u64(1);
    l = vceqq_u64(l, r);
    return  VQDU_ASYU(l);
}


INLINE(Vqbu,VQBU_UNOL) (Rc(0, 8) n)
{
#define     VQBU_UNOL(N)   vdupq_n_u8((UINT8_MAX>>(8-N)))
    return  VQBU_UNOL(n);
}

INLINE(Vqbi,VQBI_UNOL) (Rc(0, 8) n)
{
#define     VQBI_UNOL(N)    vdupq_n_s8((UINT8_MAX>>(8-N)))
    return  vreinterpretq_s8_u8( VQBU_UNOL(n) );
}

INLINE(Vqbc,VQBC_UNOL) (Rc(0, 8) n)
{
#define     VQBC_UNOL(N)    VQBU_ASBC(vdupq_n_u8((UINT8_MAX>>(8-N))))
    return  VQBC_UNOL(n);
}


INLINE(Vqhu,VQHU_UNOL) (Rc(0, 16) n)
{
#define     VQHU_UNOL(N)   vdupq_n_u16((UINT16_MAX>>(16-N)))
    return  VQHU_UNOL(n);
}

INLINE(Vqhi,VQHI_UNOL) (Rc(0, 16) n)
{
#define     VQHI_UNOL(N) \
vreinterpretq_s16_u16(vdupq_n_u16( (UINT16_MAX>>(16-N)) ))

    return  VQHI_UNOL(n);
}


INLINE(Vqwu,VQWU_UNOL) (Rc(0, 32) n)
{
#define     VQWU_UNOL(N)    vdupq_n_u32((UINT32_MAX>>(32-N)))
    return  VQWU_UNOL(n);
}

INLINE(Vqwi,VQWI_UNOL) (Rc(0, 32) n)
{
#define     VQWI_UNOL(N)    vreinterpretq_s32_u32(VQWU_UNOL(N))
    return  VQWI_UNOL(n);
}


INLINE(Vqdu,VQDU_UNOL) (Rc(0, 64) n)
{
#define     VQDU_UNOL(N)    vdupq_n_u64((UINT64_MAX>>(64-N)))
    return  VQDU_UNOL(n);
}

INLINE(Vqdi,VQDI_UNOL) (Rc(0, 64) n)
{
#define     VQDI_UNOL(N)    vreinterpretq_s64_u64(VQDU_UNOL(N))
    return  VQDU_UNOL(n);
}

#define     QQZ_UNOL(N) \
vcombine_u64(\
    (\
        (N>64)\
        ?   vdup_n_u64(-1)\
        :   vdup_n_u64((UINT64_MAX>>(64-N)))\
    ),\
    (\
        (N>64)\
        ?   vdup_n_u64((UINT64_MAX>>(128-N)))\
        :   vdup_n_u64(0)\
    )\
)

INLINE(Vqqu,VQQU_UNOL) (Rc(0, 128) n)
{
#define     VQQU_UNOL(N) ((Vqqu){QQZ_UNOL(N)})
    return  VQQU_UNOL(n);
}

INLINE(Vqqi,VQQI_UNOL) (Rc(0, 128) n)
{
#define     VQQI_UNOL(N) ((Vqqi){vreinterpretq_s64_u64(QQZ_UNOL(N))})
    return  VQQI_UNOL(n);
}

#if 0 // _LEAVE_ARM_UNOL
}
#endif

#if 0 // _ENTER_ARM_UNOR
{
#endif


INLINE(Vwyu,VWYU_UNOR) (Rc(0, 1) n)
{
#define     VWYU_UNOR(N) VWWU_ASYU(UINT_ASTV(((N==1)?-1u:0u)))
    return  VWYU_UNOR(n);
}


INLINE(Vwbu,VWBU_UNOR) (Rc(0, 8) n)
{
#define     WBZ_UNOR(N)                     \
(                                           \
    (!N) ? 0.0f :                           \
    (N==8)                                  \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u8(            \
                vdup_n_u8(UINT8_MAX)        \
            ),                              \
            0                               \
        )                                   \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u8(            \
                vshl_n_u8(                  \
                    vdup_n_u8(UINT8_MAX),   \
                    ((8-!(N&7))-(7&N))      \
                )                           \
            ),                              \
            0                               \
        )                                   \
)

#define     VWBU_UNOR(N)    WBU_ASTV(WBZ_UNOR(N))
    uint8x8_t   v = vdup_n_u8(UINT8_MAX);
    int8x8_t    l = vdup_n_s8(8);
    l = vsub_s8(l, vdup_n_s8(n));
    v = vshl_u8(v, l);
    float32x2_t m = vreinterpret_f32_u8(v);
    float       f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_UNOR) (Rc(0, 8) n)
{
#define     VWBI_UNOR(N)    WBI_ASTV(WBZ_UNOR(N))
    uint8x8_t   v = vdup_n_u8(UINT8_MAX);
    int8x8_t    l = vdup_n_s8(8);
    l = vsub_s8(l, vdup_n_s8(n));
    v = vshl_u8(v, l);
    float32x2_t m = vreinterpret_f32_u8(v);
    float       f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_UNOR) (Rc(0, 8) n)
{
#define     VWBC_UNOR(N)    WBC_ASTV(WBZ_UNOR(N))
    uint8x8_t   v = vdup_n_u8(UINT8_MAX);
    int8x8_t    l = vdup_n_s8(8);
    l = vsub_s8(l, vdup_n_s8(n));
    v = vshl_u8(v, l);
    float32x2_t m = vreinterpret_f32_u8(v);
    float       f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
}


INLINE(Vwhu,VWHU_UNOR) (Rc(0, 16) n)
{
#define     WHZ_UNOR(N)                     \
(                                           \
    (000==N) ? 0.0f :                       \
    (010==N)                                \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u16(           \
                vdup_n_u16(UINT16_MAX)      \
            ),                              \
            0                               \
        )                                   \
    :   vget_lane_f32(                      \
            vreinterpret_f32_u16(           \
                vshl_n_u16(                 \
                    vdup_n_u16(UINT16_MAX), \
                    ((16-!(15&N))-(15&N))   \
                )                           \
            ),                              \
            0                               \
        )                                   \
)

#define     VWHU_UNOR(N)    WHU_ASTV(WHZ_UNOR(N))
    uint16x4_t   v = vdup_n_u16(UINT16_MAX);
    int16x4_t    l = vdup_n_s16(16);
    l = vsub_s16(l, vdup_n_s16(n));
    v = vshl_u16(v, l);
    float32x2_t m = vreinterpret_f32_u16(v);
    float       f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_UNOR) (Rc(0, 16) n)
{
#define     VWHI_UNOR(N)    WHI_ASTV(WHZ_UNOR(N))
    uint16x4_t   v = vdup_n_u16(UINT16_MAX);
    int16x4_t    l = vdup_n_s16(16);
    l = vsub_s16(l, vdup_n_s16(n));
    v = vshl_u16(v, l);
    float32x2_t m = vreinterpret_f32_u16(v);
    float       f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_UNOR) (Rc(0, 32) n)
{
#define     WWZ_UNOR(N)                     \
(                                           \
    (000==N) ? 0.0f :                       \
    (010==N)                                \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u32(           \
                vdup_n_u32(UINT32_MAX)      \
            ),                              \
            0                               \
        )                                   \
    :   vget_lane_f32(                      \
            vreinterpret_f32_u32(           \
                vshl_n_u32(                 \
                    vdup_n_u32(UINT32_MAX), \
                    ((32-!(31&N))-(31&N))   \
                )                           \
            ),                              \
            0                               \
        )                                   \
)

#define     VWWU_UNOR(N)    WWU_ASTV(WWZ_UNOR(N))
    uint32x2_t   v = vdup_n_u32(UINT32_MAX);
    int32x2_t    l = vdup_n_s32(32);
    l = vsub_s32(l,  vdup_n_s32(n));
    v = vshl_u32(v, l);
    float32x2_t m = vreinterpret_f32_u32(v);
    float       f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_UNOR) (Rc(0, 32) n)
{
#define     VWWI_UNOR(N)    WWI_ASTV(WWU_UNOR(N))
    uint32x2_t   v = vdup_n_u32(UINT32_MAX);
    int32x2_t    l = vdup_n_s32(32);
    l = vsub_s32(l,  vdup_n_s32(n));
    v = vshl_u32(v, l);
    float32x2_t m = vreinterpret_f32_u32(v);
    float       f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}


INLINE(Vdyu,VDYU_UNOR) (Rc(0, 1) n)
{
#define     VDYU_UNOR(N)                    \
(                                           \
    (N == 1)                                \
    ?   VDDU_ASYU(vdup_n_u64(UINT64_MAX))   \
    :   ((Vdyu){0})                         \
)
    uint64x1_t  c = vceq_u64(vdup_n_u64(n), vdup_n_u64(1));
    return  VDDU_ASYU(c);
}


INLINE(Vdbu,VDBU_UNOR) (Rc(0, 8) n)
{
#define     VDBU_UNOR(N)   vdup_n_u8(UINT8_UNOR(N))
    uint8x8_t v = vdup_n_u8(UINT8_MAX);
    int8x8_t  l = vsub_s8(vdup_n_s8(8), vdup_n_s8(n));
    return  vshl_u8(v, l);
}

INLINE(Vdbi,VDBI_UNOR) (Rc(0, 8) n)
{
#define     VDBI_UNOR(N)   vdup_n_s8(INT8_UNOR(N))
    return  VDBU_ASTI(((VDBU_UNOR)(n)));
}

INLINE(Vdbc,VDBC_UNOR) (Rc(0, 8) n)
{
#if CHAR_MIN
#   define  VDBC_UNOR(N)   VDBU_ASBC(vdup_n_s8(INT8_UNOR(N)))
#else
#   define  VDBC_UNOR(N)   VDBI_ASBC(vdup_n_u8(UINT8_UNOR(N)))
#endif

    return  VDBU_ASBC(((VDBU_UNOR)(n)));
}


INLINE(Vdhu,VDHU_UNOR) (Rc(0, 16) n)
{
#define     VDHU_UNOR(N)   vdup_n_u16(UINT16_UNOR(N))
    uint16x4_t v = vdup_n_u16(UINT16_MAX);
    int16x4_t  l = vsub_s16(vdup_n_s16(16), vdup_n_s16(n));
    return  vshl_u16(v, l);
}

INLINE(Vdhi,VDHI_UNOR) (Rc(0, 16) n)
{
#define     VDHI_UNOR(N)   vdup_n_s16(INT16_UNOR(N))
    return  VDHU_ASTI(((VDHU_UNOR)(n)));
}


INLINE(Vdwu,VDWU_UNOR) (Rc(0, 32) n)
{
#define     VDWU_UNOR(N)   vdup_n_u32(UINT32_UNOR(N))
    uint32x2_t v = vdup_n_u32(UINT32_MAX);
    int32x2_t  l = vsub_s32(vdup_n_s32(32), vdup_n_s32(n));
    return  vshl_u32(v, l);
}

INLINE(Vdwi,VDWI_UNOR) (Rc(0, 32) n)
{
#define     VDWI_UNOR(N)   vdup_n_s32(INT32_UNOR(N))
    return  VDWU_ASTI(((VDWU_UNOR)(n)));
}


INLINE(Vddu,VDDU_UNOR) (Rc(0, 64) n)
{
#define     VDDU_UNOR(N)   vdup_n_u64(UINT64_UNOR(N))
    uint64x1_t v = vdup_n_u64(UINT64_MAX);
    int64x1_t  l = vsub_s64(vdup_n_s64(64), vdup_n_s64(n));
    return  vshl_u64(v, l);
}

INLINE(Vddi,VDDI_UNOR) (Rc(0, 64) n)
{
#define     VDDI_UNOR(N)   vdup_n_s64(INT64_UNOR(N))
    return  VDDU_ASTI(((VDDU_UNOR)(n)));
}


INLINE(Vqyu,VQYU_UNOR) (Rc(0, 1) n)
{
#define     VQYU_UNOR(N)                    \
(                                           \
    (N == 1)                                \
    ?   VQDU_ASYU(vdupq_n_u64(UINT64_MAX))  \
    :   ((Vqyu){0})                         \
)
    uint64x2_t  c = vceqq_u64(vdupq_n_u64(n), vdupq_n_u64(1));
    return  VQDU_ASYU(c);
}


INLINE(Vqbu,VQBU_UNOR) (Rc(0, 8) n)
{
#define     VQBU_UNOR(N)   vdupq_n_u8(UINT8_UNOR(N))
    uint8x16_t  v = vdupq_n_u8(UINT8_MAX);
    int8x16_t   l = vsubq_s8(vdupq_n_s8(8), vdupq_n_s8(n));
    return  vshlq_u8(v, l);
}

INLINE(Vqbi,VQBI_UNOR) (Rc(0, 8) n)
{
#define     VQBI_UNOR(N)   vdupq_n_s8(INT8_UNOR(N))
    return  VQBU_ASTI(((VQBU_UNOR)(n)));
}

INLINE(Vqbc,VQBC_UNOR) (Rc(0, 8) n)
{
#if CHAR_MIN
#   define  VQBC_UNOR(N)   VQBU_ASBC(vdupq_n_s8(INT8_UNOR(N)))
#else
#   define  VQBC_UNOR(N)   VQBI_ASBC(vdupq_n_u8(UINT8_UNOR(N)))
#endif

    return  VQBU_ASBC(((VQBU_UNOR)(n)));
}


INLINE(Vqhu,VQHU_UNOR) (Rc(0, 16) n)
{
#define     VQHU_UNOR(N)   vdupq_n_u16(UINT16_UNOR(N))
    uint16x8_t  v = vdupq_n_u16(UINT16_MAX);
    int16x8_t   l = vsubq_s16(vdupq_n_s16(16), vdupq_n_s16(n));
    return  vshlq_u16(v, l);
}

INLINE(Vqhi,VQHI_UNOR) (Rc(0, 16) n)
{
#define     VQHI_UNOR(N)   vdupq_n_s16(INT16_UNOR(N))
    return  VQHU_ASTI(((VQHU_UNOR)(n)));
}


INLINE(Vqwu,VQWU_UNOR) (Rc(0, 32) n)
{
#define     VQWU_UNOR(N)   vdupq_n_u32(UINT32_UNOR(N))
    uint32x4_t  v = vdupq_n_u32(UINT32_MAX);
    int32x4_t   l = vsubq_s32(vdupq_n_s32(32), vdupq_n_s32(n));
    return  vshlq_u32(v, l);
}

INLINE(Vqwi,VQWI_UNOR) (Rc(0, 32) n)
{
#define     VQWI_UNOR(N)   vdupq_n_s32(INT32_UNOR(N))
    return  VQWU_ASTI(((VQWU_UNOR)(n)));
}


INLINE(Vqdu,VQDU_UNOR) (Rc(0, 64) n)
{
#define     VQDU_UNOR(N)   vdupq_n_u64(UINT64_UNOR(N))
    uint64x2_t  v = vdupq_n_u64(UINT64_MAX);
    int64x2_t   l = vsubq_s64(vdupq_n_s64(64), vdupq_n_s64(n));
    return  vshlq_u64(v, l);
}

INLINE(Vqdi,VQDI_UNOR) (Rc(0, 64) n)
{
#define     VQDI_UNOR(N)   vdupq_n_s64(INT64_UNOR(N))
    return  VQDU_ASTI(((VQDU_UNOR)(n)));
}


#define     QQZ_UNOR(N) \
vcombine_u64(\
    (\
        (N>64)\
        ?   vdup_n_u64((UINT64_MAX<<(128-N)))\
        :   vdup_n_u64(0)\
    ),\
    (\
        (N>64)\
        ?   vdup_n_u64(-1)\
        :   vdup_n_u64((UINT64_MAX<<(64-N)))\
    )\
)

INLINE(Vqqu,VQQU_UNOR) (Rc(0, 128) n)
{
#define     VQQU_UNOR(N) ((Vqqu){QQZ_UNOR(N)})
    return  VQQU_UNOR(n);
}

INLINE(Vqqi,VQQI_UNOR) (Rc(0, 128) n)
{
#define     VQQI_UNOR(N) ((Vqqi){vreinterpretq_s64_u64(QQZ_UNOR(N))})
    return  VQQI_UNOR(n);
}

#if 0 // _LEAVE_ARM_UNOR
}
#endif


#if 0 // _ENTER_ARM_PERM
{
#endif
/*
    TBX <Vd>.<Ta>, { <Vn>.16B }, <Vm>.<Ta>
    TBX <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B }, <Vm>.<Ta>
    TBX <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B }, <Vm>.<Ta>
    TBX <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B, <Vn+3>.16B }, <Vm>.<Ta>

uint8x16t vqtbx1q_u8(
    uint8x16_t      a (  <Vd.16B>  ),
    uint8x16_t      t ( {<Vn.16B>} ),
    uint8x16_t      i (  <Vm.16B>  )
) => TBX <Vd.16B>, {<Vn.16B>}, <Vm.16B>

uint8x16t vqtbx2q_u8(
    uint8x16_t      a (  <Vd.16B>  ),
    uint8x16_t      t ( {<Vn.16B>, <Vn+1.16B} ),
    uint8x16_t      i (  <Vm.16B>  )
) => TBX <Vd.16B>, {<Vn.16B>}, <Vm.16B>

*/

#define     V2_PERM(F,A,B,K0,K1)        F((A),(B),(K0),(K1))
#define     V4_PERM(F,A,B,K0,K1,K2,K3)  F((A),(B),(K0),(K1),(K2),(K3))
#define     V8_PERM(F,A,B,                  \
    K0,  K1,  K2,  K3,  K4,  K5,  K6,  K7)  \
F(                                          \
    (A),(B),                                \
    (K0),(K1),(K2),(K3),(K4),(K5),(K6),(K7) \
)

#define     V16_PERM(                               \
    F,A,B,                                          \
    K0,  K1,  K2,  K3,  K4,  K5,  K6,  K7,          \
    K8,  K9,  K10, K11, K12, K13, K14, K15  )       \
F(                                                  \
    (A),(B),                                         \
    (K0 ),(K1 ),(K2 ),(K3 ),(K4 ),(K5 ),(K6 ),(K7 ),\
    (K8 ),(K9 ),(K10),(K11),(K12),(K13),(K14),(K15) \
)

#define     V32_PERM(                   \
    F, A, B,                            \
    K0, K1, K2, K3, K4, K5, K6, K7,     \
    K8, K9, K10,K11,K12,K13,K14,K15,    \
    K16,K17,K18,K19,K20,K21,K22,K23,    \
    K24,K25,K26,K27,K28,K29,K30,K31)    \
F(                                      \
    (A),(B),                            \
    (K0 ),(K1 ),(K2 ),(K3 ),            \
    (K4 ),(K5 ),(K6 ),(K7 ),            \
    (K8 ),(K9 ),(K10),(K11),            \
    (K12),(K13),(K14),(K15),            \
    (K16),(K17),(K18),(K19),            \
    (K20),(K21),(K22),(K23),            \
    (K24),(K25),(K26),(K27),            \
    (K28),(K29),(K30),(K31)             \
)

#define     V64_PERM(                   \
    F, A, B,                            \
    K0, K1, K2, K3, K4, K5, K6, K7,     \
    K8, K9, K10,K11,K12,K13,K14,K15,    \
    K16,K17,K18,K19,K20,K21,K22,K23,    \
    K24,K25,K26,K27,K28,K29,K30,K31,    \
    K32,K33,K34,K35,K36,K37,K38,K39,    \
    K40,K41,K42,K43,K44,K45,K46,K47,    \
    K48,K49,K50,K51,K52,K53,K54,K55,    \
    K56,K57,K58,K59,K60,K61,K62,K63)    \
F(                                      \
    (A),(B),                            \
    (K0 ),(K1 ),(K2 ),(K3 ),(K4 ),(K5 ),(K6 ),(K7 ),\
    (K8 ),(K9 ),(K10),(K11),(K12),(K13),(K14),(K15),\
    (K16),(K17),(K18),(K19),(K20),(K21),(K22),(K23),\
    (K24),(K25),(K26),(K27),(K28),(K29),(K30),(K31),\
    (K32),(K33),(K34),(K35),(K36),(K37),(K38),(K39),\
    (K40),(K41),(K42),(K43),(K44),(K45),(K46),(K47),\
    (K48),(K49),(K50),(K51),(K52),(K53),(K54),(K55),\
    (K56),(K57),(K58),(K59),(K60),(K61),(K62),(K63) \
)

#define     WB_PERM(A,B, K0,K1,K2,K3)           \
vget_lane_f32(                                  \
    vreinterpret_f32_u8(                        \
        DB_PERM(                                \
            vreinterpret_u8_f32(vdup_n_f32(A)), \
            vreinterpret_u8_f32(vdup_n_f32(B)), \
            K0, K1, K2, K3,                     \
            -1, -1, -1, -1                      \
        )                                       \
    ),                                          \
    0                                           \
)

#define     WH_PERM(A, B, K0, K1)               \
vget_lane_f32(                                  \
    vreinterpret_f32_u8(                        \
        DH_PERM(                                \
            vreinterpret_u8_f32(vdup_n_f32(A)), \
            vreinterpret_u8_f32(vdup_n_f32(B)), \
            K0, K1, -1, -1                      \
        )                                       \
    ),                                          \
    0                                           \
)


#define     DB_PERM(                \
    A, B,                           \
    K0, K1, K2, K3, K4, K5, K6, K7) \
vtbx1_u8(                           \
    A, B,                           \
    vreinterpret_u8_u64(            \
        vdup_n_u64(                 \
            ((K0&0xffull)<<000)     \
        |   ((K1&0xffull)<<010)     \
        |   ((K2&0xffull)<<020)     \
        |   ((K3&0xffull)<<030)     \
        |   ((K4&0xffull)<<040)     \
        |   ((K5&0xffull)<<050)     \
        |   ((K6&0xffull)<<060)     \
        |   ((K7&0xffull)<<070)     \
        )                           \
    )                               \
)

#define     DH_PERM(A, B, K0, K1, K2, K3)           \
vtbx1_u8(                                           \
    A, B,                                           \
    vreinterpret_u8_u64(                            \
        vdup_n_u64(                                 \
            (                                       \
                (K0&0xfc) ? 0xffffull :             \
                (                                   \
                    ((HALF_B0+2ull*K0)<<000)        \
                |   ((HALF_B1+2ull*K0)<<010)        \
                )                                   \
            )                                       \
        |   (                                       \
                (K1&0xfc) ? (0xffffull<<020) :      \
                (                                   \
                    ((HALF_B0+2ull*K1)<<020)        \
                |   ((HALF_B1+2ull*K1)<<030)        \
                )                                   \
            )                                       \
        |   (                                       \
                (K2&0xfc) ? (0xffffull<<040) :      \
                (                                   \
                    ((HALF_B0+2ull*K2)<<040)        \
                |   ((HALF_B1+2ull*K2)<<050)        \
                )                                   \
            )                                       \
        |   (                                       \
                (K3&0xfc) ? (0xffffull<<060) :      \
                (                                   \
                    ((HALF_B0+2ull*K3)<<060)        \
                |   ((HALF_B1+2ull*K3)<<070)        \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     DW_PERM(A, B, K0, K1)                   \
vtbx1_u8(                                           \
    A, B,                                           \
    vreinterpret_u8_u64(                            \
        vdup_n_u64(                                 \
            (                                       \
                (K0&0xfe) ? (0xffffffffull<<000) :  \
                (                                   \
                    ((WORD_B0+K0*4ull)<<000)        \
                |   ((WORD_B1+K0*4ull)<<010)        \
                |   ((WORD_B2+K0*4ull)<<020)        \
                |   ((WORD_B3+K0*4ull)<<030)        \
                )                                   \
            )                                       \
        |   (                                       \
                (K1&0xfe) ? (0xffffffffull<<040) :  \
                (                                   \
                    ((WORD_B0+K1*4ull)<<040)        \
                |   ((WORD_B1+K1*4ull)<<050)        \
                |   ((WORD_B2+K1*4ull)<<060)        \
                |   ((WORD_B3+K1*4ull)<<070)        \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     QB_PERM(                \
    A, B,                           \
    K0,  K1,  K2,  K3,              \
    K4,  K5,  K6,  K7,              \
    K8,  K9,  K10, K11,             \
    K12, K13, K14, K15)             \
vqtbx1q_u8(                         \
    A,B,                            \
    vreinterpretq_u8_u64(           \
        vcombine_u64(               \
            vdup_n_u64(             \
                ((K0 &255ull)<<000) \
            |   ((K1 &255ull)<<010) \
            |   ((K2 &255ull)<<020) \
            |   ((K3 &255ull)<<030) \
            |   ((K4 &255ull)<<040) \
            |   ((K5 &255ull)<<050) \
            |   ((K6 &255ull)<<060) \
            |   ((K7 &255ull)<<070) \
            ),                      \
            vdup_n_u64(             \
                ((K8 &255ull)<<000) \
            |   ((K9 &255ull)<<010) \
            |   ((K10&255ull)<<020) \
            |   ((K11&255ull)<<030) \
            |   ((K12&255ull)<<040) \
            |   ((K13&255ull)<<050) \
            |   ((K14&255ull)<<060) \
            |   ((K15&255ull)<<070) \
            )                       \
        )                           \
    )                               \
)

#define     QH_PERM(                \
    A, B,                           \
    K0, K1, K2, K3, K4, K5, K6, K7) \
vqtbx1q_u8(                                         \
    A,B,                                            \
    vreinterpretq_u8_u64(                           \
        vcombine_u64(                               \
            vdup_n_u64(                             \
                (                                   \
                    (K0&0xf8) ? 0xffffull :         \
                    (                               \
                        ((HALF_B0+2ull*K0)<<000)    \
                    |   ((HALF_B1+2ull*K0)<<010)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K1&0xf8) ? (0xffffull<<020) :  \
                    (                               \
                        ((HALF_B0+2ull*K1)<<020)    \
                    |   ((HALF_B1+2ull*K1)<<030)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K2&0xf8) ? (0xffffull<<040) :  \
                    (                               \
                        ((HALF_B0+2ull*K2)<<040)    \
                    |   ((HALF_B1+2ull*K2)<<050)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K3&0xf8) ? (0xffffull<<060) :  \
                    (                               \
                        ((HALF_B0+2ull*K3)<<060)    \
                    |   ((HALF_B1+2ull*K3)<<070)    \
                    )                               \
                )                                   \
            ),                                      \
            vdup_n_u64(                             \
                (                                   \
                    (K4&0xf8) ? 0xffffull :         \
                    (                               \
                        ((2ull*K4+HALF_B0)<<000)    \
                    |   ((2ull*K4+HALF_B1)<<010)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K5&0xf8) ? (0xffffull<<020) :  \
                    (                               \
                        ((HALF_B0+2ull*K5)<<020)    \
                    |   ((HALF_B1+2ull*K5)<<030)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K6&0xf8) ? (0xffffull<<040) :  \
                    (                               \
                        ((HALF_B0+2ull*K6)<<040)    \
                    |   ((HALF_B1+2ull*K6)<<050)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K7&0xf8) ? (0xffffull<<060) :  \
                    (                               \
                        ((HALF_B0+2ull*K7)<<060)    \
                    |   ((HALF_B1+2ull*K7)<<070)    \
                    )                               \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     QW_PERM(A, B, K0, K1, K2, K3)               \
vqtbx1q_u8(                                             \
    A, B,                                               \
    vreinterpretq_u8_u64(                               \
        vcombine_u64(                                   \
            vdup_n_u64(                                 \
                (                                       \
                    (K0&0x80) ? (0xffffffffull<<000) :  \
                    (                                   \
                        ((WORD_B0+K0*4ull)<<000)        \
                    |   ((WORD_B1+K0*4ull)<<010)        \
                    |   ((WORD_B2+K0*4ull)<<020)        \
                    |   ((WORD_B3+K0*4ull)<<030)        \
                    )                                   \
                )                                       \
                |                                       \
                (                                       \
                    (K1&0x80) ? (0xffffffffull<<040) :  \
                    (                                   \
                        ((WORD_B0+K1*4ull)<<040)        \
                    |   ((WORD_B1+K1*4ull)<<050)        \
                    |   ((WORD_B2+K1*4ull)<<060)        \
                    |   ((WORD_B3+K1*4ull)<<070)        \
                    )                                   \
                )                                       \
            ),                                          \
            vdup_n_u64(                                 \
                (                                       \
                    (K2&0x80) ? (0xffffffffull<<000) :  \
                    (                                   \
                        ((WORD_B0+K2*4ull)<<000)        \
                    |   ((WORD_B1+K2*4ull)<<010)        \
                    |   ((WORD_B2+K2*4ull)<<020)        \
                    |   ((WORD_B3+K2*4ull)<<030)        \
                    )                                   \
                )                                       \
                |                                       \
                (                                       \
                    (K3&0x80) ? (0xffffffffull<<040) :  \
                    (                                   \
                        ((WORD_B0+K3*4ull)<<040)        \
                    |   ((WORD_B1+K3*4ull)<<050)        \
                    |   ((WORD_B2+K3*4ull)<<060)        \
                    |   ((WORD_B3+K3*4ull)<<070)        \
                    )                                   \
                )                                       \
            )                                           \
        )                                               \
    )                                                   \
)

#define     QD_PERM(A, B, K0, K1)   \
vqtbx1q_u8(                         \
    A, B,                           \
    vcombine_u8(                    \
        vdup_n_u8(254&K0 ? -1 : K0),\
        vdup_n_u8(254&K1 ? -1 : K1) \
    )                               \
)

#define     WBU_PERM(A, B, ...)     V4_PERM(WB_PERM,(A),(B),__VA_ARGS__)
#define     WBI_PERM(A, B, ...)     V4_PERM(WB_PERM,(A),(B),__VA_ARGS__)
#define     WBC_PERM(A, B, ...)     V4_PERM(WB_PERM,(A),(B),__VA_ARGS__)

#define     WHU_PERM(A, B, ...)     V2_PERM(WH_PERM,(A),(B),__VA_ARGS__)
#define     WHI_PERM(A, B, ...)     V2_PERM(WH_PERM,(A),(B),__VA_ARGS__)
#define     WHF_PERM(A, B, ...)     V2_PERM(WH_PERM,(A),(B),__VA_ARGS__)

#define     DBU_PERM(A, B, ...) \
         V8_PERM(DB_PERM,        (A),         (B), __VA_ARGS__)

#define     DBI_PERM(A, B, ...) \
DBU_ASBI(V8_PERM(DB_PERM,DBI_ASBU(A), DBI_ASBU(B), __VA_ARGS__))

#define     DBC_PERM(A, B, ...) \
DBU_ASBC(V8_PERM(DB_PERM,DBC_ASBU(A), DBC_ASBU(B), __VA_ARGS__))


#define     DHU_PERM(A, B, ...) \
DBU_ASHU(V4_PERM(DH_PERM,DHU_ASBU(A), DHU_ASBU(B), __VA_ARGS__))

#define     DHI_PERM(A, B, ...) \
DBU_ASHI(V4_PERM(DH_PERM,DHI_ASBU(A), DHI_ASBU(B), __VA_ARGS__))

#define     DHF_PERM(A, B, ...) \
DBU_ASHF(V4_PERM(DH_PERM,DHF_ASBU(A), DHF_ASBU(B), __VA_ARGS__))


#define     DWU_PERM(A, B, ...) \
DBU_ASWU(V2_PERM(DW_PERM,DWU_ASBU(A), DWU_ASBU(B), __VA_ARGS__))

#define     DWI_PERM(A, B, ...) \
DBU_ASWI(V2_PERM(DW_PERM,DWI_ASBU(A), DWI_ASBU(B), __VA_ARGS__))

#define     DWF_PERM(A, B, ...) \
DBU_ASWF(V2_PERM(DW_PERM,DWF_ASBU(A), DWF_ASBU(B), __VA_ARGS__))


#define     QBU_PERM(A, B, ...) \
         V16_PERM(QB_PERM,         (A),         (B), __VA_ARGS__)

#define     QBI_PERM(A, B, ...) \
QBU_ASBI(V16_PERM(QB_PERM, QBI_ASBU(A), QBI_ASBU(B), __VA_ARGS__))

#define     QBC_PERM(A, B, ...) \
QBU_ASBC(V16_PERM(QB_PERM, QBC_ASBU(A), QBC_ASBU(B), __VA_ARGS__))


#define     QHU_PERM(A, B, ...) \
QBU_ASHU( V8_PERM(QH_PERM, QHU_ASBU(A), QHU_ASBU(B), __VA_ARGS__))

#define     QHI_PERM(A, B, ...) \
QBU_ASHI( V8_PERM(QH_PERM, QHI_ASBU(A), QHI_ASBU(B), __VA_ARGS__))

#define     QHF_PERM(A, B, ...) \
QBU_ASHF( V8_PERM(QH_PERM, QHF_ASBU(A), QHU_ASBU(B), __VA_ARGS__))


#define     QWU_PERM(A, B, ...) \
QBU_ASWU( V4_PERM(QW_PERM, QWU_ASBU(A), QWU_ASBU(B), __VA_ARGS__))

#define     QWI_PERM(A, B, ...) \
QBU_ASWI( V4_PERM(QW_PERM, QWI_ASBU(A), QWI_ASBU(B), __VA_ARGS__))

#define     QWF_PERM(A, B, ...) \
QBU_ASWF( V4_PERM(QW_PERM, QWF_ASBU(A), QWF_ASBU(B), __VA_ARGS__))


#define     QDU_PERM(A, B, ...) \
QBU_ASDU( V2_PERM(QD_PERM, QDU_ASBU(A), QDU_ASBU(B), __VA_ARGS__))

#define     QDI_PERM(A, B, ...) \
QBU_ASDI( V2_PERM(QD_PERM, QDI_ASBU(A), QDI_ASBU(B), __VA_ARGS__))

#define     QDF_PERM(A, B, ...) \
QBU_ASDF( V2_PERM(QD_PERM, QDF_ASBU(A), QDF_ASBU(B), __VA_ARGS__))

INLINE(Vwbu,VWBU_PERM)
(
    Vwbu a, Vwbu b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VWBU_PERM(A, B, ...)    \
WBU_ASTV(                           \
    WBU_PERM(                       \
        VWBU_ASTM(A),               \
        VWBU_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWBU_PERM(a, b, k0,k1,k2,k3);
}

INLINE(Vwbi,VWBI_PERM)
(
    Vwbi a, Vwbi b,
    Rc(-1,3) k0, Rc(-1,3) k1, Rc(-1,3) k2, Rc(-1,3) k3
)
{
#define     VWBI_PERM(A, B, ...)    \
WBI_ASTV(                           \
    WBI_PERM(                       \
        VWBI_ASTM(A),               \
        VWBI_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWBI_PERM(a, b, k0,k1,k2,k3);
}

INLINE(Vwbc,VWBC_PERM)
(
    Vwbc a, Vwbc b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VWBC_PERM(A, B, ...)    \
WBC_ASTV(                           \
    WBC_PERM(                       \
        VWBC_ASTM(A),               \
        VWBC_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWBC_PERM(a, b, k0,k1,k2,k3);
}


INLINE(Vwhu,VWHU_PERM)
(
    Vwhu a, Vwhu b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VWHU_PERM(A, B, ...)    \
WHU_ASTV(                           \
    WHU_PERM(                       \
        VWHU_ASTM(A),               \
        VWHU_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWHU_PERM(a, b, k0,k1);
}


INLINE(Vwhi,VWHI_PERM)
(
    Vwhi a, Vwhi b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VWHI_PERM(A, B, ...)    \
WHI_ASTV(                           \
    WHI_PERM(                       \
        VWHI_ASTM(A),               \
        VWHI_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWHI_PERM(a, b, k0,k1);
}

INLINE(Vwhf,VWHF_PERM)
(
    Vwhf a, Vwhf b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VWHF_PERM(A, B, ...)    \
WHF_ASTV(                           \
    WHF_PERM(                       \
        VWHF_ASTM(A),               \
        VWHF_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWHF_PERM(a, b, k0,k1);
}


INLINE(Vdbu,VDBU_PERM)
(
    Vdbu a, Vdbu b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VDBU_PERM(...)  DBU_PERM(__VA_ARGS__)
    return  DBU_PERM(a,b, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vdbi,VDBI_PERM)
(
    Vdbi a, Vdbi b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VDBI_PERM(...)  DBI_PERM(__VA_ARGS__)
    return  DBI_PERM(a,b, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vdbc,VDBC_PERM)
(
    Vdbc a, Vdbc b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VDBC_PERM(A, B, ...)    \
DBC_ASTV(                           \
    DBC_PERM(                       \
        VDBC_ASTM(A),               \
        VDBC_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VDBC_PERM(a,b, k0,k1,k2,k3,k4,k5,k6,k7);
}


INLINE(Vdhu,VDHU_PERM)
(
    Vdhu a, Vdhu b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VDHU_PERM(...)   DHU_PERM(__VA_ARGS__)
    return  DHU_PERM(a,b, k0,k1,k2,k3);
}

INLINE(Vdhi,VDHI_PERM)
(
    Vdhi a, Vdhi b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VDHI_PERM(...)   DHI_PERM(__VA_ARGS__)
    return  DHI_PERM(a,b, k0,k1,k2,k3);
}

INLINE(Vdhf,VDHF_PERM)
(
    Vdhf a, Vdhf b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VDHF_PERM(...)   DHF_PERM(__VA_ARGS__)
    return  DHF_PERM(a,b, k0,k1,k2,k3);
}


INLINE(Vdwu,VDWU_PERM)
(
    Vdwu a, Vdwu b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VDWU_PERM(...)  DWU_PERM(__VA_ARGS__)
    return  DWU_PERM(a,b, k0,k1);
}

INLINE(Vdwi,VDWI_PERM)
(
    Vdwi a, Vdwi b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VDWI_PERM(...)  DWI_PERM(__VA_ARGS__)
    return  DWI_PERM(a,b, k0,k1);
}

INLINE(Vdwf,VDWF_PERM)
(
    Vdwf a, Vdwf b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VDWF_PERM(...)  DWF_PERM(__VA_ARGS__)
    return  DWF_PERM(a,b, k0,k1);
}


INLINE(Vqbu,VQBU_PERM)
(
    Vqbu a, Vqbu b,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBU_PERM(...)  QBU_PERM(__VA_ARGS__)
    return  QBU_PERM(
        a, b,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqbi,VQBI_PERM)
(
    Vqbi a, Vqbi b,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBI_PERM(...)  QBI_PERM(__VA_ARGS__)
    return  QBI_PERM(
        a, b,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqbc,VQBC_PERM)
(
    Vqbc a, Vqbc b,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBC_PERM(A, B, ...)    \
QBC_ASTV(                           \
    QBC_PERM(                       \
        VQBC_ASTM(A),               \
        VQBC_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VQBC_PERM(
        a,b,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqhu,VQHU_PERM)
(
    Vqhu a, Vqhu b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VQHU_PERM(...)  QHU_PERM(__VA_ARGS__)
    return  QHU_PERM(a,b, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vqhi,VQHI_PERM)
(
    Vqhi a, Vqhi b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VQHI_PERM(...)  QHI_PERM(__VA_ARGS__)
    return  QHI_PERM(a,b, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vqhf,VQHF_PERM)
(
    Vqhf a, Vqhf b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VQHF_PERM(...)  QHF_PERM(__VA_ARGS__)
    return  QHF_PERM(a,b, k0,k1,k2,k3, k4,k5,k6,k7);
}


INLINE(Vqwu,VQWU_PERM)
(
    Vqwu a, Vqwu b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VQWU_PERM(...)  QWU_PERM(__VA_ARGS__)
    return  QWU_PERM(a,b, k0,k1,k2,k3);
}

INLINE(Vqwi,VQWI_PERM)
(
    Vqwi a, Vqwi b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VQWI_PERM(...)  QWI_PERM(__VA_ARGS__)
    return  QWI_PERM(a,b, k0,k1,k2,k3);
}

INLINE(Vqwf,VQWF_PERM)
(
    Vqwf a, Vqwf b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VQWF_PERM(...)  QWF_PERM(__VA_ARGS__)
    return  QWF_PERM(a,b, k0,k1,k2,k3);
}

INLINE(Vqdu,VQDU_PERM)
(
    Vqdu a, Vqdu b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VQDU_PERM(...)  QDU_PERM(__VA_ARGS__)
    return  QDU_PERM(a,b, k0,k1);
}

INLINE(Vqdi,VQDI_PERM)
(
    Vqdi a, Vqdi b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VQDI_PERM(...)  QDI_PERM(__VA_ARGS__)
    return  QDI_PERM(a,b, k0,k1);
}

INLINE(Vqdf,VQDF_PERM)
(
    Vqdf a, Vqdf b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VQDF_PERM(...)  QDF_PERM(__VA_ARGS__)
    return  QDF_PERM(a,b, k0,k1);
}

#if 0 // _LEAVE_ARM_PERM_
}
#endif

#if 0 // _ENTER_ARM_PERS
{
#endif

#define     V2_PERS(f, v, k0, k1)           f((v),(k0),(k1))
#define     V4_PERS(f, v, k0, k1, k2, k3)   f((v),(k0),(k1),(k2),(k3))
#define     V8_PERS(f, v,                   \
    k0,  k1,  k2,  k3,  k4,  k5,  k6,  k7)  \
f(                                          \
    (v),                                    \
    (k0),(k1),(k2),(k3),(k4),(k5),(k6),(k7) \
)

#define     V16_PERS(                               \
    f, v,                                           \
    k0,  k1,  k2,  k3,  k4,  k5,  k6,  k7,          \
    k8,  k9,  k10, k11, k12, k13, k14, k15  )       \
f(                                                  \
    (v),                                            \
    (k0 ),(k1 ),(k2 ),(k3 ),(k4 ),(k5 ),(k6 ),(k7 ),\
    (k8 ),(k9 ),(k10),(k11),(k12),(k13),(k14),(k15) \
)

#define     V32_PERS(                   \
    f, v,                               \
    k0, k1, k2, k3, k4, k5, k6, k7,     \
    k8, k9, k10,k11,k12,k13,k14,k15,    \
    k16,k17,k18,k19,k20,k21,k22,k23,    \
    k24,k25,k26,k27,k28,k29,k30,k31)    \
f(                                      \
    (v),                                \
    (k0 ),(k1 ),(k2 ),(k3 ),(k4 ),(k5 ),(k6 ),(k7 ),\
    (k8 ),(k9 ),(k10),(k11),(k12),(k13),(k14),(k15),\
    (k16),(k17),(k18),(k19),(k20),(k21),(k22),(k23),\
    (k24),(k25),(k26),(k27),(k28),(k29),(k30),(k31) \
)

#define     V64_PERS(                   \
    f, v,                               \
    k0, k1, k2, k3, k4, k5, k6, k7,     \
    k8, k9, k10,k11,k12,k13,k14,k15,    \
    k16,k17,k18,k19,k20,k21,k22,k23,    \
    k24,k25,k26,k27,k28,k29,k30,k31,    \
    k32,k33,k34,k35,k36,k37,k38,k39,    \
    k40,k41,k42,k43,k44,k45,k46,k47,    \
    k48,k49,k50,k51,k52,k53,k54,k55,    \
    k56,k57,k58,k59,k60,k61,k62,k63)    \
f(                                      \
    v,                                  \
    (k0 ),(k1 ),(k2 ),(k3 ),(k4 ),(k5 ),(k6 ),(k7 ),\
    (k8 ),(k9 ),(k10),(k11),(k12),(k13),(k14),(k15),\
    (k16),(k17),(k18),(k19),(k20),(k21),(k22),(k23),\
    (k24),(k25),(k26),(k27),(k28),(k29),(k30),(k31),\
    (k32),(k33),(k34),(k35),(k36),(k37),(k38),(k39),\
    (k40),(k41),(k42),(k43),(k44),(k45),(k46),(k47),\
    (k48),(k49),(k50),(k51),(k52),(k53),(k54),(k55),\
    (k56),(k57),(k58),(k59),(k60),(k61),(k62),(k63) \
)

#define     WB_PERS(v, B0, B1, B2, B3)          \
vget_lane_f32(                                  \
    vreinterpret_f32_u8(                        \
        DB_PERS(                                \
            vreinterpret_u8_f32(vdup_n_f32(v)), \
            B0, B1, B2, B3,                     \
            -1, -1, -1, -1                      \
        )                                       \
    ),                                          \
    0                                           \
)

#define     WH_PERS(v, H0, H1)                  \
vget_lane_f32(                                  \
    vreinterpret_f32_u8(                        \
        DH_PERS(                                \
            vreinterpret_u8_f32(vdup_n_f32(v)), \
            H0, H1, -1, -1                      \
        )                                       \
    ),                                          \
    0                                           \
)


#define     DB_PERS(v, B0,B1,B2,B3,B4,B5,B6,B7) \
vtbl1_u8(                       \
    v,                          \
    vreinterpret_u8_u64(        \
        vdup_n_u64(             \
            ((0xffull&B0)<<000) \
        |   ((0xffull&B1)<<010) \
        |   ((0xffull&B2)<<020) \
        |   ((0xffull&B3)<<030) \
        |   ((0xffull&B4)<<040) \
        |   ((0xffull&B5)<<050) \
        |   ((0xffull&B6)<<060) \
        |   ((0xffull&B7)<<070) \
        )                       \
    )                           \
)

#define     DH_PERS(v, H0, H1, H2, H3)              \
vtbl1_u8(                                           \
    v,                                              \
    vreinterpret_u8_u64(                            \
        vdup_n_u64(                                 \
            (                                       \
                (H0&0x80) ? 0xffffull :             \
                (                                   \
                    ((HALF_B0+2ull*H0)<<000)        \
                |   ((HALF_B1+2ull*H0)<<010)        \
                )                                   \
            )                                       \
        |   (                                       \
                (H1&0x80) ? (0xffffull<<020) :      \
                (                                   \
                    ((HALF_B0+2ull*H1)<<020)        \
                |   ((HALF_B1+2ull*H1)<<030)        \
                )                                   \
            )                                       \
        |   (                                       \
                (H2&0x80) ? (0xffffull<<040) :      \
                (                                   \
                    ((HALF_B0+2ull*H2)<<040)        \
                |   ((HALF_B1+2ull*H2)<<050)        \
                )                                   \
            )                                       \
        |   (                                       \
                (H3&0x80) ? (0xffffull<<060) :      \
                (                                   \
                    ((HALF_B0+2ull*H3)<<060)        \
                |   ((HALF_B1+2ull*H3)<<070)        \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     DW_PERS(v, W0, W1)                      \
vtbl1_u8(                                           \
    v,                                              \
    vreinterpret_u8_u64(                            \
        vdup_n_u64(                                 \
            (                                       \
                (W0&0x80) ? (0xffffffffull<<000) :  \
                (                                   \
                    ((WORD_B0+W0*4ull)<<000)        \
                |   ((WORD_B1+W0*4ull)<<010)        \
                |   ((WORD_B2+W0*4ull)<<020)        \
                |   ((WORD_B3+W0*4ull)<<030)        \
                )                                   \
            )                                       \
        |   (                                       \
                (W1&0x80) ? (0xffffffffull<<040) :  \
                (                                   \
                    ((WORD_B0+W1*4ull)<<040)        \
                |   ((WORD_B1+W1*4ull)<<050)        \
                |   ((WORD_B2+W1*4ull)<<060)        \
                |   ((WORD_B3+W1*4ull)<<070)        \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     QB_PERS(                \
    v,                              \
    B0,  B1,  B2,  B3,              \
    B4,  B5,  B6,  B7,              \
    B8,  B9,  B10, B11,             \
    B12, B13, B14, B15              \
)                                   \
vqtbl1q_u8(                         \
    v,                              \
    vreinterpretq_u8_u64(           \
        vcombine_u64(               \
            vdup_n_u64(             \
                ((B0 &255ull)<<000) \
            |   ((B1 &255ull)<<010) \
            |   ((B2 &255ull)<<020) \
            |   ((B3 &255ull)<<030) \
            |   ((B4 &255ull)<<040) \
            |   ((B5 &255ull)<<050) \
            |   ((B6 &255ull)<<060) \
            |   ((B7 &255ull)<<070) \
            ),                      \
            vdup_n_u64(             \
                ((B8 &255ull)<<000) \
            |   ((B9 &255ull)<<010) \
            |   ((B10&255ull)<<020) \
            |   ((B11&255ull)<<030) \
            |   ((B12&255ull)<<040) \
            |   ((B13&255ull)<<050) \
            |   ((B14&255ull)<<060) \
            |   ((B15&255ull)<<070) \
            )                       \
        )                           \
    )                               \
)

#define     QH_PERS(v, H0,H1,H2,H3,H4,H5,H6,H7)     \
vqtbl1q_u8(                                         \
    v,                                              \
    vreinterpretq_u8_u64(                           \
        vcombine_u64(                               \
            vdup_n_u64(                             \
                (                                   \
                    (H0&0x80) ? 0xffffull :         \
                    (                               \
                        ((HALF_B0+2ull*H0)<<000)    \
                    |   ((HALF_B1+2ull*H0)<<010)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H1&0x80) ? (0xffffull<<020) :  \
                    (                               \
                        ((HALF_B0+2ull*H1)<<020)    \
                    |   ((HALF_B1+2ull*H1)<<030)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H2&0x80) ? (0xffffull<<040) :  \
                    (                               \
                        ((HALF_B0+2ull*H2)<<040)    \
                    |   ((HALF_B1+2ull*H2)<<050)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H3&0x80) ? (0xffffull<<060) :  \
                    (                               \
                        ((HALF_B0+2ull*H3)<<060)    \
                    |   ((HALF_B1+2ull*H3)<<070)    \
                    )                               \
                )                                   \
            ),                                      \
            vdup_n_u64(                             \
                (                                   \
                    (H4&0x80) ? 0xffffull :         \
                    (                               \
                        ((HALF_B0+2ull*H4)<<000)    \
                    |   ((HALF_B1+2ull*H4)<<010)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H5&0x80) ? (0xffffull<<020) :  \
                    (                               \
                        ((HALF_B0+2ull*H5)<<020)    \
                    |   ((HALF_B1+2ull*H5)<<030)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H6&0x80) ? (0xffffull<<040) :  \
                    (                               \
                        ((HALF_B0+2ull*H6)<<040)    \
                    |   ((HALF_B1+2ull*H6)<<050)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H7&0x80) ? (0xffffull<<060) :  \
                    (                               \
                        ((HALF_B0+2ull*H7)<<060)    \
                    |   ((HALF_B1+2ull*H7)<<070)    \
                    )                               \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     QW_PERS(v, W0, W1, W2, W3)                  \
vqtbl1q_u8(                                             \
    v,                                                  \
    vreinterpretq_u8_u64(                               \
        vcombine_u64(                                   \
            vdup_n_u64(                                 \
                (                                       \
                    (W0&0x80) ? (0xffffffffull<<000) :  \
                    (                                   \
                        ((WORD_B0+W0*4ull)<<000)        \
                    |   ((WORD_B1+W0*4ull)<<010)        \
                    |   ((WORD_B2+W0*4ull)<<020)        \
                    |   ((WORD_B3+W0*4ull)<<030)        \
                    )                                   \
                )                                       \
                |                                       \
                (                                       \
                    (W1&0x80) ? (0xffffffffull<<040) :  \
                    (                                   \
                        ((WORD_B0+W1*4ull)<<040)        \
                    |   ((WORD_B1+W1*4ull)<<050)        \
                    |   ((WORD_B2+W1*4ull)<<060)        \
                    |   ((WORD_B3+W1*4ull)<<070)        \
                    )                                   \
                )                                       \
            ),                                          \
            vdup_n_u64(                                 \
                (                                       \
                    (W2&0x80) ? (0xffffffffull<<000) :  \
                    (                                   \
                        ((WORD_B0+W2*4ull)<<000)        \
                    |   ((WORD_B1+W2*4ull)<<010)        \
                    |   ((WORD_B2+W2*4ull)<<020)        \
                    |   ((WORD_B3+W2*4ull)<<030)        \
                    )                                   \
                )                                       \
                |                                       \
                (                                       \
                    (W3&0x80) ? (0xffffffffull<<040) :  \
                    (                                   \
                        ((WORD_B0+W3*4ull)<<040)        \
                    |   ((WORD_B1+W3*4ull)<<050)        \
                    |   ((WORD_B2+W3*4ull)<<060)        \
                    |   ((WORD_B3+W3*4ull)<<070)        \
                    )                                   \
                )                                       \
            )                                           \
        )                                               \
    )                                                   \
)

#define     QD_PERS(v, D0, D1)      \
vqtbl1q_u8(                         \
    v,                              \
    vcombine_u8(                    \
        vdup_n_u8(128&D0 ? -1 : D0),\
        vdup_n_u8(128&D1 ? -1 : D1) \
    )                               \
)

#define     WBU_PERS(M, ...)    V4_PERS(WB_PERS,(M),__VA_ARGS__)
#define     WBI_PERS(M, ...)    V4_PERS(WB_PERS,(M),__VA_ARGS__)
#define     WBC_PERS(M, ...)    V4_PERS(WB_PERS,(M),__VA_ARGS__)

#define     WHU_PERS(M, ...)    V2_PERS(WH_PERS,(M),__VA_ARGS__)
#define     WHI_PERS(M, ...)    V2_PERS(WH_PERS,(M),__VA_ARGS__)
#define     WHF_PERS(M, ...)    V2_PERS(WH_PERS,(M),__VA_ARGS__)


#define     DBU_PERS(M, ...)    \
        V8_PERS(DB_PERS,          (M), __VA_ARGS__)

#define     DBI_PERS(M, ...)    \
DBU_ASBI(V8_PERS(DB_PERS, DBI_ASBU(M), __VA_ARGS__))

#define     DBC_PERS(M, ...)    \
DBU_ASBC(V8_PERS(DB_PERS, DBC_ASBU(M), __VA_ARGS__))


#define     DHU_PERS(M, ...)    \
DBU_ASHU(V4_PERS(DH_PERS, DHU_ASBU(M), __VA_ARGS__))

#define     DHI_PERS(M, ...)    \
DBU_ASHI(V4_PERS(DH_PERS, DHI_ASBU(M), __VA_ARGS__))

#define     DHF_PERS(M, ...)    \
DBU_ASHF(V4_PERS(DH_PERS, DHF_ASBU(M), __VA_ARGS__))


#define     DWU_PERS(M, ...)    \
DBU_ASWU(V2_PERS(DW_PERS, DWU_ASBU(M), __VA_ARGS__))

#define     DWI_PERS(M, ...)    \
DBU_ASWI(V2_PERS(DW_PERS, DWI_ASBU(M), __VA_ARGS__))

#define     DWF_PERS(M,...)     \
DBU_ASWF(V2_PERS(DW_PERS, DWF_ASBU(M), __VA_ARGS__))


#define     QBU_PERS(M, ...)    \
         V16_PERS(QB_PERS,         (M), __VA_ARGS__)

#define     QBI_PERS(M, ...)    \
QBU_ASBI(V16_PERS(QB_PERS, QBI_ASBU(M), __VA_ARGS__))

#define     QBC_PERS(M, ...)    \
QBU_ASBC(V16_PERS(QB_PERS, QBC_ASBU(M), __VA_ARGS__))


#define     QHU_PERS(M, ...)    \
QBU_ASHU(V8_PERS(QH_PERS, QHU_ASBU(M), __VA_ARGS__))

#define     QHI_PERS(M, ...)    \
QBU_ASHI(V8_PERS(QH_PERS, QHI_ASBU(M), __VA_ARGS__))

#define     QHF_PERS(M, ...)    \
QBU_ASHF(V8_PERS(QH_PERS, QHF_ASBU(M), __VA_ARGS__))


#define     QWU_PERS(M, ...)    \
QBU_ASWU(V4_PERS(QW_PERS, QWU_ASBU(M), __VA_ARGS__))

#define     QWI_PERS(M, ...)    \
QBU_ASWI(V4_PERS(QW_PERS, QWI_ASBU(M), __VA_ARGS__))

#define     QWF_PERS(M, ...)    \
QBU_ASWF(V4_PERS(QW_PERS, QWF_ASBU(M), __VA_ARGS__))


#define     QDU_PERS(M, ...)    \
QBU_ASDU(V2_PERS(QD_PERS, QDU_ASBU(M), __VA_ARGS__))

#define     QDI_PERS(M, ...)    \
QBU_ASDI(V2_PERS(QD_PERS, QDI_ASBU(M), __VA_ARGS__))

#define     QDF_PERS(M, ...)    \
QBU_ASDF(V2_PERS(QD_PERS, QDF_ASBU(M), __VA_ARGS__))

INLINE(uint32_t,UINT32_PERB)
(
    uint32_t v,
    Rc(0,+31) k0,  Rc(0,+31) k1,  Rc(0,+31) k2,  Rc(0,+31) k3,
    Rc(0,+31) k4,  Rc(0,+31) k5,  Rc(0,+31) k6,  Rc(0,+31) k7
)
{
    return (
        (((v>>k0)&1))
    |   (((v>>k1)&1)<<1)
    |   (((v>>k2)&1)<<2)
    |   (((v>>k3)&1)<<3)
    |   (((v>>k4)&1)<<4)
    |   (((v>>k5)&1)<<5)
    |   (((v>>k6)&1)<<6)
    |   (((v>>k7)&1)<<7)
    );
}

INLINE(uint32_t,UINT32_PERH)
(
    uint32_t v,
    Rc(0,+31) k0,  Rc(0,+31) k1,  Rc(0,+31) k2,  Rc(0,+31) k3,
    Rc(0,+31) k4,  Rc(0,+31) k5,  Rc(0,+31) k6,  Rc(0,+31) k7,
    Rc(0,+31) k8,  Rc(0,+31) k9,  Rc(0,+31) k10, Rc(0,+31) k11,
    Rc(0,+31) k12, Rc(0,+31) k13, Rc(0,+31) k14, Rc(0,+31) k15
)
{
    return (
        (((v>>k0)&1))
    |   (((v>>k1)&1)<<1)
    |   (((v>>k2)&1)<<2)
    |   (((v>>k3)&1)<<3)
    |   (((v>>k4)&1)<<4)
    |   (((v>>k5)&1)<<5)
    |   (((v>>k6)&1)<<6)
    |   (((v>>k7)&1)<<7)
    |   (((v>>k8)&1)<<8)
    |   (((v>>k9)&1)<<9)
    |   (((v>>k10)&1)<<10)
    |   (((v>>k11)&1)<<11)
    |   (((v>>k12)&1)<<12)
    |   (((v>>k13)&1)<<13)
    |   (((v>>k14)&1)<<14)
    |   (((v>>k15)&1)<<15)
    );
}


INLINE(Vwbu,VWBU_PERS)
(
    Vwbu v,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VWBU_PERS(V, ...) WBU_ASTV(WBU_PERS(VWBU_ASTM(V),__VA_ARGS__))
    return  VWBU_PERS(v, k0,k1,k2,k3);
}

INLINE(Vwbi,VWBI_PERS)
(
    Vwbi v,
    Rc(-1,3) k0, Rc(-1,3) k1, Rc(-1,3) k2, Rc(-1,3) k3
)
{
#define     VWBI_PERS(V, ...) WBI_ASTV(WBI_PERS(VWBI_ASTM(V),__VA_ARGS__))

    return  VWBI_PERS(v, k0,k1,k2,k3);
}

INLINE(Vwbc,VWBC_PERS)
(
    Vwbc v,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VWBC_PERS(V, ...) WBC_ASTV(WBC_PERS(VWBC_ASTM(V),__VA_ARGS__))
    return  VWBC_PERS(v, k0,k1,k2,k3);
}

INLINE(Vwhu,VWHU_PERS) (Vwhu v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VWHU_PERS(V, ...) WHU_ASTV(WHU_PERS(VWHU_ASTM(V),__VA_ARGS__))
    return  VWHU_PERS(v, k0,k1);
}

INLINE(Vwhi,VWHI_PERS) (Vwhi v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VWHI_PERS(V, ...) WHI_ASTV(WHI_PERS(VWHI_ASTM(V),__VA_ARGS__))
    return  VWHI_PERS(v, k0,k1);
}

INLINE(Vwhf,VWHF_PERS) (Vwhf v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VWHF_PERS(V, ...) WHF_ASTV(WHF_PERS(VWHF_ASTM(V),__VA_ARGS__))
    return  VWHF_PERS(v, k0,k1);
}


INLINE(Vdbu,VDBU_PERS)
(
    Vdbu v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VDBU_PERS(...)  DBU_PERS(__VA_ARGS__)
    return  VDBU_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vdbi,VDBI_PERS)
(
    Vdbi v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VDBI_PERS(...)  DBI_PERS(__VA_ARGS__)
    return  VDBI_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vdbc,VDBC_PERS)
(
    Vdbc v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VDBC_PERS(V, ...)  \
DBC_ASTV(DBC_PERS(VDBC_ASTM(V),__VA_ARGS__))
    return  VDBC_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}


INLINE(Vdhu,VDHU_PERS)
(
    Vdhu v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VDHU_PERS(...)  DHU_PERS(__VA_ARGS__)
    return  VDHU_PERS(v, k0,k1,k2,k3);
}

INLINE(Vdhi,VDHI_PERS)
(
    Vdhi v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VDHI_PERS(...)  DHI_PERS(__VA_ARGS__)
    return  VDHI_PERS(v, k0,k1,k2,k3);
}

INLINE(Vdhf,VDHF_PERS)
(
    Vdhf v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VDHF_PERS(...)  DHF_PERS(__VA_ARGS__)
    return  VDHF_PERS(v, k0,k1,k2,k3);
}


INLINE(Vdwu,VDWU_PERS) (Vdwu v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VDWU_PERS(...)  DWU_PERS(__VA_ARGS__)
    return  VDWU_PERS(v, k0,k1);
}

INLINE(Vdwi,VDWI_PERS) (Vdwi v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VDWI_PERS(...)  DWI_PERS(__VA_ARGS__)
    return  VDWI_PERS(v, k0,k1);
}

INLINE(Vdwf,VDWF_PERS) (Vdwf v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VDWF_PERS(...)  DWF_PERS(__VA_ARGS__)
    return  VDWF_PERS(v, k0,k1);
}


INLINE(Vqbu,VQBU_PERS)
(
    Vqbu v,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBU_PERS(...)  QBU_PERS(__VA_ARGS__)
    return  VQBU_PERS(
        v,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqbi,VQBI_PERS)
(
    Vqbi v,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBI_PERS(...)   QBI_PERS(__VA_ARGS__)
    return  VQBI_PERS(
        v,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqbc,VQBC_PERS)
(
    Vqbc v,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBC_PERS(V, ...)  \
    QBC_ASTV(QBC_PERS(VQBC_ASTM(V),__VA_ARGS__))
    return  VQBC_PERS(
        v,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqhu,VQHU_PERS)
(
    Vqhu v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VQHU_PERS(...)  QHU_PERS(__VA_ARGS__)
    return  VQHU_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vqhi,VQHI_PERS)
(
    Vqhi v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VQHI_PERS(...)  QHI_PERS(__VA_ARGS__)
    return  VQHI_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vqhf,VQHF_PERS)
(
    Vqhf v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VQHF_PERS(...)  QHF_PERS(__VA_ARGS__)
    return  VQHF_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}


INLINE(Vqwu,VQWU_PERS)
(
    Vqwu v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VQWU_PERS(...)   QWU_PERS(__VA_ARGS__)
    return  VQWU_PERS(v, k0,k1,k2,k3);
}

INLINE(Vqwi,VQWI_PERS)
(
    Vqwi v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VQWI_PERS(...)   QWI_PERS(__VA_ARGS__)
    return  VQWI_PERS(v, k0,k1,k2,k3);
}

INLINE(Vqwf,VQWF_PERS)
(
    Vqwf v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VQWF_PERS(...)   QWF_PERS(__VA_ARGS__)
    return  VQWF_PERS(v, k0,k1,k2,k3);
}

INLINE(Vqdu,VQDU_PERS) (Vqdu v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VQDU_PERS(...)   QDU_PERS(__VA_ARGS__)
    return  VQDU_PERS(v, k0,k1);
}

INLINE(Vqdi,VQDI_PERS) (Vqdi v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VQDI_PERS(...)   QDI_PERS(__VA_ARGS__)
    return  VQDI_PERS(v, k0,k1);
}

INLINE(Vqdf,VQDF_PERS) (Vqdf v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VQDF_PERS(...)   QDF_PERS(__VA_ARGS__)
    return  VQDF_PERS(v, k0,k1);
}

#if 0 // ice cream cone
#endif // ice cream cone

#if 0 // _LEAVE_ARM_PERS_
}
#endif


#if 0 // _ENTER_ARM_BLNM
{
#endif

INLINE(Vwbu,VWBU_BLNM)
(
    Vwbu a, Vwbu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     WBU_BLNM(A, B, K0, K1, K2, K3)          \
vget_lane_f32(                                      \
    vreinterpret_f32_u8(                            \
        vtbl1_u8(                                   \
            vreinterpret_u8_f32(                    \
                vset_lane_f32(                      \
                    B,                              \
                    vdup_n_f32(A),                  \
                    1                               \
                )                                   \
            ),                                      \
            vcreate_u8(                             \
                0ull                                \
                |   (K0?0x00000004u:0x00000000u)    \
                |   (K1?0x00000500u:0x00000100u)    \
                |   (K2?0x00060000u:0x00020000u)    \
                |   (K3?0x07000000u:0x03000000u)    \
            )                                       \
        )                                           \
    ),                                              \
    0                                               \
)

#define     VWBU_BLNM(A, B, ...) \
WBU_ASTV(WBU_BLNM(VWBU_ASTM(A), VWBU_ASTM(B), __VA_ARGS__))

    return  VWBU_BLNM(a, b, k0, k1, k2, k3);
}

INLINE(Vwbi,VWBI_BLNM)
(
    Vwbi a, Vwbi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VWBI_BLNM(A, B, ...) \
WBI_ASTV(WBU_BLNM(VWBI_ASTM(A),VWBI_ASTM(B),__VA_ARGS__))

    return  VWBI_BLNM(a, b, k0, k1, k2, k3);
}

INLINE(Vwbc,VWBC_BLNM)
(
    Vwbc a, Vwbc b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VWBC_BLNM(A, B, ...) \
WBC_ASTV(WBU_BLNM(VWBC_ASTM(A),VWBC_ASTM(B),__VA_ARGS__))

    return  VWBC_BLNM(a, b, k0, k1, k2, k3);
}


INLINE(Vwhu,VWHU_BLNM)
(
    Vwhu a, Vwhu b,
    Rc(0, 1) k0, Rc(0, 1) k1
)
{
#define     WHU_BLNM(A, B, K0, K1)                  \
vget_lane_f32(                                      \
    vreinterpret_f32_u8(                            \
        vtbl1_u8(                                   \
            vreinterpret_u8_f32(                    \
                vset_lane_f32(                      \
                    B,                              \
                    vdup_n_f32(A),                  \
                    1                               \
                )                                   \
            ),                                      \
            vcreate_u8(                             \
                0ull                                \
                |   (K0?0x00000504u:0x00000100u)    \
                |   (K1?0x07060000u:0x03020000u)    \
            )                                       \
        )                                           \
    ),                                              \
    0                                               \
)

#define     VWHU_BLNM(A, B, ...) \
WHU_ASTV(WHU_BLNM(VWHU_ASTM(A),VWHU_ASTM(B),__VA_ARGS__))

    return  VWHU_BLNM(a, b, k0, k1);
}

INLINE(Vwhi,VWHI_BLNM)
(
    Vwhi a, Vwhi b,
    Rc(0, 1) k0, Rc(0, 1) k1
)
{
#define     VWHI_BLNM(A, B, ...) \
WHI_ASTV(WHU_BLNM(VWHI_ASTM(A),VWHI_ASTM(B),__VA_ARGS__))

    return  VWHI_BLNM(a, b, k0, k1);
}

INLINE(Vwhf,VWHF_BLNM)
(
    Vwhf a, Vwhf b,
    Rc(0, 1) k0, Rc(0, 1) k1
)
{
#define     VWHF_BLNM(A, B, ...) \
WHF_ASTV(WHU_BLNM(VWHF_ASTM(A),VWHF_ASTM(B),__VA_ARGS__))

    return  VWHF_BLNM(a, b, k0, k1);
}


INLINE(Vdbu,VDBU_BLNM)
(
    Vdbu a, Vdbu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VDBU_BLND(                  \
    A, B,                               \
    K0, K1, K2, K3,  K4, K5, K6, K7     \
)                                       \
vqtbl1_u8(                              \
    vcombine_u8(A, B),                  \
    vreinterpret_u8_u64(                \
        vdup_n_u64(                     \
            ((K0?0x08ull:0x00ull)<<0000)\
        |   ((K1?0x09ull:0x01ull)<<0010)\
        |   ((K2?0x0aull:0x02ull)<<0020)\
        |   ((K3?0x0bull:0x03ull)<<0030)\
        |   ((K4?0x0cull:0x04ull)<<0040)\
        |   ((K5?0x0dull:0x05ull)<<0050)\
        |   ((K6?0x0eull:0x06ull)<<0060)\
        |   ((K7?0x0full:0x07ull)<<0070)\
        )                               \
    )                                   \
)
    return  VDBU_BLND(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7
    );
}

INLINE(Vdbi,VDBI_BLNM)
(
    Vdbi a, Vdbi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VDBI_BLND(                  \
    A, B,                               \
    K0, K1, K2, K3, K4, K5, K6, K7      \
)                                       \
vqtbl1_s8(                              \
    vcombine_s8(A, B),                  \
    vreinterpret_u8_u64(                \
        vdup_n_u64(                     \
            ((K0?0x08ull:0x00ull)<<0000)\
        |   ((K1?0x09ull:0x01ull)<<0010)\
        |   ((K2?0x0aull:0x02ull)<<0020)\
        |   ((K3?0x0bull:0x03ull)<<0030)\
        |   ((K4?0x0cull:0x04ull)<<0040)\
        |   ((K5?0x0dull:0x05ull)<<0050)\
        |   ((K6?0x0eull:0x06ull)<<0060)\
        |   ((K7?0x0full:0x07ull)<<0070)\
        )                               \
    )                                   \
)
    return  VDBI_BLND(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7
    );
}

INLINE(Vdbc,VDBC_BLNM)
(
    Vdbc a, Vdbc b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VDBC_BLND(A, B, ...) \
VDBU_ASBC(VDBU_BLNM(VDBC_ASBU(A),VDBC_ASBU(B),__VA_ARGS__))

    return  VDBC_BLND(a, b, k0, k1, k2, k3, k4, k5, k6, k7);
}


INLINE(Vdhu,VDHU_BLNM)
(
    Vdhu a, Vdhu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VDHU_BLND(A, B,  K0, K1, K2, K3)    \
vreinterpret_u16_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_u16(                   \
            vcombine_u16(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0908ull:0x0100ull)<<0000)\
            |   ((K1?0x0b0aull:0x0302ull)<<0020)\
            |   ((K2?0x0d0cull:0x0504ull)<<0040)\
            |   ((K3?0x0f0eull:0x0706ull)<<0060)\
            )                                   \
        )                                       \
    )                                           \
)

    return  VDHU_BLND(a, b, k0, k1, k2, k3);
}

INLINE(Vdhi,VDHI_BLNM)
(
    Vdhi a, Vdhi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VDHI_BLND(A, B,  K0, K1, K2, K3)    \
vreinterpret_s16_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_s16(                   \
            vcombine_s16(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0908ull:0x0100ull)<<0000)\
            |   ((K1?0x0b0aull:0x0302ull)<<0020)\
            |   ((K2?0x0d0cull:0x0504ull)<<0040)\
            |   ((K3?0x0f0eull:0x0706ull)<<0060)\
            )                                   \
        )                                       \
    )                                           \
)

    return  VDHI_BLND(a, b, k0, k1, k2, k3);
}

INLINE(Vdhf,VDHF_BLNM)
(
    Vdhf a, Vdhf b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VDHF_BLND(A, B,  K0, K1, K2, K3)    \
vreinterpret_f16_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_f16(                   \
            vcombine_f16(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0908ull:0x0100ull)<<0000)\
            |   ((K1?0x0b0aull:0x0302ull)<<0020)\
            |   ((K2?0x0d0cull:0x0504ull)<<0040)\
            |   ((K3?0x0f0eull:0x0706ull)<<0060)\
            )                                   \
        )                                       \
    )                                           \
)
    return  VDHF_BLND(a, b, k0, k1, k2, k3);
}



INLINE(Vdwu,VDWU_BLNM) (Vdwu a, Vdwu b, Rc(0,1) k0, Rc(0,1) k1)
{
#define     VDWU_BLND(A, B, K0, K1)             \
vreinterpret_u32_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_u32(                   \
            vcombine_u32(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0b0a0908ull:0x03020100ull)<<0000)\
            |   ((K1?0x0f0e0d0cull:0x07060504ull)<<0040)\
            )                                   \
        )                                       \
    )                                           \
)

    return  VDWU_BLND(a, b, k0, k1);
}

INLINE(Vdwi,VDWI_BLNM) (Vdwi a, Vdwi b, Rc(0,1) k0, Rc(0,1) k1)
{
#define     VDWI_BLND(A, B, K0, K1)             \
vreinterpret_s32_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_s32(                   \
            vcombine_s32(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0b0a0908ull:0x03020100ull)<<0000)\
            |   ((K1?0x0f0e0d0cull:0x07060504ull)<<0040)\
            )                                   \
        )                                       \
    )                                           \
)

    return  VDWI_BLND(a, b, k0, k1);
}

INLINE(Vdwf,VDWF_BLNM) (Vdwf a, Vdwf b, Rc(0,1) k0, Rc(0,1) k1)
{
#define     VDWF_BLND(A, B, K0, K1)             \
vreinterpret_f32_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_f32(                   \
            vcombine_f32(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0b0a0908ull:0x03020100ull)<<0000)\
            |   ((K1?0x0f0e0d0cull:0x07060504ull)<<0040)\
            )                                   \
        )                                       \
    )                                           \
)

    return  VDWI_BLNM(a, b, k0, k1);
}


INLINE(Vqbu,VQBU_BLNM)
(
    Vqbu a, Vqbu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7,
    Rc(0, 1) k8, Rc(0, 1) k9, Rc(0, 1) k10,Rc(0, 1) k11,
    Rc(0, 1) k12,Rc(0, 1) k13,Rc(0, 1) k14,Rc(0, 1) k15
)
{
#define     VQBU_BLNQ(                          \
    A, B,                                       \
    K0, K1, K2, K3, K4, K5, K6, K7,             \
    K8, K9, K10,K11,K12,K13,K14,K15             \
) \
(                                               \
    vqtbx1q_u8(                                 \
        A,                                      \
        B,                                      \
        vreinterpretq_u8_u64(                   \
            vcombine_u64(                       \
                vdup_n_u64(                     \
                    ((K0 ?0x00ull:0xffull)<<000)\
                |   ((K1 ?0x01ull:0xffull)<<010)\
                |   ((K2 ?0x02ull:0xffull)<<020)\
                |   ((K3 ?0x03ull:0xffull)<<030)\
                |   ((K4 ?0x04ull:0xffull)<<040)\
                |   ((K5 ?0x05ull:0xffull)<<050)\
                |   ((K6 ?0x06ull:0xffull)<<060)\
                |   ((K7 ?0x07ull:0xffull)<<070)\
                ),                              \
                vdup_n_u64(                     \
                    ((K8 ?0x08ull:0xffull)<<000)\
                |   ((K9 ?0x09ull:0xffull)<<010)\
                |   ((K10?0x0aull:0xffull)<<020)\
                |   ((K11?0x0bull:0xffull)<<030)\
                |   ((K12?0x0cull:0xffull)<<040)\
                |   ((K13?0x0dull:0xffull)<<050)\
                |   ((K14?0x0eull:0xffull)<<060)\
                |   ((K15?0x0full:0xffull)<<070)\
                )                               \
            )                                   \
        )                                       \
    )                                           \
)

    return  VQBU_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7,
        k8, k9, k10,k11,k12,k13,k14,k15
    );
}

INLINE(Vqbi,VQBI_BLNM)
(
    Vqbi a, Vqbi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7,
    Rc(0, 1) k8, Rc(0, 1) k9, Rc(0, 1) k10,Rc(0, 1) k11,
    Rc(0, 1) k12,Rc(0, 1) k13,Rc(0, 1) k14,Rc(0, 1) k15
)
{
#define     VQBI_BLNQ(                          \
    A, B,                                       \
    K0, K1, K2, K3, K4, K5, K6, K7,             \
    K8, K9, K10,K11,K12,K13,K14,K15             \
)                                               \
(                                               \
    vqtbx1q_u8(                                 \
        A,                                      \
        B,                                      \
        vreinterpretq_u8_u64(                   \
            vcombine_u64(                       \
                vdup_n_u64(                     \
                    ((K0 ?0x00ull:0xffull)<<000)\
                |   ((K1 ?0x01ull:0xffull)<<010)\
                |   ((K2 ?0x02ull:0xffull)<<020)\
                |   ((K3 ?0x03ull:0xffull)<<030)\
                |   ((K4 ?0x04ull:0xffull)<<040)\
                |   ((K5 ?0x05ull:0xffull)<<050)\
                |   ((K6 ?0x06ull:0xffull)<<060)\
                |   ((K7 ?0x07ull:0xffull)<<070)\
                ),                              \
                vdup_n_u64(                     \
                    ((K8 ?0x08ull:0xffull)<<000)\
                |   ((K9 ?0x09ull:0xffull)<<010)\
                |   ((K10?0x0aull:0xffull)<<020)\
                |   ((K11?0x0bull:0xffull)<<030)\
                |   ((K12?0x0cull:0xffull)<<040)\
                |   ((K13?0x0dull:0xffull)<<050)\
                |   ((K14?0x0eull:0xffull)<<060)\
                |   ((K15?0x0full:0xffull)<<070)\
                )                               \
            )                                   \
        )                                       \
    )                                           \
)

    return  VQBI_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7,
        k8, k9, k10,k11,k12,k13,k14,k15
    );
}

INLINE(Vqbc,VQBC_BLNM)
(
    Vqbc a, Vqbc b,
    Rc(0,1) k0, Rc(0,1) k1, Rc(0,1) k2, Rc(0,1) k3,
    Rc(0,1) k4, Rc(0,1) k5, Rc(0,1) k6, Rc(0,1) k7,
    Rc(0,1) k8, Rc(0,1) k9, Rc(0,1) k10,Rc(0,1) k11,
    Rc(0,1) k12,Rc(0,1) k13,Rc(0,1) k14,Rc(0,1) k15
)
{
#define     VQBC_BLNQ(A, B, ...) \
VQBU_ASBC(VQBU_BLNM(VQBC_ASBU(A),VQBC_ASBU(B),__VA_ARGS__))

    return  VQBC_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7,
        k8, k9, k10,k11,k12,k13,k14,k15
    );
}


INLINE(Vqhu,VQHU_BLNM)
(
    Vqhu a, Vqhu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VQHU_BLNQ(                              \
    A, B,                                           \
    K0, K1, K2, K3, K4, K5, K6, K7                  \
)                                                   \
vreinterpretq_u16_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_u16(A),                    \
        vreinterpretq_u8_u16(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x0100ull:0xffffull)<<000) \
                |   ((K1?0x0302ull:0xffffull)<<020) \
                |   ((K2?0x0504ull:0xffffull)<<040) \
                |   ((K3?0x0706ull:0xffffull)<<060) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K4?0x0908ull:0xffffull)<<000) \
                |   ((K5?0x0b0aull:0xffffull)<<020) \
                |   ((K6?0x0d0cull:0xffffull)<<040) \
                |   ((K7?0x0f0eull:0xffffull)<<060) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQHU_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7
    );
}

INLINE(Vqhi,VQHI_BLNM)
(
    Vqhi a, Vqhi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VQHI_BLNQ(                              \
    A, B,                                           \
    K0, K1, K2, K3, K4, K5, K6, K7                  \
)                                                   \
vreinterpretq_s16_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_s16(A),                    \
        vreinterpretq_u8_s16(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x0100ull:0xffffull)<<000) \
                |   ((K1?0x0302ull:0xffffull)<<020) \
                |   ((K2?0x0504ull:0xffffull)<<040) \
                |   ((K3?0x0706ull:0xffffull)<<060) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K4?0x0908ull:0xffffull)<<000) \
                |   ((K5?0x0b0aull:0xffffull)<<020) \
                |   ((K6?0x0d0cull:0xffffull)<<040) \
                |   ((K7?0x0f0eull:0xffffull)<<060) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQHI_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7
    );
}

INLINE(Vqhf,VQHF_BLNM)
(
    Vqhf a, Vqhf b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VQHF_BLNQ(                              \
    A, B,                                           \
    K0, K1, K2, K3, K4, K5, K6, K7                  \
)                                                   \
vreinterpretq_f16_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_f16(A),                    \
        vreinterpretq_u8_f16(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x0100ull:0xffffull)<<000) \
                |   ((K1?0x0302ull:0xffffull)<<020) \
                |   ((K2?0x0504ull:0xffffull)<<040) \
                |   ((K3?0x0706ull:0xffffull)<<060) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K4?0x0908ull:0xffffull)<<000) \
                |   ((K5?0x0b0aull:0xffffull)<<020) \
                |   ((K6?0x0d0cull:0xffffull)<<040) \
                |   ((K7?0x0f0eull:0xffffull)<<060) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQHF_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7
    );
}


INLINE(Vqwu,VQWU_BLNM)
(
    Vqwu a, Vqwu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VQWU_BLNQ(A, B, K0, K1, K2, K3)         \
vreinterpretq_u32_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_u32(A),                    \
        vreinterpretq_u8_u32(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x03020100ull:0xffffffffull)<<000) \
                |   ((K1?0x07060504ull:0xffffffffull)<<040) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K2?0x0b0a0908ull:0xffffffffull)<<000) \
                |   ((K3?0x0f0e0d0cull:0xffffffffull)<<040) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQWU_BLNQ(a, b, k0, k1, k2, k3);
}

INLINE(Vqwi,VQWI_BLNM)
(
    Vqwi a, Vqwi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VQWI_BLNQ(A, B, K0, K1, K2, K3)         \
vreinterpretq_s32_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_s32(A),                    \
        vreinterpretq_u8_s32(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x03020100ull:0xffffffffull)<<000) \
                |   ((K1?0x07060504ull:0xffffffffull)<<040) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K2?0x0b0a0908ull:0xffffffffull)<<000) \
                |   ((K3?0x0f0e0d0cull:0xffffffffull)<<040) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQWI_BLNQ(a, b, k0, k1, k2, k3);
}

INLINE(Vqwf,VQWF_BLNM)
(
    Vqwf a, Vqwf b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VQWF_BLNQ(A, B, K0, K1, K2, K3)         \
vreinterpretq_f32_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_f32(A),                    \
        vreinterpretq_u8_f32(B),                    \
        vreinterpretq_u8_f64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x03020100ull:0xffffffffull)<<000) \
                |   ((K1?0x07060504ull:0xffffffffull)<<040) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K2?0x0b0a0908ull:0xffffffffull)<<000) \
                |   ((K3?0x0f0e0d0cull:0xffffffffull)<<040) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQWF_BLNQ(a, b, k0, k1, k2, k3);
}


INLINE(Vqdu,VQDU_BLNM) (Vqdu a, Vqdu b, Rc(0, 1) k0, Rc(0, 1) k1)
{
#define     VQDU_BLNQ(A, B, K0, K1)                 \
vreinterpretq_u64_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_u64(A),                    \
        vreinterpretq_u8_u64(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    K0                              \
                    ?   0x0706050403020100ull       \
                    :   0xffffffffffffffffull       \
                ),                                  \
                vdup_n_u64(                         \
                    K1                              \
                    ?   0x0f0e0d0c0b0a0908ull       \
                    :   0xffffffffffffffffull       \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQDU_BLNQ(a, b, k0, k1);
}

INLINE(Vqdi,VQDI_BLNM) (Vqdi a, Vqdi b, Rc(0, 1) k0, Rc(0, 1) k1)
{
#define     VQDI_BLNQ(A, B, K0, K1)                 \
vreinterpretq_s64_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_s64(A),                    \
        vreinterpretq_u8_s64(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    K0                              \
                    ?   0x0706050403020100ull       \
                    :   0xffffffffffffffffull       \
                ),                                  \
                vdup_n_u64(                         \
                    K1                              \
                    ?   0x0f0e0d0c0b0a0908ull       \
                    :   0xffffffffffffffffull       \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQDI_BLNQ(a, b, k0, k1);
}

INLINE(Vqdf,VQDF_BLNM) (Vqdf a, Vqdf b, Rc(0, 1) k0, Rc(0, 1) k1)
{
#define     VQDF_BLNQ(A, B, K0, K1)                 \
vreinterpretq_f64_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_f64(A),                    \
        vreinterpretq_u8_f64(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    K0                              \
                    ?   0x0706050403020100ull       \
                    :   0xffffffffffffffffull       \
                ),                                  \
                vdup_n_u64(                         \
                    K1                              \
                    ?   0x0f0e0d0c0b0a0908ull       \
                    :   0xffffffffffffffffull       \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQDF_BLNQ(a, b, k0, k1);
}

#if 0 // _LEAVE_ARM_BLNM
}
#endif

#if 0 // _ENTER_ARM_MANT
{
#endif

INLINE(uint16_t,manthf) (flt16_t x)
{
    union {
        flt16_t F;
        struct {uint16_t Mant:10, Expo:5, Sign:1, :0;};
    } v = {x};
    return  v.Mant;
}

INLINE(uint32_t,mantwf) (float x)
{
    union {
        float F;
        struct {uint32_t Mant:23, Expo:8, Sign:1, :0;};
    } v = {x};
    return  v.Mant;
}

INLINE(uint64_t,mantdf) (double x)
{
    union {
        double F;
        struct {uint64_t Mant:52, Expo:11, Sign:1, :0;};
    } v = {x};
    return  v.Mant;
}

INLINE(Vdhu,mantdhf) (Vdhf x)
{
    return  vand_u16(
        vreinterpret_u16_f16(x),
        vdup_n_u16(0x3ff)
    );
}

INLINE(Vdwu,mantdwf) (Vdwf x)
{
    return  vand_u32(
        vreinterpret_u32_f32(x),
        vdup_n_u32(0x7fffff)
    );
}

INLINE(Vddu,mantddf) (Vddf x)
{
    return  vand_u64(
        vreinterpret_u64_f64(x),
        vdup_n_u64(0xfffffffffffffULL)
    );
}

INLINE(Vqhu,mantqhf) (Vqhf x)
{
    return  vandq_u16(
        vreinterpretq_u16_f16(x),
        vdupq_n_u16(0x3ff)
    );
}

INLINE(Vqwu,mantqwf) (Vqwf x)
{
    return  vandq_u32(
        vreinterpretq_u32_f32(x),
        vdupq_n_u32(0x7fffff)
    );
}

INLINE(Vqdu,mantqdf) (Vqdf x)
{
    return  vandq_u64(
        vreinterpretq_u64_f64(x),
        vdupq_n_u64(0xfffffffffffffULL)
    );
}

#if 0 // _LEAVE_ARM_MANT
}
#endif

#if 0 // _ENTER_ARM_EXPO
{
#endif

INLINE(uint16_t,expohf) (flt16_t x)
{
    union {
        flt16_t F;
        struct {uint16_t Mant:10, Expo:5, Sign:1, :0;};
    } v = {x};
    return  v.Expo;
}

INLINE(uint32_t,expowf) (float x)
{
    union {
        float F;
        struct {uint32_t Mant:23, Expo:8, Sign:1, :0;};
    } v = {x};
    return  v.Expo;
}

INLINE(uint64_t,expodf) (double x)
{
    union {
        double F;
        struct {uint64_t Mant:52, Expo:11, Sign:1, :0;};
    } v = {x};
    return  v.Expo;
}

INLINE(Vdhu,expodhf) (Vdhf x)
{
#define     expodhf(X)          \
vand_u16(                       \
    vdup_n_u16(0x1f),           \
    vshr_n_u16(                 \
        vreinterpret_u16_f16(X),\
        10                      \
    )                           \
)

    return  expodhf(x);
}

INLINE(Vdwu,expodwf) (Vdwf x)
{
#define     expodwf(X)          \
vand_u32(                       \
    vdup_n_u32(0xff),           \
    vshr_n_u32(                 \
        vreinterpret_u32_f32(X),\
        23                      \
    )                           \
)

    return  expodwf(x);
}

INLINE(Vddu,expoddf) (Vddf x)
{
#define     expoddf(X)          \
vand_u64(                       \
    vdup_n_u64(0x7ff),          \
    vshr_n_u64(                 \
        vreinterpret_u64_f64(X),\
        52                      \
    )                           \
)

    return  expoddf(x);
}


INLINE(Vqhu,expoqhf) (Vqhf x)
{
#define     expoqhf(X)              \
vandq_u16(                          \
    vdupq_n_u16(0x1f),              \
    vshrq_n_u16(                    \
        vreinterpretq_u16_f16(X),   \
        10                          \
    )                               \
)

    return  expoqhf(x);
}

INLINE(Vqwu,expoqwf) (Vqwf x)
{
#define     expoqwf(X)              \
vandq_u32(                          \
    vdupq_n_u32(0xff),              \
    vshrq_n_u32(                    \
        vreinterpretq_u32_f32(X),   \
        23                          \
    )                               \
)

    return  expoqwf(x);
}

INLINE(Vqdu,expoqdf) (Vqdf x)
{
#define     expoqdf(X)              \
vandq_u64(                          \
    vdupq_n_u64(0x7ff),             \
    vshrq_n_u64(                    \
        vreinterpretq_u64_f64(X),   \
        52                          \
    )                               \
)

    return  expoqdf(x);
}


#if 0 // _LEAVE_ARM_EXPO
}
#endif

#if 0 // _ENTER_ARM_CVYU
{
#endif

INLINE(_Bool, FLT16_CVYU) (flt16_t x) 
{
#define     FLT16_CVYU(X) ((_Bool) ((flt16_t) X))
    return  x;
}

INLINE(_Bool, FLT_CVYU) (float x) 
{
#define     FLT_CVYU(X) ((_Bool) ((float) X))
    return  x;
}

INLINE(_Bool, DBL_CVYU) (double x) 
{
#define     DBL_CVYU(X) ((_Bool) ((double) X))
    return  x;
}

INLINE(Vwyu,VWBU_CVYU) (Vwbu x)
{
    float       m = VWBU_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   z = vreinterpret_u8_f32(v);
    z = vcgt_u8(z, vdup_n_u8(0));
    uint32x2_t  w = vreinterpret_u32_u8(z);
    uint32x4_t  q = vcombine_u32(w, w);
    uint32x2_t  l = vcreate_u32(0x0000020000000001ULL);
    uint32x2_t  r = vcreate_u32(0x0800000000040000ULL);
    q = vandq_u32(q, vcombine_u32(l, r));
    l = vcreate_u32(0xfffffff900000000ULL);
    r = vcreate_u32(0xffffffebfffffff2ULL);
    uint32x4_t  n = vcombine_u32(l, r);
    q = vshlq_u32(q, vreinterpretq_s32_u32(n));
    l = vget_low_u32(q);    // w0, w1
    r = vget_high_u32(q);   // w2, w3
    w = vorr_u32(l, r);  
    l = vzip1_u32(w, vdup_n_u32(0));
    r = vzip2_u32(w, vdup_n_u32(0));
    w = vorr_u32(l, r);
    v = vreinterpret_f32_u32(w);
    m = vget_lane_f32(v, 0);
    return  WYU_ASTV(m);
}

INLINE(Vwyu,VWBI_CVYU) (Vwbi x)
{
    return  VWBU_CVYU(VWBI_ASTU(x));
}

INLINE(Vwyu,VWBC_CVYU) (Vwbc x)
{
    return  VWBU_CVYU(VWBC_ASTU(x));
}


INLINE(Vwyu,VWHU_CVYU) (Vwhu x)
{
    float       m = VWHU_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    uint16x4_t  l = vreinterpret_u16_f32(v);
    l = vcgt_u16(l, vdup_n_u16(0));
    uint32_t    p = vget_lane_u16(l, 0)&0x00000001;
    uint32_t    q = vget_lane_u16(l, 1)&0x00010000;
    q >>= 15;
    uint32x2_t  w = vdup_n_u32((p|q));
    v = vreinterpret_f32_u32(w);
    m = vget_lane_f32(v, 0);
    return  WYU_ASTV(m);
}

INLINE(Vwyu,VWHI_CVYU) (Vwhi x)
{
    return  VWHU_CVYU(VWHI_ASTU(x));
}

INLINE(Vwyu,VWHF_CVYU) (Vwhf x)
{
    float       m = VWHF_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
    uint16x4_t  c;
#if defined(SPC_ARM_FP16_SIMD)
    c = vceqz_f16(f);
#else
    uint32x4_t  y = vceqzq_f32(vcvt_f32_f16(f));
    c = vmovn_u32(y);
#endif
    uint32_t    p = (0x00000001&vget_lane_u16(c,0));
    uint32_t    q = (0x00010000&vget_lane_u16(c,1))>>15;
    uint32x2_t  w = vdup_n_u32((p|q));
    v = vreinterpret_f32_u32(w);
    m = vget_lane_f32(v, 0);
    return  WYU_ASTV(m);
}


INLINE(Vwyu,VWWU_CVYU) (Vwwu x)
{
    uint32_t    n = VWWU_ASTV(x);
    return  VWWU_ASYU(UINT32_ASTV( (n ? 1 : 0)));
}

INLINE(Vwyu,VWWI_CVYU) (Vwwi x)
{
    uint32_t    n = VWWI_ASTV(x);
    return  VWWU_ASYU(UINT32_ASTV( (n ? 1 : 0)));
}

INLINE(Vwyu,VWWF_CVYU) (Vwwf x)
{
    uint32_t    n = VWWF_ASTV(x) ? 1 : 0;
    return  VWWU_ASYU(UINT32_ASTV( n ));
}


INLINE(Vdyu,VDBU_CVYU) (Vdbu x)
{
    x = vcgt_u8(x, vdup_n_u8(0));
    x = vand_u8(x, vdup_n_u8(1));
    uint64x1_t  m = vreinterpret_u64_u8(x);
    uint64_t    l, r, t, c=vget_lane_u64(m, 0);
    l = (c>>(000-000)); // r[0] = c[0]
    r = (c>>(010-001)); // r[1] = c[8]
    t = l|r;
    l = (c>>(020-002)); // r[2] = c[16]
    r = (c>>(030-003)); // r[3] = c[24]
    t = l|t|r;
    l = (c>>(040-004)); // r[4] = c[32]
    r = (c>>(050-005)); // r[5] = c[40]
    t = l|t|r;
    l = (c>>(060-006)); // r[6] = c[48]
    r = (c>>(070-007)); // r[7] = c[56]
    t = 0b11111111&(l|t|r);
    return  ((Vdyu){vdup_n_u64(t)});
}

INLINE(Vdyu,VDBI_CVYU) (Vdbi x)
{
    return  VDBU_CVYU(VDBI_ASTU(x));
}

INLINE(Vdyu,VDBC_CVYU) (Vdbc x)
{
    return  VDBU_CVYU(VDBC_ASTU(x));
}


INLINE(Vdyu,VDHU_CVYU) (Vdhu x)
{
    x = vcgt_u16(x, vdup_n_u16(0));
    x = vand_u16(x, vdup_n_u16(1));
    uint64x1_t  m = vreinterpret_u64_u8(x);
    uint64_t    l, r, t, c=vget_lane_u64(m, 0);
    l = (c>>(000-000)); // r[0] = c[0]
    r = (c>>(020-001)); // r[1] = c[16]
    t = l|r;
    l = (c>>(040-002)); // r[2] = c[32]
    l = (c>>(060-003)); // r[3] = c[48]
    t = 0b1111&(l|t|r);
    return  ((Vdyu){vdup_n_u64(t)});
}

INLINE(Vdyu,VDHI_CVYU) (Vdhi x)
{
    return  VDHU_CVYU(VDHI_ASTU(x));
}

INLINE(Vdyu,VDHF_CVYU) (Vdhf x)
{
#if defined(SPC_ARM_FP16)
    uint16x4_t d = vceqz_f16(x);
#else
    uint32x4_t q = vceqzq_f32(vcvt_f32_f16(x));
    uint16x4_t d = vmovn_u32(q);
#endif
    d = vmvn_u16(d);
    d = vand_u16(d, vdup_n_u16(1));
    uint64_t    l, t, r;
    uint64_t    c = vget_lane_u64(vreinterpret_u64_u16(d), 0);
    l = (c>>(000-000)); // r[0] = c[0]
    r = (c>>(020-001)); // r[1] = c[16]
    t = l|r;
    l = (c>>(040-002)); // r[2] = c[32]
    l = (c>>(060-003)); // r[3] = c[48]
    t = 0b1111&(l|t|r);
    return  ((Vdyu){vdup_n_u64(t)});
}


INLINE(Vdyu,VDWU_CVYU) (Vdwu x)
{
    x = vcgt_u32(x, vdup_n_u32(0));
    x = vand_u32(x, vdup_n_u32(1));
    uint64x1_t  l = vreinterpret_u64_u32(x);
    uint64x1_t  r = vshr_n_u64(l, 31);
    l = vorr_u64(l, r);
    l = vand_u64(l, vdup_n_u64(0b11));
    return  ((Vdyu){l});
}

INLINE(Vdyu,VDWI_CVYU) (Vdwi x)
{
    return  VDWU_CVYU(VDWI_ASTU(x));
}

INLINE(Vdyu,VDWF_CVYU) (Vdwf x)
{
    uint32x2_t  z = vceqz_f32(x);
    z = vmvn_u32(z);
    z = vand_u32(z, vdup_n_u32(1));
    uint64x1_t  l = vreinterpret_u64_u32(z);
    uint64x1_t  r = vshr_n_u64(l, 31);
    l = vorr_u64(l, r);
    l = vand_u64(l, vdup_n_u64(0b11));
    return  ((Vdyu){l});
}


INLINE(Vdyu,VDDU_CVYU) (Vddu x)
{
    x = vcgt_u64(x, vdup_n_u64(0));
    x = vand_u64(x, vdup_n_u64(1));
    return  ((Vdyu){x});
}

INLINE(Vdyu,VDDI_CVYU) (Vddi x)
{
    return VDDU_CVYU(vreinterpret_u64_s64(x));
}

INLINE(Vdyu,VDDF_CVYU) (Vddf x)
{
    uint64x1_t d = vceqz_f64(x);
    uint32x2_t w = vreinterpret_u32_u64(d);
    w = vmvn_u32(w);
    d = vreinterpret_u64_u32(w);
    d = vand_u64(d, vdup_n_u64(1));
    return  ((Vdyu){d});
}

INLINE(Vqyu,VQBU_CVYU) (Vqbu x)
{
    x = vcgtq_u8(x, vdupq_n_u8(0));
    x = vandq_u8(x, vdupq_n_u8(1));
    uint8x8_t   b = vget_low_u8(x);
    uint64x1_t  d = vreinterpret_u64_u8(b);
    uint64_t    l, r, t, y, c=vget_lane_u64(d, 0);
    l = (c>>(000-000)); // r[0] = c[0]
    r = (c>>(010-001)); // r[1] = c[8]
    t = (l|r);
    l = (c>>(020-002)); // r[2] = c[16]
    r = (c>>(030-003)); // r[3] = c[24]
    t = (l|t|r);
    l = (c>>(040-004)); // r[4] = c[32]
    r = (c>>(050-005)); // r[5] = c[40]
    t = (l|t|r);
    l = (c>>(060-006)); // r[6] = c[48]
    r = (c>>(070-007)); // r[7] = c[56]
    t = 0b0000000011111111&(l|t|r);
    b = vget_high_u8(x);
    d = vreinterpret_u64_u8(b);
    c = vget_lane_u64(d, 0);
    l = (c>>(000-000)); // r[8] = Hi[0]
    r = (c>>(010-001)); // r[9] = Hi[8]
    y = l|r;
    l = (c>>(020-002)); // r[10] = Hi[16]
    r = (c>>(030-003)); // r[11] = Hi[24]
    y = l|y|r;
    l = (c>>(040-004)); // r[12] = Hi[32]
    r = (c>>(050-005)); // r[13] = Hi[40]
    y = l|y|r;
    l = (c>>(060-006)); // r[14] = Hi[48]
    r = (c>>(070-007)); // r[15] = Hi[56]
    y = t|(0b1111111100000000&((l|y|r)<<8));
    d = vdup_n_u64(y);
    uint64x2_t m = vcombine_u64(d, vdup_n_u64(0));
    return  ((Vqyu){m});
}

INLINE(Vqyu,VQBI_CVYU) (Vqbi x)
{
    return  VQBU_CVYU(VQBI_ASTU(x));
}

INLINE(Vqyu,VQBC_CVYU) (Vqbc x)
{
    return  VQBU_CVYU(VQBC_ASTU(x));
}


INLINE(uint64x2_t,MY_CVYUQHU) (uint16x8_t x)
{
    x = vandq_u16(x, vdupq_n_u16(1));
    uint16x4_t  z = vget_low_u16(x);
    uint64x1_t  d = vreinterpret_u64_u16(z);
    uint64_t    l, r, t, y, c=vget_lane_u64(d, 0);
    l = (c>>(000-000)); // r[0] = c[0]
    r = (c>>(020-001)); // r[1] = c[16]
    t = (l|r);
    l = (c>>(040-002)); // r[2] = c[32]
    r = (c>>(060-003)); // r[3] = c[48]
    t = 0b00001111&(l|t|r);
    z = vget_high_u16(x);
    d = vreinterpret_u64_u16(z);
    c = vget_lane_u64(d, 0);
    l = (c>>(000-000)); // r[0] = c[0]
    r = (c>>(020-001)); // r[1] = c[16]
    y = (l|r);
    l = (c>>(040-002)); // r[2] = c[32]
    r = (c>>(060-003)); // r[3] = c[48]
    y = t|(0b11110000&((l|y|r)<<4));
    return  vcombine_u64(vdup_n_u64(y), vdup_n_u64(0));
}

INLINE(Vqyu,VQHU_CVYU) (Vqhu x)
{
    x = vcgtq_u16(x, vdupq_n_u16(0));
    uint64x2_t c = MY_CVYUQHU(x);
    return ((Vqyu){c});
}

INLINE(Vqyu,VQHI_CVYU) (Vqhi x)
{
    return  VQHU_CVYU(VQHI_ASTU(x));
}

INLINE(Vqyu,VQHF_CVYU) (Vqhf x)
{
#if defined(SPC_ARM_FP16)
    uint16x8_t  m = vceqzq_f16(x);
#else
    uint32x4_t  lo = vceqzq_f32(vcvt_f32_f16(vget_low_f16(x)));
    uint32x4_t  hi = vceqzq_f32(vcvt_high_f32_f16(x));
    uint16x8_t  m = vcombine_u16(vmovn_u32(lo), vmovn_u32(hi));
#endif
    m = vmvnq_u16(m);
    uint64x2_t c = MY_CVYUQHU(m);
    return ((Vqyu){c});
}


INLINE(uint64x2_t,MY_CVYUQWU) (uint32x4_t x)
{
    uint32x2_t  l = vcreate_u32(0x0000000200000001ULL);
    uint32x2_t  r = vcreate_u32(0x0000000800000002ULL);
    x = vandq_u32(x, vcombine_u64(l, r));
    uint64x1_t  d = vdup_n_u64(
        (
            vgetq_lane_u32(x, 0)
        |   vgetq_lane_u32(x, 1)
        |   vgetq_lane_u32(x, 2)
        |   vgetq_lane_u32(x, 3)
        )
    );
    return  vcombine_u64(d, vdup_n_u64(0));
}

INLINE(Vqyu,VQWU_CVYU) (Vqwu x)
{
    uint32x4_t w = vcgtq_u32(x, vdupq_n_u32(0));
    uint64x2_t d = MY_CVYUQWU(w);
    return  ((Vqyu){d});
}

INLINE(Vqyu,VQWI_CVYU) (Vqwi x)
{
    return  VQWU_CVYU(VQWI_ASTU(x));
}

INLINE(Vqyu,VQWF_CVYU) (Vqwf x)
{
    uint32x4_t  m = vceqzq_f32(x);
    m = vmvnq_u32(m);
    uint64x2_t d = MY_CVYUQWU(m);
    return  ((Vqyu){d});
}

INLINE(uint64x2_t, MY_CVYUQDU) (uint64x2_t x)
{
    uint64x1_t  l = vdup_n_u64(0b01);
    uint64x1_t  r = vdup_n_u64(0b10);
    x = vandq_u64(x, vcombine_u64(l, r));
    l = vget_low_u64(x);
    r = vget_high_u64(x);
    l = vorr_u64(l, r);
    r = vdup_n_u64(0);
    return  vcombine_u64(l, r);
    
}

INLINE(Vqyu,VQDU_CVYU) (Vqdu x)
{
    uint64x2_t c = vcgtq_u64(x, vdupq_n_u64(0));
    c = MY_CVYUQDU(c);
    return  ((Vqyu){c});
}

INLINE(Vqyu,VQDI_CVYU) (Vqdi x)
{
    return  VQDU_CVYU(vreinterpretq_u64_s64(x));
}

INLINE(Vqyu,VQDF_CVYU) (Vqdf x)
{
    uint64x2_t c = vceqzq_f64(x);
    uint32x4_t w = vreinterpretq_u32_u64(c);
    w = vmvnq_u32(w);
    c = vreinterpretq_u64_u32(w);
    c = MY_CVYUQDU(c);
    return  ((Vqyu){c});
}

INLINE(uint64x2_t,QQU_CVYU) (uint64x2_t x)
{
    uint64x1_t l = vget_low_u64(x);
    uint64x1_t r = vget_high_u64(x);
    l = vorr_u64(l, r);
    r = vdup_n_u64(0);
    l = vcgt_u64(l, r);
    l = vand_u64(l, vdup_n_u64(1));
    return vcombine_u64(l, r);
}

INLINE(Vqyu,VQQU_CVYU) (Vqqu x)
{
    return ((Vqyu){QQU_CVYU(x.V0)});
}

INLINE(Vqyu,VQQI_CVYU) (Vqqi x)
{
    return ((Vqyu){QQU_CVYU(vreinterpretq_u64_s64(x.V0))});
}

INLINE(Vqyu,VQQF_CVYU) (Vqqf x)
{
    uint64x1_t l = vdup_n_u64(((_Bool) x.V0));
    uint64x2_t c = vcombine_u64(l, vdup_n_u64(0));
    return ((Vqyu){c});
}


#if 0 // _LEAVE_ARM_CVYU
}
#endif


#if 0 // _ENTER_ARM_CVBU
{
#endif

INLINE(uint8_t,FLT16_CVBU) (flt16_t x) {return x;}
INLINE(uint8_t,  FLT_CVBU)   (float x) {return x;}
INLINE(uint8_t,  DBL_CVBU)  (double x) {return x;}
INLINE(uint8_t, cvbuqf) (QUAD_FTYPE x) {return x;}

INLINE(Vwbu,VWBI_CVBU) (Vwbi x) {return VWBI_ASBU(x);}
INLINE(Vwbu,VWBC_CVBU) (Vwbc x) {return VWBC_ASBU(x);}

INLINE(Vdbu,VDBI_CVBU) (Vdbi x) {return VDBI_ASBU(x);}
INLINE(Vdbu,VDBC_CVBU) (Vdbc x) {return VDBC_ASBU(x);}

INLINE(Vwbu,VDHU_CVBU) (Vdhu x)
{
    uint8x8_t   b = vreinterpret_u8_u16(x);
    b = vuzp1_u8(b, b);
    float32x2_t m = vreinterpret_f32_u8(b);
    return  WBU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwbu,VDHI_CVBU) (Vdhi x)
{
    uint8x8_t   b = vreinterpret_u8_s16(x);
    b = vuzp1_u8(b, b);
    float32x2_t m = vreinterpret_f32_u8(b);
    return  WBU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwbu,VDHF_CVBU) (Vdhf x)
{
#if 0

/*
TODO: remember wtf this is
*/
    uint16x4_t u = vreinterpret_u16_f16(x);
    uint16x4_t m = vand_u16(u, vdup_n_u16(0x03ff));
    uint16x4_t e = vand_u16(u, vdup_n_u16(0x7c00));
    e = vshr_n_u16(e, 10);
    uint16x4_t s = vtst_u16(u, vdup_n_u16(0x8000));
    uint16x4_t l = vcge_u16(e, vdup_n_u16(15));
    uint16x4_t c = vclt_u16(e, vdup_n_u16(23));
    c = vand_u16(l, c);
    e = vsub_u16(
        e,
        vand_u16(c, vdup_n_u16(15))
    );
    e = vshl_u16(vdup_n_u16(1),  e);
    l = vmovn_u32(vshrq_n_u32(vmull_u16(m, e), 10));
    e = vadd_u16(e, l);
    uint8x8_t   b = vreinterpret_u8_u16(e);
    b = vtbl1_u8(b, vcreate_u8(0xffffffff06040200));
    float32x2_t v = vreinterpret_f32_u8(b);
    return  WBU_ASTV(vget_lane_f32(v, 0));
#endif

    float32x4_t f = vcvt_f32_f16(x);
    uint32x4_t  z = vcvtq_u32_f32(f);
    uint8x8_t   t = vqtbl1_u8(
        vreinterpretq_u8_s32(z),
        vcreate_u8(0x0c0804000c080400ULL)
    );
    float32x2_t r = vreinterpret_f32_u8(t);
    float       m = vget_lane_f32(r, 0);
    return  WBU_ASTV(m);
}


INLINE(Vqbu,VQBI_CVBU) (Vqbi x) {return VQBI_ASBU(x);}
INLINE(Vqbu,VQBC_CVBU) (Vqbc x) {return VQBC_ASBU(x);}

INLINE(Vdbu,VQHU_CVBU) (Vqhu x) {return vmovn_u16(x);}
INLINE(Vdbu,VQHI_CVBU) (Vqhi x) {return VDBI_ASBU(vmovn_s16(x));}
INLINE(Vdbu,VQHF_CVBU) (Vqhf x)
{
    uint16x8_t u = vreinterpretq_u16_f16(x);
    uint16x8_t m = vandq_u16(u, vdupq_n_u16(0x03ff));
    uint16x8_t e = vandq_u16(u, vdupq_n_u16(0x7c00));
    e = vshrq_n_u16(e, 10);
    uint16x8_t s = vtstq_u16(u, vdupq_n_u16(0x8000));
    uint16x8_t l = vcgeq_u16(e, vdupq_n_u16(15));
    uint16x8_t c = vcltq_u16(e, vdupq_n_u16(23));
    c = vandq_u16(l, c);
    e = vsubq_u16(
        e,
        vandq_u16(c, vdupq_n_u16(15))
    );
    e = vshlq_u16(vdupq_n_u16(1),  e);
    l = vcombine_u16(
        vmovn_u32(
            vshrq_n_u32(
                vmull_u16(
                    vget_low_u16(m),
                    vget_low_u16(e)
                ),
                10
            )
        ),
        vmovn_u32(
            vshrq_n_u32(
                vmull_u16(
                    vget_high_u16(m),
                    vget_high_u16(e)
                ),
                10
            )
        )
    );
    e = vaddq_u16(e, l);
    return  vmovn_u16(e);
}

INLINE(Vwbu,VQWU_CVBU) (Vqwu x)
{
    uint8x16_t  b = vreinterpretq_u8_u32(x);
    uint64x1_t  m = vdup_n_u64(0xffffffff0c080400ull);
    uint8x8_t   t = vreinterpret_u8_u64(m);
    t = vqtbl1_u8(b, t);
    return  WBU_ASTV(vget_lane_f32(vreinterpret_f32_u8(t), 0));
}

INLINE(Vwbu,VQWI_CVBU) (Vqwi x)
{
    return  VQWU_CVBU(VQWI_ASTU(x));
}

INLINE(Vwbu,VQWF_CVBU) (Vqwf x)
{
    return  VQWU_CVBU(vcvtq_u32_f32(x));
}

#if 0 // _LEAVE_ARM_CVBU
}
#endif

#if 0 // _ENTER_ARM_CVBI
{
#endif

INLINE(int8_t,FLT16_CVBI) (flt16_t x) {return x;}
INLINE(int8_t,  FLT_CVBI)   (float x) {return x;}
INLINE(int8_t,  DBL_CVBI)  (double x) {return x;}
INLINE(int8_t,cvbiqf) (QUAD_FTYPE x) {return x;}

INLINE(Vwbi,VWBU_CVBI) (Vwbu x) {return VWBU_ASBI(x);}

INLINE(Vwbi,VWBC_CVBI) (Vwbc x) {return VWBC_ASBI(x);}

INLINE(Vdbi,VDBU_CVBI) (Vdbu x) {return VDBU_ASBI(x);}
INLINE(Vdbi,VDBC_CVBI) (Vdbc x) {return VDBC_ASBI(x);}

INLINE(Vwbi,VDHU_CVBI) (Vdhu x) {return VWBU_ASBI(VDHU_CVBU(x));}
INLINE(Vwbi,VDHI_CVBI) (Vdhi x) {return VWBU_ASBI(VDHI_CVBU(x));}
INLINE(Vwbi,VDHF_CVBI) (Vdhf x)
{
    float32x4_t f = vcvt_f32_f16(x);
    int32x4_t   z = vcvtq_s32_f32(f);
    uint8x8_t   t = vqtbl1_u8(
        vreinterpretq_u8_s32(z),
        vcreate_u8(0x0c0804000c080400ULL)
    );
    float32x2_t r = vreinterpret_f32_u8(t);
    float       m = vget_lane_f32(r, 0);
    return  WBI_ASTV(m);
}

INLINE(Vqbi,VQBU_CVBI) (Vqbu x) {return VQBU_ASBI(x);}

INLINE(Vqbi,VQBC_CVBI) (Vqbc x) {return VQBC_ASBI(x);}

INLINE(Vdbi,VQHU_CVBI) (Vqhu x) {return vmovn_s16(VQHU_ASHI(x));}
INLINE(Vdbi,VQHI_CVBI) (Vqhi x) {return vmovn_s16(x);}
INLINE(Vdbi,VQHF_CVBI) (Vqhf x) {return VDBU_VOID;}

INLINE(Vwbi,VQWU_CVBI) (Vqwu x) {return VWBU_ASBI(VQWU_CVBU(x));}
INLINE(Vwbi,VQWI_CVBI) (Vqwi x)
{
    uint8x16_t  b = vreinterpretq_u8_s32(x);
    uint64x1_t  m = vdup_n_u64(0xffffffff0c080400ull);
    //                                    3 2 1 0
    uint8x8_t   t = vreinterpret_u8_u64(m);
    t = vqtbl1_u8(b, t);
    return WBI_ASTV(vget_lane_f32(vreinterpret_f32_u8(t), 0));
}

INLINE(Vwbi,VQWF_CVBI) (Vqwf x)
{
    return  VQWI_CVBI(vcvtq_s32_f32(x));
}

#if 0 // _LEAVE_ARM_CVBI
}
#endif

#if 0 // _ENTER_ARM_CVBC
{
#endif

INLINE(Vwbc,VWBU_CVBC) (Vwbu x) {return VWBU_ASBC(x);}
INLINE(Vwbc,VWBI_CVBC) (Vwbi x) {return VWBI_ASBC(x);}

INLINE(Vdbc,VDBU_CVBC) (Vdbu x) {return VDBU_ASBC(x);}
INLINE(Vdbc,VDBI_CVBC) (Vdbi x) {return VDBI_ASBC(x);}

INLINE(Vwbc,VDHU_CVBC) (Vdhu x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VDHU_CVBI(x));
#else
    return  VWBU_ASBC(VDHU_CVBU(x));
#endif
}

INLINE(Vwbc,VDHI_CVBC) (Vdhi x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VDHI_CVBI(x));
#else
    return  VWBU_ASBC(VDHI_CVBU(x));
#endif

}

INLINE(Vwbc,VDHF_CVBC) (Vdhf x)
{
// TODO
    return  VWBC_VOID;
}


INLINE(Vqbc,VQBU_CVBC) (Vqbu x) {return VQBU_ASBC(x);}
INLINE(Vqbc,VQBI_CVBC) (Vqbi x) {return VQBI_ASBC(x);}


INLINE(Vdbc,VQHU_CVBC) (Vqhu x)
{
#if CHAR_MIN
    return  VDBI_ASBC(VQHU_CVBI(x));
#else
    return  VDBU_ASBC(VQHU_CVBU(x));
#endif
}

INLINE(Vdbc,VQHI_CVBC) (Vqhi x)
{
#if CHAR_MIN
    return  VDBI_ASBC(VQHI_CVBI(x));
#else
    return  VDBU_ASBC(VQHI_CVBU(x));
#endif
}

INLINE(Vdbc,VQHF_CVBC) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  VQHI_CVBC(vcvtq_s16_f16(x));
#else
#   if CHAR_MIN
    return  VDBI_ASBC(VQHF_CVBI(x));
#   else
    return  VDBU_ASBC(VQHF_CVBU(x));
#   endif
#endif

}

INLINE(Vwbc,VQWU_CVBC) (Vqwu x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VQWU_CVBI(x));
#else
    return  VWBU_ASBC(VQWU_CVBU(x));
#endif
}


INLINE(Vwbc,VQWI_CVBC) (Vqwi x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VQWU_CVBI(x));
#else
    return  VWBU_ASBC(VQWU_CVBU(x));
#endif
}


INLINE(Vwbc,VQWF_CVBC) (Vqwf x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VQWF_CVBI(x));
#else
    return  VWBU_ASBC(VQWF_CVBU(x));
#endif
}

#if 0 // _LEAVE_ARM_CVBC
}
#endif

#if 0 // _ENTER_ARM_CVBZ
{
#endif

INLINE(uint8_t,  BOOL_CVBZ)  (_Bool x) {return x;}
INLINE(uint8_t, UCHAR_CVBZ)  (uchar x) {return x;}
INLINE(uint8_t, SCHAR_CVBZ)  (schar x) {return vqmovunh_s16(x);}
INLINE(uint8_t,  CHAR_CVBZ)   (char x) {return vqmovunh_s16(x);}
INLINE(uint8_t, USHRT_CVBZ) (ushort x) {return  vqmovnh_u16(x);}
INLINE(uint8_t,  SHRT_CVBZ)  (short x) {return vqmovunh_s16(x);}
INLINE(uint8_t,  UINT_CVBZ)   (uint x)
{
    return  x|vtstd_u64(x, (UINT64_MAX-UINT8_MAX));
}

INLINE(uint8_t,   INT_CVBZ)    (int x)
{
    return  vqmovunh_s16(vqmovns_s32(x));
}

INLINE(uint8_t, ULONG_CVBZ)  (ulong x)
{
    return  x|vtstd_u64(x, (UINT64_MAX-UINT8_MAX));
}

INLINE(uint8_t,  LONG_CVBZ)   (long x)
{
#if DWRD_NLONG == 2
    return  vqmovunh_s16(vqmovns_s32(x));
#else
    uint32_t m = vqmovund_s64(x);
    return  m|vtstd_u64(m, (UINT64_MAX-UINT8_MAX));
#endif
}

INLINE(uint8_t,ULLONG_CVBZ) (ullong x)
{
    return  x|vtstd_u64(x, (UINT64_MAX-UINT8_MAX));
}

INLINE(uint8_t, LLONG_CVBZ)  (llong x)
{
    uint32_t m = vqmovund_s64(x);
    return  m|vtstd_u64(m, (UINT64_MAX-UINT8_MAX));
}

INLINE(uint8_t, FLT16_CVBZ) (flt16_t x)
{
    uint32_t m = vcvts_u32_f32(x);
    return   m|vtstd_u64(m, (UINT64_MAX-UINT8_MAX));
}

INLINE(uint8_t,   FLT_CVBZ)   (float x)
{
#define     FLT_CVBZ    FLT_CVBZ
    uint32_t m = vcvts_u32_f32(x);
    return m|vtstd_u64(m, (UINT64_MAX-UINT8_MAX));
}

INLINE(uint8_t,   DBL_CVBZ) (double x)
{
#define     DBL_CVBZ    DBL_CVBZ
    uint64_t m = vcvtd_u64_f64(x);
    return m|vtstd_u64(m, (UINT64_MAX-UINT8_MAX));
}

INLINE(uint8_t,cvbzqu) (QUAD_UTYPE x)
{
    return (x >= UINT8_MAX) ? UINT8_MAX : x;
}

INLINE(uint8_t,cvbzqi) (QUAD_ITYPE x)
{
    if (x >= UINT8_MAX) return UINT8_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE(uint8_t,cvbzqf) (QUAD_FTYPE x)
{
    if (x >= UINT8_MAX) return UINT8_MAX;
    if (x <= 0) return 0;
    return x;
}


INLINE(float,WBI_CVBZ) (float m)
{
    float32x2_t d = vdup_n_f32(m);
    int16x8_t   q = vmovl_s8(vreinterpret_s8_f32(d));
    d = vreinterpret_f32_u8(vqmovun_s16(q));
    return  vget_lane_f32(d, 0);
}

INLINE(Vwbu,VWBU_CVBZ) (Vwbu x) {return x;}
INLINE(Vwbu,VWBI_CVBZ) (Vwbi x)
{
    return  WBU_ASTV(WBI_CVBZ(VWBI_ASTM(x)));
}

INLINE(Vwbu,VWBC_CVBZ) (Vwbc v)
{
#if CHAR_MIN
    return  WBU_ASTV(WBI_CVBZ(VWBC_ASTM(v)));
#else
    return  VWBC_ASBU(v);
#endif
}


INLINE(Vdbu,VDBU_CVBZ) (Vdbu x) {return x;}
INLINE(Vdbu,VDBI_CVBZ) (Vdbi x) {return vqmovun_s16(vmovl_s8(x));}
INLINE(Vdbu,VDBC_CVBZ) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_CVBZ(VDBC_ASBI(x));
#else
    return  VDBC_ASBU(x);
#endif
}

INLINE(Vwbu,VDHU_CVBZ) (Vdhu x)
{
#define     DHU_CVBZ(V)     \
vget_lane_f32(              \
    vreinterpret_f32_u8(    \
        vqmovn_u16(         \
            vcombine_u16(   \
                V,          \
                VDHU_VOID   \
            )               \
        )                   \
    ),                      \
    0                   \
)

    return WBU_ASTV(DHU_CVBZ(x));
}

INLINE(Vwbu,VDHI_CVBZ) (Vdhi x)
{
#define     DHI_CVBZ(V)     \
vget_lane_f32(              \
    vreinterpret_f32_s8(    \
        vqmovun_s16(        \
            vcombine_s16(   \
                V,          \
                VDHI_VOID   \
            )               \
        )                   \
    ),                      \
    0                   \
)

    return  WBU_ASTV(DHI_CVBZ(x));
}

INLINE(Vwbu,VDHF_CVBZ) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CVBZ(M)             \
vget_lane_f32(                      \
    vreinterpret_f32_u8(            \
        vqmovn_u16(                 \
            vcombine_u16(           \
                vcvt_u16_f16(M),    \
                vdup_n_u16(0)       \
            )                       \
        )                           \
    ),                              \
    0                           \
)
#   define  VDHF_CVBZ(V)        WBU_ASTV(DHF_CVBZ(VDHF_ASTM(V)))
    return  VDHF_CVBZ(x);
#else
    float32x4_t qwf = vcvt_f32_f16(x);
    uint32x4_t  qwu = vcvtq_u32_f32(qwf);
    uint16x4_t  dhu = vqmovn_u32(qwu);
    uint16x8_t  qhu = vcombine_u16(dhu, VDHU_VOID);
    uint8x8_t   dbu = vqmovn_u16(qhu);
    float32x2_t dwf = vreinterpret_f32_u8(dbu);
    return  WBU_ASTV(vget_lane_f32(dwf, 0));
#endif
}

INLINE(Vqbu,VQBU_CVBZ) (Vqbu x) {return x;}
INLINE(Vqbu,VQBI_CVBZ) (Vqbi x)
{
    return vcombine_u8(
        vqmovun_s16(vmovl_s8(vget_low_s8(x))),
        vqmovun_s16(vmovl_s8(vget_high_s8(x)))
    );
}
INLINE(Vqbu,VQBC_CVBZ) (Vqbc x)
{
#if CHAR_MIN
    return  VQBU_CVBZ(VQBC_ASBI(x));
#else
    return  VQBC_ASBU(x);
#endif
}

INLINE(Vdbu,VQHU_CVBZ) (Vqhu x) {return  vqmovn_u16(x);}
INLINE(Vdbu,VQHI_CVBZ) (Vqhi x) {return vqmovun_s16(x);}
INLINE(Vdbu,VQHF_CVBZ) (Vqhf x) {return VDBU_VOID;}

INLINE(Vwbu,VQWU_CVBZ) (Vqwu v)
{
#define     QWU_CVBZ(M)     DHU_CVBZ(vqmovn_u32(M))
#define     VQWU_CVBZ(V)    WBU_ASTV(QWU_CVBZ(V))
    return  VQWU_CVBZ(v);
}

INLINE(Vwbu,VQWI_CVBZ) (Vqwi v)
{
#define     QWI_CVBZ(M)     DHU_CVBZ(vqmovun_s32(M))
#define     VQWI_CVBZ(V)    WBU_ASTV(QWI_CVBZ(V))
    return  VQWI_CVBZ(v);
}

INLINE(Vwbu,VQWF_CVBZ) (Vqwf v)
{
#define     QWF_CVBZ(M)             \
vget_lane_f32(                      \
    vreinterpret_f32_u8(            \
        vqmovn_u16(                 \
            vcombine_u16(           \
                vqmovn_u32(         \
                    vcvtq_u32_f32(M)\
                ),                  \
                VDHU_VOID           \
            )                       \
        )                           \
    ),                              \
    0                               \
)

#define     VQWF_CVBZ(V)            WBU_ASTV(QWF_CVBZ(VQWF_ASTM(V)))
    return  VQWI_CVBZ(v);
}

#if 0 // _LEAVE_ARM_CVBZ
}
#endif

#if 0 // _ENTER_ARM_CVBS
{
#endif

INLINE(int8_t,  BOOL_CVBS)   (_Bool x) {return x;}
INLINE(int8_t, UCHAR_CVBS)   (uchar x) {return vqmovnh_s16(x);}
INLINE(int8_t, SCHAR_CVBS)   (schar x) {return x;}
INLINE(int8_t,  CHAR_CVBS)    (char x) {return vqmovnh_s16(x);}
INLINE(int8_t, USHRT_CVBS)  (ushort x) {return vqmovnh_s16(vqmovns_s32(x));}
INLINE(int8_t,  SHRT_CVBS)   (short x) {return vqmovnh_s16(x);}

INLINE(int8_t,  UINT_CVBS)    (uint x)
{
    return (
            (vtstd_u64(x, (UINT_MAX-INT8_MAX))>>57)
        |   (INT8_MAX&x)
    );
}

INLINE(int8_t,   INT_CVBS)     (int x) {return vqmovnh_s16(vqmovns_s32(x));}
INLINE(int8_t, ULONG_CVBS)   (ulong x) 
{
    return (
            (vtstd_u64(x, (ULONG_MAX-INT8_MAX))>>57)
        |   (INT8_MAX&x)
    );
}

INLINE(int8_t,  LONG_CVBS)    (long x)
{
#if DWRD_NLONG == 2
    return  INT_CVBS(x);
#else
    return  vqmovnh_s16(vqmovns_s32(vqmovnd_s64(x)));
#endif
}

INLINE(int8_t,ULLONG_CVBS)  (ullong x)
{
#if QUAD_NLLONG == 2
    return (
            (vtstd_u64(x, (ULLONG_MAX-INT8_MAX))>>57)
        |   (INT8_MAX&x)
    );
#else
    return x <= INT8_MAX ? x : INT8_MAX;
#endif
}

INLINE(int8_t, LLONG_CVBS)   (llong x)
{
#if QUAD_NLLONG == 2
    return  vqmovnh_s16(vqmovns_s32(vqmovnd_s64(x)));
#else
    if (x < INT8_MIN) return INT8_MIN;
    if (x > INT8_MAX) return INT8_MAX;
    return x;
#endif
}


INLINE(int8_t, FLT16_CVBS) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vqmovnh_s16(vcvth_s16_f16(x));
#else
    return  vqmovnh_s16(vqmovns_s32(vcvts_s32_f32(x)));
#endif
}

INLINE(int8_t,   FLT_CVBS) (float x)
{
    return  vqmovnh_s16(vqmovns_s32(vcvts_s32_f32(x)));
}

INLINE(int8_t,   DBL_CVBS) (double x)
{
    return  vqmovnh_s16(vqmovns_s32(vqmovnd_s64(vcvtd_s64_f64(x))));
}

#if QUAD_NLLONG == 2

INLINE(int8_t,cvbsqu) (QUAD_UTYPE x)
{
    return (x >= INT8_MAX) ? INT8_MAX : x;
}

INLINE(int8_t,cvbsqi) (QUAD_ITYPE x)
{
    if (x >= INT8_MAX) return INT8_MAX;
    if (x <= INT8_MIN) return INT8_MIN;
    return x;
}

INLINE(int8_t,cvbsqf) (QUAD_FTYPE x)
{
    if (x >= INT8_MAX) return INT8_MAX;
    if (x <= INT8_MIN) return INT8_MIN;
    return x;
}
#endif

#define     DBU_CVBS(X) vqmovn_s16(vreinterpretq_s16_u16(vmovl_u8(X)))
#define     DBI_CVBS(X) X

INLINE(float,WBU_CVBS) (float x)
{
    float32x2_t m = vdup_n_f32(x);
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  u = vmovl_u16(d);
    int16x8_t   i = vreinterpretq_s16_u16(u);
    int8x8_t    t = vqmovn_s16(i);
    m = vreinterpret_f32_s8(t);
    return  vget_lane_f32(m, 0);
}

INLINE(float,DHU_CVBS) (uint16x4_t x)
{
    uint16x4_t y = vdup_n_u16((UINT16_MAX-INT8_MAX));
    y = vtst_u16(x, y);
    x = vorr_u16(x, y);
    y = vdup_n_u16(INT8_MAX);
    x = vand_u16(x, y);
    uint16x8_t  q = vcombine_u16(x, x);
    uint8x8_t   d = vmovn_u16(q);
    float32x2_t m = vreinterpret_f32_u8(d);
    return  vget_lane_f32(m, 0);
}

INLINE(float,DHI_CVBS) (int16x4_t x)
{
    int16x8_t   q = vcombine_s16(x, x);
    int8x8_t    d = vqmovn_s16(q);
    float32x2_t m = vreinterpret_f32_s8(d);
    return  vget_lane_f32(m, 0);
}

#define     QBU_CVBS(X)         \
vreinterpretq_s8_u8(            \
    vandq_u8(                   \
        vdupq_n_u8(0x7f),       \
        vorrq_u8(               \
            vtstq_u8(           \
                vdupq_n_u8(128),\
                X               \
            ),                  \
            X                   \
        )                       \
    )                           \
)
#define     QBI_CVBS(X) X

#define     QHU_CVBS(X)                 \
vqmovn_s16(                             \
    vreinterpretq_s16_u16(              \
        vandq_u16(                      \
            vdupq_n_u16(0x7fff),        \
            vorrq_u16(                  \
                vtstq_u16(              \
                    vdupq_n_u16(0x8000),\
                    X                   \
                ),                      \
                X                       \
            )                           \
        )                               \
    )                                   \
)

#define     QHI_CVBS    vqmovn_s16

INLINE(float,QWU_CVBS) (uint32x4_t x)
{
    uint32x4_t y = vdupq_n_u32(UINT32_MAX-INT8_MAX);
    y = vtstq_u32(x, y);
    x = vorrq_u32(x, y);
    y = vdupq_n_u32(INT8_MAX);
    x = vandq_u32(x, y);
    uint64x1_t  m = vdup_n_u64(UINT64_C(0xffffffff0c080400));
    uint8x8_t   t = vqtbl1_u8(
        vreinterpretq_u8_u32(x),
        vreinterpret_u8_u64(m)
    );
    return  vget_lane_f32(vreinterpret_f32_u8(t), 0);
}

INLINE(float,QWI_CVBS) (int32x4_t x)
{
    int16x4_t   l = vqmovn_s32(x);
    int16x8_t   h = vcombine_s16(l, l);
    int8x8_t    b = vqmovn_s16(h);
    float32x2_t m = vreinterpret_f32_s8(b);
    return  vget_lane_f32(m, 0);
}

INLINE(float,QWF_CVBS) (float32x4_t x)
{
    return  QWI_CVBS(vcvtq_s32_f32(x));
}



INLINE(Vwbi,VWBU_CVBS) (Vwbu x)
{
    return  WBI_ASTV(WBU_CVBS(VWBU_ASTM(x)));
}

INLINE(Vwbi,VWBI_CVBS) (Vwbi x) {return x;}

INLINE(Vwbi,VWBC_CVBS) (Vwbc x)
{
#if CHAR_MIN
    return  VWBC_ASBI(x);
#else
    return  WBI_ASTV(WBU_CVBS(VWBC_ASTM(x)));
#endif
}


INLINE(Vdbi,VDBU_CVBS) (Vdbu x) {return  DBU_CVBS(x);}
INLINE(Vdbi,VDBI_CVBS) (Vdbi x) {return  DBI_CVBS(x);}
INLINE(Vdbi,VDBC_CVBS) (Vdbc x)
{
#if CHAR_MIN
    return  VDBC_ASBI(x);
#else
    return  DBU_CVBS(VDBC_ASBU(x));
#endif
}


INLINE(Vwbi,VDHU_CVBS) (Vdhu x)
{
    return  WBI_ASTV(DHU_CVBS(x));
}

INLINE(Vwbi,VDHI_CVBS) (Vdhi x)
{
    return  WBI_ASTV(DHI_CVBS(x));
}

INLINE(Vwbi,VDHF_CVBS) (Vdhf x)
{
    return  VWBI_VOID;
}


INLINE(Vqbi,VQBU_CVBS) (Vqbu x) {return QBU_CVBS(x);}
INLINE(Vqbi,VQBI_CVBS) (Vqbi x) {return QBI_CVBS(x);}
INLINE(Vqbi,VQBC_CVBS) (Vqbc x)
{
#if CHAR_MIN
    return  VQBC_ASBI(x);
#else
    return  VQBU_CVBS(VQBC_ASBU(x));
#endif
}

INLINE(Vdbi,VQHU_CVBS) (Vqhu x) {return QHU_CVBS(x);}
INLINE(Vdbi,VQHI_CVBS) (Vqhi x) {return QHI_CVBS(x);}
INLINE(Vdbi,VQHF_CVBS) (Vqhf x) {return VDBU_VOID;}

INLINE(Vwbi,VQWU_CVBS) (Vqwu x) {return WBI_ASTV(QWU_CVBS(x));}
INLINE(Vwbi,VQWI_CVBS) (Vqwi x) {return WBI_ASTV(QWI_CVBS(x));}
INLINE(Vwbi,VQWF_CVBS) (Vqwf x) {return WBI_ASTV(QWF_CVBS(x));}


#if 0 // _LEAVE_ARM_CVBS
}
#endif

#if 0 // _ENTER_ARM_CVHU
{
#endif

INLINE(uint16_t,FLT16_CVHU) (flt16_t x) {return x;}
INLINE(uint16_t,  FLT_CVHU)   (float x) {return x;}
INLINE(uint16_t,  DBL_CVHU)  (double x) {return x;}
INLINE(uint16_t, cvhuqf) (QUAD_FTYPE x) {return x;}

#define     WBU_CVHU(X)         \
vreinterpret_u16_u8(            \
    vzip1_u8(                   \
        vreinterpret_u8_f32(    \
            vdup_n_f32(X)       \
        ),                      \
        vdup_n_u8(0)            \
    )                           \
)


#define     WBI_CVHU(X)         \
vreinterpret_u16_s16(           \
    vget_low_s16(               \
        vmovl_s8(               \
            vreinterpret_s8_f32(\
                vdup_n_f32(X)   \
            )                   \
        )                       \
    )                           \
)

#if CHAR_MIN
#   define  WBC_CVHU        WBI_CVHU
#else
#   define  WBC_CVHU        WBU_CVHU
#endif

#define     WHU_CVHU(X)     (X)
#define     WHI_CVHU(X)     (X)
#define     WHF_CVHU(X)     (0.0f)

#define     DBU_CVHU(X)     vmovl_u8(X)
#define     DBI_CVHU(X)     vreinterpretq_u16_s16(vmovl_s8(X))
#if CHAR_MIN
#   define  DBC_CVHU        DBI_CVHU
#else
#   define  DBC_CVHU        DBU_CVHU
#endif

#define     DHU_CVHU(X)     (X)
#define     DHI_CVHU(X)     vreinterpret_u16_s16(X)
#define     DHF_CVHU(X)     vdup_n_u16(0)

//  {.H0={W0.B0, W0.B1}, .H1={W1.B0, W1.B1}}

#define     DWU_CVHU(X)                     \
vget_lane_f32(                              \
    vreinterpret_f32_u8(                    \
        vtbl1_u8(                           \
            vreinterpret_u8_u32(X),         \
            vreinterpret_u8_u64(            \
                vdup_n_u64(                 \
                    0x0504010005040100ull   \
                )                           \
            )                               \
        )                                   \
    ),                                      \
    0                                       \
)

#define     DWI_CVHU(X)                     \
vget_lane_f32(                              \
    vreinterpret_f32_u8(                    \
        vtbl1_u8(                           \
            vreinterpret_u8_s32(X),         \
            vreinterpret_u8_u64(            \
                vdup_n_u64(                 \
                    0x0504010005040100ull   \
                )                           \
            )                               \
        )                                   \
    ),                                      \
    0                                       \
)

#define     DWF_CVHU(X)     DWI_CVHU(vcvt_s32_f32(X))

#define     QHU_CVHU(X)     (X)
#define     QHI_CVHU(X)     vreinterpretq_u16_s16(X)
#define     QHF_CVHU(X)     vdupq_n_u16(0)

#define     QWU_CVHU(X)     vmovn_u32(X)
#define     QWI_CVHU(X)     vreinterpret_u16_s16(vmovn_s32(X))
#define     QWF_CVHU(X)     QWI_CVHU(vcvtq_s32_f32(X))

// u64×2=>u32×2; u32×2##u32×2=>u32×4; u32×4=>
#define     QDU_CVHU(X)                     \
vget_lane_f32(                              \
    vreinterpret_f32_u8(                    \
        vqtbl1_u8(                          \
            vreinterpretq_u8_u64(X),        \
            vreinterpret_u8_u64(            \
                vdup_n_u64(                 \
                    0x0908010009080100ull   \
                )                           \
            )                               \
        )                                   \
    ),                                      \
    0                                       \
)

#define     QDI_CVHU(X)                     \
vget_lane_f32(                              \
    vreinterpret_f32_u8(                    \
        vqtbl1_u8(                          \
            vreinterpretq_u8_s64(X),        \
            vreinterpret_u8_u64(            \
                vdup_n_u64(                 \
                    0x0908010009080100ull   \
                )                           \
            )                               \
        )                                   \
    ),                                      \
    0                                       \
)

#define     QDF_CVHU(X) QDI_CVHU(vcvtq_s64_f64(X))

INLINE(Vdhu,VWBU_CVHU) (Vwbu x) {return  WBU_CVHU(VWBU_ASTM(x));}
INLINE(Vdhu,VWBI_CVHU) (Vwbi x) {return  WBI_CVHU(VWBI_ASTM(x));}
INLINE(Vdhu,VWBC_CVHU) (Vwbc x) {return  WBC_CVHU(VWBC_ASTM(x));}


INLINE(Vwhu,VWHI_CVHU) (Vwhi x) {return  VWHI_ASHU(x);}
INLINE(Vwhu,VWHF_CVHU) (Vwhf x) {return  VWHU_VOID;}

INLINE(Vqhu,VDBU_CVHU) (Vdbu x) {return  DBU_CVHU(x);}
INLINE(Vqhu,VDBI_CVHU) (Vdbi x) {return  DBI_CVHU(x);}
INLINE(Vqhu,VDBC_CVHU) (Vdbc x) {return  DBC_CVHU(VDBC_ASTM(x));}

INLINE(Vdhu,VDHI_CVHU) (Vdhi x) {return  DHI_CVHU(x);}
INLINE(Vdhu,VDHF_CVHU) (Vdhf x) {return  DHF_CVHU(x);}

INLINE(Vwhu,VDWU_CVHU) (Vdwu x) {return  WHU_ASTV(DWU_CVHU(x));}
INLINE(Vwhu,VDWI_CVHU) (Vdwi x) {return  WHU_ASTV(DWI_CVHU(x));}
INLINE(Vwhu,VDWF_CVHU) (Vdwf x) {return  WHU_ASTV(DWF_CVHU(x));}

INLINE(Vqhu,VQHI_CVHU) (Vqhi x) {return  QHI_CVHU(x);}
INLINE(Vqhu,VQHF_CVHU) (Vqhf x) {return  QHF_CVHU(x);}

INLINE(Vdhu,VQWU_CVHU) (Vqwu x) {return  QWU_CVHU(x);}
INLINE(Vdhu,VQWI_CVHU) (Vqwi x) {return  QWI_CVHU(x);}
INLINE(Vdhu,VQWF_CVHU) (Vqwf x) {return  QWF_CVHU(x);}

INLINE(Vwhu,VQDU_CVHU) (Vqdu x) {return  WHU_ASTV(QDU_CVHU(x));}
INLINE(Vwhu,VQDI_CVHU) (Vqdi x) {return  WHU_ASTV(QDI_CVHU(x));}
INLINE(Vwhu,VQDF_CVHU) (Vqdf x) {return  WHU_ASTV(QDF_CVHU(x));}

#if 0 // _LEAVE_ARM_CVHU
}
#endif

#if 0 // _ENTER_ARM_CVHI
{
#endif

INLINE(int16_t,FLT16_CVHI) (flt16_t x) {return x;}
INLINE(int16_t,  FLT_CVHI)   (float x) {return x;}
INLINE(int16_t,  DBL_CVHI)  (double x) {return x;}
INLINE(int16_t, cvhiqf) (QUAD_FTYPE x) {return x;}

#define     WBU_CVHI(X)         \
vreinterpret_s16_u8(            \
    vzip1_u8(                   \
        vreinterpret_u8_f32(    \
            vdup_n_f32(X)       \
        ),                      \
        vdup_n_u8(0)            \
    )                           \
)

#define     WBI_CVHI(X)     \
vget_low_s16(vmovl_s8(vreinterpret_s8_f32(vdup_n_f32(X))))


INLINE(Vdhi,VWBU_CVHI) (Vwbu x)
{
    float32x2_t m = vdup_n_f32(VWBU_ASTM(x));
    uint16x8_t  u = vmovl_u8(vreinterpret_u8_f32(m));
    int16x8_t   i = vreinterpretq_s16_u16(u);
    return  vget_low_s16(i);
}

INLINE(Vdhi,VWBI_CVHI) (Vwbi x)
{
    float32x2_t m = vdup_n_f32(VWBI_ASTM(x));
    int16x8_t   i = vmovl_s8(vreinterpret_s8_f32(m));
    return  vget_low_s16(i);
}

INLINE(Vdhi,VWBC_CVHI) (Vwbc x)
{
    float32x2_t m = vdup_n_f32(VWBC_ASTM(x));
    int16x8_t   i;
#if CHAR_MIN
    i = vmovl_s8(vreinterpret_s8_f32(m));
#else
    uint16x8_t  u = vmovl_u8(vreinterpret_u8_f32(m));
    i = vreinterpretq_s16_u16(u);
#endif
    return  vget_low_s16(i);
}

INLINE(Vwhi,VWHU_CVHI) (Vwhu x) {return VWHU_ASHI(x);}

INLINE(Vwhi,VWHF_CVHI) (Vwhf x)
{
    float32x2_t v = vdup_n_f32(VWHF_ASTM(x));
    float16x4_t h = vreinterpret_f16_f32(v);
    int16x4_t   i;
#if defined(SPC_ARM_FP16_SIMD)
    i = vcvt_s16_f16(h);
#else
    i = vmovn_s32(vcvtq_s32_f32(vcvt_f32_f16(h)));
#endif
    v = vreinterpret_f32_s16(i);
    return  WHI_ASTV(vget_lane_f32(v, 0));
}

INLINE(Vqhi,VDBU_CVHI) (Vdbu x) {return  VQHU_ASHI(vmovl_u8(x));}
INLINE(Vqhi,VDBI_CVHI) (Vdbi x) {return  vmovl_s8(x);}
INLINE(Vqhi,VDBC_CVHI) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_CVHI(VDBC_ASBI(x));
#else
    return  VDBU_CVHI(VDBC_ASBU(x));
#endif
}

INLINE(Vdhi,VDHU_CVHI) (Vdhu x) {return VDHU_ASHI(x);}

INLINE(Vdhi,VDHF_CVHI) (Vdhf x)
{
    return VDHI_VOID;
}

INLINE(Vwhi,VDWU_CVHI) (Vdwu x)
{
#define     VDWU_CVHI(X) WHI_ASTV(DWU_CVHU(X))
    return  VDWU_CVHI(x);
}

INLINE(Vwhi,VDWI_CVHI) (Vdwi x)
{
#define     VDWI_CVHI(X) WHI_ASTV(DWU_CVHU(vreinterpret_u32_s32(X)))
    return  VDWI_CVHI(x);
}

INLINE(Vwhi,VDWF_CVHI) (Vdwf x)
{
    int32x2_t i = vcvt_s32_f32(x);
    uint8x8_t r = vreinterpret_u8_s32(i);
    r = vtbl1_u8(
        r,
        vcreate_u8(0xffffffff05040100ull)
    );
    float32x2_t m = vreinterpret_f32_u8(r);
    return  WHI_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vqhi,VQHU_CVHI) (Vqhu x) {return vreinterpretq_s16_u16(x);}

INLINE(Vqhi,VQHF_CVHI) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_s16_f16(x);
#else
    float16x4_t l = vget_low_f16(x);
    float16x4_t r = vget_high_f16(x);
    return  vcombine_s16(
        vmovn_s32(vcvtq_s32_f32(vcvt_f32_f16(l))),
        vmovn_s32(vcvtq_s32_f32(vcvt_f32_f16(r)))
    );

#endif
}

INLINE(Vdhi,VQWU_CVHI) (Vqwu x) {return VDHU_ASHI(vmovn_u32(x));}
INLINE(Vdhi,VQWI_CVHI) (Vqwi x) {return vmovn_s32(x);}
INLINE(Vdhi,VQWF_CVHI) (Vqwf x) {return vmovn_s32(vcvtq_s32_f32(x));}

INLINE(Vwhi,VQDU_CVHI) (Vqdu x)
{

#define     VQDU_CVHI(X) WHI_ASTV(QDU_CVHU(X))
    return  VQDU_CVHI(x);
}

INLINE(Vwhi,VQDI_CVHI) (Vqdi x)
{
#define     VQDI_CVHI(X) WHI_ASTV(QDI_CVHU(X))
    return  VQDI_CVHI(x);
}

INLINE(Vwhi,VQDF_CVHI) (Vqdf x)
{
#define     VQDF_CVHI(X) WHI_ASTV(QDI_CVHU(vcvtq_s64_f64(X)))
    return  VQDF_CVHI(x);
}

#if 0 // _LEAVE_ARM_CVHI
}
#endif

#if 0 // _ENTER_ARM_CVHZ
{
#endif

INLINE(uint16_t,   BOOL_CVHZ)  (_Bool x) {return x;}
INLINE(uint16_t,  UCHAR_CVHZ)  (uchar x) {return x;}
INLINE(uint16_t,  SCHAR_CVHZ)  (schar x) {return vqmovunh_s16(x);}
INLINE(uint16_t,   CHAR_CVHZ)   (char x) 
{
#if CHAR_MIN
    return  SCHAR_CVHZ(x);
#else
    return  UCHAR_CVHZ(x);
#endif
}

INLINE(uint16_t,  USHRT_CVHZ) (ushort x) {return x;}
INLINE(uint16_t,   SHRT_CVHZ)  (short x) {return vqmovuns_s32(x);}
INLINE(uint16_t,   UINT_CVHZ)   (uint x) {return  vqmovns_u32(x);}
INLINE(uint16_t,    INT_CVHZ)    (int x) {return vqmovuns_s32(x);}
INLINE(uint16_t,  ULONG_CVHZ)  (ulong x)
{
#if DWRD_NLONG == 2
    return  vqmovns_u32(x);
#else
    return  vqmovns_u32(vqmovnd_u64(x));
#endif
}

INLINE(uint16_t,   LONG_CVHZ)   (long x)
{
#if DWRD_NLONG == 2
    return  vqmovuns_s32(x);
#else
    return  vqmovns_u32(vqmovund_s64(x));
#endif
}

INLINE(uint16_t, ULLONG_CVHZ) (ullong x)
{
#if QUAD_NLLONG == 2
    return  vqmovns_u32(vqmovnd_u64(x));
#else

#endif
}

INLINE(uint16_t,  LLONG_CVHZ)  (llong x)
{
#if QUAD_NLLONG == 2
    return  vqmovns_u32(vqmovund_s64(x));
#else

#endif
}

INLINE(uint16_t, FLT16_CVHZ) (flt16_t x)
{
    return  0;
}

INLINE(uint16_t, FLT_CVHZ) (float x)
{
    return  vqmovns_u32(vcvts_u32_f32(x));
}

INLINE(uint16_t, DBL_CVHZ) (double x)
{
    return  vqmovns_u32(vqmovnd_u64(vcvtd_u64_f64(x)));
}

INLINE(uint16_t,cvhzqu) (QUAD_UTYPE x)
{
    return (x >= UINT16_MAX) ? UINT16_MAX : x;
}

INLINE(uint16_t,cvhzqi) (QUAD_ITYPE x)
{
    if (x >= UINT16_MAX) return UINT16_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE(uint16_t,cvhzqf) (QUAD_FTYPE x)
{
    if (x >= UINT16_MAX) return UINT16_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE(Vdhu,VWBU_CVHZ) (Vwbu x)
{
    float32x2_t m = vdup_n_f32(VWBU_ASTM(x));
    uint8x8_t   l = vreinterpret_u8_f32(m);
    uint8x8_t   r = vdup_n_u8(0);
    r = vzip1_u8(l, r);
    return vreinterpret_u16_u8(r);
}

INLINE(Vdhu,VWBI_CVHZ) (Vwbi x)
{
    float32x2_t m = vdup_n_f32(VWBI_ASTM(x));
    int16x8_t   q = vmovl_s8(vreinterpret_s8_f32(m));
    int16x4_t   l = vget_low_s16(q);
    int32x4_t   c = vmovl_s16(l);
    return  vqmovun_s32(c);
}

INLINE(Vdhu,VWBC_CVHZ) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVHZ(VWBC_ASBI(x));
#else
    return  VWBU_CVHZ(VWBC_ASBU(x));
#endif
}


INLINE(Vwhu,VWHU_CVHZ) (Vwhu x) {return x;}
INLINE(Vwhu,VWHI_CVHZ) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    int32x4_t   q = vmovl_s16(vreinterpret_s16_f32(m));
    uint16x4_t  v = vqmovun_s32(q);
    m = vreinterpret_f32_u16(v);
    return  WHU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwhu,VWHF_CVHZ) (Vwhf x) {return VWHU_VOID;}

INLINE(Vqhu,VDBU_CVHZ) (Vdbu x) {return vmovl_u8(x);}
INLINE(Vqhu,VDBI_CVHZ) (Vdbi x)
{
    int16x8_t m = vmovl_s8(x);
    return vcombine_u16(
        vqmovun_s32(vmovl_s16(vget_low_s16(m))),
        vqmovun_s32(vmovl_s16(vget_high_s16(m)))
    );
}

INLINE(Vqhu,VDBC_CVHZ) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_CVHZ(VDBC_ASBI(x));
#else
    return  VDBU_CVHZ(VDBC_ASBU(x));
#endif
}

INLINE(Vdhu,VDHU_CVHZ) (Vdhu x) {return x;}
INLINE(Vdhu,VDHI_CVHZ) (Vdhi x) {return vqmovun_s32(vmovl_s16(x));}
INLINE(Vdhu,VDHF_CVHZ) (Vdhf x) {return VDHU_VOID;}

INLINE(Vwhu,VDWU_CVHZ) (Vdwu x)
{
#define     DWU_CVHZ(V)     \
vget_lane_f32(              \
    vreinterpret_f32_u16(   \
        vqmovn_u32(         \
            vcombine_u32(   \
                V,          \
                VDWU_VOID   \
            )               \
        )                   \
    ),                      \
    0                   \
)
#define     VDWU_CVHZ(X)    WHU_ASTV(DWU_CVHZ(X))
    return  VDWU_CVHZ(x);
}

INLINE(Vwhu,VDWI_CVHZ) (Vdwi x)
{
#define     DWI_CVHZ(V)     \
vget_lane_f32(              \
    vreinterpret_f32_s16(   \
        vqmovun_s32(        \
            vcombine_s32(   \
                V,          \
                VDWI_VOID   \
            )               \
        )                   \
    ),                      \
    0                   \
)
#define     VDWI_CVHZ(X)    WHU_ASTV(DWI_CVHZ(X))
    return  VDWI_CVHZ(x);
}

INLINE(Vwhu,VDWF_CVHZ) (Vdwf x)
{
    return  VDWI_CVHZ(vcvt_s32_f32(x));
}

INLINE(Vqhu,VQHU_CVHZ) (Vqhu x) {return x;}
INLINE(Vqhu,VQHI_CVHZ) (Vqhi x)
{
    return vcombine_u16(
        vqmovun_s32(vmovl_s16(vget_low_s16( x))),
        vqmovun_s32(vmovl_s16(vget_high_s16(x)))
    );

}

INLINE(Vqhu,VQHF_CVHZ) (Vqhf x) {return VQHU_VOID;}

INLINE(Vdhu,VQWU_CVHZ) (Vqwu x) {return vqmovn_u32(x);}
INLINE(Vdhu,VQWI_CVHZ) (Vqwi x) {return vqmovun_s32(x);}
INLINE(Vdhu,VQWF_CVHZ) (Vqwf x) {return vqmovun_s32(vcvtq_s32_f32(x));}

INLINE(Vwhu,VQDU_CVHZ) (Vqdu x)
{
    x = vorrq_u64(
        vtstq_u64(
            vdupq_n_u64(UINT64_MAX-UINT16_MAX),
            x
        ),
        x
    );
    uint8x8_t t = vqtbl1_u8(
        vreinterpretq_u8_u64(x),
        vreinterpret_u8_u64(vdup_n_u64(UINT64_C(0xffff0908ffff0100)))
    );
    float32x2_t m = vreinterpret_f32_u8(t);
    return  WHU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwhu,VQDI_CVHZ) (Vqdi x)
{
// TODO: implement arm's 'cvhzqdi' properly
    uint16x4_t v = VDHU_VOID;
    v = vset_lane_u16(
        vqmovns_u32(vqmovund_s64(vgetq_lane_s64(x, 0))),
        v,
        0
    );
    v = vset_lane_u16(
        vqmovns_u32(vqmovund_s64(vgetq_lane_s64(x, 0))),
        v,
        0
    );
    float32x2_t m = vreinterpret_f32_u16(v);
    return  WHU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwhu,VQDF_CVHZ) (Vqdf x) {return VQDI_CVHZ(vcvtq_s64_f64(x));}


#if 0 // _LEAVE_ARM_CVHZ
}
#endif

#if 0 // _ENTER_ARM_CVHS
{
#endif

INLINE(int16_t,  BOOL_CVHS)  (_Bool x) {return x;}
INLINE(int16_t, UCHAR_CVHS)  (uchar x) {return x;}
INLINE(int16_t, SCHAR_CVHS)  (schar x) {return x;}
INLINE(int16_t,  CHAR_CVHS)   (char x) {return x;}
INLINE(int16_t, USHRT_CVHS) (ushort x) {return vqmovns_s32(x);}
INLINE(int16_t,  SHRT_CVHS)  (short x) {return vqmovns_s32(x);}
INLINE(int16_t,  UINT_CVHS)   (uint x)
{
    return (
            (vtstd_u64( (UINT_MAX-INT16_MAX), x)>>49)
        |   (INT16_MAX&x)
    );
}

INLINE(int16_t,   INT_CVHS)    (int x) {return vqmovns_s32(x);}
INLINE(int16_t, ULONG_CVHS)  (ulong x) 
{
    return (
            (vtstd_u64( (ULONG_MAX-INT16_MAX), x)>>49)
        |   (INT16_MAX&x)
    );
}

INLINE(int16_t,  LONG_CVHS)   (long x)
{
#if DWRD_NLONG == 2
    return  vqmovns_s32(x);
#else
    return  vqmovns_s32(vqmovnd_s64(x));
#endif
}

INLINE(int16_t,ULLONG_CVHS) (ullong x)
{
    return (
            (vtstd_u64( (ULLONG_MAX-INT16_MAX), x)>>49)
        |   (INT16_MAX&x)
    );
}

INLINE(int16_t, LLONG_CVHS)  (llong x)
{
#if QUAD_NLLONG == 2
    return  vqmovnh_s16(vqmovns_s32(vqmovnd_s64(x)));
#else
#   error "is there a 'vqmovnq_s128' yet?"
#endif
}

INLINE(int16_t, FLT16_CVHS) (flt16_t x) {return 0;}
INLINE(int16_t,   FLT_CVHS)  (float x)
{
    return  vqmovns_s32(vcvts_s32_f32(x));
}

INLINE(int16_t,   DBL_CVHS) (double x)
{
    return  vqmovns_s32(vqmovnd_s64(vcvtd_s64_f64(x)));
}

INLINE(int16_t,cvhsqu) (QUAD_UTYPE x)
{
    return (x >= INT16_MAX) ? INT16_MAX : x;
}

INLINE(int16_t,cvhsqi) (QUAD_ITYPE x)
{
    if (x >= INT16_MAX) return INT16_MAX;
    if (x <= INT16_MIN) return INT16_MIN;
    return x;
}

INLINE(int16_t,cvhsqf) (QUAD_FTYPE x)
{
    if (x >= INT16_MAX) return INT16_MAX;
    if (x <= INT16_MIN) return INT16_MIN;
    return x;
}


INLINE(int16x4_t, WBU_CVHS) (float x)
{
    float32x2_t m = vdup_n_f32(x);
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  u = vmovl_u16(d);
    return  vget_low_s16(vreinterpretq_s16_u16(u));
}

INLINE(int16x4_t, WBI_CVHS) (float x)
{
    float32x2_t m = vdup_n_f32(x);
    int8x8_t    d = vreinterpret_s8_f32(m);
    int16x8_t   i = vmovl_s16(d);
    return  vget_low_s16(i);
}

INLINE(float,     WHU_CVHS) (float x)
{
    float32x2_t m = vdup_n_f32(x);
    uint16x4_t  u = vreinterpret_u16_f32(m);
    u = vand_u16(
        vdup_n_u16(0x7fff),
        vorr_u16(
            vtst_u16(vdup_n_u16(0x8000), u),
            u
        )
    );
    m = vreinterpret_f32_u16(u);
    return vget_lane_f32(m, 0);
}

INLINE(float,     WHI_CVHS)      (float x) {return x;}

INLINE(float,     WHF_CVHS)      (float x) {return 0.0f;}


INLINE(int16x8_t, DBU_CVHS)   (uint8x8_t x)
{
    return vreinterpretq_s16_u16(vmovl_u8(x));
}

INLINE(int16x8_t, DBI_CVHS)    (int8x8_t x) {return vmovl_u8(x);}


INLINE(int16x4_t, DHU_CVHS)  (uint16x4_t x)
{
    return vreinterpret_s16_u16(
        vand_u16(
            vdup_n_u16(0x7fff),
            vorr_u16(
                vtst_u16(vdup_n_u16(0x8000), x),
                x
            )
        )
    );
}

INLINE(int16x4_t, DHI_CVHS)   (int16x4_t x) {return x;}
INLINE(int16x4_t, DHF_CVHS) (float16x4_t x) {return VDHI_VOID;}

INLINE(float,     DWU_CVHS)  (uint32x2_t x)
{
    x = vand_u32(
        vdup_n_u32(0x7fffffffu),
        vorr_u32(
            vtst_u32(vdup_n_u32(0x80000000u), x),
            x
        )
    );
    int32x2_t   i = vreinterpret_s32_u32(x);
    int32x4_t   s = vcombine_s32(i, i);
    int16x4_t   v = vqmovn_s32(s);
    float32x2_t m = vreinterpret_f32_s16(v);
    return vget_lane_f32(m, 0);
}

INLINE(float,     DWI_CVHS)   (int32x2_t x)
{
    int32x4_t   s = vcombine_s32(x, x);
    int16x4_t   v = vqmovn_s32(s);
    float32x2_t m = vreinterpret_f32_s16(v);
    return vget_lane_f32(m, 0);
}

INLINE(float,     DWF_CVHS) (float32x2_t x)
{
    return  DWI_CVHS(vcvt_s32_f32(x));
}

INLINE(int16x8_t, QHU_CVHS)  (uint16x8_t x)
{
    return vreinterpretq_s16_u16(
        vandq_u16(
            vdupq_n_u16(0x7fff),
            vorrq_u16(
                vtstq_u16(vdupq_n_u16(0x8000), x),
                x
            )
        )
    );
}

INLINE(int16x8_t, QHI_CVHS)   (int16x8_t x) {return x;}
INLINE(int16x8_t, QHF_CVHS) (float16x8_t x) {return VQHI_VOID;}


INLINE(int16x4_t, QWU_CVHS)  (uint32x4_t x)
{
    x = vandq_u32(
        vdupq_n_u32(0x7fffffffu),
        vorrq_u32(
            vtstq_u32(vdupq_n_u32(0x80000000u), x),
            x
        )
    );
    return vqmovn_s32(vreinterpretq_s32_u32(x));
}

INLINE(int16x4_t, QWI_CVHS)   (int32x4_t x) {return vqmovn_s32(x);}
INLINE(int16x4_t, QWF_CVHS) (float32x4_t x)
{
    return vqmovn_s32(vcvtq_s32_f32(x));
}

INLINE(float,     QDU_CVHS)  (uint64x2_t x)
{
    return  DWU_CVHS(vqmovn_u64(x));
}

INLINE(float,     QDI_CVHS)   (int64x2_t x)
{
    return  DWI_CVHS(vqmovn_s64(x));
}

INLINE(float,     QDF_CVHS) (float64x2_t x)
{
    return  QDI_CVHS(vcvtq_s64_f64(x));
}


INLINE(Vdhi,VWBU_CVHS) (Vwbu x)
{
    return  WBU_CVHS(VWBU_ASTM(x));
}

INLINE(Vdhi,VWBI_CVHS) (Vwbi x)
{
    return  WBI_CVHS(VWBI_ASTM(x));
}

INLINE(Vdhi,VWBC_CVHS) (Vwbc x)
{
#if CHAR_MIN
    return  WBI_CVHS(VWBC_ASTM(x));
#else
    return  WBU_CVHS(VWBC_ASTM(x));
#endif
}


INLINE(Vwhi,VWHU_CVHS) (Vwhu x)
{
    return  WHI_ASTV(WHU_CVHS(VWHU_ASTM(x)));
}

INLINE(Vwhi,VWHI_CVHS) (Vwhi x) {return x;}
INLINE(Vwhi,VWHF_CVHS) (Vwhf x) {return VWHI_VOID;}

INLINE(Vqhi,VDBU_CVHS) (Vdbu x) {return DBU_CVHS(x);}
INLINE(Vqhi,VDBI_CVHS) (Vdbi x) {return DBI_CVHS(x);}
INLINE(Vqhi,VDBC_CVHS) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_CVHS(VDBC_ASBI(x));
#else
    return  VDBU_CVHS(VDBC_ASBU(x));
#endif
}

INLINE(Vdhi,VDHU_CVHS) (Vdhu x) {return DHU_CVHS(x);}
INLINE(Vdhi,VDHI_CVHS) (Vdhi x) {return DHI_CVHS(x);}
INLINE(Vdhi,VDHF_CVHS) (Vdhf x) {return DHF_CVHS(x);}

INLINE(Vwhi,VDWU_CVHS) (Vdwu x) {return WHI_ASTV(DWU_CVHS(x));}
INLINE(Vwhi,VDWI_CVHS) (Vdwi x) {return WHI_ASTV(DWI_CVHS(x));}
INLINE(Vwhi,VDWF_CVHS) (Vdwf x) {return WHI_ASTV(DWF_CVHS(x));}

//     Vohi,VQBU_CVHS
INLINE(Vqhi,VQHU_CVHS) (Vqhu x) {return QHU_CVHS(x);}
INLINE(Vqhi,VQHI_CVHS) (Vqhi x) {return QHI_CVHS(x);}
INLINE(Vqhi,VQHF_CVHS) (Vqhf x) {return QHF_CVHS(x);}

INLINE(Vdhi,VQWU_CVHS) (Vqwu x) {return QWU_CVHS(x);}
INLINE(Vdhi,VQWI_CVHS) (Vqwi x) {return QWI_CVHS(x);}
INLINE(Vdhi,VQWF_CVHS) (Vqwf x) {return QWF_CVHS(x);}

INLINE(Vwhi,VQDU_CVHS) (Vqdu x) {return WHI_ASTV(QDU_CVHS(x));}
INLINE(Vwhi,VQDI_CVHS) (Vqdi x) {return WHI_ASTV(QDI_CVHS(x));}
INLINE(Vwhi,VQDF_CVHS) (Vqdf x) {return WHI_ASTV(QDF_CVHS(x));}

#if 0 // _LEAVE_ARM_CVHS
}
#endif

#if 0 // _ENTER_ARM_CVHF
{
#endif

/*
INLINE(float,FLT16_CVWF) (flt16_t m)
{
    HALF_TYPE src={.F=m};
    WORD_TYPE dst;
    dst.Mant = src.Mant<<13;
    dst.Expo = src.Expo+112;
    dst.Sign = src.Sign;
    return  dst.F;
}

INLINE(flt16_t,FLT_CVHF) (float m)
{
    WORD_TYPE src = {.F=m};
    HALF_TYPE dst = {.Sign=src.Sign};
    if (src.Expo < 112)
        return  dst.F;
    if (src.Expo > 142)
        dst.Expo = 31;
    else
        dst.Expo = src.Expo-112;
    dst.Mant = src.Mant>>13;
    return  dst.F;
}

1.0f16:
        E       M
    H   15      0
    W   127     0
    D   1023    0
    Q   8191    0

FLT16_MIN:
    H   1       0
    W   113     0
    D   1009    0

FLT16_MAX:
    H   30      (1023<<0)
    W   142     (1023<<13) == 8380416
    D   1038    (1023<<42) == 4499201580859392



INLINE(flt16_t,DBL_CVHF) (double m)
{
    DWRD_TYPE src = {.F=m};
    HALF_TYPE dst = {.Sign=src.Sign};
    if (src.Expo < 1009)
        return  dst.F;
    if (src.Expo > 1038)
        dst.Expo = 31;
    else
        dst.Expo = src.Expo-1008;
    dst.Mant = src.Mant>>42;
    return  dst.F;
}

*/

INLINE(flt16_t,  BOOL_CVHF)   (_Bool x) {return x;}
INLINE(flt16_t, UCHAR_CVHF)   (uchar x) {return x;}
INLINE(flt16_t, SCHAR_CVHF)   (schar x) {return x;}
INLINE(flt16_t,  CHAR_CVHF)    (char x) {return x;}
INLINE(flt16_t, USHRT_CVHF)  (ushort x) {return x;}
INLINE(flt16_t,  SHRT_CVHF)   (short x) {return x;}
INLINE(flt16_t,   INT_CVHF)     (int x) {return x;}
INLINE(flt16_t,  UINT_CVHF)    (uint x) {return x;}
INLINE(flt16_t, ULONG_CVHF)   (ulong x) {return x;}
INLINE(flt16_t,  LONG_CVHF)    (long x) {return x;}
INLINE(flt16_t,ULLONG_CVHF)  (ullong x) {return x;}
INLINE(flt16_t, LLONG_CVHF)   (llong x) {return x;}
INLINE(flt16_t, FLT16_CVHF) (flt16_t x) {return x;}
INLINE(flt16_t,   FLT_CVHF)   (float x) {return x;}
INLINE(flt16_t,   DBL_CVHF)  (double x) {return x;}
#if QUAD_NLLONG == 2
INLINE(flt16_t, cvhfqu)  (QUAD_UTYPE x) {return x;}
INLINE(flt16_t, cvhfqi)  (QUAD_ITYPE x) {return x;}
INLINE(flt16_t, cvhfqf)  (QUAD_FTYPE x) {return x;}
#endif

INLINE(Vdhf,VWBU_CVHF) (Vwbu x)
{
    float32x2_t w = vdup_n_f32(VWBU_ASTM(x));
    uint8x8_t  bv = vreinterpret_u8_f32(w);
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvt_f16_u16(vget_low_u16(vmovl_u8(bv)));
#else
    uint8x8_t  bz = vclz_u8(bv);
    uint16x4_t  v = vget_low_u16(vmovl_u8(bv));
    uint16x4_t  z = vget_low_u16(vmovl_u8(bz));
    uint16x4_t  e = vsub_u16(vdup_n_u16(22), z);
    int16x4_t   n = vreinterpret_s16_u16(z);
    uint16x4_t  d = vshl_u16(vdup_n_u16(128), vneg_s16(n));
    uint16x4_t  m = vsub_u16(v, d);
    n = vadd_s16(vdup_n_s16(3), n);
    m = vshl_u16(m, n);
    uint16x4_t  t = vcgt_u16(v, vdup_n_u16(0));
    m = vand_u16(m, t);
    e = vand_u16(e, t);
    e = vshl_n_u16(e, 10);
    e = vorr_u16(m, e);
    return  vreinterpret_f16_u16(e);
#endif
}

INLINE(Vdhf,VWBI_CVHF) (Vwbi x)
{
    float32x2_t m = vdup_n_f32(VWBI_ASTM(x));
    int8x8_t    i = vreinterpret_s8_f32(m);
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvt_f16_s16(vget_low_s16(vmovl_s8(i)));
#else
    int16x4_t   h = vget_low_s16(vmovl_s8(i));
    int32x4_t   w = vmovl_s16(h);
    return  vcvt_f16_f32(vcvtq_f32_s32(w));
#endif
}

INLINE(Vdhf,VWBC_CVHF) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVHF(VWBC_ASBI(x));
#else
    return  VWBU_CVHF(VWBC_ASBU(x));
#endif
}


INLINE(Vwhf,VWHU_CVHF) (Vwhu x)
{
    float32x2_t w = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  h = vreinterpret_u16_f32(w);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t f = vcvt_f16_u16(h);
    w = vreinterpret_f32_f16(f);
    return  WHF_ASTV(vget_lane_f32(w, 0));
#else
    uint16x4_t  z = vclz_u16(h);
    uint16x4_t  e = vsub_u16(vdup_n_u16(22), z);
    int16x4_t   n = vreinterpret_s16_u16(z);
    uint16x4_t  d = vshl_u16(vdup_n_u16(128), vneg_s16(n));
    uint16x4_t  m = vsub_u16(h, d);
    n = vadd_s16(vdup_n_s16(3), n);
    m = vshl_u16(m, n);
    uint16x4_t  t = vcgt_u16(h, vdup_n_u16(0));
    m = vand_u16(m, t);
    e = vand_u16(e, t);
    e = vshl_n_u16(e, 10);
    h = vorr_u16(m, e);
    w = vreinterpret_f32_u16(h);
    return  WHF_ASTV(vget_lane_f32(w, 0));
#endif
}

INLINE(Vwhf,VWHI_CVHF) (Vwhi x)
{
    float32x2_t w = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   h = vreinterpret_s16_f32(w);
    float16x4_t f;
#if defined(SPC_ARM_FP16_SIMD)
    f = vcvt_f16_s16(h);
#else
    f = vcvt_f16_f32(vcvtq_f32_s32(vmovl_s16(h)));
#endif
    w = vreinterpret_f32_f16(f);
    return  WHF_ASTV(vget_lane_f32(w, 0));
}

INLINE(Vwhf,VWHF_CVHF) (Vwhf x) {return x;}


INLINE(Vqhf,VDBU_CVHF) (Vdbu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_f16_u16(vmovl_u8(x));
#else
    uint16x8_t  zh = vmovl_u8(x);
    uint32x4_t  zl = vmovl_u16(vget_low_u16(zh));
    uint32x4_t  zr = vmovl_u16(vget_high_u16(zh));
    float32x4_t fl = vcvtq_f32_u32(zl);
    float32x4_t fr = vcvtq_f32_u32(zr);
    return  vcombine_f16(vcvt_f16_f32(fl), vcvt_f16_f32(fr));
#endif
}

INLINE(Vqhf,VDBI_CVHF) (Vdbi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_f16_s16(vmovl_s8(x));
#else
    int16x8_t   z = vmovl_s8(x);
    int32x4_t   p = vmovl_s16(vget_low_s16(z));
    int32x4_t   q = vmovl_s16(vget_high_s16(z));
    float32x4_t l = vcvtq_f32_s32(p);
    float32x4_t r = vcvtq_f32_s32(q);
    return  vreinterpretq_f16_u16(
        vcombine_u16(
            vreinterpret_u16_f16(vcvt_f16_f32(l)),
            vreinterpret_u16_f16(vcvt_f16_f32(r))
        )
    );
#endif
}

INLINE(Vqhf,VDBC_CVHF) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_CVHF(VDBC_ASBI(x));
#else
    return  VDBU_CVHF(VDBC_ASBU(x));
#endif
}


INLINE(Vdhf,VDHU_CVHF) (Vdhu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvt_f16_u16(x);
#else
    return  vcvt_f16_f32(vcvtq_f32_u32(vmovl_u16(x)));
#endif
}

INLINE(Vdhf,VDHI_CVHF) (Vdhi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvt_f16_s16(x);
#else
    return  vcvt_f16_f32(vcvtq_f32_s32(vmovl_s16(x)));
#endif
}

INLINE(Vdhf,VDHF_CVHF) (Vdhf x) {return x;}


INLINE(Vwhf,VDWU_CVHF) (Vdwu x)
{
    uint8x8_t   b = vreinterpret_u8_u32(x);
    b = vtbl1_u8(
        b,
        vcreate_u8(0xffffffff05040100ULL)
    );
    uint16x4_t  h = vreinterpret_u16_u8(b);
    float16x4_t f;
#if defined(SPC_ARM_FP16_SIMD)
    f = vcvt_f16_u16(h);
#else
    f = vcvt_f16_f32(vcvtq_f32_u32(vmovl_u16(h)));
#endif
    float32x2_t m = vreinterpret_f32_f16(f);
    return  WHF_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwhf,VDWI_CVHF) (Vdwi x)
{
    uint8x8_t   b = vreinterpret_u8_s32(x);
    b = vtbl1_u8(
        b,
        vcreate_u8(0xffffffff05040100ULL)
    );
    int16x4_t  h = vreinterpret_s16_u8(b);
    float16x4_t f;
#if defined(SPC_ARM_FP16_SIMD)
    f = vcvt_f16_s16(h);
#else
    f = vcvt_f16_f32(vcvtq_f32_s32(vmovl_s16(h)));
#endif
    float32x2_t m = vreinterpret_f32_f16(f);
    return  WHF_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwhf,VDWF_CVHF) (Vdwf x)
{
    float32x4_t m = vcombine_f32(x, x);
    float16x4_t f = vcvt_f16_f32(m);
    x = vreinterpret_f32_f16(f);
    return  WHF_ASTV(vget_lane_f32(x, 0));
}

INLINE(Vqhf,VQHU_CVHF) (Vqhu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_f16_u16(x);
#else
    uint32x4_t  zl = vmovl_u16(vget_low_u16(x));
    uint32x4_t  zr = vmovl_u16(vget_high_u16(x));
    float32x4_t ml = vcvtq_f32_u32(zl);
    float32x4_t mr = vcvtq_f32_u32(zr);
    float16x4_t fl = vcvt_f16_f32(ml);
    float16x4_t fr = vcvt_f16_f32(mr);
    return  vcombine_f16(fl, fr);
#endif
}

INLINE(Vqhf,VQHI_CVHF) (Vqhi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_f16_s16(x);
#else
    int32x4_t   zl = vmovl_s16(vget_low_s16(x));
    int32x4_t   zr = vmovl_s16(vget_high_s16(x));
    float32x4_t ml = vcvtq_f32_s32(zl);
    float32x4_t mr = vcvtq_f32_s32(zr);
    float16x4_t fl = vcvt_f16_f32(ml);
    float16x4_t fr = vcvt_f16_f32(mr);
    return  vcombine_f16(fl, fr);
#endif
}


INLINE(Vqhf,VQHF_CVHF) (Vqhf x) {return x;}

INLINE(Vdhf,VQWU_CVHF) (Vqwu x) {return vcvt_f16_f32(vcvtq_f32_u32(x));}
INLINE(Vdhf,VQWI_CVHF) (Vqwi x) {return vcvt_f16_f32(vcvtq_f32_s32(x));}
INLINE(Vdhf,VQWF_CVHF) (Vqwf x) {return vcvt_f16_f32(x);}

INLINE(Vwhf,VQDU_CVHF) (Vqdu x) {return VDWU_CVHF(vmovn_u64(x));}
INLINE(Vwhf,VQDI_CVHF) (Vqdi x) {return VDWI_CVHF(vmovn_s64(x));}
INLINE(Vwhf,VQDF_CVHF) (Vqdf x)
{
//  TODO: make sure this actually works...
    uint64x2_t  du = vreinterpretq_u64_f64(x);
    uint64x2_t  dm = vandq_u64(vdupq_n_u64(0x000fffffffffffffULL), du);
    uint64x2_t  de = vandq_u64(vdupq_n_u64(0x7ff0000000000000ULL), du);
    uint64x2_t  ds = vtstq_u64(vdupq_n_u64(0x8000000000000000ULL), du);

    uint32x2_t  e = vmovn_u64(vshrq_n_u64(de, 52));
    uint32x2_t  m = vdup_n_u32(1008);
    uint32x2_t  c = vcgt_u32(e, m);

    // subtract 1008 from e
    e = vsub_u32(e, m);

    // set e=0 if !(e > 1008)
    e = vand_u32(e, c);

    // set e=31 if (e > 31)
    e = vmin_u32(e, vdup_n_u32(31));

    uint8x8_t   b = vqtbl1_u8(
        vreinterpretq_u8_u64(ds),
        vcreate_u8(0xffffffff09080100ULL)
    );
    uint16x4_t  f = vreinterpret_u16_u8(b);

    // set dst.Sign
    f = vand_u16(vdup_n_u16(0x8000), f);

    b = vtbl1_u8(
        vreinterpret_u8_u16(e),
        vcreate_u8(0xffffffff05040100ULL)
    );

    // set dst.Expo
    f = vorr_u16(f, vshl_n_u16(vreinterpret_u16_u8(b), 10));

    b = vqtbl1_u8(
        vreinterpretq_u8_u64(vshrq_n_u64(dm, 42)),
        vcreate_u8(0xffffffff09080100ULL)
    );
    // set dst.Mant
    f = vorr_u16(f, vreinterpret_u16_u8(b));

    float32x2_t v = vreinterpret_f32_u16(f);
    return  WHF_ASTV(vget_lane_f32(v, 0));
}

#if 0 // _LEAVE_ARM_CVHF
}
#endif

#if 0 // _ENTER_ARM_CVWU
{
#endif

INLINE(uint32_t,FLT16_CVWU) (flt16_t x) {return x;}
INLINE(uint32_t,  FLT_CVWU)   (float x) {return x;}
INLINE(uint32_t,  DBL_CVWU)  (double x) {return x;}
INLINE(uint32_t, cvwuqf) (QUAD_FTYPE x) {return x;}

#define     WHU_CVWU(X)         \
vreinterpret_u32_u16(           \
    vzip1_u16(                  \
        vreinterpret_u16_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        ),                      \
        vdup_n_u16(0)           \
    )                           \
)

INLINE(Vqwu,VWBU_CVWU) (Vwbu x)
{
    float32x2_t m = vdup_n_f32(VWBU_ASTM(x));
    uint8x8_t   t = vreinterpret_u8_f32(m);
    return  vcombine_u8(
        vtbl1_u8(
            t,
            vcreate_u8(
                // {0,-1,-1,-1,  4,-1,-1,-1}
                UINT64_C(0xffffff04ffffff00)
            )
        ),
        vtbl1_u8(
            t,
            vcreate_u8(
                // {8,-1,-1,-1,  12,-1,-1,-1}
                UINT64_C(0xffffff0cffffff08)
            )
        )
    );
}

INLINE(Vqwu,VWBI_CVWU) (Vwbi x)
{
    float32x2_t dwf = vdup_n_f32(VWBI_ASTM(x));
    int8x8_t    dbi = vreinterpret_s8_f32(dwf);
    int16x8_t   qhi = vmovl_s8(dbi);
    int32x4_t   qwi = vmovl_s16(vget_low_s16(qhi));
    return  vreinterpretq_u32_s32(qwi);
}

INLINE(Vqwu,VWBC_CVWU) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVWU(VWBC_ASBI(x));
#else
    return  VWBU_CVWU(VWBC_ASBU(x));
#endif
}


INLINE(Vdwu,VWHU_CVWU) (Vwhu x)
{
    float32x2_t mdwf = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  mdhu = vreinterpret_u16_f32(mdwf);
    uint32x4_t  mqwu = vmovl_u16(mdhu);
    return  vget_low_u32(mqwu);
}

INLINE(Vdwu,VWHI_CVWU) (Vwhi x)
{
    float32x2_t dwf = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   dhi = vreinterpret_s16_f32(dwf);
    int32x4_t   qwi = vmovl_s16(dhi);
    uint32x4_t  qwu = vreinterpretq_u32_s32(qwi);
    return  vget_low_u32(qwu);
}

INLINE(Vdwu,VWHF_CVWU) (Vwhf x)
{
    float32x2_t dwf = vdup_n_f32(VWHF_ASTM(x));
    float16x4_t dhf = vreinterpret_f16_f32(dwf);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_low_u32(vmovl_u16(vcvt_u16_f16(dhf)));
#else
    return  vget_low_u32(vcvtq_u32_f32(vcvt_f32_f16(dhf)));
#endif
}



INLINE(Vwwu,VWWI_CVWU) (Vwwi x) {return VWWI_ASTU(x);}
INLINE(Vwwu,VWWF_CVWU) (Vwwf x)
{
    return UINT_ASTV(vcvts_u32_f32(VWWF_ASTM(x)));
}

INLINE(Vqwu,VDHU_CVWU) (Vdhu x) {return vmovl_u16(x);}
INLINE(Vqwu,VDHI_CVWU) (Vdhi x) {return VQWI_ASTU(vmovl_s16(x));}
INLINE(Vqwu,VDHF_CVWU) (Vdhf x)
{

#if defined(SPC_ARM_FP16_SIMD)
    return  vreinterpretq_u32_s32(vmovl_s16(vcvt_s16_f16(x)));
#else
    return  vreinterpretq_u32_s32(vcvtq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vdwu,VDWI_CVWU) (Vdwi x) {return vreinterpret_u32_s32(x);}
INLINE(Vdwu,VDWF_CVWU) (Vdwf x)
{
    return  vreinterpret_u32_s32(vcvt_s32_f32(x));
}

INLINE(Vwwu,VDDU_CVWU) (Vddu x)
{
    return  UINT_ASTV(vget_lane_u64(x, 0));
}

INLINE(Vwwu,VDDI_CVWU) (Vddi x)
{
    return  UINT_ASTV(vget_lane_s64(x, 0));
}

INLINE(Vwwu,VDDF_CVWU) (Vddf x)
{
    return  UINT_ASTV(vget_lane_f64(x, 0));
}


INLINE(Vqwu,VQWI_CVWU) (Vqwi x) {return vreinterpretq_u32_s32(x);}
INLINE(Vqwu,VQWF_CVWU) (Vqwf x) {return vcvtq_u32_f32(x);}

INLINE(Vdwu,VQDU_CVWU) (Vqdu x) {return vmovn_u64(x);}
INLINE(Vdwu,VQDI_CVWU) (Vqdi x) {return vreinterpret_u32_s32(vmovn_s64(x));}
INLINE(Vdwu,VQDF_CVWU) (Vqdu x) {return vmovn_u64(vcvtq_u64_f64(x));}

INLINE(Vwwu,VQQU_CVWU) (Vqqu x)
{
    float32x4_t m = vreinterpretq_f32_u64(x.V0);
    return  ((Vwwu){vgetq_lane_f32(m, 0)});
}

INLINE(Vwwu,VQQI_CVWU) (Vqqi x)
{
    float32x4_t m = vreinterpretq_f32_s64(x.V0);
    return  ((Vwwu){vgetq_lane_f32(m, 0)});
}

INLINE(Vwwu,VQQF_CVWU) (Vqqf x)
{
    QUAD_TYPE m;
    m.W0.U = x.V0;
    return ((Vwwu){m.W0.F});
}

#if 0 // _LEAVE_ARM_CVWU
}
#endif

#if 0 // _ENTER_ARM_CVWZ
{
#endif

INLINE(uint32_t,  BOOL_CVWZ)   (_Bool x) {return x;}
INLINE(uint32_t, UCHAR_CVWZ)   (uchar x) {return x;}
INLINE(uint32_t,  CHAR_CVWZ)    (char x)
{
#if CHAR_MIN
    return  vqmovunh_s16(x);
#else
    return  x;
#endif
}
INLINE(uint32_t, SCHAR_CVWZ)   (schar x) {return vqmovunh_s16(x);}
INLINE(uint32_t, USHRT_CVWZ)  (ushort x) {return x;}
INLINE(uint32_t,  SHRT_CVWZ)   (short x) {return vqmovuns_s32(x);}
INLINE(uint32_t,  UINT_CVWZ)    (uint x) {return x;}
INLINE(uint32_t,   INT_CVWZ)     (int x) {return vqmovund_s64(x);}

INLINE(uint32_t, ULONG_CVWZ)   (ulong x)
{
#if DWRD_NLONG == 2
    return  x;
#else
    return  vqmovnd_u64(x);
#endif
}

INLINE(uint32_t,  LONG_CVWZ)    (long x) {return vqmovund_s64(x);}
INLINE(uint32_t,ULLONG_CVWZ)  (ullong x)
{
#if QUAD_NLLONG == 2
    return  vqmovnd_u64(x);
#else
#endif
}

INLINE(uint32_t, LLONG_CVWZ)   (llong x)
{
#if QUAD_NLLONG == 2
    return  vqmovund_s64(x);
#else
#endif
}

INLINE(uint32_t, FLT16_CVWZ) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvth_u16_f16(x);
#else
    return  vcvts_u32_f32(x);
#endif
}

INLINE(uint32_t,   FLT_CVWZ)   (float x) {return vcvts_u32_f32(x);}
INLINE(uint32_t,   DBL_CVWZ)  (double x)
{
    return  vqmovnd_u64(vcvtd_u64_f64(x));
}

INLINE(uint32_t,cvwzqu) (QUAD_UTYPE x)
{
    return (x >= UINT32_MAX) ? UINT32_MAX : x;
}

INLINE(uint32_t,cvwzqi) (QUAD_ITYPE x)
{
    if (x >= UINT32_MAX) return UINT32_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE(uint32_t,cvwzqf) (QUAD_FTYPE x)
{
    if (x >= UINT32_MAX) return UINT32_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE( Wwu,       WWI_CVWZ) ( Wwi m)
{
    float32x2_t f = vdup_n_f32(m);
    int64x2_t   q = vmovl_s32(vreinterpret_s32_f32(f));
    uint32x2_t  d = vqmovun_s64(q);
    f = vreinterpret_f32_u32(d);
    return vget_lane_f32(f, 0);
}

INLINE( Wwu, WWF_CVWZ) ( Wwf m)
{
    float32x2_t d = vset_lane_f32(m, d, 0);
    uint32x2_t  u = vcvt_u32_f32(d);
    d = vreinterpret_f32_u32(u);
    return vget_lane_f32(d, 0);
}

INLINE(Vqwu,VWBU_CVWZ) (Vwbu x) {return VWBU_CVWU(x);}
INLINE(Vqwu,VWBI_CVWZ) (Vwbi x)
{
/*  This is much simpler than it looks.
*/
    float32x2_t m = vdup_n_f32(VWBI_ASTM(x));
    uint8x8_t   b = vreinterpret_u8_f32(m);
    uint8x8_t   s = vtst_u8(b, vdup_n_u8(0x80));
    uint8x16_t  t = vcombine_u8(b, s);
    uint8x16_t  l = vqtbl1q_u8(
        t,
        vcombine_u8(
            vcreate_u8(UINT64_C(0x0000000000000008)),
            vcreate_u8(UINT64_C(0x0101010101010109))
        )
    );
    uint8x16_t  r = vqtbl1q_u8(
        t,
        vcombine_u8(
            vcreate_u8(UINT64_C(0x020202020202020a)),
            vcreate_u8(UINT64_C(0x030303030303030b))
        )
    );
    int64x2_t   lq = vreinterpretq_s64_u8(l);
    uint32x2_t  lo = vqmovun_s64(lq);
    int64x2_t   rq = vreinterpretq_s64_u8(r);
    uint32x2_t  hi = vqmovun_s64(rq);
    return vcombine_u32(lo, hi);
}

INLINE(Vqwu,VWBC_CVWZ) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVWZ(VWBC_ASBI(x));
#else
    return  VWBU_CVWZ(VWBC_ASBU(x));
#endif
}

INLINE(Vdwu,VWHU_CVWZ) (Vwhu x) {return VWHU_CVWU(x);}
INLINE(Vdwu,VWHI_CVWZ) (Vwhi x)
{
    float32x2_t dwf = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   dhi = vreinterpret_s16_f32(dwf);
    int32x4_t   qwi = vmovl_s16(dhi);
    int64x2_t   qdi = vmovl_s32(vget_low_s32(qwi));
    return  vqmovun_s64(qdi);
}
INLINE(Vdwu,VWHF_CVWZ) (Vwhf x) {return VWHF_CVWU(x);}

INLINE(Vwwu,VWWU_CVWZ) (Vwwu x) {return x;}
INLINE(Vwwu,VWWI_CVWZ) (Vwwi v)
{
#define     VWWI_CVWZ(V)    WWU_ASTV(WWI_CVWZ(VWWI_ASTM(V)))
    return  VWWI_CVWZ(v);
}

INLINE(Vwwu,VWWF_CVWZ) (Vwwf v)
{
#define     VWWF_CVWZ(V)    WWU_ASTV(WWF_CVWZ(VWWF_ASTM(V)))
    return  VWWF_CVWZ(v);
}

INLINE(Vqwu,VDHU_CVWZ) (Vdhu x) {return VDHU_CVWU(x);}
INLINE(Vqwu,VDHI_CVWZ) (Vdhi x)
{
    int32x4_t m = vmovl_s16(x);
    int64x2_t l = vmovl_s32(vget_low_s32(m));
    int64x2_t r = vmovl_s32(vget_high_s32(m));
    return  vcombine_u32(
        vqmovun_s64(l),
        vqmovun_s64(r)
    );
}

INLINE(Vqwu,VDHF_CVWZ) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmovl_u16(vcvt_u16_f16(x));
#else
    return  vcvtq_u32_f32(vcvt_f32_f16(x));
#endif
}

INLINE(Vdwu,VDWU_CVWZ) (Vdwu x) {return x;}
INLINE(Vdwu,VDWI_CVWZ) (Vdwi x) {return vqmovun_s64(vmovl_s32(x));}
INLINE(Vdwu,VDWF_CVWZ) (Vdwf x) {return vcvt_u32_f32(x);}

INLINE(Vwwu,VDDU_CVWZ) (Vddu v)
{
#define     DDU_CVWZ(V)     \
vget_lane_f32(              \
    vreinterpret_f32_u32(   \
        vqmovn_u64(         \
            vcombine_u64(   \
                V,          \
                VDDU_VOID   \
            )               \
        )                   \
    ),                      \
    0                   \
)

#define     VDDU_CVWZ(V)    WWU_ASTV(DDU_CVWZ(V))
    return  VDDU_CVWZ(v);
}

INLINE(Vwwu,VDDI_CVWZ) (Vddi v)
{
#define     DDI_CVWZ(M)     \
vget_lane_f32(              \
    vreinterpret_f32_u32(   \
        vqmovun_s64(        \
            vcombine_s64(   \
                M,          \
                VDDI_VOID   \
            )               \
        )                   \
    ),                      \
    0                   \
)

#define     VDDI_CVWZ(V)    WWU_ASTV(DDI_CVWZ(V))
    return  VDDI_CVWZ(v);
}

INLINE(Vwwu,VDDF_CVWZ) (Vddf x)
{
    uint64x1_t  m = vcvt_u64_f64(x);
    return UINT_ASTV(vqmovnd_u64(vget_lane_u64(m, 0)));
}
INLINE(Vqwu,VQWU_CVWZ) (Vqwu x) {return x;}
INLINE(Vqwu,VQWI_CVWZ) (Vqwi x)
{
    return vcombine_s32(
        vqmovun_s64(vmovl_s32(vget_low_s32(x))),
        vqmovun_s64(vmovl_s32(vget_high_s32(x)))
    );
}
INLINE(Vqwu,VQWF_CVWZ) (Vqwf x) {return vcvtq_u32_f32(x);}

INLINE(Vdwu,VQDU_CVWZ) (Vqdu x) {return vqmovn_u64(x);}
INLINE(Vdwu,VQDI_CVWZ) (Vqdi x) {return vqmovun_s64(x);}
INLINE(Vdwu,VQDF_CVWZ) (Vqdf x) {return vqmovn_u64(vcvtq_u64_f64(x));}

#if 0 // _LEAVE_ARM_CVWZ
}
#endif

#if 0 // _ENTER_ARM_CVWI
{
#endif

INLINE(int32_t,FLT16_CVWI) (flt16_t x) {return x;}
INLINE(int32_t,  FLT_CVWI)   (float x) {return x;}
INLINE(int32_t,  DBL_CVWI)  (double x) {return x;}
INLINE(int32_t, cvwiqf) (QUAD_FTYPE x) {return x;}


#define     WHU_CVWI(X)         \
vreinterpret_s32_u16(           \
    vzip1_u16(                  \
        vreinterpret_u16_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        ),                      \
        vdup_n_u16(0)           \
    )                           \
)


#define     WHI_CVWI(X)         \
vget_low_s32(                   \
    vmovl_s16(                  \
        vreinterpret_s16_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        )                       \
    )                           \
)

INLINE(Vqwi,VWBU_CVWI) (Vwbu x)
{
/*  TODO: test the perf difference between VWBU_CVWU and
    VWBU_CVWI.
*/
    float32x2_t dwf = vdup_n_f32(VWBU_ASTM(x));
    uint8x8_t   dbu = vreinterpret_u8_f32(dwf);
    uint16x8_t  qhu = vmovl_u8(dbu);
    uint16x4_t  dhu = vget_low_u16(qhu);
    uint32x4_t  qwu = vmovl_u16(dhu);
    return  vreinterpretq_s32_u32(qwu);
}

INLINE(Vqwi,VWBI_CVWI) (Vwbi x)
{
    float32x2_t dwf = vdup_n_f32(VWBI_ASTM(x));
    int8x8_t    dbi = vreinterpret_s8_f32(dwf);
    int16x8_t   qhi = vmovl_s8(dbi);
    return  vmovl_s16(vget_low_s16(qhi));
}

INLINE(Vqwi,VWBC_CVWI) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVWI(VWBC_ASBI(x));
#else
    return  VWBU_CVWI(VWBC_ASBU(x));
#endif
}


INLINE(Vdwi,VWHU_CVWI) (Vwhu x)
{
    float32x2_t dwf = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  dhu = vreinterpret_u16_f32(dwf);
    uint32x4_t  qwu = vmovl_u16(dhu);
    return  vreinterpret_s32_u32(vget_low_u32(qwu));
}

INLINE(Vdwi,VWHI_CVWI) (Vwhi x)
{
    float32x2_t dwf = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   dhi = vreinterpret_s16_f32(dwf);
    int32x4_t   qwi = vmovl_s16(dhi);
    return  vget_low_u32(qwi);
}

INLINE(Vdwi,VWHF_CVWI) (Vwhf x)
{
    float32x2_t dwf = vdup_n_f32(VWHF_ASTM(x));
    float16x4_t dhf = vreinterpret_f16_f32(dwf);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_low_s32(vmovl_s16(vcvt_s16_f16(dhf)));
#else
    return  vget_low_s32(vcvtq_s32_f32(vcvt_f32_f16(dhf)));
#endif
}


INLINE(Vwwi,VWWU_CVWI) (Vwwu x) {return VWWU_ASTI(x);}

INLINE(Vwwi,VWWF_CVWI) (Vwwf x)
{
    return  INT_ASTV(vcvts_s32_f32(VWWF_ASTM(x)));
}

INLINE(Vqwi,VDHU_CVWI) (Vdhu x) {return VQWU_ASTI(vmovl_u16(x));}
INLINE(Vqwi,VDHI_CVWI) (Vdhi x) {return           vmovl_s16(x);}
INLINE(Vqwi,VDHF_CVWI) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmovl_s16(vcvt_s16_f16(x));
#else
    return  vcvtq_u32_f32(vcvt_f32_f16(x));
#endif
}

INLINE(Vdwi,VDWU_CVWI) (Vdwu x) {return vreinterpret_s32_u32(x);}

INLINE(Vdwi,VDWF_CVWI) (Vdwf x) {return vcvt_s32_f32(x);}

INLINE(Vwwi,VDDU_CVWI) (Vddu x)
{
    return  INT_ASTV(vget_lane_u64(x, 0));
}

INLINE(Vwwi,VDDI_CVWI) (Vddi x)
{
    return  INT_ASTV(vget_lane_s64(x, 0));
}

INLINE(Vwwi,VDDF_CVWI) (Vddf x)
{
    return  INT_ASTV(vget_lane_f64(x, 0));
}

INLINE(Vqwi,VQWU_CVWI) (Vqwu x) {return vreinterpretq_s32_u32(x);}
INLINE(Vqwi,VQWF_CVWI) (Vqwf x) {return vcvtq_u32_f32(x);}

INLINE(Vdwi,VQDU_CVWI) (Vqdu x) {return vreinterpret_s32_u32(vmovn_u64(x));}
INLINE(Vdwi,VQDI_CVWI) (Vqdi x) {return                      vmovn_s64(x);}
INLINE(Vdwi,VQDF_CVWI) (Vqdu x) {return vmovn_s64(vcvtq_s64_f64(x));}

INLINE(Vwwi,VQQU_CVWI) (Vqqu x)
{
    QUAD_VTYPE q = {.Q.U=x};
    Vwwi v = {vgetq_lane_f32(q.W.F, 0)};
    return v;
}

INLINE(Vwwi,VQQI_CVWI) (Vqqi x)
{
    QUAD_VTYPE q = {.Q.I=x};
    Vwwi v = {vgetq_lane_f32(q.W.F, 0)};
    return v;
}

#if 0 // _LEAVE_ARM_CVWI
}
#endif

#if 0 // _ENTER_ARM_CVWS
{
#endif

INLINE(int32_t,  BOOL_CVWS)   (_Bool x) {return x;}
INLINE(int32_t, UCHAR_CVWS)   (uchar x) {return x;}
INLINE(int32_t, SCHAR_CVWS)   (schar x) {return x;}
INLINE(int32_t,  CHAR_CVWS)    (char x) {return x;}
INLINE(int32_t, USHRT_CVWS)  (ushort x) {return x;}
INLINE(int32_t,  SHRT_CVWS)   (short x) {return x;}
INLINE(int32_t,  UINT_CVWS)    (uint x)
{
    uint64_t m = vtstd_u64((UINT32_MAX-INT32_MAX), x)>>33;
    return  (x&INT32_MAX)|m;
}
INLINE(int32_t,   INT_CVWS)     (int x) {return x;}
INLINE(int32_t, ULONG_CVWS)   (ulong x) {return UINT_CVWS(x);}
INLINE(int32_t,  LONG_CVWS)    (long x)
{
#if DWRD_NLONG == 2
    return  x;
#else
    return  vqmovnd_s64(x);
#endif
}

INLINE(int32_t,ULLONG_CVWS)  (ullong x)
{
    uint64_t m = vtstd_u64((UINT64_MAX-INT32_MAX), x)>>33;
    return  (x&INT32_MAX)|m;
}

INLINE(int32_t, LLONG_CVWS)   (llong x)
{
#if QUAD_NLLONG == 2
    return  vqmovnd_s64(x);
#else
// TODO
#endif
}

INLINE(int32_t, FLT16_CVWS)  (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvth_s16_f16(x);
#else
    return  vcvts_s32_f32(x);
#endif
}

INLINE(int32_t,   FLT_CVWS) (float x) {return vcvts_s32_f32(x);}
INLINE(int32_t,   DBL_CVWS) (double x)
{
    return vqmovnd_s64(vcvtd_s64_f64(x));
}

INLINE(int32_t,cvwsqu) (QUAD_UTYPE x)
{
    return (x >= INT32_MAX) ? INT32_MAX : x;
}

INLINE(int32_t,cvwsqi) (QUAD_ITYPE x)
{
    if (x >= INT32_MAX) return INT32_MAX;
    if (x <= INT32_MIN) return INT32_MIN;
    return x;
}

INLINE(int32_t,cvwsqf) (QUAD_FTYPE x)
{
    if (x >= INT32_MAX) return INT32_MAX;
    if (x <= INT32_MIN) return INT32_MIN;
    return x;
}


INLINE(Vqwi,VWBU_CVWS) (Vwbu x) {return VWBU_CVWI(x);}
INLINE(Vqwi,VWBI_CVWS) (Vwbi x) {return VWBI_CVWI(x);}
INLINE(Vqwi,VWBC_CVWS) (Vwbc x) {return VWBC_CVWI(x);}

INLINE(Vdwi,VWHU_CVWS) (Vwhu x) {return VWHU_CVWI(x);}
INLINE(Vdwi,VWHI_CVWS) (Vwhi x) {return VWHI_CVWI(x);}
INLINE(Vdwi,VWHF_CVWS) (Vwhf x) {return VWHF_CVWI(x);}

INLINE(Vwwi,VWWU_CVWS) (Vwwu x)
{
    float32x2_t dwf = vdup_n_f32(VWWU_ASTM(x));
    uint32x2_t  dwu = vreinterpret_u32_f32(dwf);
    uint32x2_t  tmp = vtst_u32(vdup_n_u32(0x80000000u), dwu);
    tmp = vorr_u32(dwu, tmp);
    int32x2_t   dwi = vand_s32(
        vdup_n_s32(INT32_MAX),
        vreinterpret_s32_u32(tmp)
    );
    dwf = vreinterpret_f32_s32(dwi);
    return  WWI_ASTV(vget_lane_f32(dwf, 0));
}

INLINE(Vwwi,VWWI_CVWS) (Vwwi x) {return x;}
INLINE(Vwwi,VWWF_CVWS) (Vwwf x) {return VWWF_CVWI(x);}

INLINE(Vqwi,VDHU_CVWS) (Vdhu x) {return VDHU_CVWI(x);}
INLINE(Vqwi,VDHI_CVWS) (Vdhi x) {return VDHU_CVWI(x);}
INLINE(Vqwi,VDHF_CVWS) (Vdhf x) {return VDHF_CVWI(x);}


INLINE(Vdwi,VDWU_CVWS) (Vdwu x)
{
    uint32x2_t  tmp = vtst_u32(vdup_n_u32(0x80000000u), x);
    tmp = vorr_u32(x, tmp);
    return vand_s32(
        vdup_n_s32(INT32_MAX),
        vreinterpret_s32_u32(tmp)
    );
}

INLINE(Vdwi,VDWI_CVWS) (Vdwi x) {return x;}
INLINE(Vdwi,VDWF_CVWS) (Vdwf x) {return vcvt_s32_f32(x);}

INLINE(Vwwi,VDDU_CVWS) (Vddu x)
{
    uint64x1_t m = vtst_u64(
        vdup_n_u64(UINT64_MAX-INT32_MAX),
        x
    );
    x = vorr_u64(x, m);
    x = vand_u64(x, vdup_n_u64(INT32_MAX));
    return  INT_ASTV(vget_lane_u64(x, 0));
}

INLINE(Vwwi,VDDI_CVWS) (Vddi x)
{
    return  INT_ASTV(
        vqmovnd_s64(vget_lane_s64(x, 0))
    );
}

INLINE(Vwwi,VDDF_CVWS) (Vddf x)
{
    double  f = vget_lane_f64(x, 0);
    int64_t i = vcvtd_s64_f64(f);
    return  INT_ASTV(vqmovnd_s64(i));
}


INLINE(Vqwi,VQWU_CVWS) (Vqwu x)
{
    uint32x4_t  tmp = vtstq_u32(vdupq_n_u32(0x80000000u), x);
    tmp = vorrq_u32(x, tmp);
    return vandq_s32(
        vdupq_n_s32(INT32_MAX),
        vreinterpretq_s32_u32(tmp)
    );
}

INLINE(Vqwi,VQWI_CVWS) (Vqwi x) {return x;}
INLINE(Vqwi,VQWF_CVWS) (Vqwf x) {return vcvtq_s32_f32(x);}

INLINE(Vdwi,VQDU_CVWS) (Vqdu x) {return vqmovn_u64(x);}
INLINE(Vdwi,VQDI_CVWS) (Vqdi x) {return vqmovn_s64(x);}
INLINE(Vdwi,VQDF_CVWS) (Vqdf x) {return vqmovn_s64(vcvtq_s64_f64(x));}


#if 0 // _LEAVE_ARM_CVWS
}
#endif

#if 0 // _ENTER_ARM_CVWF
{
#endif
/*  Convert each operand element to an IEEE 754 single
    precision floating point value.
*/
#if 0
INLINE(float,  BOOL_CVWF)      (_Bool x) {return x;}
INLINE(float, UCHAR_CVWF)      (uchar x) {return x;}
INLINE(float, SCHAR_CVWF)      (schar x) {return x;}
INLINE(float,  CHAR_CVWF)       (char x) {return x;}
INLINE(float, USHRT_CVWF)     (ushort x) {return x;}
INLINE(float,  SHRT_CVWF)      (short x) {return x;}
INLINE(float,  UINT_CVWF)       (uint x) {return x;}
INLINE(float,   INT_CVWF)        (int x) {return x;}
INLINE(float, ULONG_CVWF)      (ulong x) {return x;}
INLINE(float,  LONG_CVWF)       (long x) {return x;}
INLINE(float,ULLONG_CVWF)     (ullong x) {return x;}
INLINE(float, LLONG_CVWF)      (llong x) {return x;}
INLINE(float, FLT16_CVWF)    (flt16_t x) {return x;}
INLINE(float,   FLT_CVWF)      (float x) {return x;}
INLINE(float,   DBL_CVWF)     (double x) {return x;}
#if QUAD_NLLONG == 2
INLINE(float,cvwfqu) (QUAD_UTYPE x) {return x;}
INLINE(float,cvwfqi) (QUAD_ITYPE x) {return x;}
INLINE(float,cvwfqf) (QUAD_FTYPE x) {return x;}
#endif
#endif


INLINE(Vqwf,VWBU_CVWF) (Vwbu x)
{
    float32x2_t m = vdup_n_f32(VWBU_ASTM(x));
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint8x16_t  q = vcombine_u8(
        vtbl1_u8(d, vcreate_u8(0xffffff01ffffff00ull)),
        vtbl1_u8(d, vcreate_u8(0xffffff03ffffff02ull))
    );
    uint32x4_t  w = vreinterpretq_u32_u8(q);
    return  vcvtq_f32_u32(w);
}

INLINE(Vqwf,VWBI_CVWF) (Vwbi x)
{
    int32x4_t   m = VWBI_CVWI(x);
    return vcvtq_f32_s32(m);
}

INLINE(Vqwf,VWBC_CVWF) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVWF(VWBC_ASBI(x));
#else
    return  VWBU_CVWF(VWBC_ASBU(x));
#endif
}


INLINE(Vdwf,VWHU_CVWF) (Vwhu x)
{
    float32x2_t m = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  u = vreinterpret_u16_f32(m);
    uint32x4_t  q = vmovl_u16(u);
    uint32x2_t  r = vget_low_u32(q);
    return  vcvt_f32_u32(r);
}

INLINE(Vdwf,VWHI_CVWF) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   i = vreinterpret_s16_f32(m);
    int32x4_t   q = vmovl_s16(i);
    int32x2_t   r = vget_low_s32(q);
    return  vcvt_f32_s32(r);
}

INLINE(Vdwf,VWHF_CVWF) (Vwhf x)
{
    float32x2_t m = vdup_n_f32(VWHF_ASTM(x));
    float16x4_t f = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(f);
    return  vget_low_f32(q);
}

INLINE(Vwwf,VWWU_CVWF) (Vwwu x)
{
    float32x2_t m = vdup_n_f32(VWWU_ASTM(x));
    uint32x2_t  z = vreinterpret_u32_f32(m);
    m = vcvt_f32_u32(z);
    return  WWF_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwwf,VWWI_CVWF) (Vwwi x)
{
    float32x2_t m = vdup_n_f32(VWWI_ASTM(x));
    int32x2_t   z = vreinterpret_s32_f32(m);
    m = vcvt_f32_s32(z);
    return  WWF_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwwf,VWWF_CVWF) (Vwwf x) {return x;}

INLINE(Vqwf,VDHU_CVWF) (Vdhu x) {return vcvtq_f32_u32(vmovl_u16(x));}
INLINE(Vqwf,VDHI_CVWF) (Vdhi x) {return vcvtq_f32_s32(vmovl_s16(x));}
INLINE(Vqwf,VDHF_CVWF) (Vdhf x) {return  vcvt_f32_f16(x);}

INLINE(Vdwf,VDWU_CVWF) (Vdwu x) {return vcvt_f32_u32(x);}
INLINE(Vdwf,VDWI_CVWF) (Vdwi x) {return vcvt_f32_s32(x);}
INLINE(Vdwf,VDWF_CVWF) (Vdwf x) {return x;}

INLINE(Vwwf,VDDU_CVWF) (Vddu x) {return WWF_ASTV(vget_lane_u64(x, 0));}
INLINE(Vwwf,VDDI_CVWF) (Vddi x) {return WWF_ASTV(vget_lane_s64(x, 0));}
INLINE(Vwwf,VDDF_CVWF) (Vddf x) {return WWF_ASTV(vget_lane_f64(x, 0));}

INLINE(Vqwf,VQWU_CVWF) (Vqwu x) {return vcvtq_f32_u32(x);}
INLINE(Vqwf,VQWI_CVWF) (Vqwi x) {return vcvtq_f32_u32(x);}
INLINE(Vqwf,VQWF_CVWF) (Vqwf x) {return x;}

INLINE(Vdwf,VQDU_CVWF) (Vqdu x) {return vcvt_f32_f64(vcvtq_f64_u64(x));}
INLINE(Vdwf,VQDI_CVWF) (Vqdi x) {return vcvt_f32_f64(vcvtq_f64_s64(x));}
INLINE(Vdwf,VQDF_CVWF) (Vqdf x) {return vcvt_f32_f64(x);}

#if 0 // _LEAVE_ARM_CVWF
}
#endif

#if 0 // _ENTER_ARM_CVDU
{
#endif

INLINE(uint64_t,FLT16_CVDU) (flt16_t x) {return x;}
INLINE(uint64_t,  FLT_CVDU)   (float x) {return x;}
INLINE(uint64_t,  DBL_CVDU)  (double x) {return x;}
INLINE(uint64_t, cvduqf) (QUAD_FTYPE x) {return x;}

#define     WWU_CVDU(X)         \
vreinterpret_u64_u32(           \
    vzip1_u32(                  \
        vreinterpret_u32_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        ),                      \
        vdup_n_u32(0)           \
    )                           \
)

INLINE(Vqdu,VWHU_CVDU) (Vwhu x)
{
    float32x2_t m = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  h = vreinterpret_u16_f32(m);
    uint32x4_t  w = vmovl_u16(h);
    uint32x2_t  l = vget_low_u32(w);
    return  vmovl_u32(l);
}

INLINE(Vqdu,VWHI_CVDU) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   h = vreinterpret_s16_f32(m);
    int32x4_t   w = vmovl_s16(h);
    int32x2_t   l = vget_low_s32(w);
    int64x2_t   q = vmovl_s32(l);
    return  vreinterpretq_u64_s64(q);
}

INLINE(Vqdu,VWHF_CVDU) (Vwhf x)
{
    float32x2_t m = vdup_n_f32(VWHF_ASTM(x));
    float32x4_t w = vcvt_f32_f16(m);
    int32x4_t   s = vcvtq_s32_f32(w);
    int32x2_t   l = vget_low_s32(s);
    int64x2_t   q = vmovl_s32(l);
    return  vreinterpretq_u64_s64(q);
}


INLINE(Vddu,VWWU_CVDU) (Vwwu x)
{
    return UINT64_ASTV(VWWU_ASTV(x));
}

INLINE(Vddu,VWWI_CVDU) (Vwwi x)
{
    return UINT64_ASTV(VWWI_ASTV(x));
}

INLINE(Vddu,VWWF_CVDU) (Vwwf x)
{
    return UINT64_ASTV(VWWF_ASTM(x));
}


INLINE(Vqdu,VDWU_CVDU) (Vdwu x) {return vmovl_u32(x);}
INLINE(Vqdu,VDWI_CVDU) (Vdwi x) {return VQDI_ASTU(vmovl_s32(x));}
INLINE(Vqdu,VDWF_CVDU) (Vdwf x) {return vcvtq_u64_f64(vcvt_f64_f32(x));}

INLINE(Vddu,VDDI_CVDU) (Vddi x) {return vreinterpret_u64_s64(x);}
INLINE(Vddu,VDDF_CVDU) (Vddf x) {return vcvt_u64_f64(x);}

INLINE(Vqdu,VQDI_CVDU) (Vqdi x) {return vreinterpretq_u64_s64(x);}
INLINE(Vqdu,VQDF_CVDU) (Vqdf x) {return vcvtq_u64_f64(x);}

INLINE(Vddu,VQQU_CVDU) (Vqqu x)
{
    QUAD_VTYPE q = {.Q.U=x};
    return  vget_low_u64(q.D.U);
}

INLINE(Vddu,VQQI_CVDU) (Vqqi x)
{
    QUAD_VTYPE  q = {.Q.I=x};
    return  vget_low_u64(q.D.U);
}


#if 0 // _LEAVE_ARM_CVDU
}
#endif

#if 0 // _ENTER_ARM_CVDI
{
#endif

INLINE(int64_t,FLT16_CVDI) (flt16_t x) {return x;}
INLINE(int64_t,  FLT_CVDI)   (float x) {return x;}
INLINE(int64_t,  DBL_CVDI)  (double x) {return x;}
INLINE(int64_t, cvdiqf) (QUAD_FTYPE x) {return x;}


#define     WWU_CVDI(X)         \
vreinterpret_s64_u32(           \
    vzip1_u32(                  \
        vreinterpret_u32_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        ),                      \
        vdup_n_u32(0)           \
    )                           \
)

#define     WWI_CVDI(X)         \
vget_low_s64(                   \
    vmovl_s32(                  \
        vreinterpret_s32_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        )                       \
    )                           \
)

INLINE(Vqdi,VWHU_CVDI) (Vwhu x)
{
    float32x2_t m = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  h = vreinterpret_u16_f32(m);
    uint32x4_t  w = vmovl_u16(h);
    uint32x2_t  l = vget_low_u32(w);
    int32x2_t   i = vreinterpret_s32_u32(l);
    return  vmovl_s32(l);
}

INLINE(Vqdi,VWHI_CVDI) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   h = vreinterpret_s16_f32(m);
    int32x4_t   w = vmovl_s16(h);
    int32x2_t   l = vget_low_s32(w);
    int64x2_t   q = vmovl_s32(l);
    return  q;
}

INLINE(Vqdi,VWHF_CVDI) (Vwhf x)
{
    float32x2_t m = vdup_n_f32(VWHF_ASTM(x));
    float32x4_t w = vcvt_f32_f16(m);
    int32x4_t   s = vcvtq_s32_f32(w);
    int32x2_t   l = vget_low_s32(s);
    return vmovl_s32(l);
}


INLINE(Vddi,VWWU_CVDI) (Vwwu x)
{
    return INT64_ASTV(VWWU_ASTV(x));
}

INLINE(Vddi,VWWI_CVDI) (Vwwi x)
{
    return INT64_ASTV(VWWI_ASTV(x));
}

INLINE(Vddi,VWWF_CVDI) (Vwwf x)
{
    return INT64_ASTV(VWWF_ASTM(x));
}


INLINE(Vqdi,VDWU_CVDI) (Vdwu x) {return vreinterpretq_s64_u64(vmovl_u32(x));}
INLINE(Vqdi,VDWI_CVDI) (Vdwi x) {return vmovl_s32(x);}
INLINE(Vqdi,VDWF_CVDI) (Vdwf x) {return vcvtq_s64_f64(vcvt_f64_f32(x));}

INLINE(Vddi,VDDU_CVDI) (Vddu x) {return vreinterpret_s64_u64(x);}
INLINE(Vddi,VDDF_CVDI) (Vddf x) {return vcvt_s64_f64(x);}

INLINE(Vqdi,VQDU_CVDI) (Vqdu x) {return vreinterpretq_s64_u64(x);}
INLINE(Vqdi,VQDF_CVDI) (Vqdf x) {return vcvtq_s64_f64(x);}

INLINE(Vddi,VQQU_CVDI) (Vqqu x)
{
    QUAD_VTYPE q = {.Q.U=x};
    return  vget_low_s64(q.D.I);
}

INLINE(Vddi,VQQI_CVDI) (Vqqi x)
{
    QUAD_VTYPE q = {.Q.I=x};
    return  vget_low_s64(q.D.I);
}

#if 0 // _LEAVE_ARM_CVDI
}
#endif

#if 0 // _ENTER_ARM_CVDZ
{
#endif

INLINE(uint64_t,  BOOL_CVDZ)   (_Bool x) {return x;}
INLINE(uint64_t, UCHAR_CVDZ)   (uchar x) {return x;}
INLINE(uint64_t,  CHAR_CVDZ)    (char x)
{
#if CHAR_MIN
    return  vqmovunh_s16(x);
#else
    return  x;
#endif
}

INLINE(uint64_t, SCHAR_CVDZ)   (schar x) {return vqmovunh_s16(x);}
INLINE(uint64_t, USHRT_CVDZ)  (ushort x) {return x;}
INLINE(uint64_t,  SHRT_CVDZ)   (short x) {return vqmovuns_s32(x);}
INLINE(uint64_t,  UINT_CVDZ)    (uint x) {return x;}
INLINE(uint64_t,   INT_CVDZ)     (int x) {return vqmovund_s64(x);}
INLINE(uint64_t, ULONG_CVDZ)   (ulong x) {return x;}
INLINE(uint64_t,  LONG_CVDZ)    (long x)
{
#if DWRD_NLONG == 2
    return  vqmovund_s64(x);
#else
    if (x >= INT64_MAX) return INT64_MAX;
    if (x <= 0) return 0;
    return  x;
#endif
}

INLINE(uint64_t,ULLONG_CVDZ)  (ullong x)
{
#if QUAD_NLLONG == 2
    return  x;
#else
    return  (x >= UINT64_MAX) ? UINT64_MAX : x;
#endif
}

INLINE(uint64_t, LLONG_CVDZ)   (llong x)
{
    if (x >= INT64_MAX) return INT64_MAX;
    if (x <= 0) return 0;
    return  x;
}

#if QUAD_NLLONG == 2

INLINE(uint64_t,cvdzqu)   (QUAD_UTYPE x)
{
    return x > UINT64_MAX ? UINT64_MAX : x;
}

INLINE(uint64_t,cvdzqi)   (QUAD_ITYPE x)
{
    if (x >= UINT64_MAX) return UINT64_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE(uint64_t,cvdzqf)   (QUAD_FTYPE x)
{
    if (x >= UINT64_MAX) return UINT64_MAX;
    if (x <= 0) return 0;
    return x;
}

#endif

INLINE(uint64_t,FLT16_CVDZ) (flt16_t x) {return  vcvtd_u64_f64(x);}
INLINE(uint64_t,  FLT_CVDZ)   (float x) {return  vcvts_u32_f32(x);}
INLINE(uint64_t,  DBL_CVDZ)  (double x) {return  vcvtd_u64_f64(x);}

INLINE(Vqdu,VWHU_CVDZ) (Vwhu x) {return VWHU_CVDU(x);}
INLINE(Vqdu,VWHI_CVDZ) (Vwhi x)
{
    float32x2_t dwf = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   dhi = vreinterpret_s16_f32(dwf);
    int32x4_t   qwi = vmovl_s16(dhi);
    uint16x4_t  dhu = vqmovun_s32(qwi);
    uint8x8_t   dbu = vreinterpret_u8_u16(dhu);
    uint8x8_t   lhs = vtbl1_u8(
        dbu,
        vcreate_u8(0xffffffffffff0100ULL)
    );
    uint8x8_t   rhs = vtbl1_u8(
        dbu,
        vcreate_u8(0xffffffffffff0302ULL)
    );
    uint8x16_t  qbu = vcombine_u8(lhs, rhs);
    return  vreinterpretq_u64_u8(qbu);
}

INLINE(Vqdu,VWHF_CVDZ) (Vwhf x)
{
    // TODO
    return  VQDU_VOID;
}

INLINE(Vddu,VWWU_CVDZ) (Vwwu x) {return VWWU_CVDU(x);}
INLINE(Vddu,VWWI_CVDZ) (Vwwi x)
{
#define     VWWI_CVDZ(X) UINT64_ASTV(vqmovund_s64(VWWI_ASTV(X)))
    return  VWWI_CVDZ(x);
}

INLINE(Vddu,VWWF_CVDZ) (Vwwf x)
{
#define     VWWF_CVDZ(X) UINT64_ASTV(vcvtd_u64_f64(VWWF_ASTM(X)))
    return  VWWF_CVDZ(x);
}

INLINE(Vqdu,VDWU_CVDZ) (Vdwu x) {return VDWU_CVDU(x);}
INLINE(Vqdu,VDWI_CVDZ) (Vdwi x)
{
#define     VDWI_CVDZ(X) vmovl_u32(vqmovun_s64(vmovl_s32(X)))
    return  VDWI_CVDZ(x);
}

INLINE(Vqdu,VDWF_CVDZ) (Vdwf x)
{
#define     VDWF_CVDZ(X) vcvtq_u64_f64(vcvt_f64_f32(X))
    return  VDWF_CVDZ(x);
}

INLINE(Vddu,VDDU_CVDZ) (Vddu x) {return x;}
INLINE(Vddu,VDDI_CVDZ) (Vddi x)
{
    uint64x1_t m = vtst_u64(
        vreinterpret_u64_s64(x),
        vdup_n_u64(0x8000000000000000ULL)
    );
    m = vreinterpret_u64_u32(
        vmvn_u32(vreinterpret_u32_u64(m))
    );
    m = vshr_n_u64(m, 1);
    return  vand_u64(m, vreinterpret_u64_s64(x));
}

INLINE(Vddu,VDDF_CVDZ) (Vddf x) {return vcvt_u64_f64(x);}

INLINE(Vqdu,VQDU_CVDZ) (Vqdu x) {return x;}
INLINE(Vqdu,VQDI_CVDZ) (Vqdi x)
{
    uint64x2_t m = vtstq_u64(
        vreinterpretq_u64_s64(x),
        vdupq_n_u64(0x8000000000000000ULL)
    );
    m = vreinterpretq_u64_u32(
        vmvnq_u32(vreinterpretq_u32_u64(m))
    );
    m = vshrq_n_u64(m, 1);
    return  vandq_u64(m, vreinterpretq_u64_s64(x));
}

INLINE(Vqdu,VQDF_CVDZ) (Vqdf x) {return vcvtq_u64_f64(x);}

INLINE(Vddu,VQQU_CVDZ) (Vqqu x)
{
    QUAD_VTYPE  q = {.Q.U=x};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE  r = {.D.U=vget_high_u64(q.D.U)};
    r.D.U = vcgt_u64(r.D.U, vdup_n_u64(0));
    return  vorr_u64(l.D.U, r.D.U);
}

INLINE(Vddu,VQQI_CVDZ) (Vqqi x)
{
    QUAD_VTYPE q = {.Q.I=x};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    DWRD_VTYPE m;

    // if any bit in x[64:128] is set, the result is 
    // saturated. First, set the intermediate result to
    // x[0:64]|(UINT64_MAX if ANY(x[64:128]) else 0)
    m.D.U = vtst_u64(r.D.U, vdup_n_u64(INT64_MAX));
    l.D.U = vorr_u64(l.D.U, m.D.U);

    // if x was neg, we shift left by -64 bits to cap 
    // the negative value to zero
    r.D.I = vshr_n_s64(r.D.I, 63);
    r.D.I = vand_s64(r.D.I, vdup_n_u64(0xffffffffffffffc0ULL));

    return  vshl_u64(l.D.U, r.D.I);
}

#if 0 // _LEAVE_ARM_CVDZ
}
#endif

#if 0 // _ENTER_ARM_CVDS
{
#endif

INLINE(int64_t,   BOOL_CVDS)   (_Bool x) {return x;}
INLINE(int64_t,  UCHAR_CVDS)   (uchar x) {return x;}
INLINE(int64_t,  SCHAR_CVDS)   (schar x) {return x;}
INLINE(int64_t,   CHAR_CVDS)    (char x) {return x;}
INLINE(int64_t,  USHRT_CVDS)  (ushort x) {return x;}
INLINE(int64_t,   SHRT_CVDS)   (short x) {return x;}
INLINE(int64_t,   UINT_CVDS)    (uint x) {return x;}
INLINE(int64_t,    INT_CVDS)     (int x) {return x;}

#if DWRD_NLONG == 2

INLINE(int64_t,  ULONG_CVDS)   (ulong x) {return x;}
INLINE(int64_t,   LONG_CVDS)    (long x) {return x;}

#else

INLINE(int64_t,ULONG_CVDS) (ulong x)
{
    return x >= INT64_MAX ? INT64_MAX : x;
}

INLINE(int64_t,LONG_CVDS) (long x) {return x;}

#endif

INLINE(int64_t,ULLONG_CVDS) (ullong x)
{
    return x >= INT64_MAX ? INT64_MAX : x;
}

INLINE(int64_t,LLONG_CVDS) (llong x)
{
#if QUAD_NLLONG == 2
    return x ;
#else
    if (x >= INT64_MAX) return INT64_MAX;
    if (x <= INT64_MIN) return INT64_MIN;
    return x;
#endif
}

INLINE(int64_t, FLT16_CVDS) (flt16_t x) {return x;}
INLINE(int64_t,   FLT_CVDS)   (float x) {return vcvtd_s64_f64(x);}
INLINE(int64_t,   DBL_CVDS)  (double x) {return vcvtd_s64_f64(x);}

#if QUAD_NLLONG == 2

INLINE(int64_t,cvdsqu) (QUAD_UTYPE x)
{
    return x > INT64_MAX ? INT64_MAX : x;
}

INLINE(int64_t,cvdsqi) (QUAD_ITYPE x)
{
    if (x >= INT64_MAX) return INT64_MAX;
    if (x <= INT64_MIN) return INT64_MIN;
    return  x;
}

INLINE(int64_t,cvdsqf) (QUAD_FTYPE x)
{
    if (x >= INT64_MAX) return INT64_MAX;
    if (x <= INT64_MIN) return INT64_MIN;
    return  x;
}

#endif

INLINE(Vqdi,VWHU_CVDS) (Vwhu x)
{
#define     VWHU_CVDS(X) VWHU_CVDI(X)
    return  VWHU_CVDS(x);
}

INLINE(Vqdi,VWHI_CVDS) (Vwhi x)
{
#define     VWHI_CVDS(X) VWHI_CVDI(X)
    return  VWHI_CVDS(x);
}

INLINE(Vqdi,VWHF_CVDS) (Vwhf x)
{
// TODO
    return  VQDI_VOID;
}

INLINE(Vddi,VWWU_CVDS) (Vwwu x) {return VWWU_CVDI(x);}
INLINE(Vddi,VWWI_CVDS) (Vwwi x) {return VWWI_CVDI(x);}
INLINE(Vddi,VWWF_CVDS) (Vwwf x)
{
#define     VWWF_CVDS(X) INT64_ASTV(vcvtd_s64_f64(VWWF_ASTM(X)))
    return  VWWF_CVDS(x);
}

INLINE(Vqdi,VDWU_CVDS) (Vdwu x) {return VDWU_CVDI(x);}
INLINE(Vqdi,VDWI_CVDS) (Vdwi x) {return VDWI_CVDI(x);}
INLINE(Vqdi,VDWF_CVDS) (Vdwf x) {return vcvtq_s64_f64(vcvt_f64_f32(x));}


INLINE(Vddi,VDDU_CVDS) (Vddu x)
{
    uint64x1_t m = vtst_u64(
        vdup_n_u64(0x8000000000000000ull),
        x
    );
    return  vand_s64(
        vdup_n_s64(INT64_MAX),
        vreinterpret_s64_u64(m)
    );
}

INLINE(Vddi,VDDI_CVDS) (Vddi x) {return x;}
INLINE(Vddi,VDDF_CVDS) (Vddf x) {return vcvt_s64_f64(x);}

INLINE(Vqdi,VQDU_CVDS) (Vqdu x)
{
    uint64x2_t m = vtstq_u64(
        vdupq_n_u64(0x8000000000000000ull),
        x
    );
    return  vandq_s64(
        vdupq_n_s64(INT64_MAX),
        vreinterpretq_s64_u64(m)
    );
}

INLINE(Vqdi,VQDI_CVDS) (Vqdi x) {return x;}
INLINE(Vqdi,VQDF_CVDS) (Vqdf x) {return vcvtq_s64_f64(x);}

INLINE(Vddi,VQQU_CVDS) (Vqqu x)
{
    QUAD_VTYPE q = {.Q.U=x};
    DWRD_VTYPE d = {.D.U=vdup_n_u64(UINT64_MAX)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};

//  set l = UINT64_MAX if ANY(x[64:128]) else l
    r.D.U = vtst_u64(r.D.U, d.D.U);
    l.D.U = vorr_u64(l.D.U, r.D.U);

//  set d = INT64_MAX
    d.D.U = vshr_n_u64(d.D.U, 1);

//  set l = UINT64_MAX if (l > d) else l
    r.D.U = vcgt_u64(l.D.U, d.D.U);
    l.D.U = vorr_u64(l.D.U, r.D.U);

//  ret INT64_MAX&l
    return  vand_s64(d.D.I, l.D.I);
}

INLINE(Vddi,VQQI_CVDS) (Vqqi x)
{
    QUAD_VTYPE  q = {.Q.I=x};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE  r = {.D.U=vget_high_u64(q.D.U)};
    DWRD_VTYPE  s = {.I=vget_lane_s64(r.D.U, 0)};
    if (s.I < 0)
    {
        return (s.I == -1) ? l.D.I : vdup_n_s64(INT64_MIN);
    }
    DWRD_VTYPE  m = {.I=INT64_MAX};
    if (s.I > 0)
    {
        return  m.D.I;
    }
    m.D.U = vcgt_u64(l.D.U, m.D.U);
    m.D.U = vshr_n_u64(m.D.U, 1);
    return  vorr_s64(m.D.I, l.D.I);
}

#if 0 // _LEAVE_ARM_CVDS
}
#endif

#if 0 // _ENTER_ARM_CVDF
{
#endif

#if 0
INLINE(double,   BOOL_CVDF)   (_Bool x) {return x;}
INLINE(double,  UCHAR_CVDF)   (uchar x) {return x;}
INLINE(double,  SCHAR_CVDF)   (schar x) {return x;}
INLINE(double,   CHAR_CVDF)    (char x) {return x;}
INLINE(double,  USHRT_CVDF)  (ushort x) {return x;}
INLINE(double,   SHRT_CVDF)   (short x) {return x;}
INLINE(double,   UINT_CVDF)    (uint x) {return x;}
INLINE(double,    INT_CVDF)     (int x) {return x;}
INLINE(double,  ULONG_CVDF)   (ulong x) {return x;}
INLINE(double,   LONG_CVDF)    (long x) {return x;}
INLINE(double, ULLONG_CVDF)  (ullong x) {return x;}
INLINE(double,  LLONG_CVDF)   (llong x) {return x;}
INLINE(double,  FLT16_CVDF) (flt16_t x) {return x;}
INLINE(double,    FLT_CVDF)   (float x) {return x;}
INLINE(double,    DBL_CVDF)  (double x) {return x;}
#if QUAD_NLLONG == 2

INLINE(double,cvdfqu) (QUAD_UTYPE x) {return x;}
INLINE(double,cvdfqi) (QUAD_ITYPE x) {return x;}
INLINE(double,cvdfqf) (QUAD_FTYPE x) {return x;}
#endif
#endif


INLINE(Vqdf,VWHU_CVDF) (Vwhu x)
{
    float32x2_t m = vdup_n_f32(VWHU_ASTM(x));
    uint8x8_t   d = vreinterpret_u8_f32(m);
    d = vtbl1_u8(
        d,
        vcreate_u8(0xffff0302ffff0100ULL)
    );
    uint64x2_t  q = vmovl_u32(vreinterpret_u32_u8(d));
    return  vcvtq_f64_u64(q);
}

INLINE(Vqdf,VWHI_CVDF) (Vwhi x)
{
//  TODO: optimize
    return  vcvtq_f64_s64(VWHI_CVDI(x));
}

INLINE(Vqdf,VWHF_CVDF) (Vwhf x)
{
//  TODO
    return  VQDF_VOID;
}


INLINE(Vddf,VWWU_CVDF) (Vwwu x) {return vdup_n_f64(VWWU_ASTV(x));}
INLINE(Vddf,VWWI_CVDF) (Vwwi x) {return vdup_n_f64(VWWI_ASTV(x));}
INLINE(Vddf,VWWF_CVDF) (Vwwf x) {return vdup_n_f64(VWWF_ASTV(x));}

INLINE(Vqdf,VDWU_CVDF) (Vdwu x) {return vcvtq_f64_u64(vmovl_u32(x));}
INLINE(Vqdf,VDWI_CVDF) (Vdwi x) {return vcvtq_f64_u64(vmovl_s32(x));}
INLINE(Vqdf,VDWF_CVDF) (Vdwf x) {return vcvt_f64_f32(x);}

INLINE(Vddf,VDDU_CVDF) (Vddu x) {return vcvt_f64_u64(x);}
INLINE(Vddf,VDDI_CVDF) (Vddi x) {return vcvt_f64_s64(x);}
INLINE(Vddf,VDDF_CVDF) (Vddf x) {return x;}

INLINE(Vqdf,VQDU_CVDF) (Vqdu x) {return vcvtq_f64_u64(x);}
INLINE(Vqdf,VQDI_CVDF) (Vqdi x) {return vcvtq_f64_s64(x);}
INLINE(Vqdf,VQDF_CVDF) (Vqdf x) {return x;}

#if 0 // _LEAVE_ARM_CVDF
}
#endif


#if 0 // _ENTER_ARM_CVQU
{
#endif

INLINE(QUAD_UTYPE,FLT16_CVQU) (flt16_t x) 
{
    return x;
}

INLINE(QUAD_UTYPE,FLT_CVQU) (float x) 
{
    return x;
}

INLINE(QUAD_UTYPE,DBL_CVQU) (double x) 
{
    return x;
}

INLINE(QUAD_UTYPE,cvquqf) (QUAD_FTYPE x) 
{
    return x;
}

INLINE(Vqqu,VWWU_CVQU) (Vwwu x)
{
    QUAD_TYPE   v = {.W0.F=x.V0};
    uint64x1_t  l = vdup_n_u64(v.Lo.U);
    uint64x1_t  r = vdup_n_u64(v.Hi.U);
    uint64x2_t  c = vcombine_u64(l, r);
    return  ((Vqqu){c});
}

INLINE(Vqqu,VWWI_CVQU) (Vwwi x)
{
    QUAD_TYPE   v = {.W0.F=x.V0};
    v.I = INT32_CVQU(v.W0.I);
    uint64x1_t  l = vdup_n_u64(v.Lo.U);
    uint64x1_t  r = vdup_n_u64(v.Hi.U);
    uint64x2_t  c = vcombine_u64(l, r);
    return  ((Vqqu){c});
}

INLINE(Vqqu,VWWF_CVQU) (Vwwf x)
{
    QUAD_TYPE   v = {.W0.F=x.V0};
    v.I = FLT_CVQU(v.W0.I);
    uint64x1_t  l = vdup_n_u64(v.Lo.U);
    uint64x1_t  r = vdup_n_u64(v.Hi.U);
    uint64x2_t  c = vcombine_u64(l, r);
    return  ((Vqqu){c});
}


INLINE(Vqqu,VDDU_CVQU) (Vddu l)
{
    uint64x1_t  r = vdup_n_u64(0);
    uint64x2_t  c = vcombine_u64(l, r);
    return  ((Vqqu){c});
}

INLINE(Vqqu,VDDI_CVQU) (Vddi l)
{
    int64x1_t   r = vshr_n_s64(l, 64);
    int64x2_t   c = vcombine_u64(l, r);
    uint64x2_t  m = vreinterpretq_u64_s64(c);
    return ((Vqqu){m});
}

INLINE(Vqqu,VDDF_CVQU) (Vddf l)
{
    QUAD_TYPE v = {.U=vget_lane_f64(l, 0)};
    return  astvqu(v.U);
}

INLINE(Vqqu,VQQI_CVQU) (Vqqi v)
{
    return ((Vqqu){vreinterpretq_u64_s64(v.V0)});
}

INLINE(Vqqu,VQQF_CVQU) (Vqqf v)
{
    QUAD_TYPE q = {.F=v.V0};
    q.U = q.F;
    return  astvqu(q.U);
}


#if 0 // _LEAVE_ARM_CVQU
}
#endif

#if 0 // _ENTER_ARM_CVQI
{
#endif
INLINE(QUAD_ITYPE,FLT16_CVQI) (flt16_t x) 
{
    return x;
}

INLINE(QUAD_ITYPE,FLT_CVQI) (float x) 
{
    return x;
}

INLINE(QUAD_ITYPE,DBL_CVQI) (double x) 
{
    return x;
}

INLINE(QUAD_ITYPE,cvqiqf) (QUAD_FTYPE x) 
{
    return x;
}

INLINE(Vqqi,VWWU_CVQI) (Vwwu x)
{
    QUAD_TYPE   v = {.W0.F=x.V0};
    int64x1_t   l = vdup_n_s64(v.Lo.I);
    int64x1_t   r = vdup_n_s64(v.Hi.I);
    int64x2_t   c = vcombine_s64(l, r);
    return  ((Vqqi){c});
}

INLINE(Vqqi,VWWI_CVQI) (Vwwi x)
{
    QUAD_TYPE   v = {.W0.F=x.V0};
    v.I = INT32_CVQI(v.W0.I);
    int64x1_t   l = vdup_n_s64(v.Lo.I);
    int64x1_t   r = vdup_n_s64(v.Hi.I);
    int64x2_t   c = vcombine_s64(l, r);
    return  ((Vqqi){c});
}

INLINE(Vqqi,VWWF_CVQI) (Vwwf x)
{
    QUAD_TYPE   v = {.W0.F=x.V0};
    v.I = FLT_CVQI(v.W0.I);
    int64x1_t   l = vdup_n_s64(v.Lo.I);
    int64x1_t   r = vdup_n_s64(v.Hi.I);
    int64x2_t   c = vcombine_s64(l, r);
    return  ((Vqqi){c});
}


INLINE(Vqqi,VDDU_CVQI) (Vddu l)
{
    uint64x1_t  r = vdup_n_u64(0);
    uint64x2_t  c = vcombine_u64(l, r);
    return  ((Vqqi){vreinterpretq_s64_u64(c)});
}

INLINE(Vqqi,VDDI_CVQI) (Vddi l)
{
    int64x1_t   r = vshr_n_s64(l, 64);
    int64x2_t   c = vcombine_s64(l, r);
    return  ((Vqqi){c});
}

INLINE(Vqqi,VDDF_CVQI) (Vddf l)
{
    QUAD_TYPE v = {.I=vget_lane_f64(l, 0)};
    return  astvqi(v.I);
}

INLINE(Vqqi,VQQU_CVQI) (Vqqu v)
{
    return  ((Vqqi){vreinterpretq_s64_u64(v.V0)});
}

INLINE(Vqqi,VQQF_CVQI) (Vqqf v)
{
    QUAD_TYPE q = {.F=v.V0};
    q.I = q.F;
    return  astvqi(q.I);
}


#if 0 // _LEAVE_ARM_CVQI
}
#endif

#if 0 // _ENTER_ARM_CVQF
{
#endif

#if 0 // _LEAVE_ARM_CVQF
}
#endif


#if 0 // _ENTER_ARM_DUPW
{
#endif

INLINE(float,MY_DUPWBZ) (signed a, Rc(-4, +3) b)
{
    DWRD_TYPE d = {.B0.U=a, .B1.U=a, .B2.U=a, .B3.U=a};
    if (b < 0)
    {
        d.U <<= 8*(4-(3&-b));
        return  d.Lo.F;
    }
    if (b)
    {
        d.U >>= 8*(4-b);
    }
    return  d.Lo.F;
}
            
INLINE(float,MY_DUPWHZ) (signed a, Rc(-2, +1) b)
{
    WORD_TYPE w={.H0.U=a, .H1.U=a};
    if (b < 0)
    {
        w.U <<= 16*(2-(1&-b));
        return  w.F;
    }
    w.U >>= 16*b;
    return  w.F;
}
            
INLINE(float,MY_DUPWHF) (flt16_t a, Rc(-2, +1) b)
{
    WORD_TYPE w={.H0.F=a, .H1.F=a};
    if (b < 0)
    {
        w.U <<= 16*(2-(1&-b));
        return  w.F;
    }
    w.U >>= 16*b;
    return  w.F;
}


INLINE(Vwyu, BOOL_DUPW)    (_Bool a, Rc(-32, +31) b)
{
#define     MY_DUPWYU(A, B)         \
(                                   \
    ((63&-B) > 31)                  \
    ?   vget_lane_f32(              \
            vreinterpret_f32_u32(   \
                vshr_n_u32(         \
                    vcreate_u32((A?UINT32_MAX:0)),\
                    (32-(31&B))     \
                )                   \
            ),                      \
            0                       \
        )                           \
    :   vget_lane_f32(              \
            vreinterpret_f32_u32(   \
                vshl_n_u32(         \
                    vcreate_u32((A?UINT32_MAX:0)),\
                    (31&B)          \
                )                   \
            ),                      \
            0                       \
        )                           \
)
#define  MY_BOOL_DUPW(A, B, ...) ((Vwyu){MY_DUPWYU(A,B)})
#define     BOOL_DUPW(...) MY_BOOL_DUPW(__VA_ARGS__,0)
    uint64x1_t  w = vcreate_u64((a ? UINT32_MAX : 0));
    int64x1_t   n;
    if ((63&-b) > 31)   n = vdup_n_s64(-(32-(31&b)));
    else                n = vdup_n_s64((31&b));
    w = vshl_u64(w, n);
    float32x2_t m = vreinterpret_f32_u64(w);
    Vwyu        v = {vget_lane_f32(m, 0)};
    return  v;
}

INLINE(Vwbu,UCHAR_DUPW) (unsigned a, Rc(-4, +3) b)
{
#define  MY_UCHAR_DUPW(A, B, ...) ((Vwbu){MY_DUPWBZ(A,B)})
#define     UCHAR_DUPW(...) MY_UCHAR_DUPW(__VA_ARGS__,0)    
    return  UCHAR_DUPW(a, b);
}

INLINE(Vwbi,SCHAR_DUPW)   (signed a, Rc(-4, +3) b)
{
#define  MY_SCHAR_DUPW(A, B, ...) ((Vwbi){MY_DUPWBZ(A,B)})
#define     SCHAR_DUPW(...) MY_SCHAR_DUPW(__VA_ARGS__,0)    
    return  SCHAR_DUPW(a, b);
}

INLINE(Vwbc, CHAR_DUPW)      (int a, Rc(-4, +3) b)
{
#define  MY_CHAR_DUPW(A, B, ...) ((Vwbc){MY_DUPWBZ(A,B)})
#define     CHAR_DUPW(...) MY_CHAR_DUPW(__VA_ARGS__,0)    
    return  CHAR_DUPW(a, b);
}

INLINE(Vwhu,USHRT_DUPW) (unsigned a, Rc(-2, +1) b)
{
#define  MY_USHRT_DUPW(A, B, ...) ((Vwhu){MY_DUPWHZ(A,B)})
#define     USHRT_DUPW(...) MY_USHRT_DUPW(__VA_ARGS__,0)    
    return  USHRT_DUPW(a, b);
}

INLINE(Vwhi, SHRT_DUPW)   (signed a, Rc(-2, +1) b)
{
#define  MY_SHRT_DUPW(A, B, ...) ((Vwhi){MY_DUPWHZ(A,B)})
#define     SHRT_DUPW(...) MY_SHRT_DUPW(__VA_ARGS__,0)    
    return  SHRT_DUPW(a, b);
}

INLINE(Vwwu, UINT_DUPW)     (uint a, Rc(-1, 0) b)
{
#define  MY_UINT_DUPW(A, B, ...) ((Vwwu){((WORD_TYPE){.U=(1&B)?0:A}).F})
#define     UINT_DUPW(...) MY_UINT_DUPW(__VA_ARGS__,0)    
    return  UINT_DUPW(a, b);
}

INLINE(Vwwi,  INT_DUPW)      (int a, Rc(-1, 0) b)
{
#define  MY_INT_DUPW(A, B, ...) ((Vwwi){((WORD_TYPE){.I=(1&B)?0:A}).F})
#define     INT_DUPW(...) MY_INT_DUPW(__VA_ARGS__,0)    
    return  INT_DUPW(a, b);
}

#if DWRD_NLONG == 2

INLINE(Vwwu,ULONG_DUPW)    (ulong a, Rc(-1, 0) b)
{
#define  MY_ULONG_DUPW(A, B, ...) ((Vwwu){((WORD_TYPE){.U=(1&B)?0:A}).F})
#define     ULONG_DUPW(...) MY_ULONG_DUPW(__VA_ARGS__,0)    
    return  ULONG_DUPW(a, b);
}

INLINE(Vwwi, LONG_DUPW)     (long a, Rc(-1, 0) b)
{
#define  MY_LONG_DUPW(A, B, ...) ((Vwwi){((WORD_TYPE){.I=(1&B)?0:A}).F})
#define     LONG_DUPW(...) MY_LONG_DUPW(__VA_ARGS__,0)    
    return  LONG_DUPW(a, b);
}

#endif

INLINE(Vwhf,FLT16_DUPW) (flt16_t a, Rc(-2, +1) b)
{
#define  MY_FLT16_DUPW(A, B, ...) ((Vwhf){MY_DUPWHF(A,B)})
#define     FLT16_DUPW(...) MY_FLT16_DUPW(__VA_ARGS__,0)    
    return  FLT16_DUPW(a, b);
}

INLINE(Vwwf,  FLT_DUPW)   (float a, Rc(-1, 0) b)
{
#define  MY_FLT_DUPW(A, B, ...) ((Vwwf){(1&B)?0.0F:A})
#define     FLT_DUPW(...) MY_FLT_DUPW(__VA_ARGS__,0)    
    return  FLT_DUPW(a, b);
}


INLINE(float,WYU_DUPW) (float a, Rc(-32, +31) b)
{
#define     WYU_DUPW(A, B) \
(((WORD_TYPE){.U=((((WORD_TYPE){.F=A}).U&(1U<<(31&B)))?-1:0),}).F)
    
    return  WYU_DUPW(a, b);
}

INLINE(float,WBR_DUPW) (float a, Rc(-4, +3) b)
{
#define  WBR_DUPW(A, B)    \
vget_lane_f32(          \
    vreinterpret_f32_u8(\
        vdup_lane_u8(   \
            vreinterpret_u8_f32(((float32x2_t){A})),\
            (3&B)       \
        )               \
    ),                  \
    0                   \
)

    float32x2_t m = {a};
    uint8x8_t   v = vreinterpret_u8_f32(m);
    uint8x8_t   t = vdup_n_u8((b&3));
    v = vtbl1_u8(v, t);
    m = vreinterpret_f32_u8(v);
    return vget_lane_f32(m, 0);
}

INLINE(float,WHR_DUPW) (float a, Rc(-2, +1) b)
{
#define  WHR_DUPW(A, B)    \
vget_lane_f32(\
    vreinterpret_f32_u16(\
        vdup_lane_u16(\
            vreinterpret_u16_f32(((float32x2_t){A})),\
            (1&B)\
        )\
    ),\
    0\
)

    float32x2_t m = {a};
    uint8x8_t   v = vreinterpret_u8_f32(m);
    uint8x8_t   t = vdup_n_u8((2*(b&1)));
    t = vadd_u8(t, vcreate_u8(0x01000100ULL));
    v = vtbl1_u8(v, t);
    m = vreinterpret_f32_u8(v);
    return  vget_lane_f32(m, 0);
}


INLINE(Vwyu,VWYU_DUPW) (Vwyu a, Rc(-32, +31) b)
{
#define  MY_VWYU_DUPW(A, B, ...) ((Vwyu){WYU_DUPW(A.V0,B)})
#define     VWYU_DUPW(...) MY_VWYU_DUPW(__VA_ARGS__,0)
    return  VWYU_DUPW(a, b);
}

INLINE(Vwbu,VWBU_DUPW) (Vwbu a, Rc(-4, +3) b)
{
#define  MY_VWBU_DUPW(A,B,...) ((Vwbu){WBR_DUPW(A.V0,B)})
#define     VWBU_DUPW(...) MY_VWBU_DUPW(__VA_ARGS__,0)
    a.V0 = (WBR_DUPW)(a.V0, b);
    return  a;
}

INLINE(Vwbi,VWBI_DUPW) (Vwbi a, Rc(-4, +3) b)
{
#define  MY_VWBI_DUPW(A,B,...) ((Vwbi){WBR_DUPW(A.V0,B)})
#define     VWBI_DUPW(...) MY_VWBI_DUPW(__VA_ARGS__,0)
    a.V0 = (WBR_DUPW)(a.V0, b);
    return  a;
}

INLINE(Vwbc,VWBC_DUPW) (Vwbc a, Rc(-4, +3) b)
{
#define  MY_VWBC_DUPW(A,B,...) ((Vwbc){WBR_DUPW(A.V0,B)})
#define     VWBC_DUPW(...) MY_VWBC_DUPW(__VA_ARGS__,0)
    a.V0 = (WBR_DUPW)(a.V0, b);
    return  a;
}

INLINE(Vwhu,VWHU_DUPW) (Vwhu a, Rc(-2, +1) b)
{
#define  MY_VWHU_DUPW(A,B,...) ((Vwhu){WHR_DUPW(A.V0,B)})
#define     VWHU_DUPW(...) MY_VWHU_DUPW(__VA_ARGS__,0)
    a.V0 = (WHR_DUPW)(a.V0, b);
    return  a;
}

INLINE(Vwhi,VWHI_DUPW) (Vwhi a, Rc(-2, +1) b)
{
#define  MY_VWHI_DUPW(A,B,...) ((Vwhi){WHR_DUPW(A.V0,B)})
#define     VWHI_DUPW(...) MY_VWHI_DUPW(__VA_ARGS__,0)
    a.V0 = (WHR_DUPW)(a.V0, b);
    return  a;
}

INLINE(Vwhf,VWHF_DUPW) (Vwhf a, Rc(-2, +1) b)
{
#define  MY_VWHF_DUPW(A,B,...) ((Vwhf){WHR_DUPW(A.V0,B)})
#define     VWHF_DUPW(...) MY_VWHF_DUPW(__VA_ARGS__,0)
    a.V0 = (WHR_DUPW)(a.V0, b);
    return  a;
}

INLINE(Vwwu,VWWU_DUPW) (Vwwu a, Rc(-1, +0) b)
{
#define     VWWU_DUPW(A, ...) ((Vwwu){A.V0})
    return  a;
}

INLINE(Vwwi,VWWI_DUPW) (Vwwi a, Rc(-1, +0) b)
{
#define     VWWI_DUPW(A, ...) ((Vwwi){A.V0})
    return  a;
}

INLINE(Vwwf,VWWF_DUPW) (Vwwf a, Rc(-1, +0) b)
{
#define     VWWF_DUPW(A, ...) ((Vwwf){A.V0})
    return  a;
}


INLINE(float,DYU_DUPW) (uint64x1_t a, Rc(-64, +64) b)
{
#define  DYU_DUPW(A, B)             \
vget_lane_f32(                      \
    vreinterpret_f32_u64(           \
        vtst_u64(                   \
            A,                      \
            vshl_n_u64(             \
                vdup_n_u64(1),      \
                (63&B)              \
            )                       \
        )                           \
    ),                              \
    0                               \
)

    uint64x1_t  k = vshl_u64(
        vcreate_u64(1),
        vdup_n_s64((63&b)) 
    );
    a = vtst_u64(a, k);
    float32x2_t m = vreinterpret_f32_u64(a);
    return  vget_lane_f32(m, 0);
}

INLINE(float,DBU_DUPW) (uint8x8_t a, Rc(-8, +7) b)
{
#define     DBU_DUPW(A, B)    \
vget_lane_f32(vreinterpret_f32_u8(vdup_lane_u8(A,(7&B))),0)

    a = vtbl1_u8(a, vdup_n_u8((7&b)));
    float32x2_t m = vreinterpret_f32_u8(a);
    return  vget_lane_f32(m, 0);
}

INLINE(float,DHU_DUPW) (uint16x4_t a, Rc(-4, +3) b)
{
#define     DHU_DUPW(A, B)    \
vget_lane_f32(vreinterpret_f32_u16(vdup_lane_u16(A,(3&B))),0)

    uint8x8_t   v = vreinterpret_u8_u16(a);
    uint8x8_t   t = vdup_n_u8((2*(3&b)));
    t = vadd_u8(t, vcreate_u8(0x01000100));
    v = vtbl1_u8(v, t);
    float32x2_t m = vreinterpret_f32_u8(v);
    return  vget_lane_f32(m, 0);
}

INLINE(float,DWU_DUPW) (uint32x2_t a, Rc(-2, +1) b)
{
#define     DWU_DUPW(A, B)    \
vget_lane_f32(vreinterpret_f32_u32(vdup_lane_u32(A,(1&B))),0)

    uint8x8_t   v = vreinterpret_u8_u32(a);
    uint8x8_t   t = vdup_n_u8((4*(1&b)));
    t = vadd_u8(t, vcreate_u8(0x03020100));
    v = vtbl1_u8(v, t);
    float32x2_t m = vreinterpret_f32_u8(v);
    return  vget_lane_f32(m, 0);
}


INLINE(Vwyu,VDYU_DUPW) (Vdyu a, Rc(-64, +63) b)
{

#define  MY_VDYU_DUPW(A, B, ...) ((Vwyu){DYU_DUPW(A.V0,B)})
#define     VDYU_DUPW(...) MY_VDYU_DUPW(__VA_ARGS__,0)

    Vwyu    v = {(DYU_DUPW)(a.V0, b)};
    return  v;
}


INLINE(Vwbu,VDBU_DUPW) (Vdbu a, Rc(-8, +7) b)
{
#define  MY_VDBU_DUPW(A, B, ...) ((Vwbu){DBU_DUPW(A,B)})
#define     VDBU_DUPW(...) MY_VDBU_DUPW(__VA_ARGS__,0)
    Vwbu v = {(DBU_DUPW)(a, b)};
    return v;
}

INLINE(Vwbi,VDBI_DUPW) (Vdbi a, Rc(-8, +7) b)
{
#define  MY_VDBI_DUPW(A, B, ...) ((Vwbi){DBU_DUPW(VDBI_ASTU(A),B)})
#define     VDBI_DUPW(...) MY_VDBI_DUPW(__VA_ARGS__,0)
    uint8x8_t m = vreinterpret_u8_s8(a);
    Vwbi v = {(DBU_DUPW)(m, b)};
    return v;
}

INLINE(Vwbc,VDBC_DUPW) (Vdbc a, Rc(-8, +7) b)
{
#define  MY_VDBC_DUPW(A, B, ...) ((Vwbc){DBU_DUPW(VDBC_ASTU(A),B)})
#define     VDBC_DUPW(...) MY_VDBC_DUPW(__VA_ARGS__,0)
    uint8x8_t m = VDBC_ASBU(a);
    Vwbc v = {(DBU_DUPW)(m, b)};
    return v;
}


INLINE(Vwhu,VDHU_DUPW) (Vdhu a, Rc(-4, +3) b)
{
#define  MY_VDHU_DUPW(A, B, ...) ((Vwhu){DHU_DUPW(A,B)})
#define     VDHU_DUPW(...) MY_VDHU_DUPW(__VA_ARGS__,0)
    Vwhu v = {(DHU_DUPW)(a, b)};
    return v;
}

INLINE(Vwhi,VDHI_DUPW) (Vdhi a, Rc(-4, +3) b)
{
#define  MY_VDHI_DUPW(A, B, ...) ((Vwhi){DHU_DUPW(VDHI_ASTU(A),B)})
#define     VDHI_DUPW(...) MY_VDHI_DUPW(__VA_ARGS__,0)
    uint16x4_t m = VDHI_ASTU(a);
    Vwhi v = {(DHU_DUPW)(m, b)};
    return v;
}

INLINE(Vwhf,VDHF_DUPW) (Vdhf a, Rc(-4, +3) b)
{
#define  MY_VDHF_DUPW(A, B, ...) ((Vwhf){DHU_DUPW(VDHF_ASTU(A),B)})
#define     VDHF_DUPW(...) MY_VDHF_DUPW(__VA_ARGS__,0)
    uint16x4_t m = VDHF_ASTU(a);
    Vwhf v = {(DHU_DUPW)(m, b)};
    return v;
}


INLINE(Vwwu,VDWU_DUPW) (Vdwu a, Rc(-2, +1) b)
{
#define  MY_VDWU_DUPW(A, B, ...) ((Vwwu){DWU_DUPW(A,B)})
#define     VDWU_DUPW(...) MY_VDWU_DUPW(__VA_ARGS__,0)
    Vwwu v = {(DWU_DUPW)(a, b)};
    return v;
}

INLINE(Vwwi,VDWI_DUPW) (Vdwi a, Rc(-4, +3) b)
{
#define  MY_VDWI_DUPW(A, B, ...) ((Vwwi){DWU_DUPW(VDWI_ASTU(A),B)})
#define     VDWI_DUPW(...) MY_VDWI_DUPW(__VA_ARGS__,0)
    uint32x2_t m = vreinterpret_u32_s32(a);
    Vwwi v = {(DWU_DUPW)(m, b)};
    return v;
}

INLINE(Vwwf,VDWF_DUPW) (Vdwf a, Rc(-4, +3) b)
{
#define  MY_VDWF_DUPW(A, B, ...) ((Vwwf){DWU_DUPW(VDWF_ASHU(A),B)})
#define     VDWF_DUPW(...) MY_VDWF_DUPW(__VA_ARGS__,0)
    uint32x2_t m = VDWF_ASTU(a);
    Vwwf v = {(DWU_DUPW)(m, b)};
    return v;
}


INLINE(float,QYU_DUPW) (uint64x2_t a, Rc(-128, +127) b)
{
#define     QYU_DUPW(A, B)                      \
vget_lane_f32(                                  \
    vreinterpret_f32_u64(                       \
        vtst_u64(                               \
            (                                   \
                ((127&B) < 64)                  \
                ?   vget_low_u64(A)             \
                :   vget_high_u64(A)            \
            ),                                  \
            vdup_n_u64((UINT64_C(1)<<(63&B)))   \
        ),                                      \
    ),                                          \
    0                                           \
)
    
    if ((127&b) < 64) 
        return ((DYU_DUPW)( vget_low_u64(a),      b));
    else                
        return ((DYU_DUPW)(vget_high_u64(a), (63&b)));
}

INLINE(float,QBU_DUPW) (uint8x16_t a, Rc(-16, +15) b)
{
#define     QBU_DUPW(A, B)    \
vget_lane_f32(\
    vreinterpret_f32_u8(\
        vdup_laneq_u8(\
            A,  \
            (15&B)\
        ),\
    ),\
    0\
)

    uint8x8_t   v = vqtbl1_u8(a, vdup_n_u8((15&b)));
    float32x2_t m = vreinterpret_f32_u8(v);
    return  vget_lane_f32(m, 0);
}

INLINE(float,QHU_DUPW) (uint8x16_t a, Rc(-8, +7) b)
{
#define     QHU_DUPW(A, B)    \
vget_lane_f32(\
    vreinterpret_f32_u16(\
        vdup_laneq_u16(\
            A,  \
            (7&B)\
        ),\
    ),\
    0\
)

    uint8x8_t   t = vdup_n_u8((2*(7&b)));
    t = vadd_u8(t, vcreate_u8(0x01000100));
    uint8x8_t   v = vqtbl1_u8(a, t);
    float32x2_t m = vreinterpret_f32_u8(v);
    return  vget_lane_f32(m, 0);
}

INLINE(float,QWU_DUPW) (uint8x16_t a, Rc(-4, +3) b)
{
#define     QWU_DUPW(A, B)    \
vget_lane_f32(\
    vreinterpret_f32_u32(\
        vdup_laneq_u32(\
            A,  \
            (3&B)\
        ),\
    ),\
    0\
)
    uint8x8_t   t = vdup_n_u8((4*(3&b)));
    t = vadd_u8(t, vcreate_u8(0x03020100));
    uint8x8_t   v = vqtbl1_u8(a, t);
    float32x2_t m = vreinterpret_f32_u8(v);
    return  vget_lane_f32(m, 0);
}


INLINE(Vwyu,VQYU_DUPW) (Vqyu a, Rc(-128, +127) b)
{
#define  MY_VQYU_DUPW(A, B, ...) ((Vwyu){QYU_DUPW(A.V0,B)})
#define     VQYU_DUPW(...) MY_VQYU_DUPW(__VA_ARGS__,0)
    return  ((Vwyu){ ((QYU_DUPW)(a.V0, b)) });
}


INLINE(Vwbu,VQBU_DUPW) (Vqbu a, Rc(-16, +15) b)
{
#define  MY_VQBU_DUPW(A, B, ...) ((Vwbu){QBU_DUPW(VQBU_ASBU(A),B)})
#define     VQBU_DUPW(...) MY_VQBU_DUPW(__VA_ARGS__,0)
    Vwbu v = {((QBU_DUPW)(VQBU_ASBU(a), b))};
    return v;
}

INLINE(Vwbi,VQBI_DUPW) (Vqbi a, Rc(-16, +15) b)
{
#define  MY_VQBI_DUPW(A, B, ...) ((Vwbi){QBU_DUPW(VQBI_ASBU(A),B)})
#define     VQBI_DUPW(...) MY_VQBI_DUPW(__VA_ARGS__,0)
    Vwbi v = {((QBU_DUPW)(VQBI_ASBU(a), b))};
    return v;
}

INLINE(Vwbc,VQBC_DUPW) (Vqbc a, Rc(-16, +15) b)
{
#define  MY_VQBC_DUPW(A, B, ...) ((Vwbc){QBU_DUPW(VQBC_ASBU(A),B)})
#define     VQBC_DUPW(...) MY_VQBC_DUPW(__VA_ARGS__,0)
    Vwbc v = {((QBU_DUPW)(VQBC_ASBU(a), b))};
    return v;
}


INLINE(Vwhu,VQHU_DUPW) (Vqhu a, Rc(-8, +7) b)
{
#define  MY_VQHU_DUPW(A, B, ...) ((Vwhu){QHU_DUPW(VQHU_ASBU(A),B)})
#define     VQHU_DUPW(...) MY_VQHU_DUPW(__VA_ARGS__,0)
    Vwhu v = {((QHU_DUPW)(VQHU_ASBU(a), b))};
    return v;
}

INLINE(Vwhi,VQHI_DUPW) (Vqhi a, Rc(-8, +7) b)
{
#define  MY_VQHI_DUPW(A, B, ...) ((Vwhi){QHU_DUPW(VQHI_ASBU(A),B)})
#define     VQHI_DUPW(...) MY_VQHI_DUPW(__VA_ARGS__,0)
    Vwhi v = {((QHU_DUPW)(VQHI_ASBU(a), b))};
    return v;
}

INLINE(Vwhf,VQHF_DUPW) (Vqhf a, Rc(-8, +7) b)
{
#define  MY_VQHF_DUPW(A, B, ...) ((Vwhf){QHU_DUPW(VQHF_ASBU(A),B)})
#define     VQHF_DUPW(...) MY_VQHF_DUPW(__VA_ARGS__,0)
    Vwhf v = {((QHU_DUPW)(VQHF_ASBU(a), b))};
    return v;
}


INLINE(Vwwu,VQWU_DUPW) (Vqwu a, Rc(-4, +3) b)
{
#define  MY_VQWU_DUPW(A, B, ...) ((Vwwu){QWU_DUPW(VQWU_ASBU(A),B)})
#define     VQWU_DUPW(...) MY_VQWU_DUPW(__VA_ARGS__,0)
    Vwwu v = {((QWU_DUPW)(VQWU_ASBU(a), b))};
    return v;
}

INLINE(Vwwi,VQWI_DUPW) (Vqwi a, Rc(-4, +3) b)
{
#define  MY_VQWI_DUPW(A, B, ...) ((Vwwi){QWU_DUPW(VQWI_ASBU(A),B)})
#define     VQWI_DUPW(...) MY_VQWI_DUPW(__VA_ARGS__,0)
    Vwwi v = {((QWU_DUPW)(VQWI_ASBU(a), b))};
    return v;
}

INLINE(Vwwf,VQWF_DUPW) (Vqwf a, Rc(-4, +3) b)
{
#define  MY_VQWF_DUPW(A, B, ...) ((Vwwf){QWU_DUPW(VQWF_ASBU(A),B)})
#define     VQWF_DUPW(...) MY_VQWF_DUPW(__VA_ARGS__,0)
    Vwwf v = {((QWU_DUPW)(VQWF_ASBU(a), b))};
    return v;
}


#if 0 // _LEAVE_ARM_DUPW
}
#endif

#if 0 // _ENTER_ARM_DUPD
{
#endif

INLINE(Vdyu,BOOL_DUPD)    (_Bool a, Rc(-64, +63) b)
{
#if 1
#   define  MY_BOOL_DUPD(A, B, ...) \
(\
    (Vdyu)\
    {\
        (!A)      ? vdup_n_u64(0) :\
        (0 ==  B) ? vdup_n_u64(-1) :\
        (B <= -1) ? vdup_n_u64((UINT64_MAX<<(64+B))) \
                  : vdup_n_u64((UINT64_MAX>>(64-B))) \
    }\
)
#endif

#define     BOOL_DUPD(...)    MY_BOOL_DUPD(__VA_ARGS__,0)
    Vdyu    v = {vdup_n_u64((0ULL-a))};
    v.V0 = vshl_u64(
        v.V0,
        vdup_n_s64(
            ((-64+(127&b))+(!(63&(b<0?-b:b))))
        )
    );
    return v;
}

INLINE(Vdbu,UCHAR_DUPD) (unsigned a, Rc(-8,+7) b) 
{

#define  MY_UCHAR_DUPD(A, B, ...)   \
(                                   \
    (B == -8) ? vdup_n_u8(0) :      \
    (B <= -1) ? vext_u8(            \
        vdup_n_u8(0),               \
        vdup_n_u8(A),               \
        (7&-B)                      \
    )  :                            \
    vext_u8(                        \
        vdup_n_u8(A),               \
        vdup_n_u8(0),               \
        (7&(1+(7-B)))               \
    )                               \
)

#define     UCHAR_DUPD(...) MY_UCHAR_DUPD(__VA_ARGS__,0)
    uint8x8_t   v = vdup_n_u8(a);
    uint64x1_t  m = vreinterpret_u64_u8(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (8*((-8+(15&b))+((!(7&(b<0?-b:b)))<<3)))
        )
    );
    return  vreinterpret_u8_u64(m);
}

INLINE(Vdbi,SCHAR_DUPD)   (signed a, Rc(-8,+7) b) 
{
#define  MY_SCHAR_DUPD(A, B, ...)   \
(                                   \
    (B == -8) ? vdup_n_s8(0) :      \
    (B <= -1) ? vext_s8(            \
        vdup_n_s8(0),               \
        vdup_n_s8(A),               \
        (7&-B)                      \
    )  :                            \
    vext_s8(                        \
        vdup_n_s8(A),               \
        vdup_n_s8(0),               \
        (7&(1+(7-B)))               \
    )                               \
)

#define     SCHAR_DUPD(...) MY_SCHAR_DUPD(__VA_ARGS__,0)
    int8x8_t    v = vdup_n_s8(a);
    uint64x1_t  m = vreinterpret_u64_s8(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (8*((-8+(15&b))+((!(7&(b<0?-b:b)))<<3)))
        )
    );
    return  vreinterpret_s8_u64(m);
}

INLINE(Vdbc,CHAR_DUPD)      (int a, Rc(-8,+7) b)
{
    Vdbc        v;
#if CHAR_MIN
#   define  CHAR_DUPD(...) ((Vdbc){MY_SCHAR_DUPD(__VA_ARGS__,0)})
    v.V0 = (SCHAR_DUPD)(a, b);
#else
#   define  CHAR_DUPD(...) ((Vdbc){MY_UCHAR_DUPD(__VA_ARGS__,0)})
    v.V0 = (UCHAR_DUPD)(a, b);
#endif
    return  v;
}

INLINE(Vdhu,USHRT_DUPD) (unsigned a, Rc(-4, +3) b) 
{
#define  MY_USHRT_DUPD(A, B, ...)   \
(                                   \
    (B == -4) ? vdup_n_u16(0) :     \
    (B <= -1) ? vext_u16(           \
        vdup_n_u16(0),              \
        vdup_n_u16(A),              \
        (3&-B)                      \
    )  :                            \
    vext_u16(                       \
        vdup_n_u16(A),              \
        vdup_n_u16(0),              \
        (3&(1+(3-B)))               \
    )                               \
)

#define     USHRT_DUPD(...) MY_USHRT_DUPD(__VA_ARGS__,0)
    uint16x4_t v = vdup_n_u16(a);
    uint64x1_t m = vreinterpret_u64_u16(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (16*((-4+(7&b))+((!(3&(b<0?-b:b)))<<2)))
        )
    );
    return  vreinterpret_u16_u64(m);
}

INLINE(Vdhi,SHRT_DUPD)   (signed a, Rc(-4,+3) b) 
{
#define  MY_SHRT_DUPD(A, B, ...)    \
(                                   \
    (B == -4) ? vdup_n_s16(0) :     \
    (B <= -1) ? vext_s16(           \
        vdup_n_s16(0),              \
        vdup_n_s16(A),              \
        (3&-B)                      \
    )  :                            \
    vext_s16(                       \
        vdup_n_s16(A),              \
        vdup_n_s16(0),              \
        (3&(1+(3-B)))               \
    )                               \
)
#define     SHRT_DUPD(...) MY_SHRT_DUPD(__VA_ARGS__,0)
    int16x4_t   v = vdup_n_s16(a);
    uint64x1_t  m = vreinterpret_u64_s16(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (16*((-4+(7&b))+((!(3&(b<0?-b:b)))<<2)))
        )
    );
    return  vreinterpret_s16_u64(m);
}


INLINE(Vdwu,UINT_DUPD)    (uint a, Rc(-2, +1) b) 
{
#define  MY_UINT_DUPD(A, B, ...)    \
(                                   \
    (B == -2) ? vdup_n_u32(0) :     \
    (B <= -1) ? vext_u32(           \
        vdup_n_u32(0),              \
        vdup_n_u32(A),              \
        (1&-B)                      \
    )  :                            \
    vext_u32(                       \
        vdup_n_u32(A),              \
        vdup_n_u32(0),              \
        (1&(1+(1-B)))               \
    )                               \
)

#define     UINT_DUPD(...) MY_UINT_DUPD(__VA_ARGS__,0)
    uint32x2_t  v = vdup_n_u32(a);
    uint64x1_t  m = vreinterpret_u64_u32(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (32*((-2+(3&b))+((!(1&(b<0?-b:b)))<<1)))
        )
    );
    return  vreinterpret_u32_u64(m);
}

INLINE(Vdwi,INT_DUPD)     (int a, Rc(-2,+1) b) 
{
#define  MY_INT_DUPD(A, B, ...)     \
(                                   \
    (B == -2) ? vdup_n_s32(0) :     \
    (B <= -1) ? vext_s32(           \
        vdup_n_s32(0),              \
        vdup_n_s32(A),              \
        (1&-B)                      \
    )  :                            \
    vext_s32(                       \
        vdup_n_s32(A),              \
        vdup_n_s32(0),              \
        (1&(1+(1-B)))               \
    )                               \
)

#define     INT_DUPD(...) MY_INT_DUPD(__VA_ARGS__,0)
    int32x2_t   v = vdup_n_s32(a);
    uint64x1_t  m = vreinterpret_u64_s32(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (32*((-2+(3&b))+((!(1&(b<0?-b:b)))<<1)))
        )
    );
    return  vreinterpret_s32_u64(m);
}

#if DWRD_NLONG == 2
INLINE(Vdwu,ULONG_DUPD)   (ulong a, Rc(-2, +1) b)
{
#define     ULONG_DUPD(...) MY_UINT_DUPD(__VA_ARGS__,0)
    return  ((UINT_DUPD)(a, b));
}

INLINE(Vdwi,LONG_DUPD)    (long a, Rc(-2, +1) b)
{
#define     LONG_DUPD(...) MY_INT_DUPD(__VA_ARGS__,0)
    return  ((INT_DUPD)(a, b));
}

#else

INLINE(Vddu,ULONG_DUPD)  (ulong a, Rc(-1, 0) b)
{
#define  MY_ULONG_DUPD(A, B, ...) vdup_n_u64( ((0==B)?A:0) )
#define     ULONG_DUPD(...) MY_ULONG_DUPD(__VA_ARGS__,0)
    return  ULONG_DUPD(a, b);
}

INLINE(Vddi,LONG_DUPD)    (long a, Rc(-1, 0) b)
{
#define  MY_LONG_DUPD(A, B, ...) vdup_n_s64( ((0==B)?A:0) )
#define     LONG_DUPD(...) MY_LONG_DUPD(__VA_ARGS__,0)
    return  LONG_DUPD(a, b);
}

#endif


#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_DUPD)  (ullong a, Rc(-1, +0) b)
{
#define  MY_ULLONG_DUPD(A, B, ...) vdup_n_u64( ((0==B)?A:0) )
#define     ULLONG_DUPD(...) MY_ULLONG_DUPD(__VA_ARGS__,0)
    return  ULLONG_DUPD(a, b);
}

INLINE(Vddi,LLONG_DUPD)    (llong a, Rc(-1, +0) b)
{
#define  MY_LLONG_DUPD(A, B, ...) vdup_n_s64( ((0==B)?A:0) )
#define     LLONG_DUPD(...) MY_LLONG_DUPD(__VA_ARGS__,0)
    return  LLONG_DUPD(a, b);
}

#endif

INLINE(Vdhf,FLT16_DUPD) (flt16_t a, Rc(-4, +3) b) 
{
#define  MY_FLT16_DUPD(A, B, ...)       \
(                                       \
    (B == -4) ? vdup_n_f16(0.0F16) :    \
    (B <= -1) ? vreinterpret_f16_u16(   \
        vext_u16(                       \
            vdup_n_u16(0),              \
            vreinterpret_u16_f16(       \
                vdup_n_f16(A)           \
            ),                          \
            (3&-B)                      \
        )                               \
    )  :                                \
    vreinterpret_f16_u16(               \
        vext_u16(                       \
            vreinterpret_u16_f16(       \
                vdup_n_f16(A)           \
            ),                          \
            vdup_n_u16(0),              \
            (3&(1+(3-B)))               \
        )                               \
    )                                   \
)

#define     FLT16_DUPD(...) MY_FLT16_DUPD(__VA_ARGS__,0)
    float16x4_t v = vdup_n_f16(a);
    uint64x1_t  m = vreinterpret_u64_f16(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (16*((-4+(7&b))+((!(3&(b<0?-b:b)))<<2)))
        )
    );
    return  vreinterpret_f16_u64(m);
}

INLINE(Vdwf,FLT_DUPD)  (float a, Rc(-2, +1) b) 
{
#define  MY_FLT_DUPD(A, B, ...)     \
(                                   \
    (B == -2) ? vdup_n_f32(0.0F) :  \
    (B <= -1)                       \
    ?   vext_f32(vdup_n_f32(0.0F),vdup_n_f32(   A),(1&-B))\
    :   vext_f32(vdup_n_f32(   A),vdup_n_f32(0.0F),(1&(1+(1-B))))\
)

#define     FLT_DUPD(...) MY_FLT_DUPD(__VA_ARGS__,0)
    float32x2_t v = vdup_n_f32(a);
    uint64x1_t  m = vreinterpret_u64_f32(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (32*((-2+(3&b))+((!(1&(b<0?-b:b)))<<1)))
        )
    );
    return  vreinterpret_f32_u64(m);
}

INLINE(Vddf,DBL_DUPD) (double a, Rc(-1, 0) b)
{
#define  MY_DBL_DUPD(A, B, ...) ((float64x1_t){(0==B)?A:0.0})
#define     DBL_DUPD(...) MY_DBL_DUPD(__VA_ARGS__,0)
    return  DBL_DUPD(a, b);
}

INLINE(Vdyu,VWYU_DUPD) (Vwyu a, Rc(0, +31) b)
{
#define  MY_VWYU_DUPD(A, B, ...)          \
(\
    (Vdyu)\
    {\
        vtst_u64(\
            vreinterpret_u64_f32(((Vdwf){A.V0})),\
            vdup_n_u64((1ULL<<B))\
        )\
    }\
)
#define     VWYU_DUPD(...) MY_VWYU_DUPD(__VA_ARGS__,0)
    return  VWYU_DUPD(a, b);
}

INLINE(Vdbu,VWBU_DUPD) (Vwbu a, Rc(0,  +3) b)
{
#define  MY_WBU_DUPD(A, B, ...)    \
vdup_lane_u8(vreinterpret_u8_f32(((Vdwf){A})),(3&B))

#define  MY_VWBU_DUPD(A, B, ...)  MY_WBU_DUPD(A.V0,B)
#define     VWBU_DUPD(...) MY_VWBU_DUPD(__VA_ARGS__,0)

    return  vtbl1_u8(VDWF_ASBU((Vdwf){a.V0}),vdup_n_u8(b));
}

INLINE(Vdbi,VWBI_DUPD) (Vwbi a, Rc(0,  +3) b)
{
#define  MY_WBI_DUPD(A, B, ...)    \
vdup_lane_s8(vreinterpret_s8_f32(((Vdwf){A})),(3&B))

#define  MY_VWBI_DUPD(A, B, ...)  MY_WBI_DUPD(A.V0,B)
#define     VWBI_DUPD(...) MY_VWBI_DUPD(__VA_ARGS__,0)

    return  vtbl1_s8(VDWF_ASBI((Vdwf){a.V0}),vdup_n_u8(b));
}

INLINE(Vdbc,VWBC_DUPD) (Vwbc a, Rc(0, +3) b)
{
    Vdbc v;
#if CHAR_MIN
#   define VWBC_DUPD(...) ((Vdbc){MY_VWBI_DUPD(__VA_ARGS__,0)})
    Vwbi m = {a.V0};
    v.V0 = ((VWBI_DUPD)(m, b));
#else
#   define VWBC_DUPD(...) ((Vdbc){MY_VWBU_DUPD(__VA_ARGS__,0)})
    Vwbu m = {a.V0};
    v.V0 = ((VWBU_DUPD)(m, b));
#endif
    return  v;
}

INLINE(Vdhu,WHU_DUPD) (float a, Rc(0, +1) b)
{
#define  MY_WHU_DUPD(A, B)    \
vdup_lane_u16(vreinterpret_u16_f32(((Vdwf){A})),(1&B))

    float32x2_t f = {a};    
    uint64x1_t  m = vreinterpret_u64_f32(f);
    m = vshl_u64(m, vdup_n_s64(-(b<<4)));
    uint16x4_t  v = vreinterpret_u16_u64(m);
    return  vdup_lane_u16(v, 0);
}

INLINE(Vdhu,VWHU_DUPD) (Vwhu a, Rc(0, +1) b)
{
#define  MY_VWHU_DUPD(A, B, ...)  MY_WHU_DUPD(A.V0,B)
#define     VWHU_DUPD(...) MY_VWHU_DUPD(__VA_ARGS__,0)
    return (WHU_DUPD)(a.V0, b);
}

INLINE(Vdhi,VWHI_DUPD) (Vwhi a, Rc(0, +1) b)
{
#define  MY_WHI_DUPD(A, B)    \
vdup_lane_s16(vreinterpret_s16_f32(((Vdwf){A})),(1&B))

#define  MY_VWHI_DUPD(A, B, ...)  MY_WHI_DUPD(A.V0,B)
#define     VWHI_DUPD(...) MY_VWHI_DUPD(__VA_ARGS__,0)
    Vdhu v = (WHU_DUPD)(a.V0, b);
    return  VDHU_ASTI(v);
}

INLINE(Vdhf,VWHF_DUPD) (Vwhf a, Rc(0, +1) b)
{
#define  MY_WHF_DUPD(A, B)    \
vdup_lane_f16(vreinterpret_f16_f32(((Vdwf){A})),(1&B))

#define  MY_VWHF_DUPD(A, B, ...)  MY_WHF_DUPD(A.V0,B)
#define     VWHF_DUPD(...) MY_VWHF_DUPD(__VA_ARGS__,0)
    Vdhu v = (WHU_DUPD)(a.V0, b);
    return  VDHU_ASTF(v);
}

INLINE(Vdwu,VWWU_DUPD) (Vwwu a, Rc(-2, +1) b)
{
#define  MY_VWWU_DUPD(A, B, ...)    \
(                                   \
    (B == -2) ? vdup_n_u32(0) :     \
    (B <= -1) ? vext_u32(           \
        vdup_n_u32(0),              \
        vreinterpret_u32_f32(       \
            vdup_n_f32(A.V0)        \
        ),                          \
        (1&-B)                      \
    )  :                            \
    vext_u32(                       \
        vreinterpret_u32_f32(       \
            vdup_n_f32(A.V0)        \
        ),                          \
        vdup_n_u32(0),              \
        (1&(1+(1-B)))               \
    )                               \
)

#define     VWWU_DUPD(...) MY_VWWU_DUPD(__VA_ARGS__,0)
    float32x2_t v = vdup_n_f32(a.V0);
    uint64x1_t  m = vreinterpret_u64_f32(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (32*((-2+(3&b))+((!(1&(b<0?-b:b)))<<1)))
        )
    );
    return  vreinterpret_u32_u64(m);
}

INLINE(Vdwi,VWWI_DUPD) (Vwwi a, Rc(-2, +1) b)
{
#define  MY_VWWI_DUPD(A, B, ...)    \
(                                   \
    (B == -2) ? vdup_n_s32(0) :     \
    (B <= -1) ? vext_s32(           \
        vdup_n_s32(0),              \
        vreinterpret_s32_f32(       \
            vdup_n_f32(A.V0)        \
        ),                          \
        (1&-B)                      \
    )  :                            \
    vext_s32(                       \
        vreinterpret_s32_f32(       \
            vdup_n_f32(A.V0)        \
        ),                          \
        vdup_n_s32(0),              \
        (1&(1+(1-B)))               \
    )                               \
)

#define     VWWI_DUPD(...) MY_VWWI_DUPD(__VA_ARGS__,0)
    float32x2_t v = vdup_n_f32(a.V0);
    uint64x1_t  m = vreinterpret_u64_f32(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (32*((-2+(3&b))+((!(1&(b<0?-b:b)))<<1)))
        )
    );
    return  vreinterpret_s32_u64(m);
}

INLINE(Vdwf,VWWF_DUPD) (Vwwf a, Rc(-2, +1) b)
{
#define  MY_VWWF_DUPD(A, B, ...)    \
(                                   \
    (B == -2) ? vdup_n_f32(0.0F) :  \
    (B <= -1) ? vext_f32(           \
        vdup_n_f32(0.0F),           \
        vdup_n_f32(A.V0),           \
        (1&-B)                      \
    )  :                            \
    vext_f32(                       \
        vdup_n_f32(A.V0),           \
        vdup_n_s32(0.0F),           \
        (1&(1+(1-B)))               \
    )                               \
)

#define     VWWF_DUPD(...) MY_VWWI_DUPD(__VA_ARGS__,0)
    float32x2_t v = vdup_n_f32(a.V0);
    uint64x1_t  m = vreinterpret_u64_f32(v);
    m = vshl_u64(
        m,
        vdup_n_s64(
            (32*((-2+(3&b))+((!(1&(b<0?-b:b)))<<1)))
        )
    );
    return  vreinterpret_f32_u64(m);
}

INLINE(Vdyu,VDYU_DUPD) (Vdyu a, Rc(0, +63) b)
{
#define  MY_VDYU_DUPD(A, B, ...)  \
((Vdyu){vtst_u64(A.V0,vshl_n_u64(vdup_n_u64(1),(63&B)))})

#define     VDYU_DUPD(...) MY_VDYU_DUPD(__VA_ARGS__,0)
    a.V0 = vtst_u64(a.V0, vdup_n_u64((1ULL<<b)));
    return a;
}

INLINE(Vdbu,VDBU_DUPD) (Vdbu a, Rc(0, +7) b)
{
#define  MY_DBU_DUPD(A, B, ...)  vdup_lane_u8(A,(7&B))
#define     VDBU_DUPD(...) MY_DBU_DUPD(__VA_ARGS__,0)
    return  vtbl1_u8(a, vdup_n_u8(b));
}

INLINE(Vdbi,VDBI_DUPD) (Vdbi a, Rc(0, +7) b)
{
#define  MY_DBI_DUPD(A, B, ...)  vdup_lane_s8(A,(7&B))
#define     VDBI_DUPD(...) MY_DBI_DUPD(__VA_ARGS__,0)
    return  vtbl1_s8(a, vdup_n_u8(b));
}

INLINE(Vdbc,VDBC_DUPD) (Vdbc a, Rc(0, +7) b)
{
#if CHAR_MIN
#   define  VDBC_DUPD(...) ((Vdbc){MY_DBI_DUPD(__VA_ARGS__,0)})
    a.V0 = (VDBI_DUPD)(a.V0, b);
#else
#   define  VDBC_DUPD(...) ((Vdbc){MY_DBU_DUPD(__VA_ARGS__,0)})
    a.V0 = (VDBI_DUPD)(a.V0, b);
#endif
    return  a;
}

INLINE(Vdhu,VDHU_DUPD) (Vdhu a, Rc(0, +3) b)
{
#define  MY_DHU_DUPD(A, B, ...)  vdup_lane_u16(A,(3&B))
#define     VDHU_DUPD(...) MY_DHU_DUPD(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_u16(a);
    v = vshl_u64(v, vdup_n_s64((-16*b)));
    a = vreinterpret_u16_u64(v);
    return  vdup_lane_u16(a, 0);
}

INLINE(Vdhi,VDHI_DUPD) (Vdhi a, Rc(0, +3) b)
{
#define  MY_DHI_DUPD(A, B, ...)  vdup_lane_s16(A,(3&B))
#define     VDHI_DUPD(...) MY_DHI_DUPD(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_s16(a);
    v = vshl_u64(v, vdup_n_s64((-16*b)));
    a = vreinterpret_u16_u64(v);
    return  vdup_lane_s16(a, 0);
}

INLINE(Vdhf,VDHF_DUPD) (Vdhf a, Rc(0, +3) b)
{
#define  MY_DHF_DUPD(A, B, ...)  vdup_lane_f16(A,(3&B))
#define     VDHF_DUPD(...) MY_DHF_DUPD(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_f16(a);
    v = vshl_u64(v, vdup_n_s64((-16*b)));
    a = vreinterpret_u16_u64(v);
    return  vdup_lane_f16(a, 0);
}

INLINE(Vdwu,VDWU_DUPD) (Vdwu a, Rc(0, +1) b)
{
#define  MY_DWU_DUPD(A, B, ...)  vdup_lane_u32(A,(1&B))
#define     VDWU_DUPD(...) MY_DWU_DUPD(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_u32(a);
    v = vshl_u64(v, vdup_n_s64((-32*b)));
    a = vreinterpret_u32_u64(v);
    return  vdup_lane_u32(a, 0);
}

INLINE(Vdwi,VDWI_DUPD) (Vdwi a, Rc(0, +1) b)
{
#define  MY_DWI_DUPD(A, B, ...)  vdup_lane_s32(A,(1&B))
#define     VDWI_DUPD(...) MY_DWI_DUPD(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_s32(a);
    v = vshl_u64(v, vdup_n_s64((-32*b)));
    a = vreinterpret_s32_u64(v);
    return  vdup_lane_s32(a, 0);
}

INLINE(Vdwf,VDWF_DUPD) (Vdwf a, Rc(0, +1) b)
{
#define  MY_DWF_DUPD(A, B, ...)  vdup_lane_f32(A,(1&B))
#define     VDWF_DUPD(...) MY_DWF_DUPD(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_f32(a);
    v = vshl_u64(v, vdup_n_s64((-32*b)));
    a = vreinterpret_f32_u64(v);
    return  vdup_lane_f32(a, 0);
}

INLINE(Vddu,VDDU_DUPD) (Vddu a, Rc(-1, +0) b)
{
#define  MY_VDDU_DUPD(A, B, ...) ((0==B)?vdup_lane_u64(A,0):vdup_n_u64(0))
#define     VDDU_DUPD(...) MY_VDDU_DUPD(__VA_ARGS__,0)
    return  VDDU_DUPD(a, b);
}

INLINE(Vddi,VDDI_DUPD) (Vddi a, Rc(-1, +0) b)
{
#define  MY_VDDI_DUPD(A, B, ...) ((0==B)?vdup_lane_s64(A,0):vdup_n_s64(0))
#define     VDDI_DUPD(...) MY_VDDI_DUPD(__VA_ARGS__,0)
    return  VDDI_DUPD(a, b);
}

INLINE(Vddf,VDDF_DUPD) (Vddf a, Rc(-1, +0) b)
{
#define  MY_VDDF_DUPD(A, B, ...) ((0==B)?vdup_lane_f64(A,0):vdup_n_f64(0.0))
#define     VDDF_DUPD(...) MY_VDDF_DUPD(__VA_ARGS__,0)
    return  VDDF_DUPD(a, b);
}


INLINE(Vdyu,VQYU_DUPD) (Vqyu a, Rc(0,+127) b)
{
#define  MY_VQYU_DUPD(A, B, ...) \
(\
    (Vdyu)\
    {\
        (B > 63)\
        ?   vtst_u64(\
                vget_high_u64(A.V0),\
                vdup_n_u64((1ULL<<(63&(B-64))))\
            )\
        :   vtst_u64(\
                vget_low_u64(A.V0),\
                vdup_n_u64((1ULL<<(63&B)))\
            )\
    }\
)

#define     VQYU_DUPD(...) MY_VQYU_DUPD(__VA_ARGS__,0)
    Vdyu        v;
    uint64x1_t  m;
    if (b > 63)
    {
        v.V0 = vget_high_u64(a.V0);
        m = vdup_n_u64((1ULL<<(b-64)));
    }
    else
    {
        v.V0 = vget_low_u64(a.V0);
        m = vdup_n_u64((1ULL<<b));
    }
    v.V0 = vtst_u64(v.V0, m);
    return  v;
}

INLINE(Vdbu,VQBU_DUPD) (Vqbu a, Rc(0, +15) b)
{
#define  MY_QBU_DUPD(A, B, ...)  vdup_laneq_u8(A,(15&B))
#define     VQBU_DUPD(...) MY_QBU_DUPD(__VA_ARGS__,0)
    return  vqtbl1_u8(a, vdup_n_u8(b));
}

INLINE(Vdbi,VQBI_DUPD) (Vqbi a, Rc(0, +15) b)
{
#define     QBI_DUPD(A, B, ...)  vdup_laneq_s8(A,(15&B))
#define     VQBI_DUPD(...) QBI_DUPD(__VA_ARGS__,0)
    return  vqtbl1_s8(a, vdup_n_u8(b));
}

INLINE(Vdbc,VQBC_DUPD) (Vqbc a, Rc(0, +15) b)
{
    Vdbc v;
#if CHAR_MIN
#   define  VQBC_DUPD(...) ((Vdbc){MY_QBI_DUPD(__VA_ARGS__,0)})
    v.V0 = (VQBI_DUPD)(a.V0, b);
#else
#   define  VDBC_DUPD(...) ((Vdbc){MY_QBU_DUPD(__VA_ARGS__,0)})
    v.V0 = (VQBU_DUPD)(a.V0, b);
#endif
    return  v;
}

INLINE(Vdhu,VQHU_DUPD) (Vqhu a, Rc(0, +7) b)
{
#define  MY_QHU_DUPD(A, B, ...) vdup_laneq_u16(A,(7&B))
#define     VQHU_DUPD(...) MY_QHU_DUPD(__VA_ARGS__,0)
    uint8x16_t  t = vreinterpretq_u8_u16(a);
    uint8x8_t   k = vdup_n_u8((2*b));
    k = vadd_u8(k, vcreate_u8(0x0100010001000100ULL));
    uint8x8_t   v = vqtbl1_u8(t, k);
    return  vreinterpret_u16_u8(v);
}

INLINE(Vdhi,VQHI_DUPD) (Vqhi a, Rc(0, +7) b)
{
#define  MY_QHI_DUPD(A, B, ...) vdup_laneq_s16(A,(7&B))
#define     VQHI_DUPD(...) MY_QHI_DUPD(__VA_ARGS__,0)
    uint8x16_t  t = vreinterpretq_u8_s16(a);
    uint8x8_t   k = vdup_n_u8((2*b));
    k = vadd_u8(k, vcreate_u8(0x0100010001000100ULL));
    uint8x8_t   v = vqtbl1_u8(t, k);
    return  vreinterpret_s16_u8(v);
}

INLINE(Vdhf,VQHF_DUPD) (Vqhf a, Rc(0, +7) b)
{
#define  MY_QHF_DUPD(A, B)  vdup_laneq_f16(A,(7&B))
#define     VQHF_DUPD(...) MY_QHF_DUPD(__VA_ARGS__,0)
    return  vreinterpret_f16_u8(
        vqtbl1_u8(
            vreinterpretq_u8_f16(a),
            vzip1_u8(
                vdup_n_u8((2*b)),
                vdup_n_u8((2*b+1))
            )
        )
    );
}

INLINE(Vdwu,VQWU_DUPD) (Vqwu a, Rc(0, +3) b)
{
#define  MY_QWU_DUPD(A, B, ...) vdup_laneq_u32(A,(3&B))
#define     VQWU_DUPD(...) MY_QWU_DUPD(__VA_ARGS__,0)
    uint8x16_t  t = vreinterpretq_u8_u32(a);
    uint8x8_t   k = vdup_n_u8((4*b));
    k = vadd_u8(k, vcreate_u8(0x0302010003020100ULL));
    uint8x8_t   v = vqtbl1_u8(t, k);
    return  vreinterpret_u32_u8(v);
}


INLINE(Vdwi,VQWI_DUPD) (Vqwi a, Rc(0, +3) b)
{
#define  MY_QWI_DUPD(A, B, ...) vdup_laneq_s32(A,(3&B))
#define     VQWI_DUPD(...) MY_QWI_DUPD(__VA_ARGS__,0)
    Vqwu q = VQWI_ASTU(a);
    Vdwu d = (VQWU_DUPD)(q,b);
    return  VDWU_ASTI(d);
}


INLINE(Vdwf,VQWF_DUPD) (Vqwf a, Rc(0, +3) b)
{
#define  MY_QWF_DUPD(A, B, ...) vdup_laneq_f32(A,(3&B))
#define     VQWF_DUPD(...) MY_QWF_DUPD(__VA_ARGS__,0)
    Vqwu q = VQWF_ASTU(a);
    Vdwu d = (VQWU_DUPD)(q,b);
    return  VDWU_ASTF(d);
}


INLINE(Vddu,VQDU_DUPD) (Vqdu a, Rc(0, +1) b)
{
#define  MY_QDU_DUPD(A,B,...)  vdup_laneq_u64(A,(1&B))
#define     VQDU_DUPD(...)     MY_QDU_DUPD(__VA_ARGS__,0)
    return  b?vget_high_u64(a):vget_low_u64(a);
}

INLINE(Vddi,VQDI_DUPD) (Vqdi a, Rc(0, +1) b)
{
#define  MY_QDI_DUPD(A,B,...)  vdup_laneq_s64(A,(1&B))
#define     VQDI_DUPD(...)     MY_QDI_DUPD(__VA_ARGS__,0)
    return  b?vget_high_s64(a):vget_low_s64(a);
}

INLINE(Vddf,VQDF_DUPD) (Vqdf a, Rc(0, +1) b)
{
#define  MY_QDF_DUPD(A,B,...)  vdup_laneq_f64(A,(1&B))
#define     VQDF_DUPD(...)     MY_QDF_DUPD(__VA_ARGS__,0)
    return  b?vget_high_f64(a):vget_low_f64(a);

}


INLINE(Vdyu,  BOOL_DUPDAC)   (_Bool const a[1])
{
    return  BOOL_DUPD( (*(_Bool const *) a) );
}

INLINE(Vdbu, UCHAR_DUPDAC)   (uchar const a[1]) {return vld1_dup_u8(a);}
INLINE(Vdbi, SCHAR_DUPDAC)   (schar const a[1]) {return vld1_dup_s8(a);}
INLINE(Vdbc,  CHAR_DUPDAC)    (char const a[1])
{
    return  VDBU_ASBC(vld1_dup_u8( ((void const *) a) ) );
}

INLINE(Vdhu, USHRT_DUPDAC)  (ushort const a[1]) {return vld1_dup_u16(a);}
INLINE(Vdhi,  SHRT_DUPDAC)   (short const a[1]) {return vld1_dup_s16(a);}
INLINE(Vdwu,  UINT_DUPDAC)    (uint const a[1]) {return vld1_dup_u32(a);}
INLINE(Vdwi,   INT_DUPDAC)     (int const a[1]) {return vld1_dup_s32(a);}

#if DWRD_NLONG == 2

INLINE(Vdwu, ULONG_DUPDAC)   (ulong const a[1])
{
    return  vld1_dup_u32( ((uint32_t const *) a) );
}

INLINE(Vdwi,  LONG_DUPDAC)    (long const a[1])
{
    return  vld1_dup_s32( ((int32_t const *) a) );
}

#else

INLINE(Vddu, ULONG_DUPDAC)   (ulong const a[1]) {return  vld1_dup_u64(a);}
INLINE(Vddi,  LONG_DUPDAC)    (long const a[1]) {return  vld1_dup_s64(a);}

#endif

#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_DUPDAC)  (ullong const a[1])
{
    return  vld1_dup_u64( ((uint64_t const *) a) );
}

INLINE(Vddi, LLONG_DUPDAC)   (llong const a[1])
{
    return  vld1_dup_s64( ((int64_t const *) a) );
}

#endif

INLINE(Vdhf, FLT16_DUPDAC) (flt16_t const a[1]) {return vld1_dup_f16(a);}
INLINE(Vdwf,   FLT_DUPDAC)   (float const a[1]) {return vld1_dup_f32(a);}
INLINE(Vddf,   DBL_DUPDAC)  (double const a[1]) {return vld1_dup_f64(a);}

#if 0 // _LEAVE_ARM_DUPD
}
#endif

#if 0 // _ENTER_ARM_DUPQ
{
#endif

INLINE(Vqyu,BOOL_DUPQ)     (_Bool a, Rc(-128,+127) b)
{
#define  MY_BOOL_DUPQ(A, B, ...)                    \
(                                                   \
    !A ? ((Vqyu){0}) :                              \
    (                                               \
        (Vqyu)                                      \
        {                                           \
            (B <=-128) ? vdupq_n_u64(0) :           \
            (B <= -64) ? vcombine_u64(              \
                vdup_n_u64((UINT64_MAX<<(128+B))),  \
                vdup_n_u64(-1)                      \
            ) :                                     \
            (B <= -1) ? vcombine_u64(               \
                vdup_n_u64(0),                      \
                vdup_n_u64((UINT64_MAX<<(64+B)))    \
            ) :                                     \
            (B == 0) ? vdupq_n_u64(-1) :            \
            (B <= 64) ? vcombine_u64(               \
                vdup_n_u64((UINT64_MAX>>(64-B))),   \
                vdup_n_u64(0)                       \
            ) :                                     \
            (B <= 127) ? vcombine_u64(              \
                vdup_n_u64(-1),                     \
                vdup_n_u64((UINT64_MAX>>(128-B))),  \
            ) :                                     \
            vdupq_n_u64(0)                          \
        }                                           \
    )                                               \
)

#define     BOOL_DUPQ(...) MY_BOOL_DUPQ(__VA_ARGS__,0)
    if (!a) 
        return ((Vqyu){0});
    Vqyu v;
    if (b < -64)
    {
        v.V0 = vcombine_u64(
            vshl_u64(
                vdup_n_u64(-1),
                vdup_n_s64((128+b))
            ),
            vdup_n_u64(-1)
        );
        return  v;
    }

    if (b < 0)
    {
        v.V0 = vcombine_u64(
            vdup_n_u64(0),
            vshl_u64(
                vdup_n_u64(-1),
                vdup_n_s64((64+b))
            )
        );
        return  v;
    }

    if (b == 0)
    {
        v.V0 = vdupq_n_u64(-1);
        return  v;
    }

    if (b <= 64)
    {
        v.V0 = vcombine_u64(
            vdup_n_u64((UINT64_MAX>>(64-b))),
            vdup_n_u64(0)
        );
        return  v;
    }
    
    v.V0 = vcombine_u64(
        vdup_n_u64(-1),
        vdup_n_u64((UINT64_MAX>>(64-b)))
    );
    return v;
    
}
 
INLINE(Vqbu,UCHAR_DUPQ) (unsigned a, Rc(-16, +15) b) 
{
#define  MY_UCHAR_DUPQ(A, B, ...)   \
(                                   \
    (B ==-16) ? vdupq_n_u8(0) :     \
    (B <= -1) ? vextq_u8(           \
        vdupq_n_u8(0),              \
        vdupq_n_u8(A),              \
        (15&-B)                     \
    )  :                            \
    vextq_u8(                       \
        vdupq_n_u8(A),              \
        vdupq_n_u8(0),              \
        (15&(1+(15-B)))             \
    )                               \
)

#define     UCHAR_DUPQ(...) MY_UCHAR_DUPQ(__VA_ARGS__,0)
    uint8x8_t   l, r;
    uint8x16_t  t;
    if (!b) 
        return vdupq_n_u8(a);
    if (b < 0) 
    {
        r = vcreate_u8(0x1011121314151617ULL);
        l = vcreate_u8(0x18191a1b1c1d1e1fULL);
        t = vcombine_u8(l, r);
        if (b&15)
            t = vaddq_u8(t, vdupq_n_u8(b));
    }
    else
    {
        l = vcreate_u8(0x1716151413121110ULL);
        r = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        t = vsubq_u8(t, vdupq_n_u8(b));
    }
    return  vqtbl1q_u8(vdupq_n_u8(a), t);
}

INLINE(Vqbi,SCHAR_DUPQ)   (signed a, Rc(-16, +15) b) 
{
#define  MY_SCHAR_DUPQ(A, B, ...)   \
(                                   \
    (B ==-16) ? vdupq_n_s8(0) :     \
    (B <= -1) ? vextq_s8(           \
        vdupq_n_s8(0),              \
        vdupq_n_s8(A),              \
        (15&-B)                     \
    )  :                            \
    vextq_s8(                       \
        vdupq_n_s8(A),              \
        vdupq_n_s8(0),              \
        (15&(1+(15-B)))             \
    )                               \
)

#define     SCHAR_DUPQ(...) MY_SCHAR_DUPQ(__VA_ARGS__,0)
    Vqbu u = (UCHAR_DUPQ)(a, b);
    return  VQBU_ASBI(u);
}

INLINE(Vqbc,CHAR_DUPQ)       (int a, Rc(-16, +15) b)
{
    Vqbc v;
#if CHAR_MIN
#   define  CHAR_DUPQ(...) ((Vqbc){MY_SCHAR_DUPQ(__VA_ARGS__,0)})
    v.V0 = VQBU_ASBI(((UCHAR_DUPQ)(a, b)));
#else
#   define  CHAR_DUPQ(...) ((Vqbc){MY_UCHAR_DUPQ(__VA_ARGS__,0)})
    v.V0 = (UCHAR_DUPQ)(a, b);
#endif
    return  v;
}

INLINE(Vqhu,USHRT_DUPQ) (unsigned a, Rc(-8, +7) b) 
{
#define  MY_USHRT_DUPQ(A, B, ...)   \
(                                   \
    (B == -8) ? vdupq_n_u16(0) :    \
    (B <= -1) ? vextq_u16(          \
        vdupq_n_u16(0),             \
        vdupq_n_u16(A),             \
        (7&-B)                      \
    )  :                            \
    vextq_u16(                      \
        vdupq_n_u16(A),             \
        vdupq_n_u16(0),             \
        (7&(1+(7-B)))               \
    )                               \
)

#define     USHRT_DUPQ(...) MY_USHRT_DUPQ(__VA_ARGS__,0)
    uint8x8_t   l, r;
    uint8x16_t  t;
    uint8x16_t  v;
    uint16x8_t  src = vdupq_n_u16(a);
    if (!b) 
        return  src;
    v = vreinterpretq_u8_u16(src);
    if (b < 0) 
    {
        r = vcreate_u8(0x1110131215141716ULL);
        l = vcreate_u8(0x19181b1a1d1c1f1eULL);
        t = vcombine_u8(l, r);
        if (b&7)
            t = vaddq_u8(t, vdupq_n_u8((2*b)));
    }
    else
    {
        l = vcreate_u8(0x1716151413121110ULL);
        r = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        t = vsubq_u8(t, vdupq_n_u8((2*b)));
    }
    v = vqtbl1q_u8(v, t);
    return  vreinterpretq_u16_u8(v);
}

INLINE(Vqhi,SHRT_DUPQ)    (signed a, Rc(-8,+7) b) 
{
#define  MY_SHRT_DUPQ(A, B, ...)    \
(                                   \
    (B == -8) ? vdupq_n_s16(0) :    \
    (B <= -1) ? vextq_s16(          \
        vdupq_n_s16(0),             \
        vdupq_n_s16(A),             \
        (7&-B)                      \
    )  :                            \
    vextq_s16(                      \
        vdupq_n_s16(A),             \
        vdupq_n_s16(0),             \
        (7&(1+(7-B)))               \
    )                               \
)

#define     SHRT_DUPQ(...) MY_SHRT_DUPQ(__VA_ARGS__,0)
    uint16x8_t v = (USHRT_DUPQ)(a, b);
    return  vreinterpretq_s16_u16(v);
}

INLINE(Vqwu,UINT_DUPQ)      (uint a, Rc(-4, +3) b) 
{
#define  MY_UINT_DUPQ(A, B, ...)    \
(                                   \
    (B == -4) ? vdupq_n_u32(0) :    \
    (B <= -1) ? vextq_u32(          \
        vdupq_n_u32(0),             \
        vdupq_n_u32(A),             \
        (3&-B)                      \
    )  :                            \
    vextq_u32(                      \
        vdupq_n_u32(A),             \
        vdupq_n_u32(0),             \
        (3&(1+(3-B)))               \
    )                               \
)

#define     UINT_DUPQ(...) MY_UINT_DUPQ(__VA_ARGS__,0)
    uint8x8_t   l, r;
    uint8x16_t  t;
    uint8x16_t  v;
    uint32x4_t  src = vdupq_n_u32(a);
    if (!b) 
        return  src;
    v = vreinterpretq_u8_u32(src);
    if (b < 0) 
    {
        r = vcreate_u8(0x1312111017161514ULL);
        l = vcreate_u8(0x1b1a19a81f1e1d1cULL);
        t = vcombine_u8(l, r);
        if (b&3)
            t = vaddq_u8(t, vdupq_n_u8((4*b)));
    }
    else
    {
        l = vcreate_u8(0x1716151413121110ULL);
        r = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        t = vsubq_u8(t, vdupq_n_u8((4*b)));
    }
    v = vqtbl1q_u8(v, t);
    return  vreinterpretq_u32_u8(t);
}

INLINE(Vqwi,INT_DUPQ)        (int a, Rc(-4, +3) b)
{
#define  MY_INT_DUPQ(A, B, ...)     \
(                                   \
    (B == -4) ? vdupq_n_s32(0) :    \
    (B <= -1) ? vextq_s32(          \
        vdupq_n_s32(0),             \
        vdupq_n_s32(A),             \
        (3&-B)                      \
    )  :                            \
    vextq_s32(                      \
        vdupq_n_s32(A),             \
        vdupq_n_s32(0),             \
        (3&(1+(3-B)))               \
    )                               \
)

#define     INT_DUPQ(...) MY_INT_DUPQ(__VA_ARGS__,0)
    uint32x4_t v = (UINT_DUPQ)(a, b);
    return  vreinterpretq_s32_u32(v);
}

#if DWRD_NLONG == 2

INLINE(Vqwu,ULONG_DUPQ)    (ulong a, Rc(-4, +3) b) 
{
#define     ULONG_DUPQ(...) MY_UINT_DUPQ(__VA_ARGS__,0) 
    return  (UINT_DUPQ)(a, b);
}

INLINE(Vqwi,LONG_DUPQ)      (long a, Rc(-4, +3) b) 
{
#define     LONG_DUPQ(...) MY_INT_DUPQ(__VA_ARGS__,0) 
    return  (INT_DUPQ)(a, b);
}

#else

INLINE(Vqdu,ULONG_DUPQ)    (ulong a, Rc(-2, +1) b) 
{
#define  MY_ULONG_DUPQ(A, B, ...)   \
(                                   \
    (B == -2) ? vdupq_n_u64(0) :    \
    (B <= -1) ? vextq_u64(          \
        vdupq_n_u64(0),             \
        vdupq_n_u64(A),             \
        (1&-B)                      \
    )  :                            \
    vextq_u64(                      \
        vdupq_n_u64(A),             \
        vdupq_n_u64(0),             \
        (1&(1+(1-B)))               \
    )                               \
)

#define     ULONG_DUPQ(...) MY_ULONG_DUPQ(__VA_ARGS__,0)
    uint8x8_t   l, r;
    uint8x16_t  t;
    uint64x2_t  src = vdupq_n_u64(a);
    if (!b) 
        return  src;
    uint8x16_t  v = vreinterpretq_u8_u64(src);
    if (b < 0) 
    {
        r = vcreate_u8(0x1716151413121110ULL);
        l = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        if (b&3)
            t = vaddq_u8(t, vdupq_n_u8((8*b)));
    }
    else
    {
        l = vcreate_u8(0x1716151413121110ULL);
        r = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        t = vsubq_u8(t, vdupq_n_u8((8*b)));
    }
    v = vqtbl1q_u8(v, t);
    return  vreinterpretq_u64_u8(t);
}

INLINE(Vqdi,LONG_DUPQ)      (long a, Rc(-2, +1) b) 
{
#define  MY_LONG_DUPQ(A, B, ...)    \
(                                   \
    (B == -2) ? vdupq_n_s64(0) :    \
    (B <= -1) ? vextq_s64(          \
        vdupq_n_s64(0),             \
        vdupq_n_s64(A),             \
        (1&-B)                      \
    )  :                            \
    vextq_s64(                      \
        vdupq_n_s64(A),             \
        vdupq_n_s64(0),             \
        (1&(1+(1-B)))               \
    )                               \
)

#define     LONG_DUPQ(...) MY_LONG_DUPQ(__VA_ARGS__,0)
    uint64x2_t v = (ULONG_DUPQ)(a, b);
    return  vreinterpretq_s64_u64(v);
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_DUPQ)    (ullong a, Rc(-2, +1) b) 
{
#define  MY_ULLONG_DUPQ(A, B, ...)  \
(                                   \
    (B == -2) ? vdupq_n_u64(0) :    \
    (B <= -1) ? vextq_u64(          \
        vdupq_n_u64(0),             \
        vdupq_n_u64(A),             \
        (1&-B)                      \
    )  :                            \
    vextq_u64(                      \
        vdupq_n_u64(A),             \
        vdupq_n_u64(0),             \
        (1&(1+(1-B)))               \
    )                               \
)

#define     ULLONG_DUPQ(...) MY_ULLONG_DUPQ(__VA_ARGS__,0)
    uint8x8_t   l, r;
    uint8x16_t  t;
    uint64x2_t  src = vdupq_n_u64(a);
    if (!b) 
        return  src;
    uint8x16_t  v = vreinterpretq_u8_u64(src);
    if (b < 0) 
    {
        r = vcreate_u8(0x1716151413121110ULL);
        l = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        if (b&3)
            t = vaddq_u8(t, vdupq_n_u8((8*b)));
    }
    else
    {
        l = vcreate_u8(0x1716151413121110ULL);
        r = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        t = vsubq_u8(t, vdupq_n_u8((8*b)));
    }
    v = vqtbl1q_u8(v, t);
    return  vreinterpretq_u64_u8(t);
}

INLINE(Vqdi,LLONG_DUPQ)       (llong a, Rc(-2, +1) b) 
{
#define  MY_LLONG_DUPQ(A, B, ...)   \
(                                   \
    (B == -2) ? vdupq_n_s64(0) :    \
    (B <= -1) ? vextq_s64(          \
        vdupq_n_s64(0),             \
        vdupq_n_s64(A),             \
        (1&-B)                      \
    )  :                            \
    vextq_s64(                      \
        vdupq_n_s64(A),             \
        vdupq_n_s64(0),             \
        (1&(1+(1-B)))               \
    )                               \
)


#define     LLONG_DUPQ(...) MY_LLONG_DUPQ(__VA_ARGS__,0)
    uint64x2_t v = (ULLONG_DUPQ)(a, b);
    return  vreinterpretq_s64_u64(v);
}

INLINE(Vqqu,dupqqu) (QUAD_UTYPE a, Rc(-1, +0) b) 
{
#define     MY_DUPQQU(A, B, ...) ((-1==B)?((Vqqu){0}):astvqu(A))
#   define  dupqqu(...) MY_DUPQQU(__VA_ARGS__,0)
    return  dupqqu(a, b);
}

INLINE(Vqqi,dupqqi) (QUAD_ITYPE a, Rc(-1, +0) b) 
{
#define     MY_DUPQQI(A, B, ...) ((-1==B)?((Vqqi){0}):astvqi(A))
#   define  dupqqi(...) MY_DUPQQI(__VA_ARGS__,0)
    return  dupqqi(a, b);
}

#endif

INLINE(Vqhf,FLT16_DUPQ) (flt16_t a, Rc(-8, +7) b) 
{
#define  MY_FLT16_DUPQ(A, B, ...)       \
(                                       \
    (B == -8) ? vdupq_n_f16(0) :        \
    (B <= -1) ? vreinterpretq_f16_u16(  \
        vextq_u16(                      \
            vdupq_n_u16(0),             \
            vreinterpretq_u16_f16(      \
                vdupq_n_f16(A)          \
            ),                          \
            (7&-B)                      \
        )                               \
    )  :                                \
    vreinterpretq_f16_u16(              \
        vextq_u16(                      \
            vreinterpretq_u16_f16(      \
                vdupq_n_f16(A)          \
            ),                          \
            vdupq_n_u16(0),             \
            (7&(1+(7-B)))               \
        )                               \
    )                                   \
)

#define     FLT16_DUPQ(...) MY_FLT16_DUPQ(__VA_ARGS__,0)
    uint8x8_t   l, r;
    uint8x16_t  t;
    uint8x16_t  v;
    float16x8_t src = vdupq_n_f16(a);
    if (!b) 
        return  src;
    v = vreinterpretq_u8_f16(src);
    if (b < 0) 
    {
        r = vcreate_u8(0x1110131215141716ULL);
        l = vcreate_u8(0x19181b1a1d1c1f1eULL);
        t = vcombine_u8(l, r);
        if (b&7)
            t = vaddq_u8(t, vdupq_n_u8((2*b)));
    }
    else
    {
        l = vcreate_u8(0x1716151413121110ULL);
        r = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        t = vsubq_u8(t, vdupq_n_u8((2*b)));
    }
    v = vqtbl1q_u8(v, t);
    return  vreinterpretq_f16_u8(v);
}

INLINE(Vqwf,FLT_DUPQ)   (float a, Rc(-4, 3) b) 
{
#define  MY_FLT_DUPQ(A, B, ...)     \
(                                   \
    (B == -4) ? vdupq_n_f32(0.0f) : \
    (B <= -1) ? vextq_f32(          \
        vdupq_n_f32(0.0f),          \
        vdupq_n_f32(A),             \
        (3&-B)                      \
    )  :                            \
    vextq_f16(                      \
        vdupq_n_f32(A),             \
        vdupq_n_f32(0.0f),          \
        (3&(1+(3-B)))               \
    )                               \
)

#define     FLT_DUPQ(...) MY_FLT_DUPQ(__VA_ARGS__,0)
    uint8x8_t   l, r;
    uint8x16_t  t;
    uint8x16_t  v;
    float32x4_t src = vdupq_n_f32(a);
    if (!b) 
        return  src;
    v = vreinterpretq_u8_f32(src);
    if (b < 0) 
    {
        l = vcreate_u8(0x1b1a19a81f1e1d1cULL);
        r = vcreate_u8(0x1312111017161514ULL);
        t = vcombine_u8(l, r);
        if (b&3)
            t = vaddq_u8(t, vdupq_n_u8((4*b)));
    }
    else
    {
        l = vcreate_u8(0x1716151413121110ULL);
        r = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        t = vsubq_u8(t, vdupq_n_u8((4*b)));
    }
    v = vqtbl1q_u8(v, t);
    return  vreinterpretq_f32_u8(v);
}

INLINE(Vqdf,DBL_DUPQ)  (double a, Rc(-2, +1) b) 
{
#define  MY_DBL_DUPQ(A, B, ...)     \
(                                   \
    (B == -2) ? vdupq_n_f64(0.0) :  \
    (B <= -1) ? vextq_f64(          \
        vdupq_n_f64(0.0),           \
        vdupq_n_f64(A),             \
        (1&-B)                      \
    )  :                            \
    vextq_f64(                      \
        vdupq_n_f64(A),             \
        vdupq_n_f64(0.0),           \
        (1&(1+(1-B)))               \
    )                               \
)

#define     DBL_DUPQ(...) MY_DBL_DUPQ(__VA_ARGS__,0)
    uint8x8_t   l, r;
    uint8x16_t  t;
    float64x2_t src = vdupq_n_f64(a);
    if (!b) 
        return  src;
    uint8x16_t  v = vreinterpretq_u8_f64(src);
    if (b < 0) 
    {
        r = vcreate_u8(0x1716151413121110ULL);
        l = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        if (b&3)
            t = vaddq_u8(t, vdupq_n_u8((8*b)));
    }
    else
    {
        l = vcreate_u8(0x1716151413121110ULL);
        r = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        t = vsubq_u8(t, vdupq_n_u8((8*b)));
    }
    v = vqtbl1q_u8(v, t);
    return  vreinterpretq_f64_u8(t);
}


INLINE(Vqyu,BOOL_DUPQAC) (void const *a, ptrdiff_t b) 
{
#define  MY_BOOL_DUPQAC(A, B, ...) ((BOOL_DUPQAC)(A,B))
#define     BOOL_DUPQAC(...) MY_BOOL_DUPQAC(__VA_ARGS__,0)
    uint8x16_t  m = vld1q_dup_u8((a+b/8));
    m = vtstq_u8(m, vdupq_n_u8((1<<(b&7))));
    Vqyu    v = {vreinterpretq_u64_u8(m)};
    return  v;
}

INLINE(Vqbu,UCHAR_DUPQAC) (uchar const a[1], ptrdiff_t b)
{
#define  MY_UCHAR_DUPQAC(A, B, ...) vld1q_dup_u8( ((uint8_t const *) A+B) )
#define     UCHAR_DUPQAC(...) MY_UCHAR_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_u8((a+b));
}

INLINE(Vqbi, SCHAR_DUPQAC) (schar const a[1], ptrdiff_t b) 
{
#define  MY_SCHAR_DUPQAC(A, B, ...) vld1q_dup_s8( ((int8_t const *) A+B) )
#define     SCHAR_DUPQAC(...) MY_SCHAR_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_s8((a+b));
}

INLINE(Vqbc,  CHAR_DUPQAC)  (char const a[1], ptrdiff_t b)
{
#if CHAR_MIN
#   define  CHAR_DUPQAC(...) ((Vqbc){SCHAR_DUPQAC(__VA_ARGS__)})
#else
#   define  CHAR_DUPQAC(...) ((Vqbc){UCHAR_DUPQAC(__VA_ARGS__)})
#endif
    return  CHAR_DUPQAC(a, b);
}

INLINE(Vqhu, USHRT_DUPQAC) (ushort const a[1], ptrdiff_t b) 
{
#define  MY_USHRT_DUPQAC(A, B, ...) vld1q_dup_u16( ((uint16_t const *) A+B) )
#define     USHRT_DUPQAC(...) MY_USHRT_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_u16((a+b));
}

INLINE(Vqhi,  SHRT_DUPQAC)  (short const a[1], ptrdiff_t b) 
{
#define  MY_SHRT_DUPQAC(A, B, ...) vld1q_dup_s16( ((int16_t const *) A+B) )
#define     SHRT_DUPQAC(...) MY_SHRT_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_s16((a+b));
}

INLINE(Vqwu,  UINT_DUPQAC)   (uint const a[1], ptrdiff_t b) 
{
#define  MY_UINT_DUPQAC(A, B, ...) vld1q_dup_u32( ((uint32_t const *) A+B) )
#define     UINT_DUPQAC(...) MY_UINT_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_u32((a+b));
}

INLINE(Vqwi,   INT_DUPQAC)    (int const a[1], ptrdiff_t b)
{
#define  MY_INT_DUPQAC(A, B, ...) vld1q_dup_s32( ((int32_t const *) A+B) )
#define     INT_DUPQAC(...) MY_INT_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_s32((a+b));
}

#if DWRD_NLONG == 2

INLINE(Vqwu,  ULONG_DUPQAC)   (ulong const a[1], ptrdiff_t b) 
{
#define  MY_ULONG_DUPQAC(A, B, ...) vld1q_dup_u32( ((uint32_t const *) A+B) )
#define     ULONG_DUPQAC(...) MY_ULONG_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_u32((a+b));
}

INLINE(Vqwi,   LONG_DUPQAC)    (long const a[1], ptrdiff_t b)
{
#define  MY_LONG_DUPQAC(A, B, ...) vld1q_dup_s32( ((int32_t const *) A+B) )
#define     LONG_DUPQAC(...) MY_LONG_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_s32((a+b));
}

#else

INLINE(Vqdu,  ULONG_DUPQAC)   (ulong const a[1], ptrdiff_t b) 
{
#define  MY_ULONG_DUPQAC(A, B, ...) vld1q_dup_u64( ((uint64_t const *) A+B) )
#define     ULONG_DUPQAC(...) MY_ULONG_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_u64((a+b));
}

INLINE(Vqdi,   LONG_DUPQAC)    (long const a[1], ptrdiff_t b)
{
#define  MY_LONG_DUPQAC(A, B, ...) vld1q_dup_s64( ((int64_t const *) A+B) )
#define     LONG_DUPQAC(...) MY_LONG_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_s64((a+b));
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_DUPQAC) (ullong const a[1], ptrdiff_t b) 
{
#define  MY_ULLONG_DUPQAC(A, B, ...) vld1q_dup_u64( ((uint64_t const *) A+B) )
#define     ULLONG_DUPQAC(...) MY_ULLONG_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_u64(((uint64_t const *) a+b));
}

INLINE(Vqdi, LLONG_DUPQAC)  (llong const a[1], ptrdiff_t b)
{
#define  MY_LLONG_DUPQAC(A, B, ...) vld1q_dup_s64( ((int64_t const *) A+B) )
#define     LLONG_DUPQAC(...) MY_LLONG_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_s64(((int64_t const *) a+b));
}

INLINE(Vqqu,dupqacqu) (QUAD_UTYPE const a[1], ptrdiff_t b) 
{
#define  MY_DUPQACQU(A, B, ...) ((Vqqu){vld1q_u64(((uint64_t const *) A+B))})
#define     dupqacqu(...) MY_DUPQACQU(__VA_ARGS__,0)
    return  dupqacqu(a, b);
}

INLINE(Vqqi,dupqacqi) (QUAD_ITYPE const a[1], ptrdiff_t b) 
{
#define  MY_DUPQACQI(A, B, ...) ((Vqqi){vld1q_s64(((int64_t const *) A+B))})
#define     dupqacqi(...) MY_DUPQACQI(__VA_ARGS__,0)
    return  dupqacqi(a, b);
}

#endif

INLINE(Vqhf, FLT16_DUPQAC) (flt16_t const a[1], ptrdiff_t b)
{
#define  MY_FLT16_DUPQAC(A, B, ...) vld1q_dup_f16( ((flt16_t const *) A+B) )
#define     FLT16_DUPQAC(...) MY_FLT16_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_f16((a+b));
}

INLINE(Vqwf,   FLT_DUPQAC)   (float const a[1], ptrdiff_t b)
{
#define  MY_FLT_DUPQAC(A, B, ...) vld1q_dup_f32( ((float const *) A+B) )
#define     FLT_DUPQAC(...) MY_FLT_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_f32((a+b));
}

INLINE(Vqdf,   DBL_DUPQAC)   (double const a[1], ptrdiff_t b)
{
#define  MY_DBL_DUPQAC(A, B, ...) vld1q_dup_f64( ((double const *) A+B) )
#define     DBL_DUPQAC(...) MY_DBL_DUPQAC(__VA_ARGS__,0)
    return  vld1q_dup_f64((a+b));
}


INLINE(Vqyu,VWYU_DUPQ) (Vwyu v, Rc(0, 31) k)
{
#define     WYU_DUPQ(M, K)          \
vdupq_n_u64(                        \
    vtstd_u64(                      \
        (UINT64_C(1)<<(K)),         \
        vget_lane_u32(              \
            vreinterpret_u32_f32(   \
                vdup_n_f32(M)       \
            ),                      \
            0                       \
        )                           \
    )                               \
)

#define     VWYU_DUPQ(V, K) QYU_ASTV(WYU_DUPQ(VWYU_ASTM(V), K))
    return  VWYU_DUPQ(v, k);
}


INLINE(Vqbu,VWBU_DUPQ) (Vwbu a, Rc(0,  3) b)
{
#define  MY_VWBU_DUPQ(A, B, ...) \
vdupq_lane_u8(vreinterpret_u8_f32(((float32x2_t){A.V0})),B)

#define     VWBU_DUPQ(...) MY_VWBU_DUPQ(__VA_ARGS__,0)
    float32x2_t m = {a.V0};
    uint8x8_t   t = vreinterpret_u8_f32(m);
    t = vtbl1_u8(t, vdup_n_u8(b));
    return  vcombine_u8(t, t);
}

INLINE(Vqbi,VWBI_DUPQ) (Vwbi a, Rc(0,  3) b)
{
#define  MY_VWBI_DUPQ(A, B, ...) \
vdupq_lane_s8(vreinterpret_s8_f32(((float32x2_t){A.V0})),B)

#define     VWBI_DUPQ(...) MY_VWBI_DUPQ(__VA_ARGS__,0)
    float32x2_t m = {a.V0};
    int8x8_t    t = vreinterpret_s8_f32(m);
    t = vtbl1_s8(t, vdup_n_u8(b));
    return  vcombine_s8(t, t);
}

INLINE(Vqbc,VWBC_DUPQ) (Vwbc a, Rc(0,  3) b)
{
    float32x2_t m = {a.V0};
    uint8x8_t   t = vreinterpret_u8_f32(m);
    t = vtbl1_u8(t, vdup_n_u8(b));
    uint8x16_t  q = vcombine_u8(t, t);
#if CHAR_MIN
#   define  VWBC_DUPQ(...) ((Vqbc){MY_VWBI_DUPQ(__VA_ARGS__,0)})
    return  ((Vqbc){vreinterpretq_s8_u8(q)});
#else
#   define  VWBC_DUPQ(...) ((Vqbc){MY_VWBU_DUPQ(__VA_ARGS__,0)})
    return  ((Vqbc){q});
#endif
}


INLINE(Vqhu,VWHU_DUPQ) (Vwhu a, Rc(0, 1) b)
{
#define  MY_VWHU_DUPQ(A, B, ...) \
vdupq_lane_u16(vreinterpret_u16_f32(((float32x2_t){A.V0})),B)

#define     VWHU_DUPQ(...) MY_VWHU_DUPQ(__VA_ARGS__,0)

    float32x4_t m = {a.V0};
    uint8x8_t   l = vdup_n_u8((2*b));
    uint8x8_t   r = vadd_u8(l, vcreate_u8(0x0100010001000100ULL));
    uint8x16_t  t = vcombine_u8(l, r);
    t =  vqtbl1q_u8(vreinterpretq_u8_f32(m), t);
    return  vreinterpretq_u16_u8(t);
}

INLINE(Vqhi,VWHI_DUPQ) (Vwhi a, Rc(0, 1) b)
{
#define  MY_VWHI_DUPQ(A, B, ...) \
vdupq_lane_s16(vreinterpret_s16_f32(((float32x2_t){A.V0})),B)

#define     VWHI_DUPQ(...) MY_VWHI_DUPQ(__VA_ARGS__,0)

    float32x4_t m = {a.V0};
    uint8x8_t   l = vdup_n_u8((2*b));
    uint8x8_t   r = vadd_u8(l, vcreate_u8(0x0100010001000100ULL));
    uint8x16_t  t = vcombine_u8(l, r);
    t =  vqtbl1q_u8(vreinterpretq_u8_f32(m), t);
    return  vreinterpretq_s16_u8(t);
}

INLINE(Vqhf,VWHF_DUPQ) (Vwhf a, Rc(0, 1) b)
{
#define  MY_VWHF_DUPQ(A, B, ...) \
vdupq_lane_f16(vreinterpret_f16_f32(((float32x2_t){A.V0})),B)

#define     VWHF_DUPQ(...) MY_VWHF_DUPQ(__VA_ARGS__,0)

    float32x4_t m = {a.V0};
    uint8x8_t   l = vdup_n_u8((2*b));
    uint8x8_t   r = vadd_u8(l, vcreate_u8(0x0100010001000100ULL));
    uint8x16_t  t = vcombine_u8(l, r);
    t =  vqtbl1q_u8(vreinterpretq_u8_f32(m), t);
    return  vreinterpretq_f16_u8(t);
}


INLINE(Vqwu,VWWU_DUPQ) (Vwwu a, Rc(-4, +3) b) 
{
#define  MY_VWWU_DUPQ(A, B, ...)    \
(                                   \
    (B == -4) ? vdupq_n_u32(0) :    \
    (B <= -1) ? vextq_u32(          \
        vdupq_n_u32(0),             \
        vreinterpretq_u32_f32(      \
            vdupq_n_f32(A.V0)       \
        ),                          \
        (3&-B)                      \
    )  :                            \
    vextq_u32(                      \
        vreinterpretq_u32_f32(      \
            vdupq_n_f32(A.V0)       \
        ),                          \
        vdupq_n_u32(0),             \
        (3&(1+(3-B)))               \
    )                               \
)

#define     VWWU_DUPQ(...) MY_VWWU_DUPQ(__VA_ARGS__,0)
    float32x4_t q = (FLT_DUPQ)(a.V0, b);
    return  vreinterpretq_u32_f32(q);
}

INLINE(Vqwi,VWWI_DUPQ) (Vwwi a, Rc(-4, +3) b)
{
#define  MY_VWWI_DUPQ(A, B, ...)    \
(                                   \
    (B == -4) ? vdupq_n_s32(0) :    \
    (B <= -1) ? vextq_s32(          \
        vdupq_n_s32(0),             \
        vreinterpretq_s32_f32(      \
            vdupq_n_f32(A.V0)       \
        ),                          \
        (3&-B)                      \
    )  :                            \
    vextq_u32(                      \
        vreinterpretq_u32_f32(      \
            vdupq_n_f32(A.V0)       \
        ),                          \
        vdupq_n_s32(0),             \
        (3&(1+(3-B)))               \
    )                               \
)

#define     VWWI_DUPQ(...) MY_VWWI_DUPQ(__VA_ARGS__,0)
    float32x4_t q = (FLT_DUPQ)(a.V0, b);
    return  vreinterpretq_u32_f32(q);
}

INLINE(Vqwf,VWWF_DUPQ) (Vwwf a, Rc(-4, +3) b)
{
#define  MY_VWWF_DUPQ(A, B, ...)    \
(                                   \
    (B == -2) ? vdupq_n_f32(0.0F) : \
    (B <= -1) ? vextq_f32(          \
        vdupq_n_f32(0.0F),          \
        vdupq_n_f32(A.V0),          \
        (1&-B)                      \
    )  :                            \
    vextq_f32(                      \
        vdupq_n_f32(A.V0),          \
        vdupq_n_s32(0.0F),          \
        (1&(1+(1-B)))               \
    )                               \
)

#define     VWWF_DUPQ(...)  MY_VWWF_DUPQ(__VA_ARGS__,0)
    return  (FLT_DUPQ)(a.V0, b);
}



INLINE(Vqyu,VDYU_DUPQ) (Vdyu a, Rc(0, +63) b)
{
#define  MY_VDYU_DUPQ(A, B, ...) \
((Vqyu){vdupq_lane_u64(vtst_u64(a.V0,vdup_n_u64((1ULL<<B))),0)})

#define     VDYU_DUPQ(...) MY_VDYU_DUPQ(__VA_ARGS__,0)
    uint64x1_t m = vdup_n_u64((1<<b));
    a.V0 = vtst_u64(a.V0, m);
    return ((Vqyu){vdupq_lane_u64(a.V0, 0)});
}

INLINE(Vqbu,VDBU_DUPQ) (Vdbu a, Rc(0,  +7) b)
{
#define  MY_VDBU_DUPQ(A, B, ...)  vdupq_lane_u8(A,B)
#define     VDBU_DUPQ(...)  MY_VDBU_DUPQ(__VA_ARGS__,0)
    return  vqtbl1q_u8(vcombine_u8(a,a), vdupq_n_u8(b));
}

INLINE(Vqbi,VDBI_DUPQ) (Vdbi a, Rc(0, +7) b)
{
#define  MY_VDBI_DUPQ(A, B, ...)  vdupq_lane_s8(A,B)
#define     VDBI_DUPQ(...)  MY_VDBI_DUPQ(__VA_ARGS__,0)
    a = vtbl1_s8(a, vdup_n_u8(b));
    return  vcombine_s8(a, a);
}

INLINE(Vqbc,VDBC_DUPQ) (Vdbc a, Rc(0, +7) b)
{
    Vqbc c;
#if CHAR_MIN
#   define  VDBC_DUPQ(...)  ((Vdbc){MY_VDBI_DUPQ(__VA_ARGS__,0)})
    c.V0 = (VDBI_DUPQ)(a.V0, b);
#else
#   define  VDBC_DUPQ(...)  ((Vdbc){MY_VDBU_DUPQ(__VA_ARGS__,0)})
    c.V0 = (VDBU_DUPQ)(a.V0, b);
#endif
    return  c;
}


INLINE(Vqhu,VDHU_DUPQ) (Vdhu a, Rc(0, +3) b)
{
#define  MY_VDHU_DUPQ(A, B, ...)  vdupq_lane_u16(A,B)
#define     VDHU_DUPQ(...)  MY_VDHU_DUPQ(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_u16(a);
    v = vshl_u64(v, vdup_n_s64((-2*b)));
    a = vreinterpret_u16_u64(v);
    return  vdupq_lane_u16(a, 0);
}

INLINE(Vqhi,VDHI_DUPQ) (Vdhi a, Rc(0, +3) b)
{
#define  MY_VDHI_DUPQ(A, B, ...)  vdupq_lane_16(A,B)
#define     VDHI_DUPQ(...)  MY_VDHI_DUPQ(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_s16(a);
    v = vshl_u64(v, vdup_n_s64((-2*b)));
    a = vreinterpret_s16_u64(v);
    return  vdupq_lane_s16(a, 0);
}

INLINE(Vqhf,VDHF_DUPQ) (Vdhf a, Rc(0, +3) b)
{
#define  MY_VDHF_DUPQ(A, B, ...)  vdupq_lane_f16(A,B)
#define     VDHF_DUPQ(...)  MY_VDHF_DUPQ(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_f16(a);
    v = vshl_u64(v, vdup_n_s64((-2*b)));
    a = vreinterpret_f16_u64(v);
    return  vdupq_lane_f16(a, 0);
}


INLINE(Vqwu,VDWU_DUPQ) (Vdwu a, Rc(0, +1) b)
{
#define  MY_VDWU_DUPQ(A, B, ...)  vdupq_lane_u32(A,B)
#define     VDWU_DUPQ(...)  MY_VDWU_DUPQ(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_u32(a);
    v = vshl_u64(v, vdup_n_s64((-4*b)));
    a = vreinterpret_u32_u64(v);
    return  vdupq_lane_u32(a, 0);
}

INLINE(Vqwi,VDWI_DUPQ) (Vdwi a, Rc(0, +1) b)
{
#define  MY_VDWI_DUPQ(A, B, ...)  vdupq_lane_s32(A,B)
#define     VDWI_DUPQ(...)  MY_VDWI_DUPQ(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_s32(a);
    v = vshl_u64(v, vdup_n_s64((-4*b)));
    a = vreinterpret_s32_u64(v);
    return  vdupq_lane_s32(a, 0);
}

INLINE(Vqwf,VDWF_DUPQ) (Vdwf a, Rc(0, +1) b)
{
#define  MY_VDWF_DUPQ(A, B, ...)  vdupq_lane_f32(A,B)
#define     VDWF_DUPQ(...)  MY_VDWF_DUPQ(__VA_ARGS__,0)
    uint64x1_t  v = vreinterpret_u64_f32(a);
    v = vshl_u64(v, vdup_n_s64((-4*b)));
    a = vreinterpret_f32_u64(v);
    return  vdupq_lane_f32(a, 0);
}


INLINE(Vqdu,VDDU_DUPQ) (Vddu a, Rc(-2, +1) b)
{
#define  MY_VDDU_DUPQ(A, B, ...)    \
(                                   \
    (B == -2) ? vdupq_n_u64(0) :    \
    (B <= -1) ? vextq_u64(          \
        vdupq_n_u64(0),             \
        vdupq_lane_u64(A,0),        \
        (1&-B)                      \
    )  :                            \
    vextq_u64(                      \
        vdupq_lane_u64(A,0),        \
        vdupq_n_u64(0),             \
        (1&(1+(1-B)))               \
    )                               \
)

#define     VDDU_DUPQ(...) MY_VDDU_DUPQ(__VA_ARGS__,0)
    uint8x8_t   l, r;
    uint8x16_t  t;
    uint64x2_t  src = vdupq_lane_u64(a,0);
    if (!b) 
        return  src;
    uint8x16_t  v = vreinterpretq_u8_u64(src);
    if (b < 0) 
    {
        r = vcreate_u8(0x1716151413121110ULL);
        l = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        if (b&3)
            t = vaddq_u8(t, vdupq_n_u8((8*b)));
    }
    else
    {
        l = vcreate_u8(0x1716151413121110ULL);
        r = vcreate_u8(0x1f1e1d1c1b1a1918ULL);
        t = vcombine_u8(l, r);
        t = vsubq_u8(t, vdupq_n_u8((8*b)));
    }
    v = vqtbl1q_u8(v, t);
    return  vreinterpretq_u64_u8(t);

}

INLINE(Vqdi,VDDI_DUPQ) (Vddi a, Rc(-2, +1) b)
{
#define  MY_VDDI_DUPQ(A, B, ...)    \
(                                   \
    (B == -2) ? vdupq_n_s64(0) :    \
    (B <= -1) ? vextq_s64(          \
        vdupq_n_s64(0),             \
        vdupq_lane_s64(A,0),        \
        (1&-B)                      \
    )  :                            \
    vextq_s64(                      \
        vdupq_lane_s64(A,0),        \
        vdupq_n_s64(0),             \
        (1&(1+(1-B)))               \
    )                               \
)

#define     VDDI_DUPQ(...) MY_VDDI_DUPQ(__VA_ARGS__,0)
    uint64x1_t d = vreinterpret_u64_s64(a);
    uint64x2_t q = (VDDU_DUPQ)(d, 0);
    return  vreinterpretq_s64_u64(q);
}

INLINE(Vqdf,VDDF_DUPQ) (Vddf a, Rc(-2, +1) b)
{
#define  MY_VDDF_DUPQ(A, B, ...)    \
(                                   \
    (B == -2) ? vdupq_n_f64(0.0) :  \
    (B <= -1) ? vextq_f64(          \
        vdupq_n_f64(0.0),           \
        vdupq_lane_f64(A,0),        \
        (1&-B)                      \
    )  :                            \
    vextq_f64(                      \
        vdupq_lane_f64(A,0),        \
        vdupq_n_f64(0.0),           \
        (1&(1+(1-B)))               \
    )                               \
)

#define     VDDF_DUPQ(...) MY_VDDF_DUPQ(__VA_ARGS__,0)
    uint64x1_t d = vreinterpret_u64_f64(a);
    uint64x2_t q = (VDDU_DUPQ)(d, 0);
    return  vreinterpretq_f64_u64(q);
}


INLINE(Vqyu,VQYU_DUPQ) (Vqyu a, Rc(0,+127) b)
{
#define  MY_VQYU_DUPQ(A, B, ...) ((VQYU_DUPQ)(A,B))
#define     VQYU_DUPQ(...) MY_VQYU_DUPQ(__VA_ARGS__,0)
    uint64x1_t  v, m;
    int64x1_t   n;
    if (b > 63)
    {
        v = vget_high_u64(a.V0);
        m = vdup_n_u64((1ULL<<(b-64)));
    }
    else
    {
        v = vget_low_u64(a.V0);
        m = vdup_n_u64((1ULL<<b));
    }
    v = vtst_u64(v, m);
    a.V0 = vdupq_lane_u64(v, 0);
    return a;
}


INLINE(Vqbu,VQBU_DUPQ) (Vqbu a, Rc(0, +15) b)
{
#define  MY_VQBU_DUPQ(A, B, ...) vdupq_laneq_u8(A,(15&B))
#define     VQBU_DUPQ(...) MY_VQBU_DUPQ(__VA_ARGS__,0)
    return  vqtbl1q_u8(a, vdupq_n_u8(b));
}

INLINE(Vqbi,VQBI_DUPQ) (Vqbi a, Rc(0, +15) b)
{
#define  MY_VQBI_DUPQ(A, B, ...) vdupq_laneq_s8(A,(15&B))
#define     VQBI_DUPQ(...) MY_VQBI_DUPQ(__VA_ARGS__,0)
    return  vqtbl1q_s8(a, vdupq_n_u8(b));
}

INLINE(Vqbc,VQBC_DUPQ) (Vqbc a, Rc(0, +15) b)
{
#if CHAR_MIN
#   define  VQBC_DUPQ(...)  ((Vqbc){MY_VQBI_DUPQ(__VA_ARGS__,0)})
    a.V0 = (VQBI_DUPQ)(a.V0, b);
#else
#   define  VDBC_DUPQ(...)  ((Vqbc){MY_VQBU_DUPQ(__VA_ARGS__,0)})
    a.V0 = (VDBU_DUPQ)(a.V0, b);
#endif
    return  a;
}


INLINE(Vqhu,VQHU_DUPQ) (Vqhu a, Rc(0, +7) b)
{
#define  MY_VQHU_DUPQ(A, B, ...) vdupq_laneq_u16(A,(7&B))
#define     VQHU_DUPQ(...) MY_VQHU_DUPQ(__VA_ARGS__,0)
    uint8x16_t  t = vreinterpretq_u8_u16(a);
    t = vqtbl1q_u8(
        t,
        vzip1q_u8(
            vdupq_n_u8((2*b)),
            vdupq_n_u8((2*b+1))
        )
    );
    return  vreinterpretq_u16_u8(t);
}

INLINE(Vqhi,VQHI_DUPQ) (Vqhi a, Rc(0, +7) b)
{
#define  MY_VQHI_DUPQ(A, B, ...) vdupq_laneq_s16(A,(7&B))
#define     VQHI_DUPQ(...) MY_VQHI_DUPQ(__VA_ARGS__,0)
    uint8x16_t  t = vreinterpretq_u8_s16(a);
    t = vqtbl1q_u8(
        t,
        vzip1q_u8(
            vdupq_n_u8((2*b)),
            vdupq_n_u8((2*b+1))
        )
    );
    return  vreinterpretq_s16_u8(t);
}

INLINE(Vqhf,VQHF_DUPQ) (Vqhf a, Rc(0, +7) b)
{
#define  MY_VQHF_DUPQ(A, B, ...) vdupq_laneq_f16(A,(7&B))
#define     VQHF_DUPQ(...) MY_VQHF_DUPQ(__VA_ARGS__,0)
    uint8x16_t  t = vreinterpretq_u8_f16(a);
    t = vqtbl1q_u8(
        t,
        vzip1q_u8(
            vdupq_n_u8((2*b)),
            vdupq_n_u8((2*b+1))
        )
    );
    return  vreinterpretq_f16_u8(t);
}


INLINE(Vqwu,VQWU_DUPQ) (Vqwu a, Rc(0, +3) b)
{
#define  MY_VQWU_DUPQ(A, B, ...) vdupq_laneq_u32(A,(3&B))
#define     VQWU_DUPQ(...) MY_VQWU_DUPQ(__VA_ARGS__,0)
    uint32x2_t  m = vcreate_u32(0x0302010003020100ULL);
    uint8x8_t   k = vreinterpret_u8_u32(m);
    k = vadd_u8(k, vdup_n_u8((4*b)));
    uint8x16_t  t = vreinterpretq_u8_u32(a);
    t = vqtbl1q_u8(t, vcombine_u8(k, k) );
    return  vreinterpretq_u32_u8(t);
}

INLINE(Vqwi,VQWI_DUPQ) (Vqwi a, Rc(0, +3) b)
{
#define  MY_VQWI_DUPQ(A, B, ...) vdupq_laneq_s32(A,(3&B))
#define     VQWI_DUPQ(...) MY_VQWI_DUPQ(__VA_ARGS__,0)
    uint8x16_t  t = vreinterpretq_u8_s32(a);
    uint32x2_t  m = vcreate_u32(0x0302010003020100ULL);
    uint8x8_t   k = vreinterpret_u8_u32(m);
    k = vadd_u8(k, vdup_n_u8((4*b)));
    t = vqtbl1q_u8(t, vcombine_u8(k, k) );
    return  vreinterpretq_s32_u8(t);
}

INLINE(Vqwf,VQWF_DUPQ) (Vqwf a, Rc(0, +3) b)
{
#define  MY_VQWF_DUPQ(A, B, ...) vdupq_laneq_f32(A,(3&B))
#define     VQWF_DUPQ(...) MY_VQWF_DUPQ(__VA_ARGS__,0)
    uint8x16_t  t = vreinterpretq_u8_f32(a);
    uint32x2_t  m = vcreate_u32(0x0302010003020100ULL);
    uint8x8_t   k = vreinterpret_u8_u32(m);
    k = vadd_u8(k, vdup_n_u8((4*b)));
    t = vqtbl1q_u8(t, vcombine_u8(k, k) );
    return  vreinterpretq_f32_u8(t);
}


INLINE(Vqdu,VQDU_DUPQ) (Vqdu a, Rc(0, +1) b)
{
#define  MY_VQDU_DUPQ(A, B, ...)  vdupq_laneq_u64(A,(1&B))
#define     VQDU_DUPQ(...) MY_VQDU_DUPQ(__VA_ARGS__,0)
    return  (b==1)?vdupq_laneq_u64(a,1):vdupq_laneq_u64(a,0);
}

INLINE(Vqdi,VQDI_DUPQ) (Vqdi a, Rc(0, +1) b)
{
#define  MY_VQDI_DUPQ(A, B, ...)  vdupq_laneq_s64(A,(1&B))
#define     VQDI_DUPQ(...) MY_VQDI_DUPQ(__VA_ARGS__,0)
    return  (b==1)?vdupq_laneq_s64(a,1):vdupq_laneq_s64(a,0);
}

INLINE(Vqdf,VQDF_DUPQ) (Vqdf a, Rc(0, 1) b)
{
#define  MY_VQDF_DUPQ(A, B, ...)  vdupq_laneq_f64(A,(1&B))
#define     VQDF_DUPQ(...) MY_VQDF_DUPQ(__VA_ARGS__,0)
    return  (b==1)?vdupq_laneq_f64(a,1):vdupq_laneq_f64(a,0);
}


INLINE(Vqqu,VQQU_DUPQ) (Vqqu a, Rc(-1, +0) b)
{
#define  MY_VQQU_DUPQ(A, B, ...)  ((Vqqu){(0==B)?A.V0:vdupq_n_u64(0)})
#define     VQQU_DUPQ(...) MY_VQQU_DUPQ(__VA_ARGS__,0)
    return  VQQU_DUPQ(a, b);
}

INLINE(Vqqi,VQQI_DUPQ) (Vqqi a, Rc(-1, +0) b)
{
#define  MY_VQQI_DUPQ(A, B, ...)  ((Vqqi){(0==B)?A.V0:vdupq_n_s64(0)})
#define     VQQI_DUPQ(...) MY_VQQI_DUPQ(__VA_ARGS__,0)
    return  VQQI_DUPQ(a, b);
}

INLINE(Vqqf,VQQF_DUPQ) (Vqqf a, Rc(-1, +0) b)
{
#define  MY_VQQF_DUPQ(A, B, ...)  ((Vqqf){(0==B)?A.V0:0.0L})
#define     VQQF_DUPQ(...) MY_VQQF_DUPQ(__VA_ARGS__,0)
    return  VQQF_DUPQ(a, b);
}

#if 0 // _LEAVE_ARM_DUPQ
}
#endif

#if 0 // _ENTER_ARM_DUPL
{
#endif

INLINE(Vwyu,VWYU_DUPL) (Vwyu x)
{
#define     VWYU_DUPL(X) VWYU_DUPW(X, 0)
    return  VWYU_DUPL(x);
}

INLINE(Vwbu,VWBU_DUPL) (Vwbu x) {return VWBU_DUPW(x, 0);}
INLINE(Vwbi,VWBI_DUPL) (Vwbi x) {return VWBI_DUPW(x, 0);}
INLINE(Vwbc,VWBC_DUPL) (Vwbc x) {return VWBC_DUPW(x, 0);}
INLINE(Vwhu,VWHU_DUPL) (Vwhu x) {return VWHU_DUPW(x, 0);}
INLINE(Vwhi,VWHI_DUPL) (Vwhi x) {return VWHI_DUPW(x, 0);}
INLINE(Vwhf,VWHF_DUPL) (Vwhf x) {return VWHF_DUPW(x, 0);}

INLINE(Vdyu,VDYU_DUPL) (Vdyu x)
{
#define     VDYU_DUPL(X) VDDU_ASYU(vtst_u64(vdup_n_u64(1ull),VDYU_ASDU(X)))
    return  VDYU_DUPL(x);
}

INLINE(Vdbu,VDBU_DUPL) (Vdbu x) {return vdup_lane_u8(x, 0);}
INLINE(Vdbi,VDBI_DUPL) (Vdbi x) {return vdup_lane_s8(x, 0);}
INLINE(Vdbc,VDBC_DUPL) (Vdbc x)
{
#   define  VDBC_DUPL(X) VDBU_ASBC(vdup_lane_u8(VDBC_ASBU(X),0))
    return  VDBC_DUPL(x);
}

INLINE(Vdhu,VDHU_DUPL) (Vdhu x) {return vdup_lane_u16(x, 0);}
INLINE(Vdhi,VDHI_DUPL) (Vdhi x) {return vdup_lane_s16(x, 0);}
INLINE(Vdhf,VDHF_DUPL) (Vdhf x)
{
// TODO: verify this convolution is necessary
#if defined(SPC_ARM_FP16_SIMD)
#   define  VDHF_DUPL(X) vdup_lane_f16(x, 0);
#else
#   define  VDHF_DUPL(X) \
vreinterpret_f16_u16(vdup_lane_u16(vreinterpret_u16_f16(X), 0))
#endif
    return VDHF_DUPL(x);
}

INLINE(Vdwu,VDWU_DUPL) (Vdwu x) {return vdup_lane_u32(x, 0);}
INLINE(Vdwi,VDWI_DUPL) (Vdwi x) {return vdup_lane_s32(x, 0);}
INLINE(Vdwf,VDWF_DUPL) (Vdwf x) {return vdup_lane_f32(x, 0);}


INLINE(Vqyu,VQYU_DUPL) (Vqyu x)
{
#define     QYU_DUPL(M) \
vdupq_lane_u64(         \
    vtst_u64(           \
        vget_low_u64(M),\
        vdup_n_u64(1ull)\
    ),                  \
    0                   \
)

#define     VQYU_DUPL(X) VQDU_ASYU(QYU_DUPL(VQYU_ASTM(X)))
    return  VQYU_DUPL(x);
}

INLINE(Vqbu,VQBU_DUPL) (Vqbu x) {return vdupq_laneq_u8(x, 0);}
INLINE(Vqbi,VQBI_DUPL) (Vqbi x) {return vdupq_laneq_s8(x, 0);}
INLINE(Vqbc,VQBC_DUPL) (Vqbc x)
{
#   define  VQBC_DUPL(X) VQBU_ASBC(vdupq_laneq_u8(VQBC_ASBU(X),0))
    return  VQBC_DUPL(x);
}

INLINE(Vqhu,VQHU_DUPL) (Vqhu x) {return vdupq_laneq_u16(x, 0);}
INLINE(Vqhi,VQHI_DUPL) (Vqhi x) {return vdupq_laneq_s16(x, 0);}
INLINE(Vqhf,VQHF_DUPL) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
#   define  VQHF_DUPL(X) vdupq_laneq_f16(x, 0);
#else
#   define  VQHF_DUPL(X) \
vreinterpretq_f16_u16(vdupq_laneq_u16(vreinterpretq_u16_f16(X), 0))
#endif
    return VQHF_DUPL(x);
}

INLINE(Vqwu,VQWU_DUPL) (Vqwu x) {return vdupq_laneq_u32(x, 0);}
INLINE(Vqwi,VQWI_DUPL) (Vqwi x) {return vdupq_laneq_s32(x, 0);}
INLINE(Vqwf,VQWF_DUPL) (Vqwf x) {return vdupq_laneq_f32(x, 0);}
INLINE(Vqdu,VQDU_DUPL) (Vqdu x) {return vdupq_laneq_u64(x, 0);}
INLINE(Vqdi,VQDI_DUPL) (Vqdi x) {return vdupq_laneq_s64(x, 0);}
INLINE(Vqdf,VQDF_DUPL) (Vqdf x) {return vdupq_laneq_f64(x, 0);}

#if 0 // _LEAVE_ARM_DUPL
}
#endif

#if 0 // _ENTER_ARM_ZIPL
{
#endif

INLINE(float,WBU_ZIPL) (float a, float b)
{
    uint8x8_t l = vreinterpret_u8_f32(vdup_n_f32(a));
    uint8x8_t r = vreinterpret_u8_f32(vdup_n_f32(b));
    uint8x8_t c = vzip1_u8(l, r);
    return  vget_lane_f32(vreinterpret_f32_u8(c), 0);
}

INLINE(float,WHU_ZIPL) (float a, float b)
{
    uint16x4_t l = vreinterpret_u16_f32(vdup_n_f32(a));
    uint16x4_t r = vreinterpret_u16_f32(vdup_n_f32(b));
    uint16x4_t c = vzip1_u16(l, r);
    return  vget_lane_f32(vreinterpret_f32_u16(c), 0);
}

INLINE(uint64_t, UINT64_ZIPL) (uint64_t a, uint64_t b)
{
    return  UINT_ZIPP(a, b);
}

INLINE(uint64_t, UINT64_ZIPR) (uint64_t a, uint64_t b)
{
    return UINT_ZIPP((a>>32), (b>>32));
}


INLINE(Vwyu,VWYU_ZIPL) (Vwyu a, Vwyu b)
{
    uint32_t x = FLT_ASTU(VWYU_ASTM(a));
    uint32_t y = FLT_ASTU(VWYU_ASTM(b));
    uint32x2_t z = vdup_n_u32(USHRT_ZIPP(x, y));
    return WYU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(z),
            0
        )
    );
}


INLINE(Vwbu,VWBU_ZIPL) (Vwbu a, Vwbu b)
{
    return  WBU_ASTV(WBU_ZIPL(VWBU_ASTM(b), VWBU_ASTM(b)));
}

INLINE(Vwbi,VWBI_ZIPL) (Vwbi a, Vwbi b)
{
    return  WBI_ASTV(WBU_ZIPL(VWBI_ASTM(a), VWBI_ASTM(a)));
}

INLINE(Vwbc,VWBC_ZIPL) (Vwbc a, Vwbc b)
{
    return  WBC_ASTV(WBU_ZIPL(VWBC_ASTM(a), VWBC_ASTM(a)));
}


INLINE(Vwhu,VWHU_ZIPL) (Vwhu a, Vwhu b)
{
    return  WHU_ASTV(WHU_ZIPL(VWHU_ASTM(b), VWHU_ASTM(b)));
}

INLINE(Vwhi,VWHI_ZIPL) (Vwhi a, Vwhi b)
{
    return  WHI_ASTV(WHU_ZIPL(VWHI_ASTM(b), VWHI_ASTM(b)));
}

INLINE(Vwhf,VWHF_ZIPL) (Vwhf a, Vwhf b)
{
    return  WHF_ASTV(WHU_ZIPL(VWHF_ASTM(b), VWHF_ASTM(b)));
}


INLINE(Vdyu,VDYU_ZIPL) (Vdyu a, Vdyu b)
{
    return  VDDU_ASYU(
        vdup_n_u64(
            UINT_ZIPP(
                vget_lane_u64(VDYU_ASDU(a), 0),
                vget_lane_u64(VDYU_ASDU(b), 0)
            )
        )
    );
}

INLINE(Vdbu,VDBU_ZIPL) (Vdbu a, Vdbu b) {return vzip1_u8(a, b);}
INLINE(Vdbi,VDBI_ZIPL) (Vdbi a, Vdbi b) {return vzip1_s8(a, b);}
INLINE(Vdbc,VDBC_ZIPL) (Vdbc a, Vdbc b)
{
    return  VDBU_ASBC(vzip1_u8(VDBC_ASBU(a), VDBC_ASBU(b)));
}

INLINE(Vdhu,VDHU_ZIPL) (Vdhu a, Vdhu b) {return vzip1_u16(a, b);}
INLINE(Vdhi,VDHI_ZIPL) (Vdhi a, Vdhi b) {return vzip1_s16(a, b);}
INLINE(Vdhf,VDHF_ZIPL) (Vdhf a, Vdhf b)
{
    return  VDHU_ASHF(vzip1_u16(VDHF_ASHU(a), VDHF_ASHU(b)));
}

INLINE(Vdwu,VDWU_ZIPL) (Vdwu a, Vdwu b) {return vzip1_u32(a, b);}
INLINE(Vdwi,VDWI_ZIPL) (Vdwi a, Vdwi b) {return vzip1_s32(a, b);}
INLINE(Vdwf,VDWF_ZIPL) (Vdwf a, Vdwf b) {return vzip1_f32(a, b);}


INLINE(Vqyu,VQYU_ZIPL) (Vqyu a, Vqyu b)
{
    uint64_t p = vget_lane_u64(vget_low_u64(VQYU_ASTM(a)), 0);
    uint64_t q = vget_lane_u64(vget_low_u64(VQYU_ASTM(b)), 0);
    uint64_t l = UINT_ZIPP(p, q);
    uint64_t r = UINT_ZIPP(p>>32, q>>32);
    return  VQDU_ASYU(
        vcombine_u64(
            vdup_n_u64(l),
            vdup_n_u64(r)
        )
    );
}

INLINE(Vqbu,VQBU_ZIPL) (Vqbu a, Vqbu b) {return vzip1q_u8(a, b);}
INLINE(Vqbi,VQBI_ZIPL) (Vqbi a, Vqbi b) {return vzip1q_s8(a, b);}
INLINE(Vqbc,VQBC_ZIPL) (Vqbc a, Vqbc b)
{
    return  VQBU_ASBC(vzip1q_u8(VQBC_ASBU(a), VQBC_ASBU(b)));
}

INLINE(Vqhu,VQHU_ZIPL) (Vqhu a, Vqhu b) {return vzip1q_u16(a, b);}
INLINE(Vqhi,VQHI_ZIPL) (Vqhi a, Vqhi b) {return vzip1q_s16(a, b);}
INLINE(Vqhf,VQHF_ZIPL) (Vqhf a, Vqhf b)
{
    return  VQHU_ASHF(vzip1q_u16(VQHF_ASHU(a), VQHF_ASHU(b)));
}

INLINE(Vqwu,VQWU_ZIPL) (Vqwu a, Vqwu b) {return vzip1q_u32(a, b);}
INLINE(Vqwi,VQWI_ZIPL) (Vqwi a, Vqwi b) {return vzip1q_s32(a, b);}
INLINE(Vqwf,VQWF_ZIPL) (Vqwf a, Vqwf b) {return vzip1q_f32(a, b);}

INLINE(Vqdu,VQDU_ZIPL) (Vqdu a, Vqdu b) {return vzip1q_u64(a, b);}
INLINE(Vqdi,VQDI_ZIPL) (Vqdi a, Vqdi b) {return vzip1q_s64(a, b);}
INLINE(Vqdf,VQDF_ZIPL) (Vqdf a, Vqdf b) {return vzip1q_f64(a, b);}

#if 0 // _LEAVE_ARM_ZIPL
}
#endif

#if 0 // _ENTER_ARM_ZIPR
{
#endif

INLINE(float, MY_ZIPRW) (float a, float b)
{
    float32x2_t c;
    c = vset_lane_f32(a, c, 0);
    c = vset_lane_f32(b, c, 1);
    return vget_lane_f32(vzip2_f32(c, c), 0);
}

//efine     WYU_ZIPR
#define     WBU_ZIPR    MY_ZIPRW
#define     WBI_ZIPR    MY_ZIPRW
#define     WBC_ZIPR    MY_ZIPRW
#define     WHU_ZIPR    MY_ZIPRW
#define     WHI_ZIPR    MY_ZIPRW
#define     WHF_ZIPR    MY_ZIPRW
//efine     WWU_ZIPR
//efine     WWI_ZIPR
//efine     WWF_ZIPR

#define     DBU_ZIPR    vzip2_u8
#define     DBI_ZIPR    vzip2_s8
#if CHAR_MIN
#   define  DBC_ZIPR    vzip2_s8
#else
#   define  DBC_ZIPR    vzip2_u8
#endif

#define     DHU_ZIPR    vzip2_u16
#define     DHI_ZIPR    vzip2_s16
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_ZIPR    vzip2_f16
#else
#   define  DHF_ZIPR(A, B)      \
vreinterpret_f16_u16(           \
    vzip2_u16(                  \
        vreinterpret_u16_f16(A),\
        vreinterpret_u16_f16(A) \
    )                           \
)
#endif

#define     DWU_ZIPR    vzip2_u32
#define     DWI_ZIPR    vzip2_s32
#define     DWF_ZIPR    vzip2_f32


#define     QBU_ZIPR    vzip2q_u8
#define     QBI_ZIPR    vzip2q_s8
#if CHAR_MIN
#   define  QBC_ZIPR    vzip2q_s8
#else
#   define  QBC_ZIPR    vzip2q_u8
#endif

#define     QHU_ZIPR    vzip2q_u16
#define     QHI_ZIPR    vzip2q_s16

#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_ZIPR    vzip2q_f16
#else
#   define  QHF_ZIPR(A, B)          \
vreinterpretq_f16_u16(              \
    vzip2q_u16(                     \
        vreinterpretq_u16_f16(A),   \
        vreinterpretq_u16_f16(A)    \
    )                               \
)
#endif

#define     QWU_ZIPR    vzip2q_u32
#define     QWI_ZIPR    vzip2q_s32
#define     QWF_ZIPR    vzip2q_f32
#define     QDU_ZIPR    vzip2q_u64
#define     QDI_ZIPR    vzip2q_s64
#define     QDF_ZIPR    vzip2q_f64


INLINE(Vwbu,VWBU_ZIPR) (Vwbu a, Vwbu b)
{
#define     VWBU_ZIPR(A, B) WBU_ASTV(WBU_ZIPR(VWBU_ASTM(A), VWBU_ASTM(B)))
    return  VWBU_ZIPR(a, b);
}

INLINE(Vwbi,VWBI_ZIPR) (Vwbi a, Vwbi b)
{
#define     VWBI_ZIPR(A, B) WBI_ASTV(WBI_ZIPR(VWBI_ASTM(A), VWBI_ASTM(B)))
    return  VWBI_ZIPR(a, b);
}

INLINE(Vwbc,VWBC_ZIPR) (Vwbc a, Vwbc b)
{
#define     VWBC_ZIPR(A, B) WBC_ASTV(WBC_ZIPR(VWBC_ASTM(A), VWBC_ASTM(B)))
    return  VWBC_ZIPR(a, b);
}


INLINE(Vwhu,VWHU_ZIPR) (Vwhu a, Vwhu b)
{
#define     VWHU_ZIPR(A, B) WHU_ASTV(WHU_ZIPR(VWHU_ASTM(A), VWHU_ASTM(B)))
    return  VWHU_ZIPR(a, b);
}

INLINE(Vwhi,VWHI_ZIPR) (Vwhi a, Vwhi b)
{
#define     VWHI_ZIPR(A, B) WHI_ASTV(WHI_ZIPR(VWHI_ASTM(A), VWHI_ASTM(B)))
    return  VWHI_ZIPR(a, b);
}

INLINE(Vwhf,VWHF_ZIPR) (Vwhf a, Vwhf b)
{
#define     VWHF_ZIPR(A, B) WHF_ASTV(WHF_ZIPR(VWHF_ASTM(A), VWHF_ASTM(B)))
    return  VWHF_ZIPR(a, b);
}


INLINE(Vdbu,VDBU_ZIPR) (Vdbu a, Vdbu b) {return vzip2_u8(a, b);}
INLINE(Vdbi,VDBI_ZIPR) (Vdbi a, Vdbi b) {return vzip2_s8(a, b);}
INLINE(Vdbc,VDBC_ZIPR) (Vdbc a, Vdbc b)
{
    return  VDBU_ASBC(
        vzip2_u8(
            VDBC_ASBU(a),
            VDBC_ASBU(b)
        )
    );
}

INLINE(Vdhu,VDHU_ZIPR) (Vdhu a, Vdhu b) {return vzip2_u16(a, b);}
INLINE(Vdhi,VDHI_ZIPR) (Vdhi a, Vdhi b) {return vzip2_s16(a, b);}
INLINE(Vdhf,VDHF_ZIPR) (Vdhf a, Vdhf b)
{
    return vreinterpret_f16_u16(
        vzip2_u16(
            vreinterpret_u16_f16(a),
            vreinterpret_u16_f16(b)
        )
    );
}

INLINE(Vdwu,VDWU_ZIPR) (Vdwu a, Vdwu b) {return vzip2_u32(a, b);}
INLINE(Vdwi,VDWI_ZIPR) (Vdwi a, Vdwi b) {return vzip2_s32(a, b);}
INLINE(Vdwf,VDWF_ZIPR) (Vdwf a, Vdwf b) {return vzip2_f32(a, b);}

INLINE(Vqbu,VQBU_ZIPR) (Vqbu a, Vqbu b) {return vzip2q_u8(a, b);}
INLINE(Vqbi,VQBI_ZIPR) (Vqbi a, Vqbi b) {return vzip2q_s8(a, b);}
INLINE(Vqbc,VQBC_ZIPR) (Vqbc a, Vqbc b)
{
    return  VQBU_ASBC(
        vzip2q_u8(
            VQBC_ASBU(a),
            VQBC_ASBU(b)
        )
    );
}

INLINE(Vqhu,VQHU_ZIPR) (Vqhu a, Vqhu b) {return vzip2q_u16(a, b);}
INLINE(Vqhi,VQHI_ZIPR) (Vqhi a, Vqhi b) {return vzip2q_s16(a, b);}
INLINE(Vqhf,VQHF_ZIPR) (Vqhf a, Vqhf b)
{
    return vreinterpretq_f16_u16(
        vzip2q_u16(
            vreinterpretq_u16_f16(a),
            vreinterpretq_u16_f16(b)
        )
    );
}

INLINE(Vqwu,VQWU_ZIPR) (Vqwu a, Vqwu b) {return vzip2q_u32(a, b);}
INLINE(Vqwi,VQWI_ZIPR) (Vqwi a, Vqwi b) {return vzip2q_s32(a, b);}
INLINE(Vqwf,VQWF_ZIPR) (Vqwf a, Vqwf b) {return vzip2q_f32(a, b);}

INLINE(Vqdu,VQDU_ZIPR) (Vqdu a, Vqdu b) {return vzip2q_u64(a, b);}
INLINE(Vqdi,VQDI_ZIPR) (Vqdi a, Vqdi b) {return vzip2q_s64(a, b);}
INLINE(Vqdf,VQDF_ZIPR) (Vqdf a, Vqdf b) {return vzip2q_f64(a, b);}

#if 0 // _LEAVE_ARM_ZIPR
}
#endif

#if 0 // _ENTER_ARM_ZIPP
{
#endif

INLINE(Vdyu,VWYU_ZIPP) (Vwyu a, Vwyu b)
{
    unsigned p = VWWU_ASTV(VWYU_ASWU(a));
    unsigned q = VWWU_ASTV(VWYU_ASWU(b));
    return  VDDU_ASYU(UINT64_ASTV(UINT_ZIPP(p, q)));
}

INLINE(Vdbu,VWBU_ZIPP) (Vwbu a, Vwbu b)
{
    float32x2_t l = vdup_n_f32(VWBU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBU_ASTM(b));
    return  vzip1_u8(VDWF_ASBU(l), VDWF_ASBU(r));
}

INLINE(Vdbi,VWBI_ZIPP) (Vwbi a, Vwbi b)
{
    float32x2_t l = vdup_n_f32(VWBI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBI_ASTM(b));
    return  vzip1_s8(VDWF_ASBI(l), VDWF_ASBI(r));
}

INLINE(Vdbc,VWBC_ZIPP) (Vwbc a, Vwbc b)
{
    float32x2_t l = vdup_n_f32(VWBC_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBC_ASTM(b));
    return  VDBU_ASBC(vzip1_u8(VDWF_ASBU(l), VDWF_ASBU(r)));
}


INLINE(Vdhu,VWHU_ZIPP) (Vwhu a, Vwhu b)
{
    float32x2_t l = vdup_n_f32(VWHU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHU_ASTM(b));
    return  vzip1_u16(VDWF_ASHU(l), VDWF_ASHU(r));
}

INLINE(Vdhi,VWHI_ZIPP) (Vwhi a, Vwhi b)
{
    float32x2_t l = vdup_n_f32(VWHI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHI_ASTM(b));
    return  vzip1_s16(VDWF_ASHI(l), VDWF_ASHI(r));
}

INLINE(Vdhf,VWHF_ZIPP) (Vwhf a, Vwhf b)
{
    float32x2_t l = vdup_n_f32(VWHF_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHF_ASTM(b));
    return  VDHU_ASHF(vzip1_u16(VDWF_ASHU(l), VDWF_ASHU(r)));
}


INLINE(Vdwf,VWWF_ZIPP) (Vwwf a, Vwwf b)
{
#define     WWF_ZIPP(A, B) vset_lane_f32(B,vdup_n_f32(A),0)
#define     VWWF_ZIPP(A, B) WWF_ZIPP(VWWF_ASTM(A),VWWF_ASTM(B))
    return  VWWF_ZIPP(a, b);
}

INLINE(Vdwu,VWWU_ZIPP) (Vwwu a, Vwwu b)
{
#define     VWWU_ZIPP(A, B) VDWF_ASWU(WWF_ZIPP(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_ZIPP(a, b);
}

INLINE(Vdwi,VWWI_ZIPP) (Vwwi a, Vwwi b)
{
#define     VWWI_ZIPP(A, B) VDWF_ASWI(WWF_ZIPP(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_ZIPP(a, b);
}


INLINE(Vqyu,VDYU_ZIPP) (Vdyu a, Vdyu b)
{
    uint64_t x = vget_lane_u64(VDYU_ASDU(a), 0);
    uint64_t y = vget_lane_u64(VDYU_ASDU(b), 0);
    uint64x1_t l = vdup_n_u64(UINT64_ZIPL(x, y));
    uint64x1_t r = vdup_n_u64(UINT64_ZIPR(x, y));
    return  QYU_ASTV(vcombine_u64(l, r));
}

INLINE(Vqbu,VDBU_ZIPP) (Vdbu a, Vdbu b)
{
    return  vcombine_u8(vzip1_u8(a, b), vzip2_u8(a, b));
}

INLINE(Vqbi,VDBI_ZIPP) (Vdbi a, Vdbi b)
{
    return  vcombine_s8(vzip1_s8(a, b), vzip2_s8(a, b));
}

INLINE(Vqbc,VDBC_ZIPP) (Vdbc a, Vdbc b)
{
#define     VDBC_ZIPP(A, B)  VQBU_ASBC(VDBU_ZIPP(VDBC_ASBU(A),VDBC_ASBU(B)))
    return  VDBC_ZIPP(a, b);
}

INLINE(Vqhu,VDHU_ZIPP) (Vdhu a, Vdhu b)
{
    return  vcombine_u16(vzip1_u16(a, b), vzip2_u16(a, b));
}

INLINE(Vqhi,VDHI_ZIPP) (Vdhi a, Vdhi b)
{
    return  vcombine_s16(vzip1_s16(a, b), vzip2_s16(a, b));
}

INLINE(Vqhf,VDHF_ZIPP) (Vdhf a, Vdhf b)
{
#define     VDHF_ZIPP(A, B)  VQHU_ASHF(VDHU_ZIPP(VDHF_ASHU(A),VDHF_ASHU(B)))
    return  VDHF_ZIPP(a, b);
}


INLINE(Vqwu,VDWU_ZIPP) (Vdwu a, Vdwu b)
{
    return  vcombine_u32(vzip1_u32(a, b), vzip2_u32(a, b));
}

INLINE(Vqwi,VDWI_ZIPP) (Vdwi a, Vdwi b)
{
    return  vcombine_s32(vzip1_s32(a, b), vzip2_s32(a, b));
}

INLINE(Vqwf,VDWF_ZIPP) (Vdwf a, Vdwf b)
{
    return  vcombine_f32(vzip1_f32(a, b), vzip2_f32(a, b));
}

INLINE(Vqdu,VDDU_ZIPP) (Vddu a, Vddu b) {return vcombine_u64(a, b);}
INLINE(Vqdi,VDDI_ZIPP) (Vddi a, Vddi b) {return vcombine_s64(a, b);}
INLINE(Vqdf,VDDF_ZIPP) (Vddf a, Vddf b) {return vcombine_f64(a, b);}

#if 0 // _LEAVE_ARM_ZIPP
}
#endif

#if 0 // _ENTER_ARM_XRDZ
{
#endif

INLINE(Vdhu,VWBU_XRDZ) (Vwbu x) 
{
    DWRD_VTYPE l = {.W.F={x.V0}};
    return  vzip1_u8(l.B.U, vdup_n_u8(0));
}

INLINE(Vdhi,VWBI_XRDZ) (Vwbi x) 
{
    DWRD_VTYPE l = {.W.F={x.V0}};
    return  vzip1_s8(l.B.I, vdup_n_s8(0));
}

#if CHAR_MIN
INLINE(Vdhi,VWBC_XRDZ) (Vwbc x) 
{
    Vwbi i = {x.V0};
    return  VWBI_XRDZ(i);
}
#else
INLINE(Vdhu,VWBC_XRDZ) (Vwbc x) 
{
    Vwbu i = {x.V0};
    return  VWBU_XRDZ(i);
}

#endif

INLINE(Vdwu,VWHU_XRDZ) (Vwhu x)
{
    DWRD_VTYPE l = {.W.F={x.V0}};
    return  vzip1_u16(l.H.U, vdup_n_u16(0));
}

INLINE(Vdwi,VWHI_XRDZ) (Vwhi x)
{
    DWRD_VTYPE l = {.W.F={x.V0}};
    return  vzip1_u16(l.H.I, vdup_n_s16(0));
}

INLINE(Vddu,VWWU_XRDZ) (Vwwu x)
{
    DWRD_VTYPE l = {.W.F={x.V0}};
    return  vzip1_u32(l.W.U, vdup_n_u32(0));
}

INLINE(Vddi,VWWI_XRDZ) (Vwwi x)
{
    DWRD_VTYPE l = {.W.F={x.V0}};
    return  vzip1_s32(l.W.I, vdup_n_s32(0));
}

INLINE(Vqhu,VDBU_XRDZ) (Vdbu x) {return vmovl_u8(x);}
INLINE(Vqhi,VDBI_XRDZ) (Vdbi x) 
{
    uint8x8_t   d = vreinterpret_u8_s8(x);
    uint16x8_t  q = vmovl_u8(d);
    return  vreinterpretq_s16_u16(q);
}

#if CHAR_MIN
INLINE(Vqhi,VDBC_XRDZ) (Vdbc x) {return VDBI_XRDZ(x.V0);}
#else
INLINE(Vqhu,VDBC_XRDZ) (Vdbc x) {return vmovl_u8(x.V0);}
#endif

INLINE(Vqwu,VDHU_XRDZ) (Vdhu x) {return vmovl_u16(x);}
INLINE(Vqwi,VDHI_XRDZ) (Vdhi x) 
{
    uint16x4_t  d = vreinterpret_u16_s16(x);
    uint32x4_t  q = vmovl_u16(d);
    return  vreinterpretq_s32_u32(q);
}

INLINE(Vqdu,VDWU_XRDZ) (Vdwu x) {return vmovl_u32(x);}
INLINE(Vqdi,VDWI_XRDZ) (Vdwi x) 
{
    uint32x2_t  d = vreinterpret_u32_s32(x);
    uint64x2_t  q = vmovl_u32(d);
    return  vreinterpretq_s64_u64(q);
}

INLINE(Vqqu,VDDU_XRDZ) (Vddu x) 
{
    Vqqu v = {vcombine_u64(x, vdup_n_u64(0))};
    return v;
}

INLINE(Vqqi,VDDI_XRDZ) (Vddi x) 
{
    Vqqi v = {vcombine_s64(x, vdup_n_s64(0))};
    return v;
}

#if 0 // _LEAVE_ARM_XRDZ
}
#endif

#if 0 // _ENTER_ARM_XRDS
{
#endif

INLINE(Vdhu,VWBU_XRDS) (Vwbu x) 
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.H.I=vmovl_s8(d.B.I)};
    return  vget_low_u16(q.H.U);
}

INLINE(Vdhi,VWBI_XRDS) (Vwbi x) 
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.H.I=vmovl_s8(d.B.I)};
    return  vget_low_s16(q.H.I);
}

#if CHAR_MIN
INLINE(Vdhi,VWBC_XRDS) (Vwbc x) 
{
    Vwbi i = {x.V0};
    return  VWBI_XRDS(i);
}
#else
INLINE(Vdhu,VWBC_XRDS) (Vwbc x) 
{
    Vwbu i = {x.V0};
    return  VWBU_XRDS(i);
}

#endif

INLINE(Vdwu,VWHU_XRDS) (Vwhu x)
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.W.I=vmovl_s16(d.H.I)};
    return  vget_low_u32(q.W.U);
}

INLINE(Vdwi,VWHI_XRDS) (Vwhi x)
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.W.I=vmovl_s16(d.H.I)};
    return  vget_low_s32(q.W.I);
}


INLINE(Vddu,VWWU_XRDS) (Vwwu x)
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.D.I=vmovl_s32(d.W.I)};
    return  vget_low_u64(q.D.U);
}

INLINE(Vddi,VWWI_XRDS) (Vwwi x)
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.D.I=vmovl_s32(d.W.I)};
    return  vget_low_s64(q.D.I);
}


INLINE(Vqhu,VDBU_XRDS) (Vdbu x) 
{
    DWRD_VTYPE d = {.B.U=x};
    QUAD_VTYPE q = {.H.I=vmovl_s8(d.B.I)};
    return  q.H.U;
}

INLINE(Vqhi,VDBI_XRDS) (Vdbi x) {return vmovl_s8(x);}

#if CHAR_MIN
INLINE(Vqhi,VDBC_XRDS) (Vdbc x) {return vmovl_s8(x.V0);}
#else
INLINE(Vqhu,VDBC_XRDS) (Vdbc x) {return VDBU_XRDS(x.V0);}
#endif


INLINE(Vqwu,VDHU_XRDS) (Vdhu x) 
{
    DWRD_VTYPE d = {.H.U=x};
    QUAD_VTYPE q = {.W.I=vmovl_s16(d.H.I)};
    return  q.W.U;
}

INLINE(Vqwi,VDHI_XRDS) (Vdhi x) {return vmovl_s16(x);}


INLINE(Vqdu,VDWU_XRDS) (Vdwu x) 
{
    DWRD_VTYPE d = {.W.U=x};
    QUAD_VTYPE q = {.D.I=vmovl_s32(d.W.I)};
    return  q.D.U;
}

INLINE(Vqdi,VDWI_XRDS) (Vdwi x) {return vmovl_s32(x);}

INLINE(Vqqu,VDDU_XRDS) (Vddu x) 
{
    int64x1_t   l = vreinterpret_s64_u64(x);
    int64x1_t   r = vshr_n_s64(l, 63);
    QUAD_VTYPE  c = {.D.I=vcombine_s64(l, r)};
    return  c.Q.U;
}

INLINE(Vqqi,VDDI_XRDS) (Vddi x) 
{
    int64x1_t   r = vshr_n_s64(x, 63);
    QUAD_VTYPE  c = {.D.I=vcombine_s64(x, r)};
    return  c.Q.I;
}

#if 0 // _LEAVE_ARM_XRDS
}
#endif

#if 0 // _ENTER_ARM_XRQZ
{
#endif

INLINE(Vqwu,VWBU_XRQZ) (Vwbu x) 
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.H.U=vmovl_u8(d.B.U)};
    d.H.U = vget_low_u16(q.H.U);
    return  vmovl_u16(d.H.U);
}

INLINE(Vqwi,VWBI_XRQZ) (Vwbi x) 
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.H.U=vmovl_u8(d.B.U)};
    d.H.U = vget_low_u16(q.H.U);
    q.W.U = vmovl_u16(d.H.U);
    return  q.W.I;
}

#if CHAR_MIN
INLINE(Vqwi,VWBC_XRQZ) (Vwbc x) {return VWBI_XRQZ(((Vwbi){x.V0}));}
#else
INLINE(Vqwu,VWBC_XRQZ) (Vwbc x) {return VWBU_XRQZ(((Vwbu){x.V0}));}
#endif

INLINE(Vqdu,VWHU_XRQZ) (Vwhu x)
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.W.U=vmovl_u16(d.H.U)};
    d.W.U = vget_low_u32(q.W.U);
    return  vmovl_u32(d.W.U);
}

INLINE(Vqdi,VWHI_XRQZ) (Vwhi x)
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.W.U=vmovl_u16(d.H.U)};
    d.W.U = vget_low_u32(q.W.U);
    q.D.U = vmovl_u32(d.W.U);
    return  q.D.I;
}

INLINE(Vqqu,VWWU_XRQZ) (Vwwu x)
{
    QUAD_VTYPE q = {.D.U=vdupq_n_u64(0)};
    q.W.F = vsetq_lane_f32(x.V0, q.W.F, 0);
    return  q.Q.U;
}

INLINE(Vqqi,VWWI_XRQZ) (Vwwi x)
{
    QUAD_VTYPE q = {.D.U=vdupq_n_u64(0)};
    q.W.F = vsetq_lane_f32(x.V0, q.W.F, 0);
    return  q.Q.I;
}

#if 0 // _LEAVE_ARM_XRQZ
}
#endif

#if 0 // _ENTER_ARM_XRQS
{
#endif

INLINE(Vqwu,VWBU_XRQS) (Vwbu x) 
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.H.I=vmovl_s8(d.B.I)};
    d.H.I = vget_low_s16(q.H.I);
    q.W.I = vmovl_s16(d.H.I);
    return  q.W.U;
}

INLINE(Vqwi,VWBI_XRQS) (Vwbi x) 
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.H.U=vmovl_s8(d.B.I)};
    d.H.I = vget_low_s16(q.H.I);
    return  vmovl_s16(d.H.I);
}

#if CHAR_MIN
INLINE(Vqwi,VWBC_XRQS) (Vwbc x) {return VWBI_XRQS(((Vwbi){x.V0}));}
#else
INLINE(Vqwu,VWBC_XRQS) (Vwbc x) {return VWBU_XRQS(((Vwbu){x.V0}));}
#endif

INLINE(Vqdu,VWHU_XRQS) (Vwhu x)
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.W.I=vmovl_s16(d.H.I)};
    d.W.I = vget_low_s32(q.W.I);
    q.D.I = vmovl_s32(d.W.I);
    return  q.D.U;
}

INLINE(Vqdi,VWHI_XRQS) (Vwhi x)
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.W.I=vmovl_s16(d.H.I)};
    d.W.I = vget_low_s32(q.W.I);
    return  vmovl_s32(d.W.I);
}

INLINE(Vqqu,VWWU_XRQS) (Vwwu x)
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.D.I=vmovl_s32(d.W.I)};
    d.D.I = vget_low_s64(q.D.I);
    d.D.I = vshr_n_s64(d.D.I, 63);
    q.D.I = vcopyq_lane_s64(q.D.I, 1, d.D.I, 0);
    return  q.Q.U;
}

INLINE(Vqqi,VWWI_XRQS) (Vwwi x)
{
    DWRD_VTYPE d = {.W.F={x.V0}};
    QUAD_VTYPE q = {.D.I=vmovl_s32(d.W.I)};
    d.D.I = vget_low_s64(q.D.I);
    d.D.I = vshr_n_s64(d.D.I, 63);
    q.D.I = vcopyq_lane_s64(q.D.I, 1, d.D.I, 0);
    return  q.Q.I;
}

#if 0 // _LEAVE_ARM_XRQS
}
#endif

#if 0 // _ENTER_ARM_UZPL
{
#endif

INLINE(Vwbu,VWBU_UZPL) (Vwbu a, Vwbu b)
{
    DWRD_TYPE   v = {.Lo.F=VWBU_ASTM(a), .Hi.F=VWBU_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    uint8x8_t   c = vreinterpret_u8_f64(m);
    c = vuzp1_u8(c, c);
    float32x2_t f = vreinterpret_f32_u8(c);
    return  WBU_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vwbi,VWBI_UZPL) (Vwbi a, Vwbi b)
{
    DWRD_TYPE   v = {.Lo.F=VWBI_ASTM(a), .Hi.F=VWBI_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    uint8x8_t   c = vreinterpret_u8_f64(m);
    c = vuzp1_u8(c, c);
    float32x2_t f = vreinterpret_f32_u8(c);
    return  WBI_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vwbc,VWBC_UZPL) (Vwbc a, Vwbc b)
{
    DWRD_TYPE   v = {.Lo.F=VWBC_ASTM(a), .Hi.F=VWBC_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    uint8x8_t   c = vreinterpret_u8_f64(m);
    c = vuzp1_u8(c, c);
    float32x2_t f = vreinterpret_f32_u8(c);
    return  WBC_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vwhu,VWHU_UZPL) (Vwhu a, Vwhu b)
{
    DWRD_TYPE   v = {.Lo.F=VWHU_ASTM(a), .Hi.F=VWHU_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    uint16x4_t  c = vreinterpret_u16_f64(m);
    c = vuzp1_u16(c, c);
    float32x2_t f = vreinterpret_f32_u16(c);
    return  WHU_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vwhi,VWHI_UZPL) (Vwhi a, Vwhi b)
{
    DWRD_TYPE   v = {.Lo.F=VWHI_ASTM(a), .Hi.F=VWHI_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    int16x4_t   c = vreinterpret_s16_f64(m);
    c = vuzp1_s16(c, c);
    float32x2_t f = vreinterpret_f32_s16(c);
    return  WHI_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vwhf,VWHF_UZPL) (Vwhf a, Vwhf b)
{
    DWRD_TYPE   v = {.Lo.F=VWHF_ASTM(a), .Hi.F=VWHF_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    uint16x4_t  c = vreinterpret_u16_f64(m);
    c = vuzp1_u16(c, c);
    float32x2_t f = vreinterpret_f32_u16(c);
    return  WHF_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vdyu,VDYU_UZPL) (Vdyu a, Vdyu b)
{
    uint64x1_t  x = VDYU_ASTM(a);

    // clear a's odd bits
    x = vand_u64(vdup_n_u64(0x5555555555555555ull), x);

    // x = 0x3333333333333333&(x|(x>>1));
    x = vand_u64(
        vdup_n_u64(0x3333333333333333ull),
        vorr_u64(x, vshr_n_u64(x, 1))
    );

    // x = 0x0f0f0f0f0f0f0f0f&(x|(x>>2));
    x = vand_u64(
        vdup_n_u64(0x0f0f0f0f0f0f0f0full),
        vorr_u64(x, vshr_n_u64(x, 2))
    );

    // x = 0x00ff00ff00ff00ff&(x|(x>>4));
    x = vand_u64(
        vdup_n_u64(0x00ff00ff00ff00ffull),
        vorr_u64(x, vshr_n_u64(x, 4))
    );

    // x = 0x0000ffff0000ffff&(x|(x>>8));
    x = vand_u64(
        vdup_n_u64(0x0000ffff0000ffffull),
        vorr_u64(x, vshr_n_u64(x, 8))
    );

    uint32x2_t w = vreinterpret_u32_u64(x);
    x = VDYU_ASTM(b);

    // clear b's odd bits
    x = vand_u64(vdup_n_u64(0x5555555555555555ull), x);

    x = vand_u64(
        vdup_n_u64(0x3333333333333333ull),
        vorr_u64(x, vshr_n_u64(x, 1))
    );

    x = vand_u64(
        vdup_n_u64(0x0f0f0f0f0f0f0f0full),
        vorr_u64(x, vshr_n_u64(x, 2))
    );

    x = vand_u64(
        vdup_n_u64(0x00ff00ff00ff00ffull),
        vorr_u64(x, vshr_n_u64(x, 4))
    );

    x = vand_u64(
        vdup_n_u64(0x0000ffff0000ffffull),
        vorr_u64(x, vshr_n_u64(x, 8))
    );
    w = vset_lane_u32(
        vget_lane_u64(x, 0),
        w,
        1
    );
    x = vreinterpret_u64_u32(w);
    return  VDDU_ASYU(x);
}

INLINE(Vdbu,VDBU_UZPL) (Vdbu a, Vdbi b) {return vuzp1_u8(a, b);}
INLINE(Vdbi,VDBI_UZPL) (Vdbi a, Vdbi b) {return vuzp1_s8(a, b);}

INLINE(Vdbc,VDBC_UZPL) (Vdbc a, Vdbc b)
{
    return  VDBU_ASBC(vuzp1_u8(VDBC_ASBU(a),VDBC_ASBU(b)));
}

INLINE(Vdhu,VDHU_UZPL) (Vdhu a, Vdhu b) {return vuzp1_u16(a, b);}
INLINE(Vdhi,VDHI_UZPL) (Vdhi a, Vdhi b) {return vuzp1_s16(a, b);}
INLINE(Vdhf,VDHF_UZPL) (Vdhf a, Vdhf b)
{
    return  vreinterpret_f16_u16(
        vuzp1_u16(
            vreinterpret_u16_f16(a),
            vreinterpret_u16_f16(b)
        )
    );
}

INLINE(Vdwu,VDWU_UZPL) (Vdwu a, Vdwu b) {return vuzp1_u32(a, b);}
INLINE(Vdwi,VDWI_UZPL) (Vdwi a, Vdwi b) {return vuzp1_s32(a, b);}
INLINE(Vdwf,VDWF_UZPL) (Vdwf a, Vdwf b) {return vuzp1_f32(a, b);}

INLINE(Vqbu,VQBU_UZPL) (Vqbu a, Vqbu b) {return vuzp1q_u8(a, b);}
INLINE(Vqbi,VQBI_UZPL) (Vqbi a, Vqbi b) {return vuzp1q_s8(a, b);}

INLINE(Vqbc,VQBC_UZPL) (Vqbc a, Vqbc b)
{
    return  VQBU_ASBC(vuzp1q_u8(VQBC_ASBU(a),VQBC_ASBU(b)));
}

INLINE(Vqhu,VQHU_UZPL) (Vqhu a, Vqhu b) {return vuzp1q_u16(a, b);}
INLINE(Vqhi,VQHI_UZPL) (Vqhi a, Vqhi b) {return vuzp1q_s16(a, b);}
INLINE(Vqhf,VQHF_UZPL) (Vqhf a, Vqhf b)
{
    return  vreinterpretq_f16_u16(
        vuzp1q_u16(
            vreinterpretq_u16_f16(a),
            vreinterpretq_u16_f16(b)
        )
    );
}

INLINE(Vqwu,VQWU_UZPL) (Vqwu a, Vqwu b) {return vuzp1q_u32(a, b);}
INLINE(Vqwi,VQWI_UZPL) (Vqwi a, Vqwi b) {return vuzp1q_s32(a, b);}
INLINE(Vqwf,VQWF_UZPL) (Vqwf a, Vqwf b) {return vuzp1q_f32(a, b);}

INLINE(Vqdu,VQDU_UZPL) (Vqdu a, Vqdu b) {return vuzp1q_u64(a, b);}
INLINE(Vqdi,VQDI_UZPL) (Vqdi a, Vqdi b) {return vuzp1q_s64(a, b);}
INLINE(Vqdf,VQDF_UZPL) (Vqdf a, Vqdf b) {return vuzp1q_f64(a, b);}

#if 0 // _LEAVE_ARM_UZPL
}
#endif

#if 0 // _ENTER_ARM_UZPR
{
#endif

INLINE(Vwbu,VWBU_UZPR) (Vwbu a, Vwbu b)
{
    DWRD_TYPE   v = {.Lo.F=VWBU_ASTM(a), .Hi.F=VWBU_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    uint8x8_t   c = vreinterpret_u8_f64(m);
    c = vuzp2_u8(c, c);
    float32x2_t f = vreinterpret_f32_u8(c);
    return  WBU_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vwbi,VWBI_UZPR) (Vwbi a, Vwbi b)
{
    DWRD_TYPE   v = {.Lo.F=VWBI_ASTM(a), .Hi.F=VWBI_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    uint8x8_t   c = vreinterpret_u8_f64(m);
    c = vuzp2_u8(c, c);
    float32x2_t f = vreinterpret_f32_u8(c);
    return  WBI_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vwbc,VWBC_UZPR) (Vwbc a, Vwbc b)
{
    DWRD_TYPE   v = {.Lo.F=VWBC_ASTM(a), .Hi.F=VWBC_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    uint8x8_t   c = vreinterpret_u8_f64(m);
    c = vuzp2_u8(c, c);
    float32x2_t f = vreinterpret_f32_u8(c);
    return  WBC_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vwhu,VWHU_UZPR) (Vwhu a, Vwhu b)
{
    DWRD_TYPE   v = {.Lo.F=VWHU_ASTM(a), .Hi.F=VWHU_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    uint16x4_t  c = vreinterpret_u16_f64(m);
    c = vuzp2_u16(c, c);
    float32x2_t f = vreinterpret_f32_u16(c);
    return  WHU_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vwhi,VWHI_UZPR) (Vwhi a, Vwhi b)
{
    DWRD_TYPE   v = {.Lo.F=VWHI_ASTM(a), .Hi.F=VWHI_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    int16x4_t   c = vreinterpret_s16_f64(m);
    c = vuzp2_s16(c, c);
    float32x2_t f = vreinterpret_f32_s16(c);
    return  WHI_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vwhf,VWHF_UZPR) (Vwhf a, Vwhf b)
{
    DWRD_TYPE   v = {.Lo.F=VWHF_ASTM(a), .Hi.F=VWHF_ASTM(b)};
    float64x1_t m = vdup_n_f64(v.F);
    uint16x4_t  c = vreinterpret_u16_f64(m);
    c = vuzp2_u16(c, c);
    float32x2_t f = vreinterpret_f32_u16(c);
    return  WHF_ASTV(vget_lane_f32(f, 0));
}

INLINE(Vdbu,VDBU_UZPR) (Vdbu a, Vdbu b) {return vuzp2_u8(a, b);}
INLINE(Vdbi,VDBI_UZPR) (Vdbi a, Vdbi b) {return vuzp2_s8(a, b);}

INLINE(Vdbc,VDBC_UZPR) (Vdbc a, Vdbc b)
{
    return  VDBU_ASBC(vuzp2_u8(VDBC_ASBU(a),VDBC_ASBU(b)));
}

INLINE(Vdhu,VDHU_UZPR) (Vdhu a, Vdhu b) {return vuzp2_u16(a, b);}
INLINE(Vdhi,VDHI_UZPR) (Vdhi a, Vdhi b) {return vuzp2_s16(a, b);}
INLINE(Vdhf,VDHF_UZPR) (Vdhf a, Vdhf b)
{
    return  vreinterpret_f16_u16(
        vuzp2_u16(
            vreinterpret_u16_f16(a),
            vreinterpret_u16_f16(b)
        )
    );
}

INLINE(Vdwu,VDWU_UZPR) (Vdwu a, Vdwu b) {return vuzp2_u32(a, b);}
INLINE(Vdwi,VDWI_UZPR) (Vdwi a, Vdwi b) {return vuzp2_s32(a, b);}
INLINE(Vdwf,VDWF_UZPR) (Vdwf a, Vdwf b) {return vuzp2_f32(a, b);}

INLINE(Vqbu,VQBU_UZPR) (Vqbu a, Vqbu b) {return vuzp2q_u8(a, b);}
INLINE(Vqbi,VQBI_UZPR) (Vqbi a, Vqbi b) {return vuzp2q_s8(a, b);}

INLINE(Vqbc,VQBC_UZPR) (Vqbc a, Vqbc b)
{
    return  VQBU_ASBC(vuzp2q_u8(VQBC_ASBU(a),VQBC_ASBU(b)));
}

INLINE(Vqhu,VQHU_UZPR) (Vqhu a, Vqhu b) {return vuzp2q_u16(a, b);}
INLINE(Vqhi,VQHI_UZPR) (Vqhi a, Vqhi b) {return vuzp2q_s16(a, b);}
INLINE(Vqhf,VQHF_UZPR) (Vqhf a, Vqhf b)
{
    return  vreinterpretq_f16_u16(
        vuzp2q_u16(
            vreinterpretq_u16_f16(a),
            vreinterpretq_u16_f16(b)
        )
    );
}

INLINE(Vqwu,VQWU_UZPR) (Vqwu a, Vqwu b) {return vuzp2q_u32(a, b);}
INLINE(Vqwi,VQWI_UZPR) (Vqwi a, Vqwi b) {return vuzp2q_s32(a, b);}
INLINE(Vqwf,VQWF_UZPR) (Vqwf a, Vqwf b) {return vuzp2q_f32(a, b);}

INLINE(Vqdu,VQDU_UZPR) (Vqdu a, Vqdu b) {return vuzp2q_u64(a, b);}
INLINE(Vqdi,VQDI_UZPR) (Vqdi a, Vqdi b) {return vuzp2q_s64(a, b);}
INLINE(Vqdf,VQDF_UZPR) (Vqdf a, Vqdf b) {return vuzp2q_f64(a, b);}

#if 0 // _LEAVE_ARM_UZPR
}
#endif


#if 0 // _ENTER_ARM_SETL
{
#endif

INLINE(Vdyu,VDYU_SETL) (Vdyu c, Vwyu l)
{
#define     VDYU_SETL(C, L) \
VDWF_ASYU(vset_lane_f32(VWYU_ASTM(L), VDYU_ASWF(C), 0))
    return  VDYU_SETL(c, l);
}


INLINE(Vdbu,VDBU_SETL) (Vdbu c, Vwbu l)
{
#define     VDBU_SETL(C, L) \
VDWF_ASBU(vset_lane_f32(VWBU_ASTM(L), VDBU_ASWF(C), 0))
    return  VDBU_SETL(c, l);
}

INLINE(Vdbi,VDBI_SETL) (Vdbi c, Vwbi l)
{
#define     VDBI_SETL(C, L) \
VDWF_ASBI(vset_lane_f32(VWBI_ASTM(L), VDBI_ASWF(C), 0))
    return  VDBI_SETL(c, l);
}

INLINE(Vdbc,VDBC_SETL) (Vdbc c, Vwbc l)
{
#define     VDBC_SETL(C, L) \
VDWF_ASBC(vset_lane_f32(VWBC_ASTM(L), VDBC_ASWF(C), 0))
    return  VDBC_SETL(c, l);
}


INLINE(Vdhu,VDHU_SETL) (Vdhu c, Vwhu l)
{
#define     VDHU_SETL(C, L) \
VDWF_ASHU(vset_lane_f32(VWHU_ASTM(L), VDHU_ASWF(C), 0))
    return  VDHU_SETL(c, l);
}

INLINE(Vdhi,VDHI_SETL) (Vdhi c, Vwhi l)
{
#define     VDHI_SETL(C, L) \
VDWF_ASHI(vset_lane_f32(VWHI_ASTM(L), VDHI_ASWF(C), 0))
    return  VDHI_SETL(c, l);
}

INLINE(Vdhf,VDHF_SETL) (Vdhf c, Vwhf l)
{
#define     VDHF_SETL(C, L) \
VDWF_ASHF(vset_lane_f32(VWHF_ASTM(L), VDHF_ASWF(C), 0))
    return  VDHF_SETL(c, l);
}


INLINE(Vdwu,VDWU_SETL) (Vdwu c, Vwwu l)
{
#define     VDWU_SETL(C, L) \
VDWF_ASWU(vset_lane_f32(VWWU_ASTM(L), VDWU_ASWF(C), 0))
    return  VDWU_SETL(c, l);
}

INLINE(Vdwi,VDWI_SETL) (Vdwi c, Vwwi l)
{
#define     VDWI_SETL(C, L) \
VDWF_ASWI(vset_lane_f32(VWWI_ASTM(L), VDWI_ASWF(C), 0))
    return  VDWI_SETL(c, l);
}

INLINE(Vdwf,VDWF_SETL) (Vdwf c, Vwwf l)
{
#define     VDWF_SETL(C, L) vset_lane_f32(VWWF_ASTM(L), C, 0)
    return  VDWF_SETL(c, l);
}


INLINE(Vqyu,VQYU_SETL) (Vqyu c, Vdyu l)
{
#define     VQYU_SETL(C, L) \
QYU_ASTV(vcombine_u64(VDYU_ASTM(L), vget_low_u64(VQYU_ASTM(C))))
    return  VQYU_SETL(c, l);
}

INLINE(Vqbu,VQBU_SETL) (Vqbu c, Vdbu l)
{
#define     VQBU_SETL(C, L) vcombine_u8(L, vget_high_u8(C))
    return  VQBU_SETL(c, l);
}

INLINE(Vqbi,VQBI_SETL) (Vqbi c, Vdbi l)
{
#define     VQBI_SETL(C, L) vcombine_s8(L, vget_high_s8(C))
    return  VQBI_SETL(c, l);
}

INLINE(Vqbc,VQBC_SETL) (Vqbc c, Vdbc l)
{
#define     VQBC_SETL(C, L)         \
VQDU_ASBC(                          \
    vcombine_u64(                   \
        VDBC_ASDU(L),               \
        vget_high_u64(VQBC_ASDU(C)) \
    )                               \
)
    return  VQBC_SETL(c, l);
}


INLINE(Vqhu,VQHU_SETL) (Vqhu c, Vdhu l)
{
#define     VQHU_SETL(C, L) vcombine_u16(L, vget_high_u16(C))
    return  VQHU_SETL(c, l);
}

INLINE(Vqhi,VQHI_SETL) (Vqhi c, Vdhi l)
{
#define     VQHI_SETL(C, L) vcombine_s16(L, vget_high_s16(C))
    return  VQHI_SETL(c, l);
}

INLINE(Vqhf,VQHF_SETL) (Vqhf c, Vdhf l)
{
#define     VQHF_SETL(C, L) vcombine_f16(L, vget_high_f16(C))
    return  VQHF_SETL(c, l);
}


INLINE(Vqwu,VQWU_SETL) (Vqwu c, Vdwu l)
{
#define     VQWU_SETL(C, L) vcombine_u32(L, vget_high_u32(C))
    return  VQWU_SETL(c, l);
}

INLINE(Vqwi,VQWI_SETL) (Vqwi c, Vdwi l)
{
#define     VQWI_SETL(C, L) vcombine_s32(L, vget_high_s32(C))
    return  VQWI_SETL(c, l);
}

INLINE(Vqwf,VQWF_SETL) (Vqwf c, Vdwf l)
{
#define     VQWF_SETL(C, L) vcombine_f32(L, vget_high_f32(C))
    return  VQWF_SETL(c, l);
}


INLINE(Vqdu,VQDU_SETL) (Vqdu c, Vddu l)
{
#define     VQDU_SETL(C, L) vcombine_u64(L, vget_high_u64(C))
    return  VQDU_SETL(c, l);
}

INLINE(Vqdi,VQDI_SETL) (Vqdi c, Vddi l)
{
#define     VQDI_SETL(C, L) vcombine_s64(L, vget_high_s64(C))
    return  VQDI_SETL(c, l);
}

INLINE(Vqdf,VQDF_SETL) (Vqdf c, Vddf l)
{
#define     VQDF_SETL(C, L) vcombine_f64(L, vget_high_f64(C))
    return  VQDF_SETL(c, l);
}

#if 0 // _LEAVE_ARM_SETL
}
#endif

#if 0 // _ENTER_ARM_SETR
{
#endif

INLINE(ushort,USHRT_SETR) (ushort x,  uint8_t hi)
{
    HALF_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE( short, SHRT_SETR)  (short x,  uint8_t hi)
{
    HALF_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}


INLINE(  uint, UINT_SETR)   (uint x, uint16_t hi)
{
    WORD_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE(   int,  INT_SETR)    (int x, uint16_t hi)
{
    WORD_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}

#if DWRD_NLONG == 2

INLINE( ulong, ULONG_SETR) (ulong x, uint16_t hi)
{
    WORD_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE(  long,  LONG_SETR)  (long x, uint16_t hi)
{
    WORD_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}

#else

INLINE( ulong, ULONG_SETR) (ulong x, uint32_t hi)
{
    DWRD_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE(  long,  LONG_SETR)  (long x, uint32_t hi)
{
    DWRD_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}

#endif

#if QUAD_NLLONG == 2

INLINE(ullong,ULLONG_SETR)(ullong x, uint32_t hi)
{
    DWRD_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE( llong, LLONG_SETR) (llong x, uint32_t hi)
{
    DWRD_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}

#else

INLINE(ullong,ULLONG_SETR)(ullong x, uint64_t hi)
{
    QUAD_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE( llong, LLONG_SETR) (llong x, uint64_t hi)
{
    QUAD_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}

#endif

INLINE(Vdyu,VDYU_SETR) (Vdyu c, Vwyu r)
{
#define     VDYU_SETR(C, R) \
VDWF_ASYU(vset_lane_f32(VWYU_ASTM(R), VDYU_ASWF(C), 1))
    return  VDYU_SETR(c, r);
}

INLINE(Vdbu,VDBU_SETR) (Vdbu c, Vwbu r)
{
#define     VDBU_SETR(C, R) \
VDWF_ASBU(vset_lane_f32(VWBU_ASTM(R), VDBU_ASWF(C), 1))
    return  VDBU_SETR(c, r);
}

INLINE(Vdbi,VDBI_SETR) (Vdbi c, Vwbi r)
{
#define     VDBI_SETR(C, R) \
VDWF_ASBI(vset_lane_f32(VWBI_ASTM(R), VDBI_ASWF(C), 1))
    return  VDBI_SETR(c, r);
}

INLINE(Vdbc,VDBC_SETR) (Vdbc c, Vwbc r)
{
#define     VDBC_SETR(C, R) \
VDWF_ASBC(vset_lane_f32(VWBC_ASTM(R), VDBC_ASWF(C), 1))
    return  VDBC_SETR(c, r);
}


INLINE(Vdhu,VDHU_SETR) (Vdhu c, Vwhu r)
{
#define     VDHU_SETR(C, R) \
VDWF_ASHU(vset_lane_f32(VWHU_ASTM(R), VDHU_ASWF(C), 1))
    return  VDHU_SETR(c, r);
}

INLINE(Vdhi,VDHI_SETR) (Vdhi c, Vwhi r)
{
#define     VDHI_SETR(C, R) \
VDWF_ASHI(vset_lane_f32(VWHI_ASTM(R), VDHI_ASWF(C), 1))
    return  VDHI_SETR(c, r);
}

INLINE(Vdhf,VDHF_SETR) (Vdhf c, Vwhf r)
{
#define     VDHF_SETR(C, R) \
VDWF_ASHF(vset_lane_f32(VWHF_ASTM(R), VDHF_ASWF(C), 1))
    return  VDHF_SETR(c, r);
}


INLINE(Vdwu,VDWU_SETR) (Vdwu c, Vwwu r)
{
#define     VDWU_SETR(C, R) \
VDWF_ASWU(vset_lane_f32(VWWU_ASTM(R), VDWU_ASWF(C), 1))
    return  VDWU_SETR(c, r);
}

INLINE(Vdwi,VDWI_SETR) (Vdwi c, Vwwi r)
{
#define     VDWI_SETR(C, R) \
VDWF_ASWI(vset_lane_f32(VWWI_ASTM(R), VDWI_ASWF(C), 1))
    return  VDWI_SETR(c, r);
}

INLINE(Vdwf,VDWF_SETR) (Vdwf c, Vwwf r)
{
#define     VDWF_SETR(C, R) vset_lane_f32(VWWF_ASTM(R), C, 1)
    return  VDWF_SETR(c, r);
}


INLINE(Vqyu,VQYU_SETR) (Vqyu c, Vdyu r)
{
#define     VQYU_SETR(C, R)         \
QYU_ASTV(                           \
    vcombine_u64(                   \
        vget_low_u64(VQYU_ASTM(C)), \
        VDYU_ASTM(R)                \
    )                               \
)

    return  VQYU_SETR(c, r);
}

INLINE(Vqbu,VQBU_SETR) (Vqbu c, Vdbu r)
{
#define     VQBU_SETR(C, R)     vcombine_u8(vget_low_u8(C), R)
    return  VQBU_SETR(c, r);
}

INLINE(Vqbi,VQBI_SETR) (Vqbi c, Vdbi r)
{
#define     VQBI_SETR(C, R)     vcombine_s8(vget_low_s8(C), R)
    return  VQBI_SETR(c, r);
}

INLINE(Vqbc,VQBC_SETR) (Vqbc c, Vdbc r)
{
#define     VQBC_SETR(C, R)         \
VQDU_ASBC(                          \
    vcombine_u64(                   \
        vget_low_u64(VQBC_ASDU(C)), \
        VDBC_ASDU(R)                \
    )                               \
)
    return  VQBC_SETR(c, r);
}


INLINE(Vqhu,VQHU_SETR) (Vqhu c, Vdhu r)
{
#define     VQHU_SETR(C, R)     vcombine_u16(vget_low_u16(C), R)
    return  VQHU_SETR(c, r);
}

INLINE(Vqhi,VQHI_SETR) (Vqhi c, Vdhi r)
{
#define     VQHI_SETR(C, R)     vcombine_s16(vget_low_s16(C), R)
    return  VQHI_SETR(c, r);
}

INLINE(Vqhf,VQHF_SETR) (Vqhf c, Vdhf r)
{
#define     VQHF_SETR(C, R)     vcombine_f16(vget_low_f16(C), R)
    return  VQHF_SETR(c, r);
}


INLINE(Vqwu,VQWU_SETR) (Vqwu c, Vdwu r)
{
#define     VQWU_SETR(C, R)     vcombine_u32(vget_low_u32(C), R)
    return  VQWU_SETR(c, r);
}

INLINE(Vqwi,VQWI_SETR) (Vqwi c, Vdwi r)
{
#define     VQWI_SETR(C, R)     vcombine_s32(vget_low_s32(C), R)
    return  VQWI_SETR(c, r);
}

INLINE(Vqwf,VQWF_SETR) (Vqwf c, Vdwf r)
{
#define     VQWF_SETR(C, R)     vcombine_f32(vget_low_f32(C), R)
    return  VQWF_SETR(c, r);
}


INLINE(Vqdu,VQDU_SETR) (Vqdu c, Vddu r)
{
#define     VQDU_SETR(C, R)     vcombine_u64(vget_low_u64(C), R)
    return  VQDU_SETR(c, r);
}

INLINE(Vqdi,VQDI_SETR) (Vqdi c, Vddi r)
{
#define     VQDI_SETR(C, R)     vcombine_s64(vget_low_s64(C), R)
    return  VQDI_SETR(c, r);
}

INLINE(Vqdf,VQDF_SETR) (Vqdf c, Vddf r)
{
#define     VQDF_SETR(C, R)     vcombine_f64(vget_low_f64(C), R)
    return  VQDF_SETR(c, r);
}

#if 0 // _LEAVE_ARM_SETR
}
#endif

#if 0 // _ENTER_ARM_SET1
{
#endif

INLINE(Vwyu,VWYU_SET1) (Vwyu d, Rc(0, +31) k, _Bool v)
{
#define     VWYU_SET1   VWYU_SET1
    float32x2_t w = vdup_n_f32(VWYU_ASTM(d));
    int64x1_t   s = vdup_n_s64(k);
    uint64x1_t  m = vshl_u64(vdup_n_u64(1), s);
    uint64x1_t  y = vbic_u64(VDWF_ASDU(w), m);
    y = vorr_u64(y, vshl_u64(vdup_n_u64(v), s));
    return  WYU_ASTV(vget_lane_f32(VDDU_ASWF(y), 0));
}

INLINE(Vwbu,VWBU_SET1) (Vwbu a, Rc(0, +3) b, unsigned c)
{
#define     WBU_SET1(A, B, C)   vset_lane_f32(C,((Vd){.W0.F=A}).W.F,B)
#define     VWBU_SET1(A, B, C)  ((Vwbu){WBU_SET1(A.V0,B,C)})

    Vd  dst = {.L.B.U=a};
    Vd  off = {.D0.I=b};
    Vd  src = {.D0.U=c};
    Vd  bim = {.D0.U=0xffULL};
    src.D0.U = vand_u64(src.D.U, bim.D.U);
    src.D0.U = vshl_u64(src.D.U, off.D.I);
    bim.D0.U = vshl_u64(bim.D.U, off.D.I);
    dst.D0.U = vbic_u64(dst.D.U, bim.D.U);
    dst.D0.U = vorr_u64(dst.D.U, src.D.U);
    return  dst.L.B.U;
}

INLINE(Vwbi,VWBI_SET1) (Vwbi d, Rc(0, 3) k, int8_t v)
{
#define     WBI_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_s8(        \
        vset_lane_s8(           \
            (V),                \
            vreinterpret_s8_f32(\
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    0                       \
)

#define     VWBI_SET1(D, K, V)  WBI_ASTV(WBI_SET1(VWBI_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWBI_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*8);
    uint64x1_t  m = vdup_n_u64(0xffull);
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(vdup_n_u64(v), s));
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            0
        )
    );
}

INLINE(Vwbc,VWBC_SET1) (Vwbc d, Rc(0, 3) k, char v)
{
#if CHAR_MIN
#   define  WBC_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_s8(        \
        vset_lane_s8(           \
            (V),                \
            vreinterpret_s8_f32(\
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    0                       \
)

#else
#   define  WBC_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_u8(        \
        vset_lane_u8(           \
            (V),                \
            vreinterpret_u8_f32(\
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    0                       \
)

#endif

#define     VWBC_SET1(D, K, V)  WBC_ASTV(WBC_SET1(VWBC_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWBC_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*8);
    uint64x1_t  m = vdup_n_u64(0xffull);
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(vdup_n_u64(v), s));
    return  WBC_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            0
        )
    );
}

INLINE(Vwhu,VWHU_SET1) (Vwhu d, Rc(0, 1) k, uint16_t v)
{
#define     WHU_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_u16(       \
        vset_lane_u16(          \
            (V),                \
            VDWF_ASHU(          \
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    0                       \
)

#define     VWHU_SET1(D, K, V)  WHU_ASTV(WHU_SET1(VWHU_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWHU_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*16);
    uint64x1_t  m = vdup_n_u64(0xffffull);
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(vdup_n_u64(v), s));
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            0
        )
    );
}

INLINE(Vwhi,VWHI_SET1) (Vwhi d, Rc(0, 1) k, int16_t v)
{
#define     WHI_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_s16(       \
        vset_lane_s16(          \
            (V),                \
            VDWF_ASHI(          \
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    0                       \
)

#define     VWHI_SET1(D, K, V)  WHI_ASTV(WHI_SET1(VWHI_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWHI_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*16);
    uint64x1_t  m = vdup_n_u64(0xffffull);
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(vdup_n_u64(v), s));
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            0
        )
    );
}

INLINE(Vwhf,VWHF_SET1) (Vwhf d, Rc(0, 1) k, flt16_t v)
{
#define     WHF_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_f16(       \
        vset_lane_f16(          \
            (V),                \
            VDWF_ASHF(          \
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    0                       \
)

#define     VWHF_SET1(D, K, V)  WHF_ASTV(WHF_SET1(VWHF_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWHF_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*16);
    uint64x1_t  m = vdup_n_u64(0xffffull);
    uint64x1_t  h = vdup_n_u64(FLT16_ASHU(v));
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(h, s));
    return  WHF_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            0
        )
    );
}


INLINE(Vdyu,VDYU_SET1) (Vdyu d, Rc(0,63) k, _Bool v)
{
#define     VDYU_SET1   VDYU_SET1
    int64x1_t   s = vdup_n_s64(k);
    uint64x1_t  m = vshl_u64(vdup_n_u64(1), s);
    uint64x1_t  y = vbic_u64(m, VDYU_ASTM(d));
    return DYU_ASTV(vorr_u64(y, vshl_u64(vdup_n_u64(v), s)));
}

INLINE(Vdbu,VDBU_SET1) (Vdbu d, Rc(0, 7) k, uint8_t v)
{
#define     VDBU_SET1(D, K, V)  vset_lane_u8(V, D, K)
    int64x1_t  s = vdup_n_s64(k*8);
    uint64x1_t m = vdup_n_u64(0xffull);
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_u8(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(v), s), m);
    return  vreinterpret_u8_u64(m);
}

INLINE(Vdbi,VDBI_SET1) (Vdbi d, Rc(0, 7) k, int8_t v)
{
#define     VDBI_SET1(D, K, V)  vset_lane_u8(V, D, K)
    int64x1_t  s = vdup_n_s64(k*8);
    uint64x1_t m = vdup_n_u64(0xffull);
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_s8(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(v), s), m);
    return  vreinterpret_s8_u64(m);
}

INLINE(Vdbc,VDBC_SET1) (Vdbc d, Rc(0, 7) k, char v)
{
#if CHAR_MIN
#   define  VDBC_SET1(D, K, V)  DBC_ASTV(vset_lane_s8(V, VDBC_ASTM(D), K))
#else
#   define  VDBC_SET1(D, K, V)  DBC_ASTV(vset_lane_u8(V, VDBC_ASTM(D), K))
#endif

    int64x1_t   s = vdup_n_s64(k*8);
    uint64x1_t  m = VDBC_ASDU(d);
    uint64x1_t  x;
    x = vshl_u64(vdup_n_u64(0xffull), s);
    x = vbic_u64(m, x);
    x = vorr_u64(x, vshl_u64(vdup_n_u64(v), s));
    return VDDU_ASBC(x);
}


INLINE(Vdhu,VDHU_SET1) (Vdhu d, Rc(0, 3) k, uint16_t v)
{
#define     VDHU_SET1(D, K, V)  vset_lane_u16(V, D, K)
    int64x1_t  s = vdup_n_s64(k*16);
    uint64x1_t m = vdup_n_u64(0xffffull);
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_u16(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(v), s), m);
    return vreinterpret_u16_u64(m);
}

INLINE(Vdhi,VDHI_SET1) (Vdhi d, Rc(0, 3) k, int16_t v)
{
#define     VDHI_SET1(D, K, V)  vset_lane_s16(V, D, K)
    int64x1_t   s = vdup_n_s64(k*16);
    uint64x1_t  x = vdup_n_u64(0xffffull);
    x = vshl_u64(x, s);
    x = vbic_u64(x, vreinterpret_u64_s16(d));
    x = vorr_u64(x, vshl_u64(vdup_n_u64(0xffffull&v), s));
    return vreinterpret_s16_u64(x);
}

INLINE(Vdhf,VDHF_SET1) (Vdhf d, Rc(0, 3) k, flt16_t v)
{
#define     VDHF_SET1(D, K, V)  vset_lane_f16(V, D, K)
    int64x1_t   s = vdup_n_s64(k*16);
    uint64x1_t  m = vdup_n_u64(0xffffull);
    uint64x1_t  h = vdup_n_u64(FLT16_ASHU(v));
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_u16(d), m);
    m = vorr_u64(vshl_u64(h, s), m);
    return  vreinterpret_f16_u64(m);
}


INLINE(Vdwu,VDWU_SET1) (Vdwu d, Rc(0, 1) k, uint32_t v)
{
#define     VDWU_SET1(D, K, V)  vset_lane_u32(V, D, K)
    int64x1_t  s = vdup_n_s64(k*32);
    uint64x1_t m = vdup_n_u64(0xffffffffull);
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_u32(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(v), s), m);
    return  vreinterpret_u32_u64(m);
}

INLINE(Vdwi,VDWI_SET1) (Vdwi d, Rc(0, 1) k, int32_t v)
{
#define     VDWI_SET1(D, K, V)  vset_lane_s32(V, D, K)
    int64x1_t  s = vdup_n_s64(k*32);
    uint64x1_t m = vdup_n_u64(0xffffffffull);
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_s32(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(v), s), m);
    return  vreinterpret_s32_u64(m);
}

INLINE(Vdwf,VDWF_SET1) (Vdwf d, Rc(0, 1) k, float v)
{
#define     VDWF_SET1(D, K, V)  vset_lane_f32(V, D, K)
    int64x1_t  s = vdup_n_s64(k*32);
    uint64x1_t m = vdup_n_u64(0xffffffffull);
    WORD_TYPE  w = {.F=v};
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_f32(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(w.U), s), m);
    return  vreinterpret_f32_u64(m);
}


INLINE(Vqyu,VQYU_SET1) (Vqyu d, Rc(0,127) k, _Bool v)
{
#define     VQYU_SET1   VQYU_SET1
    Vqdu  q = VQYU_ASTM(d);
    Vddu  l =  vget_low_u64(q);
    Vddu  r =  vget_high_u64(q);
    if (k > 63)
        r = VDYU_ASTM(VDYU_SET1(DYU_ASTV(r), 63&k, v));
    else
        l = VDYU_ASTM(VDYU_SET1(DYU_ASTV(l), k,    v));
    q = vcombine_u64(l, r);
    return  QYU_ASTV(q);
}

INLINE(Vqbu,VQBU_SET1) (Vqbu d, Rc(0, 15) k, uint8_t v)
{
#define     VQBU_SET1(D, K, V)  vsetq_lane_u8(V, D, K)
    uint8x8_t l = vget_low_u8(d);
    uint8x8_t r = vget_high_u8(d);
    if (k > 7)
        r = (VDBU_SET1)(r, 7&k, v);
    else
        l = (VDBU_SET1)(l,   k, v);
    return  vcombine_u8(l, r);
}

INLINE(Vqbi,VQBI_SET1) (Vqbi d, Rc(0, 15) k, int8_t v)
{
#define     VQBI_SET1(D, K, V)  vsetq_lane_s8(V, D, K)
    int8x8_t l = vget_low_s8(d);
    int8x8_t r = vget_high_s8(d);
    if (k > 7)
        r = (VDBI_SET1)(r, 7&k, v);
    else
        l = (VDBI_SET1)(l,   k, v);
    return  vcombine_u8(l, r);
}

INLINE(Vqbc,VQBC_SET1) (Vqbc d, Rc(0, 15) k, char v)
{
#if CHAR_MIN
#   define  QBC_SET1(M, K, V)   vsetq_lane_s8(V, M, K)
#else
#   define  QBC_SET1(M, K, V)   vsetq_lane_u8(V, M, K)
#endif

#define     VQBC_SET1(D, K, V)  QBC_ASTV(QBC_SET1(VQBC_ASTM(D), K, V))
    uint8x16_t  q = VQBC_ASBU(d);
    uint8x8_t   l = vget_low_u8(q);
    uint8x8_t   r = vget_high_u8(q);
    if (k > 7)
        r = (VDBU_SET1)(r, 7&k, v);
    else
        l = (VDBU_SET1)(l,   k, v);
    return  VQBU_ASBC(vcombine_u8(l, r));
}


INLINE(Vqhu,VQHU_SET1) (Vqhu d, Rc(0, 7) k, uint16_t v)
{
#define     VQHU_SET1(D, K, V)  vsetq_lane_u16(V, D, K)
    uint16x4_t l = vget_low_u16(d);
    uint16x4_t r = vget_high_u16(d);
    if (k > 3)
        r = (VDHU_SET1)(r, 3&k, v);
    else
        l = (VDHU_SET1)(l,  k,  v);
    return  vcombine_u16(l, r);
}

INLINE(Vqhi,VQHI_SET1) (Vqhi d, Rc(0, 7) k, int16_t v)
{
#define     VQHI_SET1(D, K, V)  vsetq_lane_s16(V, D, K)
    int16x4_t l = vget_low_s16(d);
    int16x4_t r = vget_high_s16(d);
    if (k > 3)
        r = (VDHI_SET1)(r, 3&k, v);
    else
        l = (VDHI_SET1)(l,  k,  v);
    return  vcombine_s16(l, r);
}

INLINE(Vqhf,VQHF_SET1) (Vqhf d, Rc(0, 7) k, flt16_t v)
{
#define     VQHF_SET1(D, K, V)  vsetq_lane_f16(V, D, K)
    float16x4_t l = vget_low_f16(d);
    float16x4_t r = vget_high_f16(d);
    if (k > 3)
        r = (VDHF_SET1)(r, 3&k, v);
    else
        l = (VDHF_SET1)(l,  k,  v);
    return  vcombine_f16(l, r);
}



INLINE(Vqwu,VQWU_SET1) (Vqwu d, Rc(0, 3) k, uint32_t v)
{
#define     VQWU_SET1(D, K, V)  vsetq_lane_u32(V, D, K)
    uint32x2_t l = vget_low_u32(d);
    uint32x2_t r = vget_high_u32(d);
    if (k > 1)
        r = (VDWU_SET1)(r, 1&k, v);
    else
        l = (VDWU_SET1)(l,  k,  v);
    return  vcombine_u32(l, r);
}

INLINE(Vqwi,VQWI_SET1) (Vqwi d, Rc(0, 3) k, int32_t v)
{
#define     VQWI_SET1(D, K, V)  vsetq_lane_s32(V, D, K)
    int32x2_t l = vget_low_s32(d);
    int32x2_t r = vget_high_s32(d);
    if (k > 1)
        r = (VDWI_SET1)(r, 1&k, v);
    else
        l = (VDWI_SET1)(l,  k,  v);
    return  vcombine_s32(l, r);
}

INLINE(Vqwf,VQWF_SET1) (Vqwf d, Rc(0, 3) k, float v)
{
#define     VQWF_SET1(D, K, V)  vsetq_lane_f32(V, D, K)
    float32x2_t l = vget_low_f32(d);
    float32x2_t r = vget_high_f32(d);
    if (k > 1)
        r = (VDWF_SET1)(r, 1&k, v);
    else
        l = (VDWF_SET1)(l,  k,  v);
    return  vcombine_f32(l, r);
}


INLINE(Vqdu,VQDU_SET1) (Vqdu d, Rc(0, 1) k, uint64_t v)
{
#define     VQDU_SET1(D, K, V) vsetq_lane_u64(V, D, K)
    return k
    ?   vsetq_lane_u64(v, d, 1)
    :   vsetq_lane_u64(v, d, 0);
}

INLINE(Vqdi,VQDI_SET1) (Vqdi d, Rc(0, 1) k, int64_t v)
{
#define     VQDI_SET1(D, K, V) vsetq_lane_s64(V, D, K)
    return k
    ?   vsetq_lane_s64(v, d, 1)
    :   vsetq_lane_s64(v, d, 0);
}

INLINE(Vqdf,VQDF_SET1) (Vqdf d, Rc(0, 1) k, double v)
{
#define     VQDF_SET1(D, K, V) vsetq_lane_f64(V, D, K)
    return k
    ?   vsetq_lane_f64(v, d, 1)
    :   vsetq_lane_f64(v, d, 0);
}

#if 0 // _LEAVE_ARM_SET1
}
#endif



#if 0 // _ENTER_ARM_LDR1
{
#endif

#define MY_LDRVAC(C, S, L, T, A) C##S(L((T const *) A))

#define MY_LDRDAC(D, SRC)       \
MY_LDRVAC(                      \
    vreinterpret_##D,   /* C */ \
    _u64,               /* S */ \
    vld1_u64,           /* L */ \
    uint64_t,           /* T */ \
    SRC                 /* A */ \
)

#define MY_LDRQAC(D, SRC)       \
MY_LDRVAC(                      \
    vreinterpretq_##D,  /* C */ \
    _p128,              /* S */ \
    vldrq_p128,         /* L */ \
    unsigned __int128,  /* T */ \
    SRC                 /* A */ \
)


INLINE(Vwyu,VWYU_LDRK) (Vwyu v, Rc(0, 31) k, _Bool volatile const a[1])
{
#define     VWYU_LDRK   VWYU_LDRK
    uint32x2_t  m = vreinterpret_u32_f32(vdup_n_f32(VWYU_ASTM(v)));
    m = vbic_u32(m, vdup_n_u32(1u<<k));
    m = vorr_u32(m, vdup_n_u32((uint32_t) *a<<k));
    return WYU_ASTV(vget_lane_f32(vreinterpret_f32_u32(m), 0));
}

INLINE(Vwbu,VWBU_LDRK) (Vwbu v, Rc(0, 3) k, uint8_t volatile const a[1])
{
#define     WBU_LDRK(M, K, A)       \
vget_lane_f32(                      \
    vreinterpret_f32_u8(             \
        vld1_lane_u8(               \
            (uint8_t *)(A),         \
            vreinterpret_u8_f32(    \
                vdup_n_f32(M)       \
            ),                      \
            K                       \
        )                           \
    ),                              \
    0                           \
)

#define     VWBU_LDRK(V,K,A)        \
WBU_ASTV(WBU_LDRK(VWBU_ASTM(V), K, A))

    int64x1_t   s = vdup_n_s64(0ll-8*k);
    float32x2_t m = vdup_n_f32(VWBU_ASTM(v));
    uint64x1_t  d = vreinterpret_u64_f32(m);
    uint8x8_t   b = vset_lane_u8(*a, vdup_n_u8(0), 0);
    d = vbic_u64(d, vshl_u64(vdup_n_u64(255ull), s));
    d = vorr_u64(d, vshl_u64(vreinterpret_u64_u8(b), s));
    m = vreinterpret_f32_u64(d);
    return WBU_ASTV(vget_lane_f32(m, 0));
}

#if 0 // _LEAVE_ARM_LDRK
}
#endif

#if 0 // _ENTER_ARM_LDRW
{
#endif

INLINE(Vwyu,BOOL_LDRW) (uint32_t s) 
{
#define     BOOL_LDRW(S) ((WORD_VTYPE){.U=S}).Y.U
    return  BOOL_LDRW(s);
}

INLINE(Vwbu,UCHAR_LDRW) (uint32_t s) 
{
#define     UCHAR_LDRW(S) ((WORD_VTYPE){.U=S}).B.U
    return  UCHAR_LDRW(s);
}

INLINE(Vwbi,SCHAR_LDRW) (uint32_t s) 
{
#define     SCHAR_LDRW(S) ((WORD_VTYPE){.U=S}).B.I
    return  SCHAR_LDRW(s);
}

INLINE(Vwbc,CHAR_LDRW) (uint32_t s) 
{
#define     CHAR_LDRW(S) ((WORD_VTYPE){.U=S}).B.C
    return  CHAR_LDRW(s);
}


INLINE(Vwhu,USHRT_LDRW) (uint32_t s) 
{
#define     USHRT_LDRW(S) ((WORD_VTYPE){.U=S}).H.U
    return  USHRT_LDRW(s);
}

INLINE(Vwhi,SHRT_LDRW) (uint32_t s) 
{
#define     SHRT_LDRW(S) ((WORD_VTYPE){.U=S}).H.I
    return  SHRT_LDRW(s);
}


INLINE(Vwwu,UINT_LDRW) (uint32_t s) 
{
#define     UINT_LDRW(S) ((WORD_VTYPE){.U=S}).W.U
    return  UINT_LDRW(s);
}

INLINE(Vwwi,INT_LDRW) (uint32_t s) 
{
#define     INT_LDRW(S) ((WORD_VTYPE){.U=S}).W.I
    return  INT_LDRW(s);
}

#if DWRD_NLONG == 2

INLINE(Vwwu,ULONG_LDRW) (uint32_t s) 
{
#define     ULONG_LDRW(S) ((WORD_VTYPE){.U=S}).W.U
    return  ULONG_LDRW(s);
}

INLINE(Vwwi,LONG_LDRW) (uint32_t s) 
{
#define     LONG_LDRW(S) ((WORD_VTYPE){.U=S}).W.I
    return  LONG_LDRW(s);
}

#endif

INLINE(Vwhf,FLT16_LDRW) (uint32_t s) 
{
#define     FLT16_LDRW(S) ((WORD_VTYPE){.U=S}).H.F
    return  FLT16_LDRW(s);
}

INLINE(Vwwf,FLT_LDRW) (uint32_t s) 
{
#define     FLT_LDRW(S) ((WORD_VTYPE){.U=S}).W.F
    return  FLT_LDRW(s);
}


#define     MY_LDRWACYU(S) ((WORD_VTYPE){.F=*(float const *) S})

INLINE(Vwyu, BOOL_LDRWAC)   (void const *s)    {return MY_LDRWACYU(s).Y.U;}
INLINE(Vwbu,UCHAR_LDRWAC)   (uchar const s[4]) {return MY_LDRWACYU(s).B.U;}
INLINE(Vwbi,SCHAR_LDRWAC)   (schar const s[4]) {return MY_LDRWACYU(s).B.I;}
INLINE(Vwbc, CHAR_LDRWAC)    (char const s[4]) {return MY_LDRWACYU(s).B.C;}
INLINE(Vwhu,USHRT_LDRWAC)  (ushort const s[2]) {return MY_LDRWACYU(s).H.U;}
INLINE(Vwhi, SHRT_LDRWAC)   (short const s[2]) {return MY_LDRWACYU(s).H.I;}
INLINE(Vwwu, UINT_LDRWAC)    (uint const s[1]) {return *(Vwwu const *) s;}
INLINE(Vwwi,  INT_LDRWAC)     (int const s[1]) {return *(Vwwi const *) s;}
#if DWRD_NLONG == 2
INLINE(Vwwu,ULONG_LDRWAC)   (ulong const s[1]) {return *(Vwwu const *) s;}
INLINE(Vwwi, LONG_LDRWAC)    (long const s[1]) {return *(Vwwi const *) s;}
#endif

INLINE(Vwhf,FLT16_LDRWAC) (flt16_t const s[2]) {return MY_LDRWACYU(s).H.F;}
INLINE(Vwwf,  FLT_LDRWAC)   (float const s[1]) {return *(Vwwf const *) s;}

INLINE(uint32_t,WYU_LDRW) (float s) {return ((WORD_TYPE){.F=s}).U;}
INLINE(uint32_t,VWYU_LDRW) (Vwyu s) {return WYU_LDRW(s.V0);}
INLINE(uint32_t,VWBU_LDRW) (Vwbu s) {return WYU_LDRW(s.V0);}
INLINE(uint32_t,VWBI_LDRW) (Vwbi s) {return WYU_LDRW(s.V0);}
INLINE(uint32_t,VWBC_LDRW) (Vwbc s) {return WYU_LDRW(s.V0);}
INLINE(uint32_t,VWHU_LDRW) (Vwhu s) {return WYU_LDRW(s.V0);}
INLINE(uint32_t,VWHI_LDRW) (Vwhi s) {return WYU_LDRW(s.V0);}
INLINE(uint32_t,VWHF_LDRW) (Vwhf s) {return WYU_LDRW(s.V0);}
INLINE(uint32_t,VWWU_LDRW) (Vwwu s) {return WYU_LDRW(s.V0);}
INLINE(uint32_t,VWWI_LDRW) (Vwwi s) {return WYU_LDRW(s.V0);}
INLINE(uint32_t,VWWF_LDRW) (Vwwf s) {return WYU_LDRW(s.V0);}

#if 0 // _LEAVE_ARM_LDRW
}
#endif

#if 0 // _ENTER_ARM_LDRD
{
#endif

// clang generates 'ldr %d, [%x]' for all or the vld1s, so
// perhaps it *doesn't* require alignment. 

INLINE(uint64x1_t,MY_LDRDACYU) (void const *s) {return vld1_u64(s);}

#if CHAR_MIN

INLINE(int8x8_t,MY_LDRDACBC) (char const *s)
{
    return vld1_s8( ((int8_t const *) s) );
}

#else
INLINE(uint8x8_t,MY_LDRDACBC) (char const *s)
{
    return vld1_u8( ((uint8_t const *) s) );
}

#endif

INLINE(Vdyu,BOOL_LDRD) (uint64_t s) 
{
#define     BOOL_LDRD(S) ((Vdyu){vcreate_u64(S)})
    return  BOOL_LDRD(s);
}


INLINE(Vdbu,UCHAR_LDRD) (uint64_t s) 
{
#define     UCHAR_LDRD vcreate_u8
    return  UCHAR_LDRD(s);
}

INLINE(Vdbi,SCHAR_LDRD) (uint64_t s) 
{
#define     SCHAR_LDRD vcreate_s8
    return  SCHAR_LDRD(s);
}

INLINE(Vdbc,CHAR_LDRD) (uint64_t s) 
{
#if CHAR_MIN
#   define  CHAR_LDRD(S) ((Vdbc){SCHAR_LDRD(S)})
#else
#   define  CHAR_LDRD(S) ((Vdbc){UCHAR_LDRD(S)})
#endif
    return  CHAR_LDRD(s);
}


INLINE(Vdhu,USHRT_LDRD) (uint64_t s) 
{
#define     USHRT_LDRD vcreate_u16
    return  USHRT_LDRD(s);
}

INLINE(Vdhi,SHRT_LDRD) (uint64_t s) 
{
#define     SHRT_LDRD vcreate_s16
    return  SHRT_LDRD(s);
}


INLINE(Vdwu,UINT_LDRD) (uint64_t s) 
{
#define     UINT_LDRD   vcreate_u32
    return  UINT_LDRD(s);
}

INLINE(Vdwi,INT_LDRD) (uint64_t s) 
{
#define     INT_LDRD    vcreate_s32
    return  INT_LDRD(s);
}

#if DWRD_NLONG == 2

INLINE(Vdwu,ULONG_LDRD) (uint64_t s) 
{
#define     ULONG_LDRD  vcreate_u32
    return  ULONG_LDRD(s);
}

INLINE(Vdwi,LONG_LDRD) (uint64_t s) 
{
#define     LONG_LDRD   vcreate_s32
    return  LONG_LDRD(s);
}

#else

INLINE(Vddu,ULONG_LDRD) (uint64_t s) 
{
#define     ULONG_LDRD   vcreate_u64
    return  ULONG_LDRD(s);
}

INLINE(Vddi, LONG_LDRD) (uint64_t s) 
{
#define     LONG_LDRD   vcreate_s64
    return  LONG_LDRD(s);
}

#endif

#if QUAD_NLLONG == 2
INLINE(Vddu,ULLONG_LDRD) (uint64_t s) 
{
#define     ULLONG_LDRD   vcreate_u64
    return  ULLONG_LDRD(s);
}

INLINE(Vddi,LLONG_LDRD) (uint64_t s) 
{
#define     LLONG_LDRD   vcreate_s64
    return  LLONG_LDRD(s);
}
#endif

INLINE(Vdhf,FLT16_LDRD) (uint64_t s) 
{
#if defined(vcreate_f16)
#   define  FLT16_LDRD vcreate_f16
#else
#   define  FLT16_LDRD(S) vreinterpret_f16_u16(UINT16_LDRD(S))
#endif
    DWRD_VTYPE d = {.H.U=vcreate_u16(s)};
    d.H.U = vrev64_u16(d.H.U);
    return d.H.F;
}

INLINE(Vdwf,FLT_LDRD) (uint64_t s) 
{
#define     FLT_LDRD vcreate_f32
    return  FLT_LDRD(s);
}

INLINE(Vddf,DBL_LDRD) (uint64_t s) 
{
#define     DBL_LDRD vcreate_f64
    return  DBL_LDRD(s);
}

INLINE(Vdyu, BOOL_LDRDAC)  (void const *s) {return ((Vdyu){MY_LDRDACYU(s)});}
INLINE(Vdbu,UCHAR_LDRDAC)  (uchar const s[8]) {return vld1_u8(s);}
INLINE(Vdbi,SCHAR_LDRDAC)  (schar const s[8]) {return vld1_s8(s);}
INLINE(Vdbc, CHAR_LDRDAC)   (char const s[8]) {return((Vdbc){MY_LDRDACBC(s)});}
INLINE(Vdhu,USHRT_LDRDAC) (ushort const s[4]) {return vld1_u16(s);}
INLINE(Vdhi, SHRT_LDRDAC)  (short const s[4]) {return vld1_s16(s);}
INLINE(Vdwu, UINT_LDRDAC)   (uint const s[2]) {return vld1_u32(s);}
INLINE(Vdwi,  INT_LDRDAC)    (int const s[2]) {return vld1_s32(s);}

#if DWRD_NLONG == 2
INLINE(Vdwu,ULONG_LDRDAC)  (ulong const s[2]) 
{
    return  vld1_u32( ((uint32_t const *) s) );
}

INLINE(Vdwi, LONG_LDRDAC)   (long const s[2]) 
{
    return  vld1_s32( ((int32_t const *) s) );
}

#else
INLINE(Vddu,ULONG_LDRDAC)  (ulong const s[1]) {return vld1_u64(s);}
INLINE(Vddi, LONG_LDRDAC)   (long const s[1]) {return vld1_s64(s);}
#endif

#if QUAD_NLLONG == 2
INLINE(Vddu,ULLONG_LDRDAC) (ullong const s[1]) 
{
    return  vld1_u64( ((uint64_t const *) s) );
}

INLINE(Vddi, LLONG_LDRDAC)  (llong const s[1]) 
{
    return  vld1_s64( ((int64_t const *) s) );
}

#endif


INLINE(Vdhf,FLT16_LDRDAC) (flt16_t const s[4])
{
    uint16x4_t u = vld1_u16( ((uint16_t const *) s) );
    return  vreinterpret_f16_u16(u);
}

INLINE(Vdwf,  FLT_LDRDAC)   (float const s[2]) {return vld1_f32(s);}
INLINE(Vddf,  DBL_LDRDAC)  (double const s[1]) {return vld1_f64(s);}

INLINE(uint64_t,WYU_LDRD) (float s) 
{
#define     WYU_LDRD(S) ((DWRD_TYPE){.Lo.F=S}).U
    return  WYU_LDRD(s);
}
INLINE(uint64_t,VWYU_LDRD) (Vwyu s) {return WYU_LDRD(s.V0);}
INLINE(uint64_t,VWBU_LDRD) (Vwbu s) {return WYU_LDRD(s.V0);}
INLINE(uint64_t,VWBI_LDRD) (Vwbi s) {return WYU_LDRD(s.V0);}
INLINE(uint64_t,VWBC_LDRD) (Vwbc s) {return WYU_LDRD(s.V0);}
INLINE(uint64_t,VWHU_LDRD) (Vwhu s) {return WYU_LDRD(s.V0);}
INLINE(uint64_t,VWHI_LDRD) (Vwhi s) {return WYU_LDRD(s.V0);}
INLINE(uint64_t,VWHF_LDRD) (Vwhf s) {return WYU_LDRD(s.V0);}
INLINE(uint64_t,VWWU_LDRD) (Vwwu s) {return WYU_LDRD(s.V0);}
INLINE(uint64_t,VWWI_LDRD) (Vwwi s) {return WYU_LDRD(s.V0);}
INLINE(uint64_t,VWWF_LDRD) (Vwwf s) {return WYU_LDRD(s.V0);}

INLINE(uint64_t,VDYU_LDRD) (Vdyu s) {return vget_lane_u64(s.V0,0);}
INLINE(uint64_t,VDBU_LDRD) (Vdbu s) {return ((DWRD_VTYPE){.B.U=s}).U;}
INLINE(uint64_t,VDBI_LDRD) (Vdbi s) {return ((DWRD_VTYPE){.B.I=s}).U;}
INLINE(uint64_t,VDBC_LDRD) (Vdbc s) {return ((DWRD_VTYPE){.B.C=s}).U;}
INLINE(uint64_t,VDHU_LDRD) (Vdhu s) {return ((DWRD_VTYPE){.H.U=s}).U;}
INLINE(uint64_t,VDHI_LDRD) (Vdhi s) {return ((DWRD_VTYPE){.H.I=s}).U;}
INLINE(uint64_t,VDHF_LDRD) (Vdhf s) {return ((DWRD_VTYPE){.H.F=s}).U;}
INLINE(uint64_t,VDWU_LDRD) (Vdwu s) {return ((DWRD_VTYPE){.W.U=s}).U;}
INLINE(uint64_t,VDWI_LDRD) (Vdwi s) {return ((DWRD_VTYPE){.W.I=s}).U;}
INLINE(uint64_t,VDWF_LDRD) (Vdwf s) {return ((DWRD_VTYPE){.W.F=s}).U;}
INLINE(uint64_t,VDDU_LDRD) (Vddu s) {return ((DWRD_VTYPE){.D.U=s}).U;}
INLINE(uint64_t,VDDI_LDRD) (Vddi s) {return ((DWRD_VTYPE){.D.I=s}).U;}
INLINE(uint64_t,VDDF_LDRD) (Vddf s) {return ((DWRD_VTYPE){.D.F=s}).U;}

#if 0 // _LEAVE_ARM_LDRD
}
#endif

#if 0 // _ENTER_ARM_LDRQ
{
#endif
INLINE(Vqyu,  BOOL_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).Y.U;}
INLINE(Vqbu, UCHAR_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).B.U;}
INLINE(Vqbi, SCHAR_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).B.I;}
INLINE(Vqbc,  CHAR_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).B.C;}
INLINE(Vqhu, USHRT_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).H.U;}
INLINE(Vqhi,  SHRT_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).H.I;}
INLINE(Vqwu,  UINT_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).W.U;}
INLINE(Vqwi,   INT_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).W.I;}

#if DWRD_NLONG == 2
INLINE(Vqwu, ULONG_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).W.U;}
INLINE(Vqwi,  LONG_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).W.U;}
#else
INLINE(Vqdu, ULONG_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).D.U;}
INLINE(Vqdi,  LONG_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).D.I;}
#endif

#if QUAD_NLLONG == 2
INLINE(Vqdu,ULLONG_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).D.U;}
INLINE(Vqdi, LLONG_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).D.I;}

INLINE(Vqqu,ldrqqu) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).Q.U;}
INLINE(Vqqi,ldrqqi) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).Q.I;}
#else
INLINE(Vqqu,ULLONG_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).Q.U;}
INLINE(Vqqi, LLONG_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).Q.I;}
#endif

INLINE(Vqhf,FLT16_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).H.F;}
INLINE(Vqwf,  FLT_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).W.F;}
INLINE(Vqdf,  DBL_LDRQ) (QUAD_UTYPE s){return ((QUAD_VTYPE){.U=s}).D.F;}


INLINE(Vqyu, BOOL_LDRQAC) (void const *s) {
    Vqyu d = {vld1q_u64(s)};
    return d;
}

INLINE(Vqbu, UCHAR_LDRQAC)   (uchar const src[16]) {return vld1q_u8(src);}

INLINE(Vqbi, SCHAR_LDRQAC)   (schar const src[16]) {return vld1q_s8(src);}

INLINE(Vqbc,  CHAR_LDRQAC)    (char const src[16])
{
#if CHAR_MIN
    return  ((Vqbc){vld1q_s8( (( int8_t const *) src) )});
#else
    return  ((Vqbc){vld1q_u8( ((uint8_t const *) src) )});
#endif
}


INLINE(Vqhu, USHRT_LDRQAC)  (ushort const src[8]) {return vld1q_u16(src);}
INLINE(Vqhi,  SHRT_LDRQAC)   (short const src[8]) {return vld1q_s16(src);}
INLINE(Vqwu,  UINT_LDRQAC)    (uint const src[4]) {return vld1q_u32(src);}
INLINE(Vqwi,   INT_LDRQAC)     (int const src[4]) {return vld1q_s32(src);}

#if DWRD_NLONG == 2

INLINE(Vqwu, ULONG_LDRQAC)   (ulong const src[4]) 
{
    return  vld1q_u32( ((uint32_t const *) src) );
}

INLINE(Vqwi,  LONG_LDRQAC)    (long const src[4])
{
    return  vld1q_s32( ((int32_t const *) src) );
}

#else

INLINE(Vqdu, ULONG_LDRQAC)   (ulong const src[2]) {return vld1q_u64(src);}
INLINE(Vqdi,  LONG_LDRQAC)    (long const src[2]) {return vld1q_s64(src);}

#endif

#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_LDRQAC)  (ullong const src[2])
{
    return  vld1q_u64( ((uint64_t const *) src) );
}

INLINE(Vqdi, LLONG_LDRQAC)   (llong const src[2])
{
    return  vld1q_s64( ((int64_t const *) src) );
}

INLINE(Vqqu,ldrqacqu) (QUAD_UTYPE const src[1])
{
    return ((Vqqu){vld1q_u64( ((uint64_t const *) src) )});
}

INLINE(Vqqi,ldrqacqi) (QUAD_ITYPE const src[1])
{
    return ((Vqqi){vld1q_s64( ((int64_t const *) src) )});
}

#endif

INLINE(Vqhf,FLT16_LDRQAC) (flt16_t const src[8]) {return vld1q_f16(src);}
INLINE(Vqwf,  FLT_LDRQAC)   (float const src[4]) {return vld1q_f32(src);}
INLINE(Vqdf,  DBL_LDRQAC)  (double const src[2]) {return vld1q_f64(src);}

INLINE(Vqqf,ldrqacqf) (QUAD_FTYPE const src[1])
{
    return ((Vqqf){*src});
}

#if 0 // _LEAVE_ARM_LDRQ
}
#endif

#if 0 // _ENTER_ARM_LDRO
{
#endif

#if CHAR_SIGNEDNESS
#   define  ABC_LDRO(p)     ((VOBI_TYPE){vld1q_s8_x2( (void const *)(p))})
#else
#   define  ABC_LDRO(p)     ((VOBU_TYPE){vld1q_u8_x2( (void const *)(p))})
#endif

#define     ABU_LDRO(p)     ((VOBU_TYPE){vld1q_u8_x2( (void const *)(p))})
#define     ABI_LDRO(p)     ((VOBI_TYPE){vld1q_s8_x2( (void const *)(p))})

#define     AHU_LDRO(p)     ((VOHU_TYPE){vld1q_u16_x2((void const *)(p))})
#define     AHI_LDRO(p)     ((VOHI_TYPE){vld1q_s16_x2((void const *)(p))})
#define     AHF_LDRO(p)     ((VOHF_TYPE){vld1q_f16_x2((void const *)(p))})

#define     AWU_LDRO(p)     ((VOWU_TYPE){vld1q_u32_x2((void const *)(p))})
#define     AWI_LDRO(p)     ((VOWI_TYPE){vld1q_s32_x2((void const *)(p))})
#define     AWF_LDRO(p)     ((VOWF_TYPE){vld1q_f32_x2((void const *)(p))})

#define     ADU_LDRO(p)     ((VODU_TYPE){vld1q_u64_x2((void const *)(p))})
#define     ADI_LDRO(p)     ((VODI_TYPE){vld1q_s64_x2((void const *)(p))})
#define     ADF_LDRO(p)     ((VODF_TYPE){vld1q_f64_x2((void const *)(p))})

#if 0 // _LEAVE_ARM_LDRO
}
#endif

#if 0 // _ENTER_ARM_LDRS
{
#endif

#if CHAR_SIGNEDNESS
#   define  ABC_LDRS(p)     ((VOBI_TYPE){vld1q_s8_x4( (void const *)(p))})
#else
#   define  ABC_LDRS(p)     ((VOBU_TYPE){vld1q_u8_x4( (void const *)(p))})
#endif
#define     ABU_LDRS(p)     ((VOBU_TYPE){vld1q_u8_x4( (void const *)(p))})
#define     ABI_LDRS(p)     ((VOBI_TYPE){vld1q_s8_x4( (void const *)(p))})

#define     AHU_LDRS(p)     ((VOHU_TYPE){vld1q_u16_x4((void const *)(p))})
#define     AHI_LDRS(p)     ((VOHI_TYPE){vld1q_s16_x4((void const *)(p))})
#define     AHF_LDRS(p)     ((VOHF_TYPE){vld1q_f16_x4((void const *)(p))})

#define     AWU_LDRS(p)     ((VOWU_TYPE){vld1q_u32_x4((void const *)(p))})
#define     AWI_LDRS(p)     ((VOWI_TYPE){vld1q_s32_x4((void const *)(p))})
#define     AWF_LDRS(p)     ((VOWF_TYPE){vld1q_f32_x4((void const *)(p))})

#define     ADU_LDRS(p)     ((VODU_TYPE){vld1q_u64_x4((void const *)(p))})
#define     ADI_LDRS(p)     ((VODI_TYPE){vld1q_s64_x4((void const *)(p))})
#define     ADF_LDRS(p)     ((VODF_TYPE){vld1q_f64_x4((void const *)(p))})

#if 0 // _LEAVE_ARM_LDRS
}
#endif

#if 0 // _ENTER_ARM_LDPD
{
#endif

INLINE(uint64x1_t,MY_LDPDYU) (uint32_t a, uint32_t b)
{
    DWRD_TYPE   c = {.Lo.U=a, .Hi.U=b};
    return  vdup_n_u64(c.U);
}

INLINE(Vdyu,  BOOL_LDPD) (uint32_t a, uint32_t b)
{
    return  ((Vdyu){MY_LDPDYU(a, b)});
}

INLINE(Vdbu, UCHAR_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).B.U;
}

INLINE(Vdbi, SCHAR_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).B.I;
}

INLINE(Vdbc,  CHAR_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).B.C;
}

INLINE(Vdhu, USHRT_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).H.U;
}

INLINE(Vdhi,  SHRT_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).H.I;
}

INLINE(Vdwu,  UINT_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).W.U;
}

INLINE(Vdwi,   INT_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).W.I;
}

#if DWRD_NLONG == 2
INLINE(Vdwu, ULONG_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).W.U;
}

INLINE(Vdwi,  LONG_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).W.I;
}

#else
INLINE(Vddu, ULONG_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).D.U;
}

INLINE(Vddi,  LONG_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).D.I;
}

#endif

#if QUAD_NLLONG == 2
INLINE(Vddu,ULLONG_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).D.U;
}

INLINE(Vddi, LLONG_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).D.I;
}

#endif

INLINE(Vdhf,FLT16_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).H.F;
}

INLINE(Vdwf,  FLT_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).W.F;
}

INLINE(Vddf,  DBL_LDPD) (uint32_t a, uint32_t b)
{
    return  ((DWRD_VTYPE){.D.U=MY_LDPDYU(a, b)}).D.F;
}

#if 0 // _LEAVE_ARM_LDPD
}
#endif

#if 0 // _ENTER_ARM_LDPQ
{
#endif

INLINE(uint64x2_t,MY_LDPQYU) (uint64_t a, uint64_t b)
{
    uint64x1_t l = vdup_n_u64(a);
    uint64x1_t r = vdup_n_u64(b);
    return  vcombine_u64(l, r);
}

INLINE(Vqyu,  BOOL_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((Vqyu){MY_LDPQYU(a, b)});
}

INLINE(Vqbu, UCHAR_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).B.U;
}

INLINE(Vqbi, SCHAR_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).B.I;
}

INLINE(Vqbc,  CHAR_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).B.C;
}

INLINE(Vqhu, USHRT_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).H.U;
}

INLINE(Vqhi,  SHRT_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).H.I;
}

INLINE(Vqwu,  UINT_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).W.U;
}

INLINE(Vqwi,   INT_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).W.I;
}


#if DWRD_NLONG == 2
INLINE(Vqwu, ULONG_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).W.U;
}

INLINE(Vqwi,  LONG_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).W.I;
}

#else
INLINE(Vqdu, ULONG_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).D.U;
}

INLINE(Vqdi,  LONG_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).D.I;
}

#endif

#if QUAD_NLLONG == 2
INLINE(Vqdu,ULLONG_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).D.U;
}

INLINE(Vqdi, LLONG_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).D.I;
}

INLINE(Vqqu,ldpqqu) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).Q.U;
}

INLINE(Vqqi,ldpqqi) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).Q.I;
}

#else
INLINE(Vqqu,ULLONG_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).Q.U;
}

INLINE(Vqqi, LLONG_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).Q.I;
}

#endif

INLINE(Vqhf,FLT16_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).H.F;
}

INLINE(Vqwf,  FLT_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).W.F;
}

INLINE(Vqdf,  DBL_LDPQ) (uint64_t a, uint64_t b)
{
    return  ((QUAD_VTYPE){.D.U=MY_LDPQYU(a, b)}).D.F;
}

#if 0 // _LEAVE_ARM_LDPQ
}
#endif

#if 0 // _ENTER_ARM_LUNN
{
#endif

/*  NOTE: keep the const qualifier for the macros; it will
    help prevent the possibility of doing something like:

        (SHRT_LUNN(dst)=(src))

*/

INLINE(  _Bool,  BOOL_LUNNAC) (void const *a) {return *(_Bool const *) a;}

INLINE(  uchar, UCHAR_LUNNAC) (void const *a) {return *(uchar const *) a;}
INLINE(  schar, SCHAR_LUNNAC) (void const *a) {return *(schar const *) a;}
INLINE(   char,  CHAR_LUNNAC) (void const *a) {return *(char const *) a;}

INLINE( ushort, USHRT_LUNNAC) (void const *a)
{
    return ((USHRT_STG(TYPE) const *) a)->M.U;
}

INLINE(   short, SHRT_LUNNAC) (void const *a)
{
    return ((SHRT_STG(TYPE) const *) a)->M.I;
}

INLINE(   uint,  UINT_LUNNAC) (void const *a)
{
    return ((UINT_STG(TYPE) const *) a)->M.U;
}

INLINE(    int,   INT_LUNNAC) (void const *a)
{
    return  ((INT_STG(TYPE) const *) a)->M.I;
}

INLINE(  ulong, ULONG_LUNNAC) (void const *a)
{
    return  ((ULONG_STG(TYPE) const *) a)->M.U;
}

INLINE(   long,  LONG_LUNNAC) (void const *a)
{
    return  ((LONG_STG(TYPE) const *) a)->M.I;
}

INLINE( ullong,ULLONG_LUNNAC) (void const *a)
{
    return  ((ULLONG_STG(TYPE) const *) a)->M.U;
}

INLINE(  llong, LLONG_LUNNAC) (void const *a)
{
    return  ((LLONG_STG(TYPE) const *) a)->M.I;
}

INLINE(flt16_t, FLT16_LUNNAC) (void const *a)
{
    return  ((FLT16_STG(TYPE) const *) a)->M.F;
}

INLINE(  float,   FLT_LUNNAC) (void const *a)
{
    return  ((FLT_STG(TYPE) const *) a)->M.F;
}

INLINE( double,   DBL_LUNNAC) (void const *a)
{
    return  ((DBL_STG(TYPE) const *) a)->M.F;
}

#if 0 // _LEAVE_ARM_LUNN
}
#endif

#if 0 // _ENTER_ARM_LUNL
{
#endif

INLINE(  _Bool,  BOOL_LUNLAC) (void const *a) {return *(_Bool const *) a;}

INLINE(  uchar, UCHAR_LUNLAC) (void const *a) {return *(uchar const *) a;}
INLINE(  schar, SCHAR_LUNLAC) (void const *a) {return *(schar const *) a;}
INLINE(   char,  CHAR_LUNLAC) (void const *a) {return  *(char const *) a;}

INLINE( ushort, USHRT_LUNLAC) (void const *a)
{
    ushort r = ((USHRT_STG(TYPE) const *) a)->M.U;
#if MY_ENDIAN == ENDIAN_BIG
    r = __revsh(r);
#endif
    return  r;
}

INLINE(   short, SHRT_LUNLAC) (void const *a)
{
    return  USHRT_LUNLAC(a);
}

INLINE(   uint,  UINT_LUNLAC) (void const *a)
{
    uint r = ((UINT_STG(TYPE) const *) a)->M.U;
#if MY_ENDIAN == ENDIAN_BIG
    r = __rev(r);
#endif
    return  r;
}

INLINE(    int,   INT_LUNLAC) (void const *a)
{
    return UINT_LUNLAC(a);
}

INLINE(  ulong, ULONG_LUNLAC) (void const *a)
{
    ulong r = ((ULONG_STG(TYPE) const *) a)->M.U;
#if MY_ENDIAN == ENDIAN_BIG
    r = __revl(r);
#endif
    return  r;
}

INLINE(   long,  LONG_LUNLAC) (void const *a)
{
    return  ULONG_LUNLAC(a);
}

INLINE( ullong,ULLONG_LUNLAC) (void const *a)
{
    ullong r = ((ULLONG_STG(TYPE) const *) a)->M.U;
#if MY_ENDIAN == ENDIAN_BIG
    r = __revll(r);
#endif
    return  r;
}

INLINE(  llong, LLONG_LUNLAC) (void const *a)
{
    return  ULLONG_LUNLAC(a);
}

INLINE(flt16_t, FLT16_LUNLAC) (void const *a)
{
    return  ((HALF_TYPE){.U=UINT16_LUNLAC(a)}).F;
}

INLINE(  float,   FLT_LUNLAC) (void const *a)
{
    return  ((WORD_TYPE){.U=UINT32_LUNLAC(a)}).F;
}

INLINE( double,   DBL_LUNLAC) (void const *a)
{
    return  ((DWRD_TYPE){.U=UINT32_LUNLAC(a)}).F;
}

#if 0 // _LEAVE_ARM_LUNL
}
#endif

#if 0 // _ENTER_ARM_LUNR
{
#endif

INLINE(  _Bool,  BOOL_LUNRAC) (void const *a) {return *(_Bool const *) a;}

INLINE(  uchar, UCHAR_LUNRAC) (void const *a) {return *(uchar const *) a;}
INLINE(  schar, SCHAR_LUNRAC) (void const *a) {return *(schar const *) a;}
INLINE(   char,  CHAR_LUNRAC) (void const *a) {return  *(char const *) a;}

INLINE( ushort, USHRT_LUNRAC) (void const *a)
{
    ushort r = ((USHRT_STG(TYPE) const *) a)->M.U;
#if MY_ENDIAN == ENDIAN_LIL
    r = __revsh(r);
#endif
    return  r;
}

INLINE(   short, SHRT_LUNRAC) (void const *a)
{
    return  USHRT_LUNRAC(a);
}

INLINE(   uint,  UINT_LUNRAC) (void const *a)
{
    uint r = ((UINT_STG(TYPE) const *) a)->M.U;
#if MY_ENDIAN == ENDIAN_LIL
    r = __rev(r);
#endif
    return  r;
}

INLINE(    int,   INT_LUNRAC) (void const *a)
{
    return UINT_LUNRAC(a);
}

INLINE(  ulong, ULONG_LUNRAC) (void const *a)
{
    ulong r = ((ULONG_STG(TYPE) const *) a)->M.U;
#if MY_ENDIAN == ENDIAN_LIL
    r = __revl(r);
#endif
    return  r;
}

INLINE(   long,  LONG_LUNRAC) (void const *a)
{
    return  ULONG_LUNRAC(a);
}

INLINE( ullong,ULLONG_LUNRAC) (void const *a)
{
    ullong r = ((ULLONG_STG(TYPE) const *) a)->M.U;
#if MY_ENDIAN == ENDIAN_LIL
    r = __revll(r);
#endif
    return  r;
}

INLINE(  llong, LLONG_LUNRAC) (void const *a)
{
    return  ULLONG_LUNRAC(a);
}

INLINE(flt16_t, FLT16_LUNRAC) (void const *a)
{
    return  ((HALF_TYPE){.U=UINT16_LUNRAC(a)}).F;
}

INLINE(  float,   FLT_LUNRAC) (void const *a)
{
    return  ((WORD_TYPE){.U=UINT32_LUNRAC(a)}).F;
}

INLINE( double,   DBL_LUNRAC) (void const *a)
{
    return  ((DWRD_TYPE){.U=UINT32_LUNRAC(a)}).F;
}

#if 0 // _LEAVE_ARM_LUNR
}
#endif

#if 0 // _ENTER_ARM_LUN1
{
#endif

INLINE(Vwyu,VWYU_LUN1) (Vwyu d, Rc(0, 31) k, void const *a)
{
    unsigned src = *(_Bool const *) a;
    unsigned dst = VWWU_ASTV(VWYU_ASWU(d));
    unsigned msk = 1<<k;
    dst = dst&(~msk);
    dst = dst|(src<<k);
    return  VWWU_ASYU(UINT_ASTV(dst));
}


INLINE(Vwbu,VWBU_LUN1) (Vwbu d, Rc(0, 3) k, void const *a)
{
#define     VWBU_LUN1(D, K, A) VWBU_SET1(D, K, (*(uint8_t const *) A))
    return (VWBU_SET1)(d, k, *(uint8_t const *) a);
}

INLINE(Vwbi,VWBI_LUN1) (Vwbi d, Rc(0, 3) k, void const *a)
{
#define     VWBI_LUN1(D, K, A) VWBI_SET1(D, K, (*(int8_t const *) A))
    return (VWBI_SET1)(d, k, *(int8_t const *) a);
}

INLINE(Vwbc,VWBC_LUN1) (Vwbc d, Rc(0, 3) k, void const *a)
{
#define     VWBC_LUN1(D, K, A) VWBC_SET1(D, K, (*(char const *) A))
    return (VWBC_SET1)(d, k, *(char const *) a);
}


INLINE(Vwhu,VWHU_LUN1) (Vwhu d, Rc(0, 1) k, void const *a)
{
#define     VWHU_LUN1(D, K, A) VWHU_SET1(D, K, ((HALF_TYPE *)(A)->M.U))
    return (VWHU_SET1)(d, k, ((HALF_TYPE const *) a)->M.U);
}

INLINE(Vwhi,VWHI_LUN1) (Vwhi d, Rc(0, 1) k, void const *a)
{
#define     VWHI_LUN1(D, K, A) VWHI_SET1(D, K, ((HALF_TYPE *)(A)->M.I))
    return (VWHI_SET1)(d, k, ((HALF_TYPE *) a)->M.I);
}

INLINE(Vwhf,VWHF_LUN1) (Vwhf d, Rc(0, 1) k, void const *a)
{
#define     VWHF_LUN1(D, K, A) VWHF_SET1(D, K, ((HALF_TYPE *)(A)->M.F))
    return (VWHF_SET1)(d, k, ((HALF_TYPE *) a)->M.F);
}


INLINE(Vdyu,VDYU_LUN1) (Vdyu d, Rc(0, 63) k, void const *a)
{
    uint64x1_t src = vdup_n_u64((*(_Bool const *) a));
    uint64x1_t dst = VDYU_ASDU(d);
    uint64x1_t msk = vdup_n_u64(1);
    int64x1_t  bit = vdup_n_s64(k);
    msk = vshl_u64(msk, bit);
    dst = vbic_u64(dst, msk);
    src = vshl_u64(src, bit);
    dst = vorr_u64(dst, src);
    return  VDDU_ASYU(dst);
}

INLINE(Vdbu,VDBU_LUN1) (Vdbu d, Rc(0, 7) k, void const *a)
{
#define     VDBU_LUN1(D, K, A) vld1_lane_u8((void *)(A), D, K)
    return (VDBU_SET1)(d,k, *(uint8_t *) a);
}

INLINE(Vdbi,VDBI_LUN1) (Vdbi d, Rc(0, 7) k, void const *a)
{
#define     VDBI_LUN1(D, K, A) vld1_lane_s8((void *)(A), D, K)
    return (VDBI_SET1)(d,k, *(int8_t *) a);
}

INLINE(Vdbc,VDBC_LUN1) (Vdbc d, Rc(0, 7) k, void const *a)
{
#define  VDBC_LUN1(D, K, A) \
VDBU_ASBC(vld1_lane_u8((void *)(A), VDBC_ASBU(D), K))
    return (VDBC_SET1)(d, k, *(char *) a);
}


INLINE(Vdhu,VDHU_LUN1) (Vdhu d, Rc(0, 3) k, void const *a)
{
#define     VDHU_LUN1(D, K, A) VDHU_SET1(D, K, (((HALF_TYPE *)(a))->M.U))
    return (VDHU_SET1)(d, k, ((HALF_TYPE *) a)->M.U);
}

INLINE(Vdhi,VDHI_LUN1) (Vdhi d, Rc(0, 3) k, void const *a)
{
#define     VDHI_LUN1(D, K, A) VDHI_SET1(D, K, (((HALF_TYPE *)(A))->M.I))
    return (VDHI_SET1)(d, k, ((HALF_TYPE *) a)->M.I);
}

INLINE(Vdhf,VDHF_LUN1) (Vdhf d, Rc(0, 3) k, void const *a)
{
#define     VDHF_LUN1(D, K, A) VDHF_SET1(D, K, (((HALF_TYPE *)(A))->M.F))
    return (VDHF_SET1)(d, k, ((HALF_TYPE *) a)->M.F);
}


INLINE(Vdwu,VDWU_LUN1) (Vdwu d, Rc(0, 1) k, void const *a)
{
#define     VDWU_LUN1(D, K, A) VDWU_SET1(D, K, (((WORD_TYPE *)(A))->M.U))
    return (VDWU_SET1)(d, k, ((WORD_TYPE *) a)->M.U);
}

INLINE(Vdwi,VDWI_LUN1) (Vdwi d, Rc(0, 1) k, void const *a)
{
#define     VDWI_LUN1(D, K, A) VDWI_SET1(D, K, (((WORD_TYPE *)(A))->M.I))
    return (VDWI_SET1)(d, k, ((WORD_TYPE *) a)->M.I);
}

INLINE(Vdwf,VDWF_LUN1) (Vdwf d, Rc(0, 1) k, void const *a)
{
#define     VDWF_LUN1(D, K, A) VDWF_SET1(D, K, (((WORD_TYPE *)(A))->M.F))
    return (VDWF_SET1)(d, k, ((WORD_TYPE *) a)->M.F);
}


INLINE(Vqyu,VQYU_LUN1) (Vqyu d, Rc(0, 127) k, void const *a)
{
#define     VQYU_LUN1(D, K, A) VQYU_SET1(D,K,(*(_Bool const *) A))
    return (VQYU_SET1)(d, k, *(_Bool const *) a);
}

INLINE(Vqbu,VQBU_LUN1) (Vqbu d, Rc(0, 15) k, void const *a)
{
#define     VQBU_LUN1(D, K, A) vld1q_lane_u8((void *)(A), D, K)
    return (VQBU_SET1)(d,k, *(uint8_t *) a);
}

INLINE(Vqbi,VQBI_LUN1) (Vqbi d, Rc(0, 15) k, void const *a)
{
#define     VQBI_LUN1(D, K, A) vld1q_lane_s8((void *)(A), D, K)
    return (VQBI_SET1)(d,k, *(int8_t *) a);
}

INLINE(Vqbc,VQBC_LUN1) (Vqbc d, Rc(0, 15) k, void const *a)
{
#define  VQBC_LUN1(D, K, A) \
VQBU_ASBC(vld1q_lane_u8((void *)(A), VQBC_ASBU(D), K))
    return (VQBC_SET1)(d, k, *(char *) a);
}


INLINE(Vqhu,VQHU_LUN1) (Vqhu d, Rc(0, 7) k, void const *a)
{
#define     VQHU_LUN1(D, K, A) VQHU_SET1(D, K, (((HALF_TYPE *)(a))->M.U))
    return (VQHU_SET1)(d, k, ((HALF_TYPE *) a)->M.U);
}

INLINE(Vqhi,VQHI_LUN1) (Vqhi d, Rc(0, 7) k, void const *a)
{
#define     VQHI_LUN1(D, K, A) VQHI_SET1(D, K, (((HALF_TYPE *)(A))->M.I))
    return (VQHI_SET1)(d, k, ((HALF_TYPE *) a)->M.I);
}

INLINE(Vqhf,VQHF_LUN1) (Vqhf d, Rc(0, 7) k, void const *a)
{
#define     VQHF_LUN1(D, K, A) VQHF_SET1(D, K, (((HALF_TYPE *)(A))->M.F))
    return (VQHF_SET1)(d, k, ((HALF_TYPE *) a)->M.F);
}


INLINE(Vqwu,VQWU_LUN1) (Vqwu d, Rc(0, 3) k, void const *a)
{
#define     VQWU_LUN1(D, K, A) VQWU_SET1(D, K, (((WORD_TYPE *)(A))->M.U))
    return (VQWU_SET1)(d, k, ((WORD_TYPE *) a)->M.F);
}

INLINE(Vqwi,VQWI_LUN1) (Vqwi d, Rc(0, 3) k, void const *a)
{
#define     VQWI_LUN1(D, K, A) VQWI_SET1(D, K, (((WORD_TYPE *)(A))->M.I))
    return (VQWI_SET1)(d, k, ((WORD_TYPE *) a)->M.I);
}

INLINE(Vqwf,VQWF_LUN1) (Vqwf d, Rc(0, 3) k, void const *a)
{
#define     VQWF_LUN1(D, K, A) VQWF_SET1(D, K, (((WORD_TYPE *)(A))->M.F))
    return (VQWI_SET1)(d, k, ((WORD_TYPE *) a)->M.F);
}


INLINE(Vqdu,VQDU_LUN1) (Vqdu d, Rc(0, 1) k, void const *a)
{
#define     VQDU_LUN1(D, K, A) VQDU_SET1(D, K, (((DWRD_TYPE *)(A))->M.U))
    return (VQDU_SET1)(d, k, ((DWRD_TYPE *) a)->M.U);
}

INLINE(Vqdi,VQDI_LUN1) (Vqdi d, Rc(0, 1) k, void const *a)
{
#define     VQDI_LUN1(D, K, A) VQDI_SET1(D, K, (((DWRD_TYPE *)(A))->M.I))
    return (VQDI_SET1)(d, k, ((DWRD_TYPE *) a)->M.I);
}

INLINE(Vqdf,VQDF_LUN1) (Vqdf d, Rc(0, 1) k, void const *a)
{
#define     VQDF_LUN1(D, K, A) VQDF_SET1(D, K, (((DWRD_TYPE *)(A))->M.F))
    return (VQDI_SET1)(d, k, ((DWRD_TYPE *) a)->M.F);
}

#if 0 // _LEAVE_ARM_LUN1
}
#endif

#if 0 // _ENTER_ARM_LUNW
{
#endif

#define     MY_LUNWAC(A) (((WORD_TYPE const *) A)->M.F)

INLINE(Vwyu, BOOL_LUNWAC) (void const *a) {return ((Vwyu){MY_LUNWAC(a)});}
INLINE(Vwbu,UCHAR_LUNWAC) (void const *a) {return ((Vwbu){MY_LUNWAC(a)});}
INLINE(Vwbi,SCHAR_LUNWAC) (void const *a) {return ((Vwbi){MY_LUNWAC(a)});}
INLINE(Vwbc, CHAR_LUNWAC) (void const *a) {return ((Vwbc){MY_LUNWAC(a)});}
INLINE(Vwhu,USHRT_LUNWAC) (void const *a) {return ((Vwhu){MY_LUNWAC(a)});}
INLINE(Vwhi, SHRT_LUNWAC) (void const *a) {return ((Vwhi){MY_LUNWAC(a)});}
INLINE(Vwwu, UINT_LUNWAC) (void const *a) {return ((Vwwu){MY_LUNWAC(a)});}
INLINE(Vwwi,  INT_LUNWAC) (void const *a) {return ((Vwwi){MY_LUNWAC(a)});}

#if DWRD_NLONG == 2
INLINE(Vwwu,ULONG_LUNWAC) (void const *a) {return ((Vwwu){MY_LUNWAC(a)});}
INLINE(Vwwi, LONG_LUNWAC) (void const *a) {return ((Vwwi){MY_LUNWAC(a)});}
#endif

INLINE(Vwhf,FLT16_LUNWAC) (void const *a) {return ((Vwhf){MY_LUNWAC(a)});}
INLINE(Vwwf,  FLT_LUNWAC) (void const *a) {return ((Vwwf){MY_LUNWAC(a)});}

#if 0 // _LEAVE_ARM_LUNW
}
#endif

#if 0 // _ENTER_ARM_LUND
{
#endif

/*  
TODO: LD1.8B has no alignment requirement; check if this
is also the case for the other formats
*/

INLINE(Vdyu,  BOOL_LUNDAC) (void const *a) {return VDBU_ASYU(vld1_u8(a));}
INLINE(Vdbu, UCHAR_LUNDAC) (void const *a) {return vld1_u8(a);}
INLINE(Vdbi, SCHAR_LUNDAC) (void const *a) {return vld1_s8(a);}
INLINE(Vdbc,  CHAR_LUNDAC) (void const *a) {return VDBU_ASBC(vld1_u8(a));}
INLINE(Vdhu, USHRT_LUNDAC) (void const *a) {return VDBU_ASHU(vld1_u8(a));}
INLINE(Vdhi,  SHRT_LUNDAC) (void const *a) {return VDBU_ASHI(vld1_u8(a));}
INLINE(Vdwu,  UINT_LUNDAC) (void const *a) {return VDBU_ASWU(vld1_u8(a));}
INLINE(Vdwi,   INT_LUNDAC) (void const *a) {return VDBU_ASWI(vld1_u8(a));}

#if DWRD_NLONG == 2
INLINE(Vdwu, ULONG_LUNDAC) (void const *a) {return VDBU_ASWU(vld1_u8(a));}
INLINE(Vdwi,  LONG_LUNDAC) (void const *a) {return VDBU_ASWI(vld1_u8(a));}
#else
INLINE(Vddu, ULONG_LUNDAC) (void const *a) {return VDBU_ASDU(vld1_u8(a));}
INLINE(Vddi,  LONG_LUNDAC) (void const *a) {return VDBU_ASDI(vld1_u8(a));}
#endif

#if QUAD_NLLONG == 2
INLINE(Vddu,ULLONG_LUNDAC) (void const *a) {return VDBU_ASDU(vld1_u8(a));}
INLINE(Vddi, LLONG_LUNDAC) (void const *a) {return VDBU_ASDI(vld1_u8(a));}
#endif

INLINE(Vdhf, FLT16_LUNDAC) (void const *a) {return VDBU_ASHF(vld1_u8(a));}
INLINE(Vdwf,   FLT_LUNDAC) (void const *a) {return VDBU_ASWF(vld1_u8(a));}
INLINE(Vddf,   DBL_LUNDAC) (void const *a) {return VDBU_ASDF(vld1_u8(a));}

#if 0 // _LEAVE_ARM_LUND
}
#endif

#if 0 // _ENTER_ARM_LUNQ
{
#endif

/*  
TODO: LD1.16B has no alignment requirement; check if this
is also the case for the other formats
*/

INLINE(Vqyu,  BOOL_LUNQAC) (void const *a) {return VQBU_ASYU(vld1q_u8(a));}
INLINE(Vqbu, UCHAR_LUNQAC) (void const *a) {return vld1q_u8(a);}
INLINE(Vqbi, SCHAR_LUNQAC) (void const *a) {return vld1q_s8(a);}
INLINE(Vqbc,  CHAR_LUNQAC) (void const *a) {return VQBU_ASBC(vld1q_u8(a));}
INLINE(Vqhu, USHRT_LUNQAC) (void const *a) {return VQBU_ASHU(vld1q_u8(a));}
INLINE(Vqhi,  SHRT_LUNQAC) (void const *a) {return VQBU_ASHI(vld1q_u8(a));}
INLINE(Vqwu,  UINT_LUNQAC) (void const *a) {return VQBU_ASWU(vld1q_u8(a));}
INLINE(Vqwi,   INT_LUNQAC) (void const *a) {return VQBU_ASWI(vld1q_u8(a));}

#if DWRD_NLONG == 2
INLINE(Vqwu, ULONG_LUNQAC) (void const *a) {return VQBU_ASWU(vld1q_u8(a));}
INLINE(Vqwi,  LONG_LUNQAC) (void const *a) {return VQBU_ASWI(vld1q_u8(a));}
#else
INLINE(Vqdu, ULONG_LUNQAC) (void const *a) {return VQBU_ASDU(vld1q_u8(a));}
INLINE(Vqdi,  LONG_LUNQAC) (void const *a) {return VQBU_ASDI(vld1q_u8(a));}
#endif

#if QUAD_NLLONG == 2
INLINE(Vqdu,ULLONG_LUNQAC) (void const *a) {return VQBU_ASDU(vld1q_u8(a));}
INLINE(Vqdi, LLONG_LUNQAC) (void const *a) {return VQBU_ASDI(vld1q_u8(a));}
INLINE(Vqqu, lunqacqu) (void const *a) {return VQBU_ASQU(vld1q_u8(a));}
INLINE(Vqqi, lunqacqi) (void const *a) {return VQBU_ASQI(vld1q_u8(a));}

#endif

INLINE(Vqhf, FLT16_LUNQAC) (void const *a) {return VQBU_ASHF(vld1q_u8(a));}
INLINE(Vqwf,   FLT_LUNQAC) (void const *a) {return VQBU_ASWF(vld1q_u8(a));}
INLINE(Vqdf,   DBL_LUNQAC) (void const *a) {return VQBU_ASDF(vld1q_u8(a));}
INLINE(Vqqf, lunqacqf) (void const *a) {return VQBU_ASQF(vld1q_u8(a));}

#if 0 // _LEAVE_ARM_LUNQ
}
#endif


#if 0 // _ENTER_ARM_SUNN
{
#endif

INLINE(flt16_t, FLT16_SUNNA) (void *dst,  flt16_t src)
{
    return  (((HALF_TYPE *) dst)->M.F=src);
}

INLINE( float,    FLT_SUNNA) (void *dst,    float src)
{
    return  (((WORD_TYPE *) dst)->M.F=src);
}

INLINE(double,    DBL_SUNNA) (void *dst,   double src)
{
    return  (((DWRD_TYPE *) dst)->M.F=src);
}

#if 0 // _LEAVE_ARM_SUNN
}
#endif

#if 0 // _ENTER_ARM_SUNL
{
#endif

INLINE(  _Bool,  BOOL_SUNLA) (void *dst,    _Bool src)
{
    return  BOOL_SUNNA(dst, src);
}


INLINE(  uchar, UCHAR_SUNLA) (void *dst, unsigned src)
{
    return  UCHAR_SUNNA(dst, src);
}

INLINE(  schar, SCHAR_SUNLA) (void *dst,   signed src)
{
    return  SCHAR_SUNNA(dst, src);
}

INLINE(   char,  CHAR_SUNLA) (void *dst,      int src)
{
    return  CHAR_SUNNA(dst, src);
}


INLINE( ushort, USHRT_SUNLA) (void *dst, unsigned src)
{
#if MY_ENDIAN == ENDIAN_BIG
    src = __revsh(src);
#endif

    return  USHRT_SUNNA(dst, src);
}

INLINE(  short,  SHRT_SUNLA) (void *dst,   signed src)
{
    return  USHRT_SUNLA(dst, src);
}


INLINE(   uint,  UINT_SUNLA) (void *dst,     uint src)
{
#if MY_ENDIAN == ENDIAN_BIG
    src = __rev(src);
#endif

    return  UINT_SUNNA(dst, src);
}

INLINE(    int,   INT_SUNLA) (void *dst,      int src)
{
    return UINT_SUNLA(dst, src);
}


INLINE(  ulong, ULONG_SUNLA) (void *dst,    ulong src)
{
#if MY_ENDIAN == ENDIAN_BIG
    src = __revl(src);
#endif

    return  ULONG_SUNNA(dst, src);
}

INLINE(   long,  LONG_SUNLA) (void *dst,     long src)
{
    return  ULONG_SUNLA(dst, src);
}


INLINE( ullong,ULLONG_SUNLA) (void *dst,   ullong src)
{
#if MY_ENDIAN == ENDIAN_BIG
    src = __revll(src);
#endif

    return  ULLONG_SUNNA(dst, src);
}

INLINE(  llong, LLONG_SUNLA)   (void *dst,   llong src)
{
    return  ULLONG_SUNLA(dst, src);
}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,sunlaqu) (void *dst, QUAD_UTYPE src)
{
#if MY_ENDIAN == ENDIAN_BIG
    src = revbqu(src);
#endif
    return  sunnaqu(dst, src);
}

INLINE(QUAD_ITYPE,sunlaqi) (void *dst, QUAD_ITYPE src)
{
#if MY_ENDIAN == ENDIAN_BIG
    src = revbqi(src);
#endif
    return  sunnaqi(dst, src);
}
#endif

INLINE(flt16_t, FLT16_SUNLA)   (void *dst, flt16_t src)
{
#if MY_ENDIAN == ENDIAN_BIG
    HALF_TYPE x = {.F=src};
    x.U = __revsh(x.U);
    src = x.F;
#endif
    return  FLT16_SUNNA(dst, src);
}

INLINE( float,    FLT_SUNLA)   (void *dst,   float src)
{
#if MY_ENDIAN == ENDIAN_BIG
    WORD_TYPE x = {.F=src};
    x.U = __rev(x.U);
    src = x.F;
#endif
    return  FLT_SUNNA(dst, src);
}

INLINE(double,    DBL_SUNLA)   (void *dst,  double src)
{
#if MY_ENDIAN == ENDIAN_BIG
    DWRD_TYPE x = {.F=src};
#   if DWRD_NLONG == 1
    x.U = __revll(x.U);
#   else
    x.U = __revll(x.U);
#   endif
    src = x.F;
#endif
    return  DBL_SUNNA(dst, src);
}


#if 0 // _LEAVE_ARM_SUNL
}
#endif

#if 0 // _ENTER_ARM_SUNR
{
#endif

INLINE(  _Bool,  BOOL_SUNRA) (void *dst,    _Bool src)
{
    return  BOOL_SUNNA(dst, src);
}


INLINE(  uchar, UCHAR_SUNRA) (void *dst, unsigned src)
{
    return  UCHAR_SUNNA(dst, src);
}

INLINE(  schar, SCHAR_SUNRA) (void *dst,   signed src)
{
    return  SCHAR_SUNNA(dst, src);
}

INLINE(   char,  CHAR_SUNRA) (void *dst,      int src)
{
    return  CHAR_SUNNA(dst, src);
}


INLINE( ushort, USHRT_SUNRA) (void *dst, unsigned src)
{
#if MY_ENDIAN == ENDIAN_LIL
    src = __revsh(src);
#endif

    return  USHRT_SUNNA(dst, src);
}

INLINE(  short,  SHRT_SUNRA) (void *dst,   signed src)
{
    return  USHRT_SUNRA(dst, src);
}


INLINE(   uint,  UINT_SUNRA) (void *dst,     uint src)
{
#if MY_ENDIAN == ENDIAN_LIL
    src = __rev(src);
#endif

    return  UINT_SUNNA(dst, src);
}

INLINE(    int,   INT_SUNRA) (void *dst,      int src)
{
    return UINT_SUNRA(dst, src);
}


INLINE(  ulong, ULONG_SUNRA) (void *dst,    ulong src)
{
#if MY_ENDIAN == ENDIAN_LIL
    src = __revl(src);
#endif

    return  ULONG_SUNNA(dst, src);
}

INLINE(   long,  LONG_SUNRA) (void *dst,     long src)
{
    return  ULONG_SUNRA(dst, src);
}


INLINE( ullong,ULLONG_SUNRA) (void *dst,   ullong src)
{
#if MY_ENDIAN == ENDIAN_LIL
    src = __revll(src);
#endif

    return  ULLONG_SUNNA(dst, src);
}

INLINE(  llong, LLONG_SUNRA)   (void *dst,   llong src)
{
    return  ULLONG_SUNRA(dst, src);
}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,sunraqu) (void *dst, QUAD_UTYPE src)
{
#if MY_ENDIAN == ENDIAN_LIL
    src = revbqu(src);
#endif
    return  sunnaqu(dst, src);
}

INLINE(QUAD_ITYPE,sunraqi) (void *dst, QUAD_ITYPE src)
{
#if MY_ENDIAN == ENDIAN_LIL
    src = revbqi(src);
#endif
    return  sunnaqi(dst, src);
}
#endif

INLINE(flt16_t, FLT16_SUNRA)   (void *dst, flt16_t src)
{
#if MY_ENDIAN == ENDIAN_LIL
    src = FLT16_REVB(src);
#endif
    return  FLT16_SUNNA(dst, src);
}

INLINE( float,    FLT_SUNRA)   (void *dst,   float src)
{
#if MY_ENDIAN == ENDIAN_LIL
    src = FLT_REVB(src);
#endif
    return  FLT_SUNNA(dst, src);
}

INLINE(double,    DBL_SUNRA)   (void *dst,  double src)
{
#if MY_ENDIAN == ENDIAN_LIL
    src = DBL_REVB(src);
#endif
    return  DBL_SUNNA(dst, src);
}


#if 0 // _LEAVE_ARM_SUNR
}
#endif

#if 0 // _ENTER_ARM_SUNW
{
#endif

INLINE(Vwyu, BOOL_SUNWA)   (void *dst, Vwyu src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWYU_ASTM(src)), src;
}

INLINE(Vwbu, UCHAR_SUNWA)   (void *dst, Vwbu src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWBU_ASTM(src)), src;
}

INLINE(Vwbi, SCHAR_SUNWA)   (void *dst, Vwbi src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWBI_ASTM(src)), src;
}

INLINE(Vwbc,  CHAR_SUNWA)   (void *dst, Vwbc src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWBC_ASTM(src)), src;
}


INLINE(Vwhu, USHRT_SUNWA)   (void *dst, Vwhu src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWHU_ASTM(src)), src;
}

INLINE(Vwhi,  SHRT_SUNWA)   (void *dst, Vwhi src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWHI_ASTM(src)), src;
}


INLINE(Vwwu,  UINT_SUNWA)   (void *dst, Vwwu src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWWU_ASTM(src)), src;
}

INLINE(Vwwi,   INT_SUNWA)   (void *dst, Vwwi src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWWI_ASTM(src)), src;
}


#if DWRD_NLONG == 2

INLINE(Vwwu, ULONG_SUNWA)   (void *dst, Vwwu src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWWU_ASTM(src)), src;
}

INLINE(Vwwi,  LONG_SUNWA)   (void *dst, Vwwi src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWWI_ASTM(src)), src;
}

#endif


INLINE(Vwhf, FLT16_SUNWA)   (void *dst, Vwhf src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWHF_ASTM(src)), src;
}

INLINE(Vwwf, FLT_SUNWA)   (void *dst, Vwwf src)
{
    ((WORD_TYPE *) dst)->M.F = VWWF_ASTM(src);
    return  src;
}


#if 0 // _LEAVE_ARM_SUNW
}
#endif

#if 0 // _ENTER_ARM_SUND
{
#endif

INLINE(Vdyu, BOOL_SUNDA)   (void *dst, Vdyu src)
{
    vst1_u8(dst, VDYU_ASBU(src));
    return  src;
}

INLINE(Vdbu, UCHAR_SUNDA) (void *dst, Vdbu src)
{
    vst1_u8(dst, src);
    return  src;
}

INLINE(Vdbi, SCHAR_SUNDA) (void *dst, Vdbi src)
{
    vst1_s8(dst, src);
    return  src;
}

INLINE(Vdbc,  CHAR_SUNDA) (void *dst, Vdbc src)
{
    vst1_u8(dst, VDBC_ASBU(src));
    return  src;
}


INLINE(Vdhu, USHRT_SUNDA) (void *dst, Vdhu src)
{
    vst1_u8(dst, vreinterpret_u8_u16(src));
    return  src;
}

INLINE(Vdhi,  SHRT_SUNDA) (void *dst, Vdhi src)
{
    vst1_u8(dst, vreinterpret_u8_s16(src));
    return  src;
}



INLINE(Vdwu,  UINT_SUNDA) (void *dst, Vdwu src)
{
    vst1_u8(dst, vreinterpret_u8_u32(src));
    return  src;
}

INLINE(Vdwi,   INT_SUNDA) (void *dst, Vdwi src)
{
    vst1_u8(dst, vreinterpret_u8_s32(src));
    return  src;
}

#if DWRD_NLONG == 2

INLINE(Vdwu, ULONG_SUNDA) (void *dst, Vdwu src)
{
    vst1_u8(dst, vreinterpret_u8_u32(src));
    return  src;
}

INLINE(Vdwi,  LONG_SUNDA) (void *dst, Vdwi src)
{
    vst1_u8(dst, vreinterpret_u8_s32(src));
    return  src;
}

#else

INLINE(Vddu, ULONG_SUNDA) (void *dst, Vddu src)
{
    vst1_u8(dst, vreinterpret_u8_u64(src));
    return  src;
}

INLINE(Vddi,  LONG_SUNDA) (void *dst, Vddi src)
{
    vst1_u8(dst, vreinterpret_u8_s64(src));
    return  src;
}

#endif


#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_SUNDA) (void *dst, Vddu src)
{
    vst1_u8(dst, vreinterpret_u8_u64(src));
    return  src;
}

INLINE(Vddi, LLONG_SUNDA) (void *dst, Vddi src)
{
    vst1_u8(dst, vreinterpret_u8_s64(src));
    return  src;
}

#endif

INLINE(Vdhf, FLT16_SUNDA) (void *dst, Vdhf src)
{
    vst1_u8(dst, vreinterpret_u8_f16(src));
    return  src;
}

INLINE(Vdwf,   FLT_SUNDA) (void *dst, Vdwf src)
{
    vst1_u8(dst, vreinterpret_u8_f32(src));
    return  src;
}

INLINE(Vddf,   DBL_SUNDA) (void *dst, Vddf src)
{
    vst1_u8(dst, vreinterpret_u8_f64(src));
    return  src;
}

#if 0 // _LEAVE_ARM_SUND
}
#endif

#if 0 // _ENTER_ARM_SUNQ
{
#endif

INLINE(Vqyu, BOOL_SUNQA)   (void *dst, Vqyu src)
{
    vst1q_u8(dst, VQYU_ASBU(src));
    return  src;
}

INLINE(Vqbu, UCHAR_SUNQA) (void *dst, Vqbu src)
{
    vst1q_u8(dst, src);
    return  src;
}

INLINE(Vqbi, SCHAR_SUNQA) (void *dst, Vqbi src)
{
    vst1q_s8(dst, src);
    return  src;
}

INLINE(Vqbc,  CHAR_SUNQA) (void *dst, Vqbc src)
{
    vst1q_u8(dst, VQBC_ASBU(src));
    return  src;
}


INLINE(Vqhu, USHRT_SUNQA) (void *dst, Vqhu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u16(src));
    return  src;
}

INLINE(Vqhi,  SHRT_SUNQA) (void *dst, Vqhi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s16(src));
    return  src;
}



INLINE(Vqwu,  UINT_SUNQA) (void *dst, Vqwu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u32(src));
    return  src;
}

INLINE(Vqwi,   INT_SUNQA) (void *dst, Vqwi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s32(src));
    return  src;
}

#if DWRD_NLONG == 2

INLINE(Vqwu, ULONG_SUNQA) (void *dst, Vqwu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u32(src));
    return  src;
}

INLINE(Vqwi,  LONG_SUNQA) (void *dst, Vqwi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s32(src));
    return  src;
}

#else

INLINE(Vqdu, ULONG_SUNQA) (void *dst, Vqdu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u64(src));
    return  src;
}

INLINE(Vqdi,  LONG_SUNQA) (void *dst, Vqdi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s64(src));
    return  src;
}

#endif


#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_SUNQA) (void *dst, Vqdu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u64(src));
    return  src;
}

INLINE(Vqdi, LLONG_SUNQA) (void *dst, Vqdi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s64(src));
    return  src;
}

INLINE(Vqqu,sunqaqu) (void *dst, Vqqu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u64(src.V0));
    return  src;
}

INLINE(Vqqi,sunqaqi) (void *dst, Vqqi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s64(src.V0));
    return  src;
}


#endif

INLINE(Vqhf, FLT16_SUNQA) (void *dst, Vqhf src)
{
    vst1q_u8(dst, vreinterpretq_u8_f16(src));
    return  src;
}

INLINE(Vqwf,   FLT_SUNQA) (void *dst, Vqwf src)
{
    vst1q_u8(dst, vreinterpretq_u8_f32(src));
    return  src;
}

INLINE(Vqdf,   DBL_SUNQA) (void *dst, Vqdf src)
{
    vst1q_u8(dst, vreinterpretq_u8_f64(src));
    return  src;
}

INLINE(Vqqf,sunqaqf) (void *dst, Vqqf src)
{
    vst1q_u8(dst, VQQF_ASBU(src));
    return  src;
}

#if 0 // _LEAVE_ARM_SUNQ
}
#endif


#if 0 // _ENTER_ARM_STRW
{
#endif

INLINE(Vwyu, BOOL_STRWA)   (void *d,    Vwyu s) {return (*(Vwyu *) d=s);}
INLINE(Vwbu,UCHAR_STRWA)   (uchar d[4], Vwbu s) {return (*(Vwbu *) d=s);}
INLINE(Vwbi,SCHAR_STRWA)   (schar d[4], Vwbi s) {return (*(Vwbi *) d=s);}
INLINE(Vwbc, CHAR_STRWA)    (char d[4], Vwbc s) {return (*(Vwbc *) d=s);}
INLINE(Vwhu,USHRT_STRWA)  (ushort d[2], Vwhu s) {return (*(Vwhu *) d=s);}
INLINE(Vwhi, SHRT_STRWA)   (short d[2], Vwhi s) {return (*(Vwhi *) d=s);}
INLINE(Vwwu, UINT_STRWA)    (uint d[1], Vwwu s) {return (*(Vwwu *) d=s);}
INLINE(Vwwi,  INT_STRWA)     (int d[1], Vwwi s) {return (*(Vwwi *) d=s);}

#if DWRD_NLONG == 2
INLINE(Vwwu,ULONG_STRWA)   (ulong d[1], Vwwu s) {return (*(Vwwu *) d=s);}
INLINE(Vwwi, LONG_STRWA)    (long d[1], Vwwi s) {return (*(Vwwi *) d=s);}
#endif

INLINE(Vwhf,FLT16_STRWA) (flt16_t d[2], Vwhf s) {return (*(Vwhf *) d=s);}
INLINE(Vwwf,  FLT_STRWA)   (float d[1], Vwwf s) {return (*(Vwwf *) d=s);}

#if 0 // _LEAVE_ARM_STRW
}
#endif

#if 0 // _ENTER_ARM_STRD
{
#endif

INLINE(Vdyu, BOOL_STRDA)   (void *dst, Vdyu src)
{
    vst1_u8(dst, VDYU_ASBU(src));
    return  src;
}

INLINE(Vdbu, UCHAR_STRDA)   (uchar dst[8], Vdbu src)
{
    vst1_u8(dst, src);
    return  src;
}

INLINE(Vdbi, SCHAR_STRDA)   (schar dst[8], Vdbi src)
{
    vst1_s8(dst, src);
    return  src;
}

INLINE(Vdbc,  CHAR_STRDA)    (char dst[8], Vdbc src)
{
    vst1_u8(((void *) dst), VDBC_ASBU(src));
    return  src;
}


INLINE(Vdhu, USHRT_STRDA)  (ushort dst[4], Vdhu src)
{
    vst1_u16(dst, src);
    return  src;
}

INLINE(Vdhi,  SHRT_STRDA)   (short dst[4], Vdhi src)
{
    vst1_s16(dst, src);
    return  src;
}



INLINE(Vdwu,  UINT_STRDA)    (uint dst[2], Vdwu src)
{
    vst1_u32(dst, src);
    return  src;
}

INLINE(Vdwi,   INT_STRDA)     (int dst[2], Vdwi src)
{
    vst1_s32(dst, src);
    return  src;
}

#if DWRD_NLONG == 2

INLINE(Vdwu, ULONG_STRDA)   (ulong dst[2], Vdwu src)
{
    vst1_u32(((uint32_t *) dst), src);
    return  src;
}

INLINE(Vdwi,  LONG_STRDA)    (long dst[2], Vdwi src)
{
    vst1_s32(((int32_t *) dst), src);
    return  src;
}

#else

INLINE(Vddu, ULONG_STRDA)   (ulong dst[1], Vddu src)
{
    vst1_u64(((uint64_t *) dst), src);
    return  src;
}

INLINE(Vddi,  LONG_STRDA)    (long dst[1], Vddi src)
{
    vst1_s64(((int64_t *) dst), src);
    return  src;
}

#endif


#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_STRDA)  (ullong dst[1], Vddu src)
{
    vst1_u64(((uint64_t *) dst), src);
    return  src;
}

INLINE(Vddi, LLONG_STRDA)   (llong dst[1], Vddi src)
{
    vst1_s64(((int64_t *) dst), src);
    return  src;
}

#endif

INLINE(Vdhf, FLT16_STRDA) (flt16_t dst[4], Vdhf src)
{
    vst1_f16(dst, src);
    return  src;
}

INLINE(Vdwf,   FLT_STRDA)   (float dst[2], Vdwf src)
{
    vst1_f32(dst, src);
    return  src;
}

INLINE(Vddf,   DBL_STRDA)  (double dst[1], Vddf src)
{
    vst1_f64(dst, src);
    return  src;
}

#if 0 // _LEAVE_ARM_STRD
}
#endif

#if 0 // _ENTER_ARM_STRQ
{
#endif

INLINE(Vqyu, BOOL_STRQA)   (void *dst, Vqyu src)
{
    vst1q_u8(dst, VQYU_ASBU(src));
    return  src;
}

INLINE(Vqbu, UCHAR_STRQA)   (uchar dst[16], Vqbu src)
{
    vst1q_u8(dst, src);
    return  src;
}

INLINE(Vqbi, SCHAR_STRQA)   (schar dst[16], Vqbi src)
{
    vst1q_s8(dst, src);
    return  src;
}

INLINE(Vqbc,  CHAR_STRQA)    (char dst[16], Vqbc src)
{
    vst1q_u8(((void *) dst), VQBC_ASBU(src));
    return  src;
}


INLINE(Vqhu, USHRT_STRQA)  (ushort dst[8], Vqhu src)
{
    vst1q_u16(dst, src);
    return  src;
}

INLINE(Vqhi,  SHRT_STRQA)   (short dst[8], Vqhi src)
{
    vst1q_s16(dst, src);
    return  src;
}



INLINE(Vqwu,  UINT_STRQA)    (uint dst[4], Vqwu src)
{
    vst1q_u32(dst, src);
    return  src;
}

INLINE(Vqwi,   INT_STRQA)     (int dst[4], Vqwi src)
{
    vst1q_s32(dst, src);
    return  src;
}

#if DWRD_NLONG == 2

INLINE(Vqwu, ULONG_STRQA)   (ulong dst[4], Vqwu src)
{
    vst1q_u32(((uint32_t *) dst), src);
    return  src;
}

INLINE(Vqwi,  LONG_STRQA)    (long dst[4], Vqwi src)
{
    vst1q_s32(((int32_t *) dst), src);
    return  src;
}

#else

INLINE(Vqdu, ULONG_STRQA)   (ulong dst[2], Vqdu src)
{
    vst1q_u64(((uint64_t *) dst), src);
    return  src;
}

INLINE(Vqdi,  LONG_STRQA)    (long dst[2], Vqdi src)
{
    vst1q_s64(((int64_t *) dst), src);
    return  src;
}

#endif


#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_STRQA)  (ullong dst[2], Vqdu src)
{
    vst1q_u64(( (uint64_t *) dst), src);
    return  src;
}

INLINE(Vqdi, LLONG_STRQA)   (llong dst[2], Vqdi src)
{
    vst1q_s64( ((int64_t *) dst), src);
    return  src;
}

INLINE(Vqqu,strqaqu) (QUAD_UTYPE dst[1], Vqqu src)
{
    vst1q_u64( ((uint64_t *) dst), src.V0);
    return src;
}

INLINE(Vqqi,strqaqi) (QUAD_ITYPE dst[1], Vqqi src)
{
    vst1q_s64( ((int64_t *) dst), src.V0);
    return src;
}

#endif

INLINE(Vqhf, FLT16_STRQA) (flt16_t dst[8], Vqhf src)
{
    vst1q_f16(dst, src);
    return  src;
}

INLINE(Vqwf,   FLT_STRQA)   (float dst[4], Vqwf src)
{
    vst1q_f32(dst, src);
    return  src;
}

INLINE(Vqdf,   DBL_STRQA)  (double dst[2], Vqdf src)
{
    vst1q_f64(dst, src);
    return  src;
}
INLINE(Vqqf, strqaqf) (QUAD_FTYPE dst[1], Vqqf src)
{
    dst[0] = src.V0;
    return  src;
}

#if 0 // _LEAVE_ARM_STRQ
}
#endif

#if 0 // _ENTER_ARM_NEWW
{
#endif

#define WBZ_NEWL(K0, K1, K2, K3)        \
(                                       \
    (                                   \
        (WORD_TYPE)                     \
        {                               \
            .U=(                        \
                    ((0xffu&K0))        \
                |   ((0xffu&K1)<<8)     \
                |   ((0xffu&K2)<<16)    \
                |   ((0xffu&K3)<<24)    \
            )                           \
        }                               \
    ).F                                 \
)

#define WHZ_NEWL(K0, K1)                \
(                                       \
    (                                   \
        (WORD_TYPE)                     \
        {                               \
            .U=(                        \
                    ((0xffffu&K0))      \
                |   ((0xffffu&K1)<<16)  \
            )                           \
        }                               \
    ).F                                 \
)

#define     WBU_NEWL(K0, K1, K2, K3) WBZ_NEWL(K0,K1,K2,K3)
#define     WBI_NEWL(K0, K1, K2, K3) WBZ_NEWL(K0,K1,K2,K3)
#define     WBC_NEWL(K0, K1, K2, K3) WBZ_NEWL(K0,K1,K2,K3)

#define     WHU_NEWL(...)   WHZ_NEWL(__VA_ARGS__)
#define     WHI_NEWL(...)   WHZ_NEWL(__VA_ARGS__)
#define     WHF_NEWL(K0, K1)  \
(((WORD_TYPE){.H0.F=K0,.H1.F=K1,}).F)

#define     WWU_NEWL(K0)    (((WORD_TYPE){.U=K0}).F)
#define     WWI_NEWL(K0)    (((WORD_TYPE){.I=K0}).F)
#define     WWF_NEWL(K0)    (((WORD_TYPE){.F=K0}).F)

#define     DBZ_NEWL(T, K0, K1, K2, K3, K4, K5, K6, K7) \
vreinterpret##T(                \
    vdup_n_u64(                 \
        ((0xffull&(K0))<<000)   \
    |   ((0xffull&(K1))<<010)   \
    |   ((0xffull&(K2))<<020)   \
    |   ((0xffull&(K3))<<030)   \
    |   ((0xffull&(K4))<<040)   \
    |   ((0xffull&(K5))<<050)   \
    |   ((0xffull&(K6))<<060)   \
    |   ((0xffull&(K7))<<070)   \
    )\
)

#define     DBU_NEWL(...)   DBZ_NEWL(_u8_u64,__VA_ARGS__)
#define     DBI_NEWL(...)   DBZ_NEWL(_s8_u64,__VA_ARGS__)
#if CHAR_MIN
#   define  DBC_NEWL(...)   DBI_NEWL(__VA_ARGS__)
#else
#   define  DBC_NEWL(...)   DBU_NEWL(__VA_ARGS__)
#endif

#define     DHZ_NEWL(T, K0, K1, K2, K3) \
vreinterpret_##T(               \
    vdup_n_u64(                 \
        ((0xffffull&K0)<<000)   \
    |   ((0xffffull&K1)<<020)   \
    |   ((0xffffull&K2)<<040)   \
    |   ((0xffffull&K3)<<060)   \
    )                           \
)


#define     DHU_NEWL(...)   DHZ_NEWL(u16_u64,__VA_ARGS__)
#define     DHI_NEWL(...)   DHZ_NEWL(s16_u64,__VA_ARGS__)
#define     DHF_NEWL(K0, K1, K2, K3)    \
VDDF_ASHF(                          \
    vdup_n_f64(                     \
        (                           \
            (DWRD_TYPE)             \
            {                       \
                .H0.F=K0, .H1.F=K1, \
                .H2.F=K2, .H3.F=K3, \
            }                       \
        ).F                         \
    )                               \
)


#define     DWZ_NEWL(T, K0, K1)     \
vreinterpret_##T(                   \
    vdup_n_u64(                     \
        ((0xffffffffull&K0)<<000)   \
    |   ((0xffffffffull&K1)<<040)   \
    )                               \
)

#define     DWU_NEWL(...)   DWZ_NEWL(u32_u64,__VA_ARGS__)
#define     DWI_NEWL(...)   DWZ_NEWL(s32_u64,__VA_ARGS__)
#define     DWF_NEWL(K0, K1)        \
VDDF_ASWF(                          \
    vdup_n_f64(                     \
        (                           \
            (DWRD_TYPE)             \
            {                       \
                .W0.F=K0, .W1.F=K1, \
            }                       \
        ).F                         \
    )                               \
)

#define     DDU_NEWL        vdup_n_u64
#define     DDI_NEWL        vdup_n_s64
#define     DDF_NEWL        vdup_n_f64


#define     QBZ_NEWL(T,             \
    K0, K1, K2, K3, K4, K5, K6, K7, \
    K8, K9, K10,K11,K12,K13,K14,K15 \
)                                   \
vreinterpretq_##T(                  \
    vcombine_u64(                   \
        vdup_n_u64(                 \
            ((0xffull&(K0 ))<<000)  \
        |   ((0xffull&(K1 ))<<010)  \
        |   ((0xffull&(K2 ))<<020)  \
        |   ((0xffull&(K3 ))<<030)  \
        |   ((0xffull&(K4 ))<<040)  \
        |   ((0xffull&(K5 ))<<050)  \
        |   ((0xffull&(K6 ))<<060)  \
        |   ((0xffull&(K7 ))<<070)  \
        ),                          \
        vdup_n_u64(                 \
            ((0xffull&(K8 ))<<000)  \
        |   ((0xffull&(K9 ))<<010)  \
        |   ((0xffull&(K10))<<020)  \
        |   ((0xffull&(K11))<<030)  \
        |   ((0xffull&(K12))<<040)  \
        |   ((0xffull&(K13))<<050)  \
        |   ((0xffull&(K14))<<060)  \
        |   ((0xffull&(K15))<<070)  \
        )                           \
    )                               \
)
#define     QBU_NEWL(...)       QBZ_NEWL(u8_u64,__VA_ARGS__)
#define     QBI_NEWL(...)       QBZ_NEWL(s8_u64,__VA_ARGS__)
#if CHAR_MIN
#   define  QBC_NEWL(...)   QBI_NEWL(__VA_ARGS__)
#else
#   define  QBC_NEWL(...)   QBU_NEWL(__VA_ARGS__)
#endif

#define     QHZ_NEWL(T, K0, K1, K2, K3, K4, K5, K6, K7) \
vreinterpretq_##T(                  \
    vcombine_u64(                   \
        vdup_n_u64(                 \
            ((0xffffull&(K0))<<000) \
        |   ((0xffffull&(K1))<<020) \
        |   ((0xffffull&(K2))<<040) \
        |   ((0xffffull&(K3))<<060) \
        ),                          \
        vdup_n_u64(                 \
            ((0xffffull&(K4))<<000) \
        |   ((0xffffull&(K5))<<020) \
        |   ((0xffffull&(K6))<<040) \
        |   ((0xffffull&(K7))<<060) \
        )                           \
    )                               \
)
#define     QHU_NEWL(...)       QHZ_NEWL(u16_u64,__VA_ARGS__)
#define     QHI_NEWL(...)       QHZ_NEWL(s16_u64,__VA_ARGS__)
#define     QHF_NEWL(K0, K1, K2, K3, K4, K5, K6, K7)    \
VQDF_ASHF(                              \
    vcombine_f64(                       \
        vdup_n_f64(                     \
            (                           \
                (DWRD_TYPE)             \
                {                       \
                    .H0.F=K0, .H1.F=K1, \
                    .H2.F=K2, .H3.F=K3, \
                }                       \
            ).F                         \
        ),                              \
        vdup_n_f64(                     \
            (                           \
                (DWRD_TYPE)             \
                {                       \
                    .H0.F=K4, .H1.F=K5, \
                    .H2.F=K6, .H3.F=K7, \
                }                       \
            ).F                         \
        )                               \
    )                                   \
)

#define     QWZ_NEWL(T, K0, K1, K2, K3) \
vreinterpretq_##T(                      \
    vcombine_u64(                       \
        vdup_n_u64(                     \
            ((0xffffffffull&(K0))<<000) \
        |   ((0xffffffffull&(K1))<<040) \
        ),                              \
        vdup_n_u64(                     \
            ((0xffffffffull&(K2))<<000) \
        |   ((0xffffffffull&(K3))<<040) \
        )                               \
    )                                   \
)

#define     QWU_NEWL(...)       QWZ_NEWL(u32_u64,__VA_ARGS__)
#define     QWI_NEWL(...)       QWZ_NEWL(s32_u64,__VA_ARGS__)
#define     QWF_NEWL(K0, K1, K2, K3)    \
VQDF_ASWF(                              \
    vcombine_f64(                       \
        vdup_n_f64(                     \
            (                           \
                (DWRD_TYPE)             \
                {                       \
                    .W0.F=K0, .W1.F=K1, \
                }                       \
            ).F                         \
        ),                              \
        vdup_n_f64(                     \
            (                           \
                (DWRD_TYPE)             \
                {                       \
                    .W0.F=K2, .W1.F=K3, \
                }                       \
            ).F                         \
        )                               \
    )                                   \
)

#define     QDU_NEWL(K0, K1)    vcombine_u64(vdup_n_u64(K0),vdup_n_u64(K1))
#define     QDI_NEWL(K0, K1)    vcombine_s64(vdup_n_s64(K0),vdup_n_s64(K1))
#define     QDF_NEWL(K0, K1)    vcombine_f64(vdup_n_f64(K0),vdup_n_f64(K1))

#define     MY_NEWG2(G, K, V, T, A, K0, K1) \
(\
    (G)\
    {\
        .K##0.V=((T const *) A)[K0], \
        .K##1.V=((T const *) A)[K1], \
    }\
)

#define MY_NEWG4(G, K, V, T, A, K0, K1, K2, K3) \
(\
    (G)\
    {\
        .K##0.V=((T const *) A)[K0], \
        .K##1.V=((T const *) A)[K1], \
        .K##2.V=((T const *) A)[K2], \
        .K##3.V=((T const *) A)[K3], \
    }\
)

#define MY_NEWG8(G,K,V,T,A,K0,K1,K2,K3,K4,K5,K6,K7) \
(\
    (G)\
    {\
        .K##0.V=((T const *) A)[K0], \
        .K##1.V=((T const *) A)[K1], \
        .K##2.V=((T const *) A)[K2], \
        .K##3.V=((T const *) A)[K3], \
        .K##4.V=((T const *) A)[K4], \
        .K##5.V=((T const *) A)[K5], \
        .K##6.V=((T const *) A)[K6], \
        .K##7.V=((T const *) A)[K7], \
    }\
)

#define MY_NEWG16(\
    G,K,V,T,A,\
    K0,K1,K2,K3,K4,K5,K6,K7,\
    K8,K9,K10,K11,K12,K13,K14,K15\
) \
(\
    (G)\
    {\
        .K## 0.V=((T const *) A)[K0], \
        .K## 1.V=((T const *) A)[K1], \
        .K## 2.V=((T const *) A)[K2], \
        .K## 3.V=((T const *) A)[K3], \
        .K## 4.V=((T const *) A)[K4], \
        .K## 5.V=((T const *) A)[K5], \
        .K## 6.V=((T const *) A)[K6], \
        .K## 7.V=((T const *) A)[K7], \
        .K## 7.V=((T const *) A)[K7], \
        .K## 8.V=((T const *) A)[K8], \
        .K## 9.V=((T const *) A)[K9], \
        .K##10.V=((T const *) A)[K10],\
        .K##11.V=((T const *) A)[K11],\
        .K##12.V=((T const *) A)[K12],\
        .K##13.V=((T const *) A)[K13],\
        .K##14.V=((T const *) A)[K14],\
        .K##15.V=((T const *) A)[K15],\
    }\
)

INLINE(Vwbu,UCHAR_NEWW) (uint  k0, uint  k1, uint k2, uint k3)
{
#define     UCHAR_NEWW(K0, K1, K2, K3) \
((Vwbu){((WORD_TYPE){.B0.U=K0,.B1.U=K1,.B2.U=K2,.B3.U=K3}).F})
    
    return  UCHAR_NEWW(k0, k1, k2, k3);
}

INLINE(Vwbi,SCHAR_NEWW) (int  k0, int  k1, int k2, int k3)
{
#define     SCHAR_NEWW(K0, K1, K2, K3) \
((Vwbi){((WORD_TYPE){.B0.I=K0,.B1.I=K1,.B2.I=K2,.B3.I=K3}).F})
    
    return  SCHAR_NEWW(k0, k1, k2, k3);
}

INLINE(Vwbc, CHAR_NEWW) (int  k0, int  k1, int k2, int k3)
{
#define     CHAR_NEWW(K0, K1, K2, K3) \
((Vwbc){((WORD_TYPE){.B0.C=K0,.B1.C=K1,.B2.C=K2,.B3.C=K3}).F})
    
    return  CHAR_NEWW(k0, k1, k2, k3);
}


INLINE(Vwhu,USHRT_NEWW) (uint  k0, uint  k1)
{
#define     USHRT_NEWW(K0, K1) ((Vwhu){((WORD_TYPE){.H0.U=K0,.H1.U=K1}).F})
    return  USHRT_NEWW(k0, k1);
}

INLINE(Vwhi,SHRT_NEWW) (int  k0, int  k1)
{
#define     SHRT_NEWW(K0, K1) ((Vwhi){((WORD_TYPE){.H0.I=K0,.H1.I=K1}).F})
    return  SHRT_NEWW(k0, k1);
}


INLINE(Vwwu,UINT_NEWW) (uint  k0)
{
#define     UINT_NEWW(K0) ((Vwwu){((WORD_TYPE){.U=K0}).F})
    return  UINT_NEWW(k0);
}

INLINE(Vwwi,INT_NEWW) (int  k0)
{
#define     INT_NEWW(K0) ((Vwwi){((WORD_TYPE){.I=K0}).F})
    return  INT_NEWW(k0);
}

#if DWRD_NLONG == 2

INLINE(Vwwu,ULONG_NEWW) (ulong  k0)
{
#define     ULONG_NEWW(K0) ((Vwwu){((WORD_TYPE){.U=K0}).F})
    return  ULONG_NEWW(k0);
}

INLINE(Vwwi,LONG_NEWW) (long  k0)
{
#define     LONG_NEWW(K0) ((Vwwi){((WORD_TYPE){.I=K0}).F})
    return  LONG_NEWW(k0);
}

#endif

INLINE(Vwhf,FLT16_NEWW) (flt16_t  k0, flt16_t  k1)
{
#define     FLT16_NEWW(K0, K1) ((Vwhf){((WORD_TYPE){.H0.F=K0,.H1.F=K1}).F})
    return  FLT16_NEWW(k0, k1);
}


INLINE(Vwwf,FLT_NEWW) (float k0)
{
#define     FLT_NEWW(K0) ((Vwwf){K0})
    return  FLT_NEWW(k0);
}



INLINE(Vwbu,UCHAR_NEWWAC) 
(
    uchar const *a,
    ptrdiff_t k0, ptrdiff_t k1, ptrdiff_t k2, ptrdiff_t k3
)
{
    return  UCHAR_NEWW(a[k0], a[k1], a[k2], a[k3]);
}

INLINE(Vwbi,SCHAR_NEWWAC) 
(
    schar const *a,
    ptrdiff_t k0, ptrdiff_t k1, ptrdiff_t k2, ptrdiff_t k3
)
{
    return  SCHAR_NEWW(a[k0], a[k1], a[k2], a[k3]);
}

INLINE(Vwbc,CHAR_NEWWAC) 
(
    char const *a,
    ptrdiff_t k0, ptrdiff_t k1, ptrdiff_t k2, ptrdiff_t k3
)
{
    return  CHAR_NEWW(a[k0], a[k1], a[k2], a[k3]);
}


INLINE(Vwhu,USHRT_NEWWAC) 
(
    ushort const *a,
    ptrdiff_t k0, ptrdiff_t k1
)
{
    return  USHRT_NEWW(a[k0], a[k1]);
}

INLINE(Vwhi,SHRT_NEWWAC) 
(
    short const *a,
    ptrdiff_t k0, ptrdiff_t k1
)
{
    return  SHRT_NEWW(a[k0], a[k1]);
}


INLINE(Vwwu,UINT_NEWWAC) (uint const *a, ptrdiff_t k0)
{
    return  UINT_NEWW(a[k0]);
}

INLINE(Vwwi, INT_NEWWAC)  (int const *a, ptrdiff_t k0)
{
    return  INT_NEWW(a[k0]);
}

#if DWRD_NLONG == 2

INLINE(Vwwu,ULONG_NEWWAC) (ulong const *a, ptrdiff_t k0)
{
    return  ULONG_NEWW(a[k0]);
}

INLINE(Vwwi, LONG_NEWWAC)  (long const *a, ptrdiff_t k0)
{
    return  LONG_NEWW(a[k0]);
}

#endif

INLINE(Vwhf,FLT16_NEWWAC) 
(
    flt16_t const *a,
    ptrdiff_t k0, ptrdiff_t k1
)
{
    return  FLT16_NEWW(a[k0], a[k1]);
}

INLINE(Vwwf,FLT_NEWWAC)  (float const *a, ptrdiff_t k0)
{
    return  FLT_NEWW(a[k0]);
}


INLINE(float,WBR_NEWW)
(
    float m,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
    float32x2_t f = {m};
    uint8x8_t   b = vreinterpret_u8_f32(f);
    uint8x8_t   t = vdup_n_u8(-1);
    t[0]=k0; t[1]=k1; t[2]=k2; t[3]=k3;
    t = vtbl1_u8(b, t);
    f = vreinterpret_f32_u8(t);
    return  vget_lane_f32(f, 0);
}

INLINE(Vwbu,VWBU_NEWW)
(
    Vwbu v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
    v.V0 = WBR_NEWW(v.V0, k0, k1, k2, k3);
    return v;
}

INLINE(Vwbi,VWBI_NEWW)
(
    Vwbi v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
    v.V0 = WBR_NEWW(v.V0, k0, k1, k2, k3);
    return v;
}

INLINE(Vwbc,VWBC_NEWW)
(
    Vwbc v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
    v.V0 = WBR_NEWW(v.V0, k0, k1, k2, k3);
    return v;
}


INLINE(float,WHR_NEWW)
(
    float m,
    Rc(-1, +1) k0, Rc(-1, +1) k1
)
{
    float32x2_t f = {m};
    uint8x8_t   b = vreinterpret_u8_f32(f);
    uint8x8_t   t = vdup_n_u8(-1);
    t[0]=k0, t[1]=k0;
    t[2]=k1, t[3]=k1;

    // only keep if 0 <= l[i] <= 1
    uint8x8_t   c = vdup_n_u8(0xfe); // 1111'1110
    uint8x8_t   x = vtst_u8(t, c); 

    // multiply lane by 2 if not -1
    c = vdup_n_u8(1);   // shift amount 
    c = vbic_u8(c, x);  // del -1s
    t = vshl_u8(t, c);
    
    // add 1 to odd lanes if not -1
    c = vbic_u8(vcreate_u8(0x0100010001000100ULL), x);
    t = vadd_u8(t, c);
    t = vtbl1_u8(b, t);
    f = vreinterpret_f32_u8(t);
    return  vget_lane_f32(f, 0);
}

INLINE(Vwhu,VWHU_NEWW)
(
    Vwhu v,
    Rc(-1, +1) k0, Rc(-1, +1) k1
)
{
    v.V0 = WHR_NEWW(v.V0, k0, k1);
    return v;
}

INLINE(Vwhi,VWHI_NEWW)
(
    Vwhi v,
    Rc(-1, +1) k0, Rc(-1, +1) k1
)
{
    v.V0 = WHR_NEWW(v.V0, k0, k1);
    return v;
}

INLINE(Vwhf,VWHF_NEWW)
(
    Vwhf v,
    Rc(-1, +1) k0, Rc(-1, +1) k1
)
{
    v.V0 = WHR_NEWW(v.V0, k0, k1);
    return v;
}


INLINE(float,WWR_NEWW) (float m, Rc(-1, +0) k0)
{
    return (k0==0) ? m : 0.0f;
}

INLINE(Vwwu,VWWU_NEWW) (Vwwu v, Rc(-1, +0) k0)
{
    v.V0 = WWR_NEWW(v.V0, k0);
    return v;
}

INLINE(Vwwi,VWWI_NEWW) (Vwwi v, Rc(-1, +0) k0)
{
    v.V0 = WWR_NEWW(v.V0, k0);
    return v;
}

INLINE(Vwwf,VWWF_NEWW) (Vwwf v, Rc(-1, +0) k0)
{
    v.V0 = WWR_NEWW(v.V0, k0);
    return v;
}



INLINE(float,DBR_NEWW)
(
    uint8x8_t b,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3
)
{
    uint8x8_t   t = vdup_n_u8(-1);
    t[0]=k0; t[1]=k1; t[2]=k2; t[3]=k3;
    b = vtbl1_u8(b, t);
    float32x2_t f = vreinterpret_f32_u8(t);
    return  vget_lane_f32(f, 0);
}

INLINE(Vwbu,VDBU_NEWW)
(
    Vdbu v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3
)
{
    return  ((Vwbu){DBR_NEWW(v, k0, k1, k2, k3)});
}

INLINE(Vwbi,VDBI_NEWW)
(
    Vdbi v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3
)
{
    uint8x8_t   m = VDBI_ASBU(v);
    return  ((Vwbi){DBR_NEWW(m, k0, k1, k2, k3)});
}

INLINE(Vwbc,VDBC_NEWW)
(
    Vdbc v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3
)
{
    uint8x8_t   m = VDBC_ASBU(v);
    return  ((Vwbc){DBR_NEWW(m, k0, k1, k2, k3)});
}


INLINE(float,DHR_NEWW)
(
    uint8x8_t b,
    Rc(-1, +3) k0, Rc(-1, +3) k1
)
{
    uint8x8_t   t = vdup_n_u8(-1);
    t[0]=k0, t[1]=k0;
    t[2]=k1, t[3]=k1;

    // only keep if 0 <= l[i] <= 1
    uint8x8_t   c = vdup_n_u8(0xfc); // 1111'1100
    uint8x8_t   x = vtst_u8(t, c); 

    // multiply lane by 2 if not -1
    c = vdup_n_u8(1);   // shift amount 
    c = vbic_u8(c, x);  // del -1s
    t = vshl_u8(t, c);
    
    // add 1 to odd lanes if not -1
    c = vbic_u8(vcreate_u8(0x0100010001000100ULL), x);
    t = vadd_u8(t, c);
    b = vtbl1_u8(b, t);
    float32x2_t f = vreinterpret_f32_u8(b);
    return  vget_lane_f32(f, 0);
}

INLINE(Vwhu,VDHU_NEWW)
(
    Vdhu v,
    Rc(-1, +3) k0, Rc(-1, +3) k1
)
{
    uint8x8_t   m = VDHU_ASBU(v);
    return  ((Vwhu){DHR_NEWW(m, k0, k1)});
}

INLINE(Vwhi,VDHI_NEWW)
(
    Vdhi v,
    Rc(-1, +3) k0, Rc(-1, +3) k1
)
{
    uint8x8_t   m = VDHI_ASBU(v);
    return  ((Vwhi){DHR_NEWW(m, k0, k1)});
}

INLINE(Vwhf,VDHF_NEWW)
(
    Vdhf v,
    Rc(-1, +3) k0, Rc(-1, +3) k1
)
{
    uint8x8_t   m = VDHF_ASBU(v);
    return  ((Vwhf){DHR_NEWW(m, k0, k1)});
}


INLINE(float,DWR_NEWW) (uint8x8_t b, Rc(-1, +1) k0)
{
    uint8x8_t   t = vdup_n_u8(-1);
    t[0]=k0, t[1]=k0, t[2]=k0, t[3]=k0;

    // only keep if 0 <= l[i] <= 1
    uint8x8_t   c = vdup_n_u8(0xfe); // 1111'1110
    uint8x8_t   x = vtst_u8(t, c); 

    // multiply lane by 4 if not -1
    c = vdup_n_u8(2);   // shift amount 
    c = vbic_u8(c, x);  // del -2s
    t = vshl_u8(t, c);
    
    // add 1 to odd lanes if not -1
    c = vbic_u8(vcreate_u8(0x0302010003020100ULL), x);
    t = vadd_u8(t, c);
    b = vtbl1_u8(b, t);
    float32x2_t f = vreinterpret_f32_u8(b);
    return  vget_lane_f32(f, 0);
}

INLINE(Vwwu,VDWU_NEWW)
(
    Vdwu v,
    Rc(-1, +1) k0
)
{
    uint8x8_t m = VDWU_ASBU(v);
    return  ((Vwwu){DWR_NEWW(m, k0)});
}

INLINE(Vwwi,VDWI_NEWW)
(
    Vdwi v,
    Rc(-1, +1) k0
)
{
    uint8x8_t m = VDWI_ASBU(v);
    return  ((Vwwi){DWR_NEWW(m, k0)});
}

INLINE(Vwwf,VDWF_NEWW)
(
    Vdwf v,
    Rc(-1, +1) k0
)
{
    uint8x8_t m = VDWF_ASBU(v);
    return  ((Vwwf){DWR_NEWW(m, k0)});
}


INLINE(float,QBR_NEWW)
(
    float32x4_t m,
    Rc(-1, +15) k0, Rc(-1, +15) k1, Rc(-1, +15) k2, Rc(-1, +15) k3
)
{
    uint8x8_t   t = vdup_n_u8(-1);
    t[0]=k0; t[1]=k1; t[2]=k2; t[3]=k3;
    uint8x16_t  b = vreinterpretq_u8_f32(m);
    t = vqtbl1_u8(b, t);
    float32x2_t f = vreinterpret_f32_u8(t);
    return  vget_lane_f32(f, 0);
}

INLINE(Vwbu,VQBU_NEWW)
(
    Vqbu v,
    Rc(-1, +15) k0, Rc(-1, +15) k1, Rc(-1, +15) k2, Rc(-1, +15) k3
)
{
    return  ((Vwbu){QBR_NEWW(v, k0, k1, k2, k3)});
}

INLINE(Vwbi,VQBI_NEWW)
(
    Vqbi v,
    Rc(-1, +15) k0, Rc(-1, +15) k1, Rc(-1, +15) k2, Rc(-1, +15) k3
)
{
    uint8x16_t m = VQBI_ASBU(v);
    return  ((Vwbi){QBR_NEWW(m, k0, k1, k2, k3)});
}

INLINE(Vwbc,VQBC_NEWW)
(
    Vqbc v,
    Rc(-1, +15) k0, Rc(-1, +15) k1, Rc(-1, +15) k2, Rc(-1, +15) k3
)
{
    uint8x16_t m = VQBC_ASBU(v);
    return  ((Vwbc){QBR_NEWW(m, k0, k1, k2, k3)});
}


INLINE(float,QHR_NEWW)
(
    float32x4_t m,
    Rc(-1, +7) k0, Rc(-1, +7) k1
)
{
    uint8x8_t   t = vdup_n_u8(-1);
    t[0]=k0, t[1]=k0;
    t[2]=k1, t[3]=k1;

    // only keep if 0 <= l[i] <= 1
    uint8x8_t   c = vdup_n_u8(0xf8); // 1111'1000
    uint8x8_t   x = vtst_u8(t, c); 

    // multiply lane by 2 if not -1
    c = vdup_n_u8(1);   // shift amount 
    c = vbic_u8(c, x);  // del -1s
    t = vshl_u8(t, c);
    
    // add 1 to odd lanes if not -1
    c = vbic_u8(vcreate_u8(0x0100010001000100ULL), x);
    t = vadd_u8(t, c);
    uint8x16_t b = vreinterpretq_u8_f32(m);
    t = vqtbl1_u8(b, t);
    float32x2_t f = vreinterpret_f32_u8(t);
    return  vget_lane_f32(f, 0);
}

INLINE(Vwhu,VQHU_NEWW)
(
    Vqhu v,
    Rc(-1, +7) k0, Rc(-1, +7) k1
)
{
    uint8x16_t m = VQHU_ASBU(v);
    return  ((Vwhu){QHR_NEWW(m, k0, k1)});
}

INLINE(Vwhi,VQHI_NEWW)
(
    Vqhi v,
    Rc(-1, +7) k0, Rc(-1, +7) k1
)
{
    uint8x16_t m = VQHI_ASBU(v);
    return  ((Vwhi){QHR_NEWW(m, k0, k1)});
}

INLINE(Vwhf,VQHF_NEWW)
(
    Vqhf v, 
    Rc(-1, +7) k0, Rc(-1, +7) k1
)
{
    uint8x16_t m = VQHF_ASBU(v);
    return  ((Vwhf){QHR_NEWW(m, k0, k1)});
}


INLINE(float,QWR_NEWW)
(
    float32x4_t m,
    Rc(-1, +3) k0
)
{
    uint8x8_t   t = vdup_n_u8(-1);
    t[0]=k0, t[1]=k0, t[2]=k0, t[3]=k0;

    // only keep if 0 <= l[i] <= 3
    uint8x8_t   c = vdup_n_u8(0xfc); // 1111'1100
    uint8x8_t   x = vtst_u8(t, c); 

    // multiply lane by 4 if not -1
    c = vdup_n_u8(2);   // shift amount 
    c = vbic_u8(c, x);  // del -1s
    t = vshl_u8(t, c);
    
    // add 1 to odd lanes if not -1
    c = vbic_u8(vcreate_u8(0x0302010003020100ULL), x);
    t = vadd_u8(t, c);
    uint8x16_t b = vreinterpretq_u8_f32(m);
    t = vqtbl1_u8(b, t);
    float32x2_t f = vreinterpret_f32_u8(t);
    return  vget_lane_f32(f, 0);
}

INLINE(Vwwu,VQWU_NEWW) (Vqwu v, Rc(-1, +3) k0)
{
    uint8x16_t m = VQWU_ASBU(v);
    return  ((Vwwu){QWR_NEWW(m, k0)});
}

INLINE(Vwwi,VQWI_NEWW) (Vqwi v, Rc(-1, +3) k0)
{
    uint8x16_t m = VQWI_ASBU(v);
    return  ((Vwwi){QWR_NEWW(m, k0)});
}

INLINE(Vwwf,VQWF_NEWW) (Vqwf v, Rc(-1, +3) k0)
{
    uint8x16_t m = VQWF_ASBU(v);
    return  ((Vwwf){QWR_NEWW(m, k0)});
}

#if 0 // _LEAVE_ARM_NEWW
}
#endif

#if 0 // _ENTER_ARM_NEWD
{
#endif

INLINE(Vdbu,UCHAR_NEWD) 
(
    unsigned  k0, unsigned  k1, unsigned k2, unsigned k3,
    unsigned  k4, unsigned  k5, unsigned k6, unsigned k7
)
{
#define     UCHAR_NEWD(K0, K1, K2, K3, K4, K5, K6, K7) \
vcreate_u8(         \
    (               \
        (DWRD_TYPE) \
        {           \
            .B0.U=K0,.B1.U=K1,.B2.U=K2,.B3.U=K3,\
            .B4.U=K4,.B5.U=K5,.B6.U=K6,.B7.U=K7,\
        }           \
    ).U             \
)

    return  UCHAR_NEWD(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vdbi,SCHAR_NEWD) 
(
    signed  k0, signed  k1, signed k2, signed k3,
    signed  k4, signed  k5, signed k6, signed k7
)
{
#define     SCHAR_NEWD(K0, K1, K2, K3, K4, K5, K6, K7) \
vcreate_s8(         \
    (               \
        (DWRD_TYPE) \
        {           \
            .B0.I=K0,.B1.I=K1,.B2.I=K2,.B3.I=K3,\
            .B4.I=K4,.B5.I=K5,.B6.I=K6,.B7.I=K7,\
        }           \
    ).U             \
)

    return  SCHAR_NEWD(k0, k1, k2, k3,  k4, k5, k6, k7);
}

INLINE(Vdbc,CHAR_NEWD) 
(
    int  k0, int  k1, int k2, int k3,
    int  k4, int  k5, int k6, int k7
)
{
#define     CHAR_NEWD(K0, K1, K2, K3, K4, K5, K6, K7) \
VDBU_ASBC(              \
    vcreate_u8(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .B0.C=K0,.B1.C=K1,.B2.C=K2,.B3.C=K3,\
                .B4.C=K4,.B5.C=K5,.B6.C=K6,.B7.C=K7,\
            }           \
        ).U             \
    )                   \
)

    return  CHAR_NEWD(k0, k1, k2, k3,  k4, k5, k6, k7);
}


INLINE(Vdhu,USHRT_NEWD) 
(
    unsigned  k0, unsigned  k1, unsigned k2, unsigned k3
)
{
#define     USHRT_NEWD(K0, K1, K2, K3) \
vcreate_u16(        \
    (               \
        (DWRD_TYPE) \
        {           \
            .H0.U=K0,.H1.U=K1,.H2.U=K2,.H3.U=K3,\
        }           \
    ).U             \
)

    return  USHRT_NEWD(k0, k1, k2, k3);
}

INLINE(Vdhi, SHRT_NEWD) 
(
    signed  k0, signed  k1, signed k2, signed k3
)
{
#define     SHRT_NEWD(K0, K1, K2, K3) \
vcreate_s16(        \
    (               \
        (DWRD_TYPE) \
        {           \
            .H0.I=K0,.H1.I=K1,.H2.I=K2,.H3.I=K3,\
        }           \
    ).U             \
)

    return  SHRT_NEWD(k0, k1, k2, k3);
}


INLINE(Vdwu,UINT_NEWD) 
(
    uint  k0, uint  k1
)
{
#define     UINT_NEWD(K0, K1) vcreate_u32(((DWRD_TYPE){.W0.U=K0,.W1.U=K1}).U)
    return  UINT_NEWD(k0, k1);
}

INLINE(Vdwi,INT_NEWD) 
(
    int  k0, int  k1
)
{
#define     INT_NEWD(K0, K1) vcreate_s32(((DWRD_TYPE){.W0.I=K0,.W1.I=K1}).U)
    return  INT_NEWD(k0, k1);
}

#if DWRD_NLONG == 2

INLINE(Vdwu,ULONG_NEWD) 
(
    ulong  k0, ulong  k1
)
{
#define     ULONG_NEWD(K0, K1) vcreate_u32(((DWRD_TYPE){.W0.U=K0,.W1.U=K1}).U)
    return  ULONG_NEWD(k0, k1);
}

INLINE(Vdwi, LONG_NEWD) 
(
    long  k0, long  k1
)
{
#define     LONG_NEWD(K0, K1) vcreate_s32(((DWRD_TYPE){.W0.I=K0,.W1.I=K1}).U)
    return  LONG_NEWD(k0, k1);
}

#else

INLINE(Vddu,ULONG_NEWD) 
(
    ulong  k0
)
{
#define     ULONG_NEWD vdup_n_u64
    return  ULONG_NEWD(k0);
}

INLINE(Vddi, LONG_NEWD) 
(
    long  k0
)
{
#define     LONG_NEWD vdup_n_s64
    return  LONG_NEWD(k0);
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_NEWD) 
(
    ullong  k0
)
{
#define     ULLONG_NEWD vdup_n_u64
    return  ULLONG_NEWD(k0);
}

INLINE(Vddi, LLONG_NEWD) 
(
    llong  k0
)
{
#define     LLONG_NEWD  vdup_n_s64
    return  LLONG_NEWD(k0);
}
#endif


INLINE(Vdhf, FLT16_NEWD) 
(
    flt16_t  k0, flt16_t  k1, flt16_t k2, flt16_t k3
)
{
#define     FLT16_NEWD(K0, K1, K2, K3) \
vreinterpret_f16_u16(   \
    vcreate_u16(        \
        (               \
            (DWRD_TYPE) \
            {           \
                .H0.F=K0,.H1.F=K1,.H2.F=K2,.H3.F=K3,\
            }           \
        ).U             \
    )                   \
)
    return  FLT16_NEWD(k0, k1, k2, k3);
}

INLINE(Vdwf,FLT_NEWD) 
(
    float k0, float  k1
)
{
#define     FLT_NEWD(K0, K1) vcreate_f32(((DWRD_TYPE){.W0.F=K0,.W1.F=K1}).U)
    return  FLT_NEWD(k0, k1);
}

INLINE(Vddf,DBL_NEWD) 
(
    double k0
)
{
#define     DBL_NEWD vdup_n_f64
    return  DBL_NEWD(k0);
}

INLINE(Vdbu,UCHAR_NEWDAC) 
(
    uchar const *a,
    ptrdiff_t k0, ptrdiff_t k1, ptrdiff_t k2, ptrdiff_t k3,
    ptrdiff_t k4, ptrdiff_t k5, ptrdiff_t k6, ptrdiff_t k7
)
{
    return  UCHAR_NEWD(a[k0],a[k1],a[k2],a[k3], a[k4],a[k5],a[k6],a[k7]);
}

INLINE(Vdbi,SCHAR_NEWDAC) 
(
    schar const *a,
    ptrdiff_t k0, ptrdiff_t k1, ptrdiff_t k2, ptrdiff_t k3,
    ptrdiff_t k4, ptrdiff_t k5, ptrdiff_t k6, ptrdiff_t k7
)
{
    return  SCHAR_NEWD(a[k0],a[k1],a[k2],a[k3], a[k4],a[k5],a[k6],a[k7]);
}

INLINE(Vdbc,CHAR_NEWDAC) 
(
    char const *a,
    ptrdiff_t k0, ptrdiff_t k1, ptrdiff_t k2, ptrdiff_t k3,
    ptrdiff_t k4, ptrdiff_t k5, ptrdiff_t k6, ptrdiff_t k7
)
{
    return  CHAR_NEWD(a[k0],a[k1],a[k2],a[k3], a[k4],a[k5],a[k6],a[k7]);
}


INLINE(Vdhu,USHRT_NEWDAC) 
(
    ushort const *a,
    ptrdiff_t k0, ptrdiff_t k1, ptrdiff_t k2, ptrdiff_t k3
)
{
    return  USHRT_NEWD(a[k0],a[k1],a[k2],a[k3]);
}

INLINE(Vdhi,SHRT_NEWDAC) 
(
    short const *a,
    ptrdiff_t k0, ptrdiff_t k1, ptrdiff_t k2, ptrdiff_t k3
)
{
    return  SHRT_NEWD(a[k0],a[k1],a[k2],a[k3]);
}


INLINE(Vdwu,UINT_NEWDAC) (uint const *a, ptrdiff_t k0, ptrdiff_t k1)
{
    return  UINT_NEWD(a[k0], a[k1]);
}

INLINE(Vdwi, INT_NEWDAC)  (int const *a, ptrdiff_t k0, ptrdiff_t k1)
{
    return  INT_NEWD(a[k0], a[k1]);
}


#if DWRD_NLONG == 2

INLINE(Vdwu,ULONG_NEWDAC) (ulong const *a, ptrdiff_t k0, ptrdiff_t k1)
{
    return  ULONG_NEWD(a[k0], a[k1]);
}

INLINE(Vdwi, LONG_NEWDAC)  (long const *a, ptrdiff_t k0, ptrdiff_t k1)
{
    return  LONG_NEWD(a[k0], a[k1]);
}

#else

INLINE(Vddu,ULONG_NEWDAC) (ulong const *a, ptrdiff_t k0)
{
    return  ULONG_NEWD(a[k0]);
}

INLINE(Vddi, LONG_NEWDAC)  (long const *a, ptrdiff_t k0)
{
    return  LONG_NEWD(a[k0]);
}

#endif


#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_NEWDAC) (ullong const *a, ptrdiff_t k0)
{
    return  ULLONG_NEWD(a[k0]);
}

INLINE(Vddi, LLONG_NEWDAC)  (llong const *a, ptrdiff_t k0)
{
    return  LLONG_NEWD(a[k0]);
}

#endif


INLINE(Vdhf,FLT16_NEWDAC) 
(
    flt16_t const *a,
    ptrdiff_t k0, ptrdiff_t k1, ptrdiff_t k2, ptrdiff_t k3
)
{
    return  FLT16_NEWD(a[k0], a[k1], a[k2], a[k3]);
}

INLINE(Vdwf,FLT_NEWDAC)  (float const *a, ptrdiff_t k0, ptrdiff_t k1)
{
    return  FLT_NEWD(a[k0], a[k1]);
}

INLINE(Vddf,DBL_NEWDAC)  (double const *a, ptrdiff_t k0)
{
    return  DBL_NEWD(a[k0]);
}


INLINE(uint8x8_t,WBR_NEWD)
(
    float m,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3,
    Rc(-1, +3) k4, Rc(-1, +3) k5, Rc(-1, +3) k6, Rc(-1, +3) k7
)
{
    float32x2_t f = {m};
    uint8x8_t   b = vreinterpret_u8_f32(f);
    uint8x8_t   t = UCHAR_NEWD(k0, k1, k2, k3, k4, k5, k6, k7);
    return  vtbl1_u8(b, t);
}

INLINE(Vdbu,VWBU_NEWD)
(
    Vwbu v,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3,
    Rc(-1,+3) k4, Rc(-1,+3) k5, Rc(-1,+3) k6, Rc(-1,+3) k7
)
{
    return  WBR_NEWD(v.V0, k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vdbi,VWBI_NEWD)
(
    Vwbi v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3,
    Rc(-1, +3) k4, Rc(-1, +3) k5, Rc(-1, +3) k6, Rc(-1, +3) k7
)
{
    uint8x8_t m = WBR_NEWD(v.V0, k0, k1, k2, k3, k4, k5, k6, k7);
    return VDBU_ASBI(m);
}

INLINE(Vdbc,VWBC_NEWD)
(
    Vwbc v,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3,
    Rc(-1,+3) k4, Rc(-1,+3) k5, Rc(-1,+3) k6, Rc(-1,+3) k7
)
{
    uint8x8_t m = WBR_NEWD(v.V0, k0, k1, k2, k3, k4, k5, k6, k7);
    return  VDBU_ASBC(m);
}


INLINE(uint8x8_t,WHR_NEWD)
(
    float m,
    Rc(-1, +1) k0, Rc(-1, +1) k1, Rc(-1, +1) k2, Rc(-1, +1) k3
)
{
    float32x2_t f = {m};
    uint8x8_t   b = vreinterpret_u8_f32(f);
    uint8x8_t   t = UCHAR_NEWD(k0,k0, k1,k1, k2,k2, k3,k3);

    // only keep if 0 <= l[i] <= 1
    uint8x8_t   c = vdup_n_u8(0xfe); // 1111'1110
    uint8x8_t   x = vtst_u8(t, c); 

    // multiply lane by 2 if not -1
    c = vdup_n_u8(1);   // shift amount 
    c = vbic_u8(c, x);  // del -1s
    t = vshl_u8(t, c);
    
    // add 1 to odd lanes if not -1
    c = vbic_u8(vcreate_u8(0x0100010001000100ULL), x);
    t = vadd_u8(t, c);
    return  vtbl1_u8(b, t);
}

INLINE(Vdhu,VWHU_NEWD)
(
    Vwhu v,
    Rc(-1,+1) k0, Rc(-1,+1) k1, Rc(-1,+1) k2, Rc(-1,+1) k3
)
{
    uint8x8_t m = WHR_NEWD(v.V0, k0, k1, k2, k3);
    return  VDBU_ASHU(m);
}

INLINE(Vdhi,VWHI_NEWD)
(
    Vwhi v,
    Rc(-1,+1) k0, Rc(-1,+1) k1, Rc(-1,+1) k2, Rc(-1,+1) k3
)
{
    uint8x8_t m = WHR_NEWD(v.V0, k0, k1, k2, k3);
    return  VDBU_ASHI(m);
}

INLINE(Vdhf,VWHF_NEWD)
(
    Vwhf v,
    Rc(-1, +1) k0, Rc(-1, +1) k1, Rc(-1, +1) k2, Rc(-1, +1) k3
)
{
    uint8x8_t m = WHR_NEWD(v.V0, k0, k1, k2, k3);
    return  VDBU_ASHF(m);
}


INLINE(uint8x8_t,WWR_NEWD) (float m, Rc(-1, +0) k0, Rc(-1, +0) k1)
{
    DWRD_TYPE v = {.W0.F=(k0==0?m:0.0f), .W1.F=(k1==0?m:0.0f)};
    return  vcreate_u8(v.U);
}


INLINE(Vdwu,VWWU_NEWD) (Vwwu v, Rc(-1, +0) k0, Rc(-1, +0) k1)
{
    uint8x8_t m = WWR_NEWD(v.V0, k0, k1);
    return  VDBU_ASWU(m);
}

INLINE(Vdwi,VWWI_NEWD) (Vwwi v, Rc(-1, +0) k0, Rc(-1, +0) k1)
{
    uint8x8_t m = WWR_NEWD(v.V0, k0, k1);
    return  VDBU_ASWI(m);
}

INLINE(Vdwf,VWWF_NEWD) (Vwwf v, Rc(-1, +0) k0, Rc(-1, +0) k1)
{
    uint8x8_t m = WWR_NEWD(v.V0, k0, k1);
    return  VDBU_ASWF(m);
}


INLINE(uint8x8_t,DBR_NEWD)
(
    uint8x8_t b,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
    uint8x8_t   t = UCHAR_NEWD(k0,k1,k2,k3, k4,k5,k6,k7);
    return  vtbl1_u8(b, t);
}

INLINE(Vdbu,VDBU_NEWD)
(
    Vdbu v,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
    uint8x8_t m = DBR_NEWD(v, k0, k1, k2, k3, k4, k5, k6, k7);
    return  m;
}

INLINE(Vdbi,VDBI_NEWD)
(
    Vdbi v,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
    uint8x8_t m = VDBI_ASBU(v);
    m = DBR_NEWD(m, k0, k1, k2, k3, k4, k5, k6, k7);
    return  VDBU_ASBI(m);
}

INLINE(Vdbc,VDBC_NEWD)
(
    Vdbc v,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
    uint8x8_t m = VDBC_ASBU(v);
    m = DBR_NEWD(m, k0, k1, k2, k3, k4, k5, k6, k7);
    return  VDBU_ASBC(m);
}


INLINE(uint8x8_t,DHR_NEWD)
(
    uint8x8_t b,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
    uint8x8_t   t = UCHAR_NEWD(k0,k0, k1,k1, k2,k2, k3,k3);

    // only keep if 0 <= l[i] <= 1
    uint8x8_t   c = vdup_n_u8(0xfc); // 1111'1100
    uint8x8_t   x = vtst_u8(t, c); 

    // multiply lane by 2 if not -1
    c = vdup_n_u8(1);   // shift amount 
    c = vbic_u8(c, x);  // del -1s
    t = vshl_u8(t, c);
    
    // add 1 to odd lanes if not -1
    c = vbic_u8(vcreate_u8(0x0100010001000100ULL), x);
    t = vadd_u8(t, c);
    return  vtbl1_u8(b, t);
}

INLINE(Vdhu,VDHU_NEWD)
(
    Vdhu v,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
    uint8x8_t m = VDHU_ASBU(v);
    m = DHR_NEWD(m, k0, k1, k2, k3);
    return  VDBU_ASHU(m);
}

INLINE(Vdhi,VDHI_NEWD)
(
    Vdhi v,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
    uint8x8_t m = VDHI_ASBU(v);
    m = DHR_NEWD(m, k0, k1, k2, k3);
    return  VDBU_ASHI(m);
}

INLINE(Vdhf,VDHF_NEWD)
(
    Vdhf v,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
    uint8x8_t m = VDHF_ASBU(v);
    m = DHR_NEWD(m, k0, k1, k2, k3);
    return  VDBU_ASHF(m);
}


INLINE(uint8x8_t,DWR_NEWD) (uint8x8_t b, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
    uint8x8_t   t = UCHAR_NEWD(k0,k0,k0,k0,  k1,k1,k1,k1);

    // only keep if 0 <= l[i] <= 1
    uint8x8_t   c = vdup_n_u8(0xfe); // 1111'1110
    uint8x8_t   x = vtst_u8(t, c); 

    // multiply lane by 4 if not -1
    c = vdup_n_u8(2);   // shift amount 
    c = vbic_u8(c, x);  // del -2s
    t = vshl_u8(t, c);
    
    // add 1 to odd lanes if not -1
    c = vbic_u8(vcreate_u8(0x0302010003020100ULL), x);
    t = vadd_u8(t, c);
    return  vtbl1_u8(b, t);
}

INLINE(Vdwu,VDWU_NEWD) (Vdwu v, Rc(-1,+1) k0, Rc(-1,+1) k1)
{
    uint8x8_t m = VDWU_ASBU(v);
    m = DWR_NEWD(m, k0, k1);
    return  VDBU_ASWU(m);
}

INLINE(Vdwi,VDWI_NEWD) (Vdwi v, Rc(-1,+1) k0, Rc(-1,+1) k1)
{
    uint8x8_t m = VDWI_ASBU(v);
    m = DWR_NEWD(m, k0, k1);
    return  VDBU_ASWI(m);
}

INLINE(Vdwf,VDWF_NEWD) (Vdwf v, Rc(-1,+1) k0, Rc(-1,+1) k1)
{
    uint8x8_t m = VDWF_ASBU(v);
    m = DWR_NEWD(m, k0, k1);
    return  VDBU_ASWF(m);
}


INLINE(uint8x8_t,DDR_NEWD) (uint8x8_t b, Rc(-1, +0) k0)
{
    return k0==0 ? b : vcreate_u8(0);
}

INLINE(Vddu,VDDU_NEWD) (Vddu v, Rc(-1,+0) k0)
{
    uint8x8_t m = VDDU_ASBU(v);
    m = DDR_NEWD(m, k0);
    return  VDBU_ASDU(m);
}

INLINE(Vddi,VDDI_NEWD) (Vddi v, Rc(-1,+0) k0)
{
    uint8x8_t m = VDDI_ASBU(v);
    m = DDR_NEWD(m, k0);
    return  VDBU_ASDI(m);
}

INLINE(Vddf,VDDF_NEWD) (Vddf v, Rc(-1,+0) k0)
{
    uint8x8_t m = VDDF_ASBU(v);
    m = DDR_NEWD(m, k0);
    return  VDBU_ASDF(m);
}


INLINE(uint8x8_t,QBR_NEWD)
(
    uint8x16_t b,
    Rc(-1,+15) k0, Rc(-1,+15) k1, Rc(-1,+15) k2, Rc(-1,+15) k3,
    Rc(-1,+15) k4, Rc(-1,+15) k5, Rc(-1,+15) k6, Rc(-1,+15) k7
)
{
    uint8x8_t   t = UCHAR_NEWD(k0,k1,k2,k3,  k4,k5,k6,k7);
    return  vqtbl1_u8(b, t);
}

INLINE(Vdbu,VQBU_NEWD)
(
    Vqbu v,
    Rc(-1,+15) k0, Rc(-1,+15) k1, Rc(-1,+15) k2, Rc(-1,+15) k3,
    Rc(-1,+15) k4, Rc(-1,+15) k5, Rc(-1,+15) k6, Rc(-1,+15) k7
)
{
    return
    QBR_NEWD(v, k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vdbi,VQBI_NEWD)
(
    Vqbi v,
    Rc(-1,+15) k0, Rc(-1,+15) k1, Rc(-1,+15) k2, Rc(-1,+15) k3,
    Rc(-1,+15) k4, Rc(-1,+15) k5, Rc(-1,+15) k6, Rc(-1,+15) k7
)
{
    uint8x16_t  q = VQBI_ASBU(v);
    uint8x8_t   d = QBR_NEWD(q, k0, k1, k2, k3, k4, k5, k6, k7);
    return  VDBU_ASBI(d);
}

INLINE(Vdbc,VQBC_NEWD)
(
    Vqbc v,
    Rc(-1,+15) k0, Rc(-1,+15) k1, Rc(-1,+15) k2, Rc(-1,+15) k3,
    Rc(-1,+15) k4, Rc(-1,+15) k5, Rc(-1,+15) k6, Rc(-1,+15) k7
)
{
    uint8x16_t  q = VQBC_ASBU(v);
    uint8x8_t   d = QBR_NEWD(q, k0, k1, k2, k3, k4, k5, k6, k7);
    return  VDBU_ASBC(d);
}


INLINE(uint8x8_t,QHR_NEWD)
(
    uint8x16_t b,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3
)
{
    uint8x8_t   t = UCHAR_NEWD(k0,k0, k1,k1, k2,k2, k3,k3);

    // only keep if 0 <= l[i] <= 1
    uint8x8_t   c = vdup_n_u8(0xf8); // 1111'1000
    uint8x8_t   x = vtst_u8(t, c); 

    // multiply lane by 2 if not -1
    c = vdup_n_u8(1);   // shift amount 
    c = vbic_u8(c, x);  // del -1s
    t = vshl_u8(t, c);
    
    // add 1 to odd lanes if not -1
    c = vbic_u8(vcreate_u8(0x0100010001000100ULL), x);
    t = vadd_u8(t, c);
    return  vqtbl1_u8(b, t);
}

INLINE(Vdhu,VQHU_NEWD)
(
    Vqhu v,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3
)
{
    uint8x16_t  q = VQHU_ASBU(v);
    uint8x8_t   m = QHR_NEWD(q, k0, k1, k2, k3);
    return  VDBU_ASHU(m);
}

INLINE(Vdhi,VQHI_NEWD)
(
    Vqhi v,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3
)
{
    uint8x16_t  q = VQHI_ASBU(v);
    uint8x8_t   m = QHR_NEWD(q, k0, k1, k2, k3);
    return  VDBU_ASHI(m);
}

INLINE(Vdhf,VQHF_NEWD)
(
    Vqhf v,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3
)
{
    uint8x16_t  q = VQHI_ASBU(v);
    uint8x8_t   m = QHR_NEWD(q, k0, k1, k2, k3);
    return  VDBU_ASHF(m);
}


INLINE(uint8x8_t,QWR_NEWD) (uint8x16_t b, Rc(-1, +3) k0, Rc(-1, +3) k1)
{
    uint8x8_t   t = UCHAR_NEWD(k0,k0,k0,k0,  k1,k1,k1,k1);

    // only keep if 0 <= l[i] <= 3
    uint8x8_t   c = vdup_n_u8(0xfc); // 1111'1100
    uint8x8_t   x = vtst_u8(t, c); 

    // multiply lane by 4 if not -1
    c = vdup_n_u8(2);   // shift amount 
    c = vbic_u8(c, x);  // del -1s
    t = vshl_u8(t, c);
    
    // add 1 to odd lanes if not -1
    c = vbic_u8(vcreate_u8(0x0302010003020100ULL), x);
    t = vadd_u8(t, c);
    return  vqtbl1_u8(b, t);
}

INLINE(Vdwu,VQWU_NEWD) (Vqwu v, Rc(-1,+3) k0, Rc(-1,+3) k1)
{
    uint8x16_t  q = VQWU_ASBU(v);
    uint8x8_t   m = QWR_NEWD(q, k0, k1);
    return  VDBU_ASWU(m);
}

INLINE(Vdwi,VQWI_NEWD) (Vqwi v, Rc(-1,+3) k0, Rc(-1,+3) k1)
{
    uint8x16_t  q = VQWI_ASBU(v);
    uint8x8_t   m = QWR_NEWD(q, k0, k1);
    return  VDBU_ASWI(m);
}

INLINE(Vdwf,VQWF_NEWD) (Vqwf v, Rc(-1,+3) k0, Rc(-1,+3) k1)
{
    uint8x16_t  q = VQWF_ASBU(v);
    uint8x8_t   m = QWR_NEWD(q, k0, k1);
    return  VDBU_ASWF(m);
}


INLINE(uint8x8_t,QDR_NEWD) (uint8x16_t b, Rc(-1,+1) k0)
{
    if (k0 == 0) return vget_low_u8(b);
    if (k0 == 1) return vget_high_u8(b);
    return  vcreate_u8(0);
}

INLINE(Vddu,VQDU_NEWD) (Vqdu v, Rc(-1,+1) k0)
{
    uint8x16_t  q = VQWF_ASBU(v);
    uint8x8_t   m = QDR_NEWD(q, k0);
    return  VDBU_ASDU(m);
}

INLINE(Vddi,VQDI_NEWD) (Vqdi v, Rc(-1,+1) k0)
{
    uint8x16_t  q = VQDI_ASBU(v);
    uint8x8_t   m = QDR_NEWD(q, k0);
    return  VDBU_ASDI(m);
}

INLINE(Vddf,VQDF_NEWD) (Vqdf v, Rc(-1,+1) k0)
{
    uint8x16_t  q = VQDF_ASBU(v);
    uint8x8_t   m = QDR_NEWD(q, k0);
    return  VDBU_ASDF(m);
}

#if 0 // _LEAVE_ARM_NEWD
}
#endif

#if 0 // _ENTER_ARM_NEWQ
{
#endif

INLINE(Vqbu,UCHAR_NEWQ)
(
    uint  k0, uint  k1, uint  k2, uint  k3,
    uint  k4, uint  k5, uint  k6, uint  k7,
    uint  k8, uint  k9, uint k10, uint k11,
    uint k12, uint k13, uint k14, uint k15
)
{
#define UCHAR_NEWQ(                 \
    K0,K1,K2,K3,K4,K5,K6,K7,        \
    K8,K9,K10,K11,K12,K13,K14,K15   \
)                       \
vcombine_u8(            \
    vcreate_u8(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .B0.U=K0,.B1.U=K1,.B2.U=K2,.B3.U=K3,\
                .B4.U=K4,.B5.U=K5,.B6.U=K6,.B7.U=K7,\
            }           \
        ).U             \
    ),                  \
    vcreate_u8(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .B0.U=K8,  .B1.U=K9,  .B2.U=K10, .B3.U=K11,\
                .B4.U=K12, .B5.U=K13, .B6.U=K14, .B7.U=K15,\
            }           \
        ).U             \
    )                   \
)

    return UCHAR_NEWQ(
        k0, k1, k2, k3,
        k4, k5, k6, k7,
        k8, k9, k10,k11,
        k12,k13,k14,k15
    );
}

INLINE(Vqbi,SCHAR_NEWQ)
(
    int  k0, int  k1, int  k2, int  k3,
    int  k4, int  k5, int  k6, int  k7,
    int  k8, int  k9, int k10, int k11,
    int k12, int k13, int k14, int k15
)
{
#define SCHAR_NEWQ(                 \
    K0,K1,K2,K3,K4,K5,K6,K7,        \
    K8,K9,K10,K11,K12,K13,K14,K15   \
)                       \
vcombine_s8(            \
    vcreate_s8(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .B0.I=K0,.B1.I=K1,.B2.I=K2,.B3.I=K3,\
                .B4.I=K4,.B5.I=K5,.B6.I=K6,.B7.I=K7,\
            }           \
        ).U             \
    ),                  \
    vcreate_s8(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .B0.I=K8,  .B1.I=K9,  .B2.I=K10, .B3.I=K11,\
                .B4.I=K12, .B5.I=K13, .B6.I=K14, .B7.I=K15,\
            }           \
        ).U             \
    )                   \
)

    return SCHAR_NEWQ(
        k0, k1, k2, k3,
        k4, k5, k6, k7,
        k8, k9, k10,k11,
        k12,k13,k14,k15
    );
}

INLINE(Vqbc,CHAR_NEWQ)
(
    int  k0, int  k1, int  k2, int  k3,
    int  k4, int  k5, int  k6, int  k7,
    int  k8, int  k9, int k10, int k11,
    int k12, int k13, int k14, int k15
)
{
#if CHAR_MIN
#   define CHAR_NEWQ(...) ((Vqbc){SCHAR_NEWQ(__VA_ARGS__)})
#else
#   define CHAR_NEWQ(...) ((Vqbc){UCHAR_NEWQ(__VA_ARGS__)})
#endif

    return  CHAR_NEWQ(
        k0,k1,k2,k3,k4,k5,k6,k7,
        k8,k9,k10,k11,k12,k13,k14,k15
    );
}


INLINE(Vqhu,USHRT_NEWQ)
(
    unsigned  k0, unsigned  k1, unsigned  k2, unsigned  k3,
    unsigned  k4, unsigned  k5, unsigned  k6, unsigned  k7
)
{
#define     USHRT_NEWQ(K0,K1,K2,K3,K4,K5,K6,K7) \
vcombine_u16(            \
    vcreate_u16(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .H0.U=K0,.H1.U=K1,\
                .H2.U=K2,.H3.U=K3,\
            }           \
        ).U             \
    ),                  \
    vcreate_u16(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .H0.U=K4, .H1.U=K5,\
                .H2.U=K6, .H3.U=K7,\
            }           \
        ).U             \
    )                   \
)

    return  USHRT_NEWQ(k0, k1, k2, k3,k4, k5, k6, k7);
}

INLINE(Vqhi,SHRT_NEWQ)
(
    signed  k0, signed  k1, signed  k2, signed  k3,
    signed  k4, signed  k5, signed  k6, signed  k7
)
{
#define     SHRT_NEWQ(K0,K1,K2,K3,K4,K5,K6,K7) \
vcombine_s16(            \
    vcreate_s16(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .H0.I=K0,.H1.I=K1,.H2.I=K2,.H3.I=K3,\
            }           \
        ).U             \
    ),                  \
    vcreate_s16(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .H0.I=K4,.H1.I=K5,.H2.I=K6,.H3.I=K7,\
            }           \
        ).U             \
    )                   \
)

    return SHRT_NEWQ(k0, k1, k2, k3,k4, k5, k6, k7);
}

INLINE(Vqwu,UINT_NEWQ)
(
    uint  k0, uint  k1, uint  k2, uint  k3
)
{
#define UINT_NEWQ(K0,K1,K2,K3) \
vcombine_u32(            \
    vcreate_u32(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .W0.U=K0,.W1.U=K1,\
            }           \
        ).U             \
    ),                  \
    vcreate_u32(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .W0.U=K2,.W1.U=K3,\
            }           \
        ).U             \
    )                   \
)

    return UINT_NEWQ(k0, k1, k2, k3);
}

INLINE(Vqwi,INT_NEWQ)
(
    int  k0, int  k1, int  k2, int  k3
)
{
#define     INT_NEWQ(K0,K1,K2,K3) \
vcombine_s32(            \
    vcreate_s32(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .W0.I=K0,.W1.I=K1,\
            }           \
        ).U             \
    ),                  \
    vcreate_s32(         \
        (               \
            (DWRD_TYPE) \
            {           \
                .W0.I=K2,.W1.I=K3,\
            }           \
        ).U             \
    )                   \
)

    return INT_NEWQ(k0, k1, k2, k3);
}

#if DWRD_NLONG == 2

INLINE(Vqwu,ULONG_NEWQ) 
(
    ulong k0, ulong k1, ulong k2, ulong k3
)
{
    return  UINT_NEWQ(k0, k1, k2, k3);
}

INLINE(Vqwi,LONG_NEWQ) 
(
    long k0, long k1, long k2, long k3
)
{
    return  INT_NEWQ(k0, k1, k2, k3);
}

#else

INLINE(Vqdu,ULONG_NEWQ) (ulong  k0, ulong  k1)
{
#define     ULONG_NEWQ(K0, K1) vcombine_u64(vdup_n_u64(K0),vdup_n_u64(K1))
    return ULONG_NEWQ(k0, k1);
}

INLINE(Vqdi,LONG_NEWQ) (long  k0, long  k1)
{
#define     LONG_NEWQ(K0, K1) vcombine_s64(vdup_n_s64(K0),vdup_n_s64(K1))
    return  LONG_NEWQ(k0, k1);
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_NEWQ) (ullong  k0, ullong  k1)
{
#define     ULLONG_NEWQ(K0, K1) vcombine_u64(vdup_n_u64(K0),vdup_n_u64(K1))
    return  ULLONG_NEWQ(k0, k1);
}

INLINE(Vqdi,LLONG_NEWQ) (llong  k0, llong  k1)
{
#define     LLONG_NEWQ(K0, K1) vcombine_s64(vdup_n_s64(K0),vdup_n_s64(K1))
    return  LLONG_NEWQ(k0, k1);
}

INLINE(Vqqu,newqqu) (QUAD_UTYPE k0) {return astvqu(k0);}
INLINE(Vqqi,newqqi) (QUAD_ITYPE k0) {return astvqi(k0);}
INLINE(Vqqf,newqqf) (QUAD_FTYPE k0) {return astvqf(k0);}

#endif

INLINE(Vqhf,FLT16_NEWQ)
(
    flt16_t  k0, flt16_t  k1, flt16_t  k2, flt16_t  k3,
    flt16_t  k4, flt16_t  k5, flt16_t  k6, flt16_t  k7
)
{
#define     FLT16_NEWQ(K0,K1,K2,K3,K4,K5,K6,K7) \
vreinterpretq_f16_u16(      \
    vcombine_u16(            \
        vcreate_u16(         \
            (               \
                (DWRD_TYPE) \
                {           \
                    .H0.F=K0,.H1.F=K1,.H2.F=K2,.H3.F=K3,\
                }           \
            ).U             \
        ),                  \
        vcreate_u16(         \
            (               \
                (DWRD_TYPE) \
                {           \
                    .H0.F=K4,.H1.F=K5,.H2.F=K6,.H3.F=K7,\
                }           \
            ).U             \
        )                   \
    )                       \
)

    return  FLT16_NEWQ(k0, k1, k2, k3,k4, k5, k6, k7);
}

INLINE(Vqwf,FLT_NEWQ)
(
    float  k0, float  k1, float  k2, float  k3
)
{
#define     FLT_NEWQ(K0,K1,K2,K3)           \
vcombine_f32(                               \
    vcreate_f32(                            \
        ((DWRD_TYPE){.W0.F=K0,.W1.F=K1}).U  \
    ),                                      \
    vcreate_f32(                            \
        ((DWRD_TYPE){.W0.F=K2,.W1.F=K3}).U  \
    )                                       \
)

    return FLT_NEWQ(k0, k1, k2, k3);
}

INLINE(Vqdf,DBL_NEWQ) (double  k0, double  k1)
{
#define     DBL_NEWQ(K0, K1) vcombine_f64(vdup_n_f64(K0),vdup_n_f64(K1))
    return  DBL_NEWQ(k0, k1);
}

#if 0 // _LEAVE_ARM_NEWQ
}
#endif

#if 0 // _ENTER_ARM_NEWL
{
#endif
// TODO: delete these

INLINE(Vwbi,VWBI_NEWL) ( int8_t  k0,  int8_t  k1,  int8_t k2,  int8_t k3)
{
    return (VWBI_TYPE){WBI_NEWL(k0, k1, k2, k3)};
}

INLINE(Vwbc,VWBC_NEWL) (   char  k0,    char  k1,    char k2,    char k3)
{
    return (VWBC_TYPE){WBC_NEWL(k0, k1, k2, k3)};
}


INLINE(Vwhu,VWHU_NEWL) (uint16_t k0, uint16_t k1)
{
    return (VWHU_TYPE){WHU_NEWL(k0, k1)};
}

INLINE(Vwhi,VWHI_NEWL) ( int16_t k0,  int16_t k1)
{
    return (VWHI_TYPE){WHI_NEWL(k0, k1)};
}

INLINE(Vwhf,VWHF_NEWL) ( flt16_t k0,  flt16_t k1)
{
    return (VWHF_TYPE){WHF_NEWL(k0, k1)};
}

INLINE(Vwwu,VWWU_NEWL) (uint32_t k0)
{
    return  UINT32_ASTV(k0);
}

INLINE(Vwwi,VWWI_NEWL) ( int32_t k0)
{
    return  INT32_ASTV(k0);
}

INLINE(Vwwf,VWWF_NEWL) (   float k0)
{
    return (VWWF_TYPE){k0};
}


INLINE(Vdbu,VDBU_NEWL)
(
    uint8_t k0, uint8_t k1, uint8_t k2, uint8_t k3,
    uint8_t k4, uint8_t k5, uint8_t k6, uint8_t k7
)
{
#define     VDBU_NEWL(K0, K1, K2, K3, K4, K5, K6, K7) \
vcreate_u8(                 \
    (                       \
        ((0xffULL&K0)<<000) \
    |   ((0xffULL&K1)<<010) \
    |   ((0xffULL&K2)<<020) \
    |   ((0xffULL&K3)<<030) \
    |   ((0xffULL&K4)<<040) \
    |   ((0xffULL&K5)<<050) \
    |   ((0xffULL&K6)<<060) \
    |   ((0xffULL&K7)<<070) \
    )                       \
)

    return  DBU_NEWL(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vdbi,VDBI_NEWL)
(
    int8_t k0, int8_t k1, int8_t k2, int8_t k3,
    int8_t k4, int8_t k5, int8_t k6, int8_t k7
)
{
#define     VDBI_NEWL(K0, K1, K2, K3, K4, K5, K6, K7) \
vcreate_s8(\
    (\
        ((0xffULL&K0)<<000) \
    |   ((0xffULL&K1)<<010) \
    |   ((0xffULL&K2)<<020) \
    |   ((0xffULL&K3)<<030) \
    |   ((0xffULL&K4)<<040) \
    |   ((0xffULL&K5)<<050) \
    |   ((0xffULL&K6)<<060) \
    |   ((0xffULL&K7)<<070) \
    )                       \
)

    return  DBI_NEWL(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vdbc,VDBC_NEWL)
(
    char k0, char k1, char k2, char k3,
    char k4, char k5, char k6, char k7
)
{
#if CHAR_MIN
#   define  VDBC_NEWL(...) VDBI_ASBC(VDBI_NEWL(__VA_ARGS__))
    return  VDBI_ASBC((VDBI_NEWL)(k0, k1, k2, k3, k4, k5, k6, k7));
#else
#   define  VDBC_NEWL(...) VDBU_ASBC(VDBU_NEWL(__VA_ARGS__))
    return  VDBU_ASBC((VDBU_NEWL)(k0, k1, k2, k3, k4, k5, k6, k7));
#endif
}


INLINE(Vdhu,VDHU_NEWL)
(
    uint16_t k0, uint16_t k1, uint16_t k2, uint16_t k3
)
{
    return  DHU_NEWL(k0, k1, k2, k3);
}

INLINE(Vdhi,VDHI_NEWL)
(
    int16_t k0, int16_t k1, int16_t k2, int16_t k3
)
{
    return  DHI_NEWL(k0, k1, k2, k3);
}

INLINE(Vdhf,VDHF_NEWL)
(
    flt16_t k0, flt16_t k1, flt16_t k2, flt16_t k3
)
{
    return  DHF_NEWL(k0, k1, k2, k3);
}


INLINE(Vdwu,VDWU_NEWL)
(
    uint32_t k0, uint32_t k1
)
{
    return  DWU_NEWL(k0, k1);
}

INLINE(Vdwi,VDWI_NEWL)
(
    int32_t k0, int32_t k1
)
{
    return  DWI_NEWL(k0, k1);
}

INLINE(Vdwf,VDWF_NEWL)
(
    float k0, float k1
)
{
    return  DWF_NEWL(k0, k1);
}


INLINE(Vddu,VDDU_NEWL) (uint64_t k0) {return UINT64_ASTV(k0);}
INLINE(Vddi,VDDI_NEWL) ( int64_t k0) {return  INT64_ASTV(k0);}
INLINE(Vddf,VDDF_NEWL) (  double k0) {return    DBL_ASTV(k0);}


INLINE(Vqbu,VQBU_NEWL)
(
    uint8_t  k0, uint8_t  k1, uint8_t  k2, uint8_t  k3,
    uint8_t  k4, uint8_t  k5, uint8_t  k6, uint8_t  k7,
    uint8_t  k8, uint8_t  k9, uint8_t k10, uint8_t k11,
    uint8_t k12, uint8_t k13, uint8_t k14, uint8_t k15
)
{
    return  QBU_NEWL(
        k0,  k1, k2,  k3,  k4,  k5,  k6,  k7,
        k8,  k9, k10, k11, k12, k13, k14, k15
    );
}

INLINE(Vqbi,VQBI_NEWL)
(
    int8_t  k0, int8_t  k1, int8_t  k2, int8_t  k3,
    int8_t  k4, int8_t  k5, int8_t  k6, int8_t  k7,
    int8_t  k8, int8_t  k9, int8_t k10, int8_t k11,
    int8_t k12, int8_t k13, int8_t k14, int8_t k15
)
{
    return  QBI_NEWL(
        k0,  k1, k2,  k3,  k4,  k5,  k6,  k7,
        k8,  k9, k10, k11, k12, k13, k14, k15
    );
}

INLINE(Vqbc,VQBC_NEWL)
(
    char  k0, char  k1, char  k2, char  k3,
    char  k4, char  k5, char  k6, char  k7,
    char  k8, char  k9, char k10, char k11,
    char k12, char k13, char k14, char k15
)
{
    return  QBC_ASTV(
        QBC_NEWL(
            k0,  k1, k2,  k3,  k4,  k5,  k6,  k7,
            k8,  k9, k10, k11, k12, k13, k14, k15
        )
    );
}

INLINE(Vqhu,VQHU_NEWL)
(
    uint16_t k0, uint16_t k1, uint16_t k2, uint16_t k3,
    uint16_t k4, uint16_t k5, uint16_t k6, uint16_t k7
)
{
    return QHU_NEWL(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vqhi,VQHI_NEWL)
(
    int16_t k0, int16_t k1, int16_t k2, int16_t k3,
    int16_t k4, int16_t k5, int16_t k6, int16_t k7
)
{
    return QHI_NEWL(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vqhf,VQHF_NEWL)
(
    flt16_t k0, flt16_t k1, flt16_t k2, flt16_t k3,
    flt16_t k4, flt16_t k5, flt16_t k6, flt16_t k7
)
{
    return QHF_NEWL(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vqwu,VQWU_NEWL)
(
    uint32_t k0, uint32_t k1, uint32_t k2, uint32_t k3
)
{
    return  QWU_NEWL(k0, k1, k2, k3);
}

INLINE(Vqwi,VQWI_NEWL)
(
    int32_t k0, int32_t k1, int32_t k2, int32_t k3
)
{
    return  QWI_NEWL(k0, k1, k2, k3);
}

INLINE(Vqwf,VQWF_NEWL)
(
    float k0, float k1, float k2, float k3
)
{
    return  QWF_NEWL(k0, k1, k2, k3);
}

INLINE(Vqdu,VQDU_NEWL) (uint64_t k0, uint64_t k1)
{
    return  QDU_NEWL(k0, k1);
}

INLINE(Vqdi,VQDI_NEWL) (int64_t k0, int64_t k1)
{
    return  QDI_NEWL(k0, k1);
}

INLINE(Vqdf,VQDF_NEWL) (double k0, double k1)
{
    return  QDF_NEWL(k0, k1);
}

#if 0 // _LEAVE_ARM_NEWL
}
#endif

#if 0 // _ENTER_ARM_NEWR
{
#endif

#define     WBZ_NEWR(K3, K2, K1, K0)    \
vget_lane_f32(                          \
    vreinterpret_f32_u64(               \
        vdup_n_u64(                     \
            ((0xffULL&(K0))<<000)       \
        |   ((0xffULL&(K1))<<010)       \
        |   ((0xffULL&(K2))<<020)       \
        |   ((0xffULL&(K3))<<030)       \
        )                               \
    ),                                  \
    0                               \
)

#define     WHZ_NEWR(K1, K0)        \
vget_lane_f32(                      \
    vreinterpret_f32_u64(           \
        vdup_n_u64(                 \
            ((0xffffULL&(K0))<<000) \
        |   ((0xffffULL&(K1))<<020) \
        )                           \
    ),                              \
    0                           \
)

#define     WWU_NEWR(K0)    ((WORD_TYPE){.U=K0}).F
#define     WWI_NEWR(K0)    ((WORD_TYPE){.I=K0}).F
#define     WWF_NEWR(K0)    ((WORD_TYPE){.F=K0}).F

#define     DBZ_NEWR(T, K7, K6, K5, K4, K3, K2, K1, K0) \
vcreate_##T(            \
    ((0xffULL&K0)<<000) \
|   ((0xffULL&K1)<<010) \
|   ((0xffULL&K2)<<020) \
|   ((0xffULL&K3)<<030) \
|   ((0xffULL&K4)<<040) \
|   ((0xffULL&K5)<<050) \
|   ((0xffULL&K6)<<060) \
|   ((0xffULL&K7)<<070) \
)

#define     DBU_NEWR(...) DBZ_NEWR(u8, __VA_ARGS__)
#define     DBI_NEWR(...) DBZ_NEWR(s8, __VA_ARGS__)
#if CHAR_MIN
#   define  DBC_NEWR    DBI_NEWR
#else
#   define  DBC_NEWR    DBU_NEWR
#endif

#define     DHZ_NEWR(T, K3, K2, K1, K0) \
vcreate_##T(                \
    ((0xffffULL&K0)<<000)   \
|   ((0xffffULL&K1)<<020)   \
|   ((0xffffULL&K2)<<040)   \
|   ((0xffffULL&K3)<<060)   \
)

#define     DHU_NEWR(...)   DHZ_NEWR(u16,__VA_ARGS__)
#define     DHI_NEWR(...)   DHZ_NEWR(s16,__VA_ARGS__)

#define     DWZ_NEWR(T, K1, K0) \
vcreate_##T(                    \
    ((0xffffffffULL&K0)<<000)   \
|   ((0xffffffffULL&K1)<<040)   \
)


#define     DWU_NEWR(...)   DWZ_NEWR(u32, __VA_ARGS__)
#define     DWI_NEWR(...)   DWZ_NEWR(s32, __VA_ARGS__)

#define     DDU_NEWR    vdup_n_u64
#define     DDI_NEWR    vdup_n_s64
#define     DDF_NEWR    vdup_n_f64

#define     QBZ_NEWR(T,             \
    K15,K14,K13,K12,K11,K10,K9, K8, \
    K7, K6, K5, K4, K3, K2, K1, K0  \
)                                   \
vcombine_##T(                       \
    DBZ_NEWR(T,K15,K14,K13,K12,K11,K10,K9,K8),  \
    DBZ_NEWR(T,K7, K6, K5, K4, K3, K2, K1,K0)   \
)

#define     QBU_NEWR(...)   QBZ_NEWR(u8,__VA_ARGS__)
#define     QBI_NEWR(...)   QBZ_NEWR(s8,__VA_ARGS__)
#if CHAR_MIN
#   define  QBC_NEWR(...)   QBI_NEWR(__VA_ARGS__)
#else
#   define  QBC_NEWR(...)   QBU_NEWR(__VA_ARGS__)
#endif

#define     QHZ_NEWR(T,K7,K6,K5,K4,K3,K2,K1,K0) \
vcombine_##T(               \
    DHZ_NEWR(T,K7,K6,K5,K4),\
    DHZ_NEWR(T,K3,K2,K1,K0) \
)

#define     QHU_NEWR(...)   QHZ_NEWR(u16,__VA_ARGS__)
#define     QHI_NEWR(...)   QHZ_NEWR(s16,__VA_ARGS__)

#define     QWZ_NEWR(T, K3,K2,K1,K0) \
vcombine_##T(DWZ_NEWR(T,K3,K2),DWZ_NEWR(T,K1,K0))

#define     QWU_NEWR(...)   QWZ_NEWR(u32,__VA_ARGS__)
#define     QWI_NEWR(...)   QWZ_NEWR(s32,__VA_ARGS__)

#define     QDZ_NEWR(T, K1, K0) vcombine_##T(vdup_n_##T(K1),vdup_n_##T(K0))

#define     QDU_NEWR(...)    QDZ_NEWR(u64,__VA_ARGS__)
#define     QDI_NEWR(...)    QDZ_NEWR(s64,__VA_ARGS__)
#define     QDF_NEWR(...)    QDZ_NEWR(f64,__VA_ARGS__)

INLINE(Vwbu,VWBU_NEWR) (uint8_t  k3, uint8_t  k2, uint8_t k1, uint8_t k0)
{
#define     VWBU_NEWR(...) ((VWBU_TYPE){WBZ_NEWR(__VA_ARGS__)})
    return  VWBU_NEWR(k3, k2, k1, k0);
}

INLINE(Vwbi,VWBI_NEWR) ( int8_t  k3,  int8_t  k2,  int8_t k1,  int8_t k0)
{
#define     VWBI_NEWR(...) ((VWBI_TYPE){WBZ_NEWR(__VA_ARGS__)})
    return  VWBI_NEWR(k3, k2, k1, k0);
}

INLINE(Vwbc,VWBC_NEWR) (   char  k3,    char  k2,    char k1,    char k0)
{
#define     VWBC_NEWR(...) ((VWBC_TYPE){WBZ_NEWR(__VA_ARGS__)})
    return  VWBC_NEWR(k3, k2, k1, k0);
}


INLINE(Vwhu,VWHU_NEWR) (uint16_t k1, uint16_t k0)
{
#define     VWHU_NEWR(...) ((VWHU_TYPE){WHZ_NEWR(__VA_ARGS__)})
    return  VWHU_NEWR(k1, k0);
}

INLINE(Vwhi,VWHI_NEWR)  (int16_t k1,  int16_t k0)
{
#define     VWHI_NEWR(...) ((VWHI_TYPE){WHZ_NEWR(__VA_ARGS__)})
    return  VWHI_NEWR(k1, k0);
}

INLINE(Vwhf,VWHF_NEWR) ( flt16_t k1,  flt16_t k0)
{
    float16x4_t r = vdup_n_f16(0);
    r = vset_lane_f16(k0, r, 0);
    r = vset_lane_f16(k1, r, 1);
    float32x2_t m = vreinterpret_f32_f16(r);
    return  WHF_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwwu,VWWU_NEWR) (uint32_t k0) {return  UINT32_ASTV(k0);}

INLINE(Vwwi,VWWI_NEWR) ( int32_t k0) {return   INT32_ASTV(k0);}

INLINE(Vwwf,VWWF_NEWR) (   float k0) {return     FLT_ASTV(k0);}


INLINE(Vdbu,VDBU_NEWR)
(
    uint8_t k7, uint8_t k6, uint8_t k5, uint8_t k4,
    uint8_t k3, uint8_t k2, uint8_t k1, uint8_t k0
)
{
#define     VDBU_NEWR(...) DBU_NEWR(__VA_ARGS__)
    return  VDBU_NEWR(k7, k6, k5, k4, k3, k2, k1, k0);
}

INLINE(Vdbi,VDBI_NEWR)
(
    int8_t k7, int8_t k6, int8_t k5, int8_t k4,
    int8_t k3, int8_t k2, int8_t k1, int8_t k0
)
{
#define     VDBI_NEWR(...) DBI_NEWR(__VA_ARGS__)
    return  VDBI_NEWR(k7, k6, k5, k4, k3, k2, k1, k0);
}

INLINE(Vdbc,VDBC_NEWR)
(
    char k0, char k1, char k2, char k3,
    char k4, char k5, char k6, char k7
)
{
#define     VDBC_NEWR(...) DBC_ASTV(DBC_NEWR(__VA_ARGS__))
    return  VDBC_NEWR(k7, k6, k5, k4, k3, k2, k1, k0);
}


INLINE(Vdhu,VDHU_NEWR)
(
    uint16_t k3, uint16_t k2, uint16_t k1, uint16_t k0
)
{
#define     VDHU_NEWR(...) DHU_NEWR(__VA_ARGS__)
    return  VDHU_NEWR(k3, k2, k1, k0);
}

INLINE(Vdhi,VDHI_NEWR)
(
    int16_t k3, int16_t k2, int16_t k1, int16_t k0
)
{
#define     VDHI_NEWR(...) DHI_NEWR(__VA_ARGS__)
    return  VDHI_NEWR(k3, k2, k1, k0);
}

INLINE(Vdhf,VDHF_NEWR)
(
    flt16_t k3, flt16_t k2, flt16_t k1, flt16_t k0
)
{
    float16x4_t r = VDHF_VOID;
    r = vset_lane_f16(k3, r, 3);
    r = vset_lane_f16(k2, r, 2);
    r = vset_lane_f16(k1, r, 1);
    r = vset_lane_f16(k0, r, 0);
    return  r;
}


INLINE(Vdwu,VDWU_NEWR) (uint32_t k1, uint32_t k0)
{
#define     VDWU_NEWR(...) DWU_NEWR(__VA_ARGS__)
    return  DWU_NEWR(k1, k0);
}

INLINE(Vdwi,VDWI_NEWR) (int32_t k1, int32_t k0)
{
#define     VDWI_NEWR(...) DWI_NEWR(__VA_ARGS__)
    return  DWI_NEWR(k1, k0);
}

INLINE(Vdwf,VDWF_NEWR) (float k1, float k0)
{
    float32x2_t r = VDWF_VOID;
    r = vset_lane_f32(k1, r, 1);
    r = vset_lane_f32(k0, r, 0);
    return  r;
}

INLINE(Vddu,VDDU_NEWR) (uint64_t k0) {return UINT64_ASTV(k0);}
INLINE(Vddi,VDDI_NEWR) ( int64_t k0) {return  INT64_ASTV(k0);}
INLINE(Vddf,VDDF_NEWR) (  double k0) {return    DBL_ASTV(k0);}


INLINE(Vqbu,VQBU_NEWR)
(
    uint8_t k15, uint8_t k14, uint8_t k13, uint8_t k12,
    uint8_t k11, uint8_t k10, uint8_t k9,  uint8_t k8,
    uint8_t k7,  uint8_t k6,  uint8_t k5,  uint8_t k4,
    uint8_t k3,  uint8_t k2,  uint8_t k1,  uint8_t k0
)
{
#define     VQBU_NEWR(...) QBU_NEWR(__VA_ARGS__)
    return  VQBU_NEWR(k15,k14,k13,k12,k11,k10,k9,k8,k7,k6,k5,k4,k3,k2,k1,k0);
}

INLINE(Vqbi,VQBI_NEWR)
(
    int8_t k15, int8_t k14, int8_t k13, int8_t k12,
    int8_t k11, int8_t k10, int8_t k9,  int8_t k8,
    int8_t k7,  int8_t k6,  int8_t k5,  int8_t k4,
    int8_t k3,  int8_t k2,  int8_t k1,  int8_t k0
)
{
#define     VQBI_NEWR(...) QBI_NEWR(__VA_ARGS__)
    return  VQBI_NEWR(k15,k14,k13,k12,k11,k10,k9,k8,k7,k6,k5,k4,k3,k2,k1,k0);
}


INLINE(Vqbc,VQBC_NEWR)
(
    char k15, char k14, char k13, char k12,
    char k11, char k10, char k9,  char k8,
    char k7,  char k6,  char k5,  char k4,
    char k3,  char k2,  char k1,  char k0
)
{
#define     VQBC_NEWR(...)  QBC_ASTV(QBC_NEWR(__VA_ARGS__))
    return  VQBC_NEWR(k15,k14,k13,k12,k11,k10,k9,k8,k7,k6,k5,k4,k3,k2,k1,k0);
}

INLINE(Vqhu,VQHU_NEWR)
(
    uint16_t k7, uint16_t k6, uint16_t k5, uint16_t k4,
    uint16_t k3, uint16_t k2, uint16_t k1, uint16_t k0
)
{
#define     VQHU_NEWR(...)  QHU_NEWR(__VA_ARGS__)
    return  VQHU_NEWR(k7, k6, k5, k4, k3, k2, k1, k0);
}

INLINE(Vqhi,VQHI_NEWR)
(
    int16_t k7, int16_t k6, int16_t k5, int16_t k4,
    int16_t k3, int16_t k2, int16_t k1, int16_t k0
)
{
#define     VQHI_NEWR(...)  QHU_NEWR(__VA_ARGS__)
    return  VQHI_NEWR(k7, k6, k5, k4, k3, k2, k1, k0);
}

INLINE(Vqhf,VQHF_NEWR)
(
    flt16_t k7, flt16_t k6, flt16_t k5, flt16_t k4,
    flt16_t k3, flt16_t k2, flt16_t k1, flt16_t k0
)
{
    float16x8_t r = VQHF_VOID;
    r = vsetq_lane_f16(k7, r, 7);
    r = vsetq_lane_f16(k6, r, 6);
    r = vsetq_lane_f16(k5, r, 5);
    r = vsetq_lane_f16(k4, r, 4);
    r = vsetq_lane_f16(k3, r, 3);
    r = vsetq_lane_f16(k2, r, 2);
    r = vsetq_lane_f16(k1, r, 1);
    r = vsetq_lane_f16(k0, r, 0);
    return  r;
}

INLINE(Vqwu,VQWU_NEWR)
(
    uint32_t k3, uint32_t k2, uint32_t k1, uint32_t k0
)
{
#define     VQWU_NEWR(...)  QWU_NEWR(__VA_ARGS__)
    return  VQWU_NEWR(k3, k2, k1, k0);
}

INLINE(Vqwi,VQWI_NEWR)
(
    int32_t k3, int32_t k2, int32_t k1, int32_t k0
)
{
#define     VQWI_NEWR(...)  QWI_NEWR(__VA_ARGS__)
    return  VQWI_NEWR(k3, k2, k1, k0);
}

INLINE(Vqwf,VQWF_NEWR)
(
    float k3, float k2, float k1, float k0
)
{
    float32x4_t r = VQWF_VOID;
    r = vsetq_lane_f32(k3, r, 3);
    r = vsetq_lane_f32(k2, r, 2);
    r = vsetq_lane_f32(k1, r, 1);
    r = vsetq_lane_f32(k0, r, 0);
    return  r;
}

INLINE(Vqdu,VQDU_NEWR) (uint64_t k1, uint64_t k0)
{
#define     VQDU_NEWR(...)  QDU_NEWR(__VA_ARGS__)
    return  VQDU_NEWR(k1, k0);
}

INLINE(Vqdi,VQDI_NEWR) (int64_t k1, int64_t k0)
{
#define     VQDI_NEWR(...)  QDI_NEWR(__VA_ARGS__)
    return  VQDI_NEWR(k1, k0);
}

INLINE(Vqdf,VQDF_NEWR) (double k1, double k0)
{
#define     VQDF_NEWR(...)  QDF_NEWR(__VA_ARGS__)
    return  VQDF_NEWR(k1, k0);
}

#if 0 // _LEAVE_ARM_NEWR
}
#endif

#if 0 // _ENTER_ARM_SEQW
{
#endif

INLINE(Vwbu, UCHAR_SEQW)  (unsigned a, signed b)
{
    uint8x8_t   v = vmla_u8(
        vdup_n_u8(a),
        vcreate_u8(0x0706050403020100ULL),
        vreinterpret_u8_s8(vdup_n_s8(b))
    );
    float32x2_t m = vreinterpret_f32_u8(v);
    float       f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi, SCHAR_SEQW)  (signed a, signed b)
{
    int8x8_t    v =  vmla_s8(
        vdup_n_s8(a),
        vcreate_s8(0x0706050403020100ULL),
        vdup_n_s8(b)
    );
    float32x2_t m = vreinterpret_f32_s8(v);
    float       f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,  CHAR_SEQW)   (int a, signed b)
{
#if CHAR_MIN
#   define  CHAR_SEQW(A, B) VWBI_ASBC(SCHAR_SEQW(A,B))
#else
#   define  CHAR_SEQW(A, B) VWBU_ASBC(UCHAR_SEQW(A,B))
#endif
    return  CHAR_SEQW(a, b);
}


INLINE(Vwhu, USHRT_SEQW) (unsigned a, signed b)
{
    uint16x4_t v =  vmla_u16(
        vdup_n_u16(a),
        vcreate_u16(0x0003000200010000ULL),
        vreinterpret_u16_s16(vdup_n_s16(b))
    );
    float32x2_t m = vreinterpret_f32_u16(v);
    float       f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,  SHRT_SEQW)  (signed a, signed b)
{
    int16x4_t v =  vmla_s16(
        vdup_n_s16(a),
        vcreate_s16(0x0003000200010000ULL),
        vdup_n_s16(b)
    );
    float32x2_t m = vreinterpret_f32_s16(v);
    float       f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


#if 0 // _LEAVE_ARM_SEQW
}
#endif

#if 0 // _ENTER_ARM_SEQD
{
#endif

INLINE(Vdbu, UCHAR_SEQD)  (unsigned a, signed b)
{
    return  vmla_u8(
        vdup_n_u8(a),
        vcreate_u8(0x0706050403020100ULL),
        vreinterpret_u8_s8(vdup_n_s8(b))
    );
}

INLINE(Vdbi, SCHAR_SEQD)  (signed a, signed b)
{
    return  vmla_s8(
        vdup_n_s8(a),
        vcreate_s8(0x0706050403020100ULL),
        vdup_n_s8(b)
    );
}

INLINE(Vdbc,  CHAR_SEQD)   (int a, signed b)
{
#if CHAR_MIN
#   define  CHAR_SEQD(A, B) VDBI_ASBC(SCHAR_SEQD(A,B))
#else
#   define  CHAR_SEQD(A, B) VDBU_ASBC(UCHAR_SEQD(A,B))
#endif
    return  CHAR_SEQD(a, b);
}


INLINE(Vdhu, USHRT_SEQD) (unsigned a,signed b)
{
    return  vmla_u16(
        vdup_n_u16(a),
        vcreate_u16(0x0003000200010000ULL),
        vreinterpret_u16_s16(vdup_n_s16(b))
    );
}

INLINE(Vdhi,  SHRT_SEQD)  (signed a, signed b)
{
    return  vmla_s16(
        vdup_n_s16(a),
        vcreate_s16(0x0003000200010000ULL),
        vdup_n_s16(b)
    );
}


INLINE(Vdwu,  UINT_SEQD)   (uint a, int32_t b)
{
    return  vmla_u32(
        vdup_n_u32(a),
        vcreate_u32(0x0000000100000000ULL),
        vreinterpret_u32_s32(vdup_n_s32(b))
    );
}

INLINE(Vdwi,   INT_SEQD)    (int a, int32_t b)
{
    return  vmla_s32(
        vdup_n_s32(a),
        vcreate_s32(0x0000000100000000ULL),
        vdup_n_s32(b)
    );
}


#if DWRD_NLONG == 2

INLINE(Vdwu, ULONG_SEQD)  (ulong a, int32_t b) {return UINT_SEQD(a, b);}
INLINE(Vdwi,  LONG_SEQD)   (long a, int32_t b) {return  INT_SEQD(a, b);}

#endif

#if 0 // _LEAVE_ARM_SEQD
}
#endif

#if 0 // _ENTER_ARM_SEQQ
{
#endif

INLINE(Vqbu, UCHAR_SEQQ)  (unsigned a, signed b)
{
    return  vmlaq_u8(
        vdupq_n_u8(a),
        vcombine_u8(
            vcreate_u8(0x0706050403020100ULL),
            vcreate_u8(0x0f0e0d0c0b0a0908ULL)
        ),
        vreinterpretq_u8_s8(vdupq_n_s8(b))
    );
}

INLINE(Vqbi, SCHAR_SEQQ)    (signed a, signed b)
{
    return  vmlaq_s8(
        vdupq_n_s8(a),
        vcombine_s8(
            vcreate_s8(0x0706050403020100ULL),
            vcreate_s8(0x0f0e0d0c0b0a0908ULL)
        ),
        vdupq_n_s8(b)
    );
}

INLINE(Vqbc,  CHAR_SEQQ)       (int a, signed b)
{
#if CHAR_MIN
#   define  CHAR_SEQQ(A, B) VQBI_ASBC(SCHAR_SEQQ(A,B))
#else
#   define  CHAR_SEQQ(A, B) VQBU_ASBC(UCHAR_SEQQ(A,B))
#endif
    return  CHAR_SEQQ(a, b);
}


INLINE(Vqhu, USHRT_SEQQ)  (unsigned a, signed b)
{
    return  vmlaq_u16(
        vdupq_n_u16(a),
        vcombine_u16(
            vcreate_u16(0x0003000200010000ULL),
            vcreate_u16(0x0007000600050004ULL)
        ),
        vreinterpretq_u16_s16(vdupq_n_s16(b))
    );
}

INLINE(Vqhi,  SHRT_SEQQ)    (signed a, signed b)
{
    return  vmlaq_s16(
        vdupq_n_s16(a),
        vcombine_s16(
            vcreate_s16(0x0003000200010000ULL),
            vcreate_s16(0x0007000600050004ULL)
        ),
        vdupq_n_s16(b)
    );
}


INLINE(Vqwu,  UINT_SEQQ)      (uint a, int32_t b)
{
    return  vmlaq_u32(
        vdupq_n_u32(a),
        vcombine_u32(
            vcreate_u32(0x0000000100000000ULL),
            vcreate_u32(0x0000000300000002ULL)
        ),
        vreinterpretq_u32_s32(vdupq_n_s32(b))
    );
}

INLINE(Vqwi,   INT_SEQQ)       (int a, int32_t b)
{
    return  vmlaq_s32(
        vdupq_n_s32(a),
        vcombine_s32(
            vcreate_s32(0x0000000100000000ULL),
            vcreate_s32(0x0000000300000002ULL)
        ),
        vdupq_n_s32(b)
    );
}

#if DWRD_NLONG == 2

INLINE(Vqwu, ULONG_SEQQ)  (ulong a, int32_t b) {return UINT_SEQQ(a, b);}
INLINE(Vqwi,  LONG_SEQQ)   (long a, int32_t b) {return  INT_SEQQ(a, b);}

#else

INLINE(Vqdu, ULONG_SEQQ)  (ulong a, int64_t b)
{
    return  vaddq_u64(
        vdupq_n_u64(a),
        vcombine_u64(
            vdup_n_u64(0),
            vreinterpret_u64_s64(vdup_n_s64(b))
        )
    );
}

INLINE(Vqdi,  LONG_SEQQ)   (long a, int64_t b)
{
    return  vaddq_s64(
        vdupq_n_s64(a),
        vcombine_u64(vdup_n_s64(0), vdup_n_s64(b))
    );
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_SEQQ) (ullong a, int64_t b)
{
    uint64x1_t l = vdup_n_u64((a));
    uint64x1_t r = vdup_n_u64((a+b));
    return vcombine_u64(l, r);
}

INLINE(Vqdi, LLONG_SEQQ)  (llong a, int64_t b)
{
    return  vaddq_s64(
        vdupq_n_s64(a),
        vcombine_u64(vdup_n_s64(0), vdup_n_s64(b))
    );
}

#endif

#if 0 // _LEAVE_ARM_SEQQ
}
#endif

#if 0 // _ENTER_ARM_CATL
{
#endif

INLINE(uint64x1_t,WYU_CATL) (float a, float b)
{
#define     WYU_CATL(A, B) ((DWRD_VTYPE){.L.W0.F=A,.R.W0.F=B}).D.U
    return  WYU_CATL(a, b);
}

INLINE(uint8x8_t,WBU_CATL) (float a, float b)
{
#define     WBU_CATL(p, q) ((DWRD_VTYPE){.L.W0.F=p, .R.W0.F=q}).B.U
    return  WBU_CATL(a, b);
}

INLINE(int8x8_t,WBI_CATL) (float a, float b)
{
#define     WBI_CATL(p, q) ((DWRD_VTYPE){.L.W0.F=p,.R.W0.F=q}).B.I
    return  WBI_CATL(a, b);
}

#if CHAR_MIN
INLINE(int8x8_t,WBC_CATL) (float a, float b)
{
#   define  WBC_CATL(p, q) ((DWRD_VTYPE){.L.W0.F=p,.R.W0.F=q}).B.I
    return  WBC_CATL(a, b);
}
#else
INLINE(uint8x8_t,WBC_CATL) (float a, float b)
{
#   define  WBC_CATL(p, q) ((DWRD_VTYPE){.L.W0.F=p,.R.W0.F=q}).B.U
    return  WBC_CATL(a, b);
}

#endif

INLINE(uint16x4_t,WHU_CATL) (float a, float b)
{
#define     WHU_CATL(A, B) ((DWRD_VTYPE){.L.W0.F=A,.R.W0.F=B}).H.U
    return  WHU_CATL(a, b);
}

INLINE(int16x4_t,WHI_CATL) (float a, float b)
{
#define     WHI_CATL(A, B) ((DWRD_VTYPE){.L.W0.F=A,.R.W0.F=B}).H.I
    return  WHI_CATL(a, b);
}

INLINE(float16x4_t,WHF_CATL) (float a, float b)
{
#define     WHF_CATL(A, B) ((DWRD_VTYPE){.L.W0.F=A,.R.W0.F=B}).H.F
    return  WHF_CATL(a, b);
}

INLINE(uint32x2_t,WWU_CATL) (float a, float b)
{
#define     WWU_CATL(A, B) ((DWRD_VTYPE){.L.W0.F=A,.R.W0.F=B}).W.U
    return  WWU_CATL(a, b);
}

INLINE(int32x2_t,WWI_CATL) (float a, float b)
{
#define     WWI_CATL(A, B) ((DWRD_VTYPE){.L.W0.F=A,.R.W0.F=B}).W.I
    return  WWI_CATL(a, b);
}

INLINE(float32x2_t,WWF_CATL) (float a, float b)
{
#define     WWF_CATL(A, B) ((DWRD_VTYPE){.L.W0.F=A,.R.W0.F=B}).W.F
    return  WWF_CATL(a, b);
}

#define     DYU_CATL    vcombine_u64
#define     DBU_CATL    vcombine_u8
#define     DBI_CATL    vcombine_s8
#if CHAR_MIN
#   define  DBC_CATL    vcombine_s8
#else
#   define  DBC_CATL    vcombine_u8
#endif

#define     DHU_CATL    vcombine_u16
#define     DHI_CATL    vcombine_s16
#define     DHF_CATL    vcombine_f16

#define     DWU_CATL    vcombine_u32
#define     DWI_CATL    vcombine_s32
#define     DWF_CATL    vcombine_f32

#define     DDU_CATL    vcombine_u64
#define     DDI_CATL    vcombine_s64
#define     DDF_CATL    vcombine_f64

INLINE(Vdyu,VWYU_CATL) (Vwyu a, Vwyu b)
{
#define     VWYU_CATL(A, B) ((Vdyu){WYU_CATL(A.V0,B.V0)})
    return  VWYU_CATL(a, b);
}

INLINE(Vdbu,VWBU_CATL) (Vwbu a, Vwbu b)
{
#define     VWBU_CATL(A, B)  WBU_CATL(A.V0,B.V0)
    return  VWBU_CATL(a, b);
}

INLINE(Vdbi,VWBI_CATL) (Vwbi a, Vwbi b)
{
#define     VWBI_CATL(A, B)  WBI_CATL(A.V0,B.V0)
    return  VWBI_CATL(a, b);
}

INLINE(Vdbc,VWBC_CATL) (Vwbc a, Vwbc b)
{
#define     VWBC_CATL(A, B)  ((Vdbc){WBI_CATL(A.V0,B.V0)})
    return  VWBC_CATL(a, b);
}

INLINE(Vdhu,VWHU_CATL) (Vwhu a, Vwhu b)
{
#define     VWHU_CATL(A, B) WHU_CATL(A.V0,B.V0)
    return  VWHU_CATL(a, b);
}

INLINE(Vdhi,VWHI_CATL) (Vwhi a, Vwhi b)
{
#define     VWHI_CATL(A, B) WHI_CATL(A.V0,B.V0)
    return  VWHI_CATL(a, b);
}

INLINE(Vdhf,VWHF_CATL) (Vwhf a, Vwhf b)
{
#define     VWHF_CATL(A, B) WHF_CATL(A.V0,B.V0)
    return  VWHF_CATL(a, b);
}

INLINE(Vdwu,VWWU_CATL) (Vwwu a, Vwwu b)
{
#define     VWWU_CATL(A, B) WWU_CATL(A.V0,B.V0)
    return  VWWU_CATL(a, b);
}

INLINE(Vdwi,VWWI_CATL) (Vwwi a, Vwwi b)
{
#define     VWWI_CATL(A, B) WWI_CATL(A.V0,B.V0)
    return  VWWI_CATL(a, b);
}

INLINE(Vdwf,VWWF_CATL) (Vwwf a, Vwwf b)
{
#define     VWWF_CATL(A, B) WWF_CATL(A.V0,B.V0)
    return  VWWF_CATL(a, b);
}

INLINE(Vqyu,VDYU_CATL) (Vdyu a, Vdyu b)
{
#define     VDYU_CATL(A, B) ((Vqyu){DYU_CATL(A.V0, B.V0)})
    return  VDYU_CATL(a, b);
}

INLINE(Vqbu,VDBU_CATL) (Vdbu a, Vdbu b) {return vcombine_u8(a, b);}
INLINE(Vqbi,VDBI_CATL) (Vdbi a, Vdbi b) {return vcombine_s8(a, b);}
INLINE(Vqbc,VDBC_CATL) (Vdbc a, Vdbc b) 
{
    return ((Vqbc){DBC_CATL(a.V0, b.V0)});
}

INLINE(Vqhu,VDHU_CATL) (Vdhu a, Vdhu b) {return vcombine_u16(a, b);}
INLINE(Vqhi,VDHI_CATL) (Vdhi a, Vdhi b) {return vcombine_s16(a, b);}
INLINE(Vqhi,VDHF_CATL) (Vdhf a, Vdhf b) {return vcombine_f16(a, b);}
INLINE(Vqwu,VDWU_CATL) (Vdwu a, Vdwu b) {return vcombine_u32(a, b);}
INLINE(Vqwi,VDWI_CATL) (Vdwi a, Vdwi b) {return vcombine_s32(a, b);}
INLINE(Vqwi,VDWF_CATL) (Vdwf a, Vdwf b) {return vcombine_f32(a, b);}
INLINE(Vqdu,VDDU_CATL) (Vddu a, Vddu b) {return vcombine_u64(a, b);}
INLINE(Vqdi,VDDI_CATL) (Vddi a, Vddi b) {return vcombine_s64(a, b);}
INLINE(Vqdi,VDDF_CATL) (Vddf a, Vddf b) {return vcombine_f64(a, b);}

#if 0 // _LEAVE_ARM_CATL
}
#endif

#if 0 // _ENTER_ARM_CATR
{
#endif

INLINE(uint64x1_t,WYU_CATR) (float l, float r)
{
    float32x2_t m = VDWF_VOID;
    m = vset_lane_f32(l, m, 0);
    m = vset_lane_f32(r, m, 1);
    uint8x8_t   x = vreinterpret_u8_f32(m);
    x = vrbit_u8(x);
    x = vrev64_u8(x);
    return  vreinterpret_u64_u8(x);
}

INLINE(uint8x8_t,WBU_CATR) (float l, float r)
{
    float32x2_t m = VDWF_VOID;
    m = vset_lane_f32(l, m, 0);
    m = vset_lane_f32(r, m, 1);
    uint8x8_t   x = vreinterpret_u8_f32(m);
    return  vrev64_u8(x);
}

#define     WBI_CATR(L, R) vreinterpret_s8_u8(WBU_CATR(L,R))
#define     WBC_CATR(L, R) VDBU_ASBC(WBU_CATR(L,R))

INLINE(uint16x4_t,WHU_CATR) (float l, float r)
{
    float32x2_t m = VDWF_VOID;
    m = vset_lane_f32(l, m, 0);
    m = vset_lane_f32(r, m, 1);
    uint16x4_t   x = vreinterpret_u16_f32(m);
    return  vrev64_u16(x);
}

#define     WHI_CATR(L, R) vreinterpret_s16_u16(WBU_CATR(L,R))
#define     WHF_CATR(L, R) vreinterpret_f16_u16(WBU_CATR(L,R))

INLINE(uint32x2_t,WWU_CATR) (float l, float r)
{
    float32x2_t m = VDWF_VOID;
    m = vset_lane_f32(l, m, 1);
    m = vset_lane_f32(r, m, 0);
    return  vreinterpret_u32_f32(m);
}

#define     WWI_CATR(L, R) vreinterpret_s32_u32(WBU_CATR(L,R))
#define     WWF_CATR(L, R) vreinterpret_f32_u32(WBU_CATR(L,R))


INLINE(uint64x2_t,DYU_CATR) (uint64x1_t l, uint64x1_t r)
{
    uint8x8_t   a = vreinterpret_u8_u64(l);
    uint8x8_t   b = vreinterpret_u8_u64(r);
    uint8x16_t  c = vcombine_u8(
        vrev64_u8(b),
        vrev64_u8(a)
    );
    c = vrbitq_u8(c);
    return  vreinterpretq_u64_u8(c);
}

#define     DBU_CATR(L, R)  vcombine_u8( vrev64_u8(R), vrev64_u8(L))
#define     DBI_CATR(L, R)  vcombine_s8( vrev64_s8(R), vrev64_s8(L))
#if CHAR_MIN
#   define  DBC_CATR    DBI_CATR
#else
#   define  DBC_CATR    DBU_CATR
#endif

#define     DHU_CATR(L, R)  vcombine_u16(vrev64_u16(R),vrev64_u16(L))
#define     DHI_CATR(L, R)  vcombine_s16(vrev64_s16(R),vrev64_s16(L))
#define     DHF_CATR(L, R)      \
vreinterpretq_f16_u16(          \
    DHU_CATR(                   \
        vreinterpret_u16_f16(L),\
        vreinterpret_u16_f16(R) \
    )                           \
)

#define     DWU_CATR(L, R)  vcombine_u32(vrev64_u32(R),vrev64_u32(L))
#define     DWI_CATR(L, R)  vcombine_s32(vrev64_s32(R),vrev64_s32(L))
#define     DWF_CATR(L, R)  vcombine_f32(vrev64_f32(R),vrev64_f32(L))
#define     DDU_CATR(L, R)  vcombine_u64(R,L)
#define     DDI_CATR(L, R)  vcombine_s64(R,L)
#define     DDF_CATR(L, R)  vcombine_f64(R,L)

INLINE(Vdyu,VWYU_CATR) (Vwyu l, Vwyu r)
{
#define     VWYU_CATR(L, R) DYU_ASTV(WYU_CATR(VWYU_ASTM(L),VWYU_ASTM(R)))
    return  VWYU_CATR(l, r);
}

INLINE(Vdbu,VWBU_CATR) (Vwbu l, Vwbu r)
{
#define     VWBU_CATR(L, R) WBU_CATR(VWBU_ASTM(L),VWBU_ASTM(R))
    return  VWBU_CATR(l, r);
}

INLINE(Vdbi,VWBI_CATR) (Vwbi l, Vwbi r)
{
#define     VWBI_CATR(L, R) VDBU_ASBI(WBU_CATR(VWBI_ASTM(L),VWBI_ASTM(R)))
    return  VWBI_CATR(l, r);
}

INLINE(Vdbc,VWBC_CATR) (Vwbc l, Vwbc r)
{
#define     VWBC_CATR(L, R) VDBU_ASBC(WBU_CATR(VWBC_ASTM(L),VWBC_ASTM(R)))
    return  VWBC_CATR(l, r);
}


INLINE(Vdhu,VWHU_CATR) (Vwhu l, Vwhu r)
{
#define     VWHU_CATR(L, R) WHU_CATR(VWHU_ASTM(L),VWHU_ASTM(R))
    return  VWHU_CATR(l, r);
}

INLINE(Vdhi,VWHI_CATR) (Vwhi l, Vwhi r)
{
#define     VWHI_CATR(L, R) VDHU_ASHI(WHU_CATR(VWHI_ASTM(L),VWHI_ASTM(R)))
    return  VWHI_CATR(l, r);
}

INLINE(Vdhf,VWHF_CATR) (Vwhf l, Vwhf r)
{
#define     VWHF_CATR(L, R) VDHU_ASHF(WHU_CATR(VWHF_ASTM(L),VWHF_ASTM(R)))
    return  VWHF_CATR(l, r);
}


INLINE(Vdwu,VWWU_CATR) (Vwwu l, Vwwu r)
{
#define     VWWU_CATR(L, R) WWU_CATR(VWWU_ASTM(L),VWWU_ASTM(R))
    return  VWWU_CATR(l, r);
}

INLINE(Vdwi,VWWI_CATR) (Vwwi l, Vwwi r)
{
#define     VWWI_CATR(L, R) VDWU_ASWI(WWU_CATR(VWWI_ASTM(L),VWWI_ASTM(R)))
    return  VWWI_CATR(l, r);
}

INLINE(Vdwf,VWWF_CATR) (Vwwf l, Vwwf r)
{
#define     VWWF_CATR(L, R) VDWU_ASWF(WWU_CATR(VWWF_ASTM(L),VWWF_ASTM(R)))
    return  VWWF_CATR(l, r);
}

INLINE(Vqyu,VDYU_CATR) (Vdyu l, Vdyu r)
{
#define     VDYU_CATR(L, R) QYU_ASTV(DYU_CATR(VDYU_ASTM(L),VDYU_ASTM(R)))
    return  VDYU_CATR(l, r);
}

INLINE(Vqbu,VDBU_CATR) (Vdbu l, Vdbu r)
{
#define     VDBU_CATR(L, R) DBU_CATR(L,R)
    return  VDBU_CATR(l, r);
}

INLINE(Vqbi,VDBI_CATR) (Vdbi l, Vdbi r)
{
#define     VDBI_CATR(L, R) DBI_CATR(L,R)
    return  VDBI_CATR(l, r);
}

INLINE(Vqbc,VDBC_CATR) (Vdbc l, Vdbc r)
{
#define     VDBC_CATR(L, R) QBC_ASTV(DBC_CATR(VDBC_ASTM(L),VDBC_ASTM(R)))
    return  VDBC_CATR(l, r);
}

INLINE(Vqhu,VDHU_CATR) (Vdhu l, Vdhu r)
{
#define     VDHU_CATR(L, R) DHU_CATR(L,R)
    return  VDHU_CATR(l, r);
}

INLINE(Vqhi,VDHI_CATR) (Vdhi l, Vdhi r)
{
#define     VDHI_CATR(L, R) DHI_CATR(L,R)
    return  VDHI_CATR(l, r);
}

INLINE(Vqhf,VDHF_CATR) (Vdhf l, Vdhf r)
{
#define     VDHF_CATR(L, R) DHF_CATR(L,R)
    return  VDHF_CATR(l, r);
}

INLINE(Vqwu,VDWU_CATR) (Vdwu l, Vdwu r)
{
#define     VDWU_CATR(L, R) DWU_CATR(L,R)
    return  VDWU_CATR(l, r);
}

INLINE(Vqwi,VDWI_CATR) (Vdwi l, Vdwi r)
{
#define     VDWI_CATR(L, R) DWI_CATR(L,R)
    return  VDWI_CATR(l, r);
}

INLINE(Vqwf,VDWF_CATR) (Vdwf l, Vdwf r)
{
#define     VDWF_CATR(L, R) DWF_CATR(L,R)
    return  VDWF_CATR(l, r);
}

INLINE(Vqdu,VDDU_CATR) (Vddu l, Vddu r)
{
#define     VDDU_CATR(L, R) DDU_CATR(L,R)
    return  VDDU_CATR(l, r);
}

INLINE(Vqdi,VDDI_CATR) (Vddi l, Vddi r)
{
#define     VDDI_CATR(L, R) DDI_CATR(L,R)
    return  VDDI_CATR(l, r);
}

INLINE(Vqdf,VDDF_CATR) (Vddf l, Vddf r)
{
#define     VDDF_CATR(L, R) DDF_CATR(L,R)
    return  VDDF_CATR(l, r);
}

#if 0 // _LEAVE_ARM_CATR
}
#endif

#if 0 // _ENTER_ARM_GETL
{
#endif

#define     DWF_GETL(x) vget_lane_f32(x,0)

INLINE(Vwyu,VDYU_GETL) (Vdyu x) 
{
    return ((Vwyu){DWF_GETL(vreinterpret_f32_u64(x.V0))});
}

INLINE(Vwbu,VDBU_GETL) (Vdbu x) {return ((Vwbu){DWF_GETL(VDBU_ASWF(x))});}
INLINE(Vwbi,VDBI_GETL) (Vdbi x) {return ((Vwbi){DWF_GETL(VDBI_ASWF(x))});}
INLINE(Vwbc,VDBC_GETL) (Vdbc x) {return ((Vwbc){DWF_GETL(VDBC_ASWF(x))});}

INLINE(Vwhu,VDHU_GETL) (Vdhu x) {return ((Vwhu){DWF_GETL(VDHU_ASWF(x))});}
INLINE(Vwhi,VDHI_GETL) (Vdhi x) {return ((Vwhi){DWF_GETL(VDHI_ASWF(x))});}
INLINE(Vwhf,VDHF_GETL) (Vdhf x) {return ((Vwhf){DWF_GETL(VDHF_ASWF(x))});}

INLINE(Vwwu,VDWU_GETL) (Vdwu x) {return ((Vwwu){DWF_GETL(VDWU_ASWF(x))});}
INLINE(Vwwi,VDWI_GETL) (Vdwi x) {return ((Vwwi){DWF_GETL(VDWI_ASWF(x))});}
INLINE(Vwwf,VDWF_GETL) (Vdwf x) {return ((Vwwf){DWF_GETL(x)});}

INLINE(Vdyu,VQYU_GETL) (Vqyu x)
{
    return ((Vdyu){vget_low_u64(x.V0)});
}

INLINE(Vdbu,VQBU_GETL) (Vqbu x) {return vget_low_u8(x);}
INLINE(Vdbi,VQBI_GETL) (Vqbi x) {return vget_low_s8(x);}
INLINE(Vdbc,VQBC_GETL) (Vqbc x)
{
#if CHAR_MIN
    return  ((Vdbc){vget_low_s8(x.V0)});
#else
    return  ((Vdbc){vget_low_u8(x.V0)});
#endif
}

INLINE(Vdhu,VQHU_GETL) (Vqhu x) {return vget_low_u16(x);}
INLINE(Vdhi,VQHI_GETL) (Vqhi x) {return vget_low_s16(x);}
INLINE(Vdhf,VQHF_GETL) (Vqhf x) {return vget_low_f16(x);}
INLINE(Vdwu,VQWU_GETL) (Vqwu x) {return vget_low_u32(x);}
INLINE(Vdwi,VQWI_GETL) (Vqwi x) {return vget_low_s32(x);}
INLINE(Vdwf,VQWF_GETL) (Vqwf x) {return vget_low_f32(x);}
INLINE(Vddu,VQDU_GETL) (Vqdu x) {return vget_low_u64(x);}
INLINE(Vddi,VQDI_GETL) (Vqdi x) {return vget_low_s64(x);}
INLINE(Vddf,VQDF_GETL) (Vqdf x) {return vget_low_f64(x);}

#if 0 // _LEAVE_ARM_GETL
}
#endif

#if 0 // _ENTER_ARM_GETR
{
#endif

#define     DWF_GETR(X)     vget_lane_f32(X,1)

INLINE(Vwyu,VDYU_GETR) (Vdyu x)
{
#define     VDYU_GETR(X) WYU_ASTV(DWF_GETR(VDYU_ASWF(X)))
    return  VDYU_GETR(x);
}

INLINE(Vwbu,VDBU_GETR) (Vdbu x)
{
#define     VDBU_GETR(X) WBU_ASTV(DWF_GETR(VDBU_ASWF(X)))
    return  VDBU_GETR(x);
}

INLINE(Vwbi,VDBI_GETR) (Vdbi x)
{
#define     VDBI_GETR(X) WBI_ASTV(DWF_GETR(VDBI_ASWF(X)))
    return  VDBI_GETR(x);
}

INLINE(Vwbc,VDBC_GETR) (Vdbc x)
{
#define     VDBC_GETR(X) WBC_ASTV(DWF_GETR(VDBC_ASWF(X)))
    return  VDBC_GETR(x);
}

INLINE(Vwhu,VDHU_GETR) (Vdhu x)
{
#define     VDHU_GETR(X) WHU_ASTV(DWF_GETR(VDHU_ASWF(X)))
    return  VDHU_GETR(x);
}

INLINE(Vwhi,VDHI_GETR) (Vdhi x)
{
#define     VDHI_GETR(X) WHI_ASTV(DWF_GETR(VDHI_ASWF(X)))
    return  VDHI_GETR(x);
}

INLINE(Vwhf,VDHF_GETR) (Vdhf x)
{
#define     VDHF_GETR(X) WHF_ASTV(DWF_GETR(VDHF_ASWF(X)))
    return  VDHF_GETR(x);
}

INLINE(Vwwu,VDWU_GETR) (Vdwu x)
{
#define     VDWU_GETR(X) WWU_ASTV(DWF_GETR(VDWU_ASWF(X)))
    return  VDWU_GETR(x);
}

INLINE(Vwwi,VDWI_GETR) (Vdwi x)
{
#define     VDWI_GETR(X) WWI_ASTV(DWF_GETR(VDWI_ASWF(X)))
    return  VDWI_GETR(x);
}

INLINE(Vwwf,VDWF_GETR) (Vdwf x)
{
#define     VDWF_GETR(X) WWF_ASTV(DWF_GETR(X))
    return  VDWF_GETR(x);
}

INLINE(Vdyu,VQYU_GETR) (Vqyu x)
{
#define     VQYU_GETR(X)    VDDU_ASYU(vget_high_u64(VQYU_ASDU(X)))
    return  VQYU_GETR(x);
}

INLINE(Vdbu,VQBU_GETR) (Vqbu x) {return vget_high_u8(x);}
INLINE(Vdbi,VQBI_GETR) (Vqbi x) {return vget_high_s8(x);}
INLINE(Vdbc,VQBC_GETR) (Vqbc x)
{
#if CHAR_MIN
#   define  VQBC_GETR(X) DBC_ASTV(vget_high_s8(VQBC_ASTM(X)))
#else
#   define  VQBC_GETR(X) DBC_ASTV(vget_high_u8(VQBC_ASTM(X)))
#endif
    return  VQBC_GETR(x);
}

INLINE(Vdhu,VQHU_GETR) (Vqhu x) {return vget_high_u16(x);}
INLINE(Vdhi,VQHI_GETR) (Vqhi x) {return vget_high_s16(x);}
INLINE(Vdhf,VQHF_GETR) (Vqhf x) {return vget_high_f16(x);}

INLINE(Vdwu,VQWU_GETR) (Vqwu x) {return vget_high_u32(x);}
INLINE(Vdwi,VQWI_GETR) (Vqwi x) {return vget_high_s32(x);}
INLINE(Vdwf,VQWF_GETR) (Vqwf x) {return vget_high_f32(x);}

INLINE(Vddu,VQDU_GETR) (Vqdu x) {return vget_high_u64(x);}
INLINE(Vddi,VQDI_GETR) (Vqdi x) {return vget_high_s64(x);}
INLINE(Vddf,VQDF_GETR) (Vqdf x) {return vget_high_f64(x);}

#if 0 // _LEAVE_ARM_GETR
}
#endif

#if 0 // _ENTER_ARM_GET1
{
#endif

INLINE(  _Bool, VWYU_GET1) (Vwyu a, Rc(0, 31) b)
{
#define     WYU_GET1(A, B) ((_Bool)(1&(FLT_ASTG(A).U>>(B))))
#define     VWYU_GET1(A, B)   WYU_GET1(VWYU_ASTM(A), B)
    return  VWYU_GET1(a, b);
}


INLINE( uint8_t,VWBU_GET1) (Vwbu a, Rc(0, 3) b)
{
#define     VWBU_GET1(A, B) \
vget_lane_u8(vreinterpret_u8_f32(vdup_n_f32(VWBU_ASTM(A))),(3&B))
    float       m = VWBU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   t = vreinterpret_u8_f32(v);
    t = vtbl1_u8(t, vdup_n_u8((3&b)));
    return  vget_lane_u8(t, 0);
}

INLINE(  int8_t,VWBI_GET1) (Vwbi a, Rc(0, 3) b)
{
#define     VWBI_GET1(A, B) \
vget_lane_s8(vreinterpret_s8_f32(vdup_n_f32(VWBI_ASTM(A))),(3&B))
    float       m = VWBI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int8x8_t    t = vreinterpret_s8_f32(v);
    t = vtbl1_s8(t, vdup_n_u8((3&b)));
    return  vget_lane_s8(t, 0);
}

INLINE(    char,VWBC_GET1) (Vwbc a, Rc(0, 3) b)
{
#if CHAR_MIN
#   define  VWBC_GET1(A, B) ((char) VWBI_GET1(VWBC_ASBI(A),B))
    return (VWBI_GET1)(VWBC_ASBI(a), b);
#else
#   define  VWBC_GET1(A, B) ((char) VWBU_GET1(VWBC_ASBU(A),B))
    return (VWBU_GET1)(VWBC_ASBU(a), b);
#endif
}


INLINE(uint16_t,VWHU_GET1) (Vwhu a, Rc(0, 1) b)
{
#define     WHU_GET1(A, B)  \
vget_lane_u16(vreinterpret_u16_f32(vdup_n_f32(A)),(1&B))

#define     VWHU_GET1(A, B)     WHU_GET1(VWHU_ASTM(A),B)
    float       m = VWHU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint32x2_t  t = vreinterpret_u32_f32(v);
    int32x2_t   n = vdup_n_s32((1&b));
    n = vshl_n_s32(n, 4);
    n = vneg_s32(n);
    t = vshl_u32(t, n);
    return  vget_lane_u32(t, 0);
}

INLINE( int16_t,VWHI_GET1) (Vwhi a, Rc(0, 1) b)
{
#define     WHI_GET1(A, B)  \
vget_lane_s16(vreinterpret_s16_f32(vdup_n_f32(A)),(1&B))

#define     VWHI_GET1(A, B)     WHI_GET1(VWHI_ASTM(A),B)
    float       m = VWHI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint32x2_t  t = vreinterpret_u32_f32(v);
    int32x2_t   n = vdup_n_s32((1&b));
    n = vshl_n_s32(n, 4);
    n = vneg_s32(n);
    t = vshl_u32(t, n);
    return  vget_lane_u32(t, 0);
}

INLINE( flt16_t,VWHF_GET1) (Vwhf a, Rc(0, 1) b)
{
#define     WHF_GET1(A, B)  \
vget_lane_f16(vreinterpret_f16_f32(vdup_n_f32(A)),(1&B))

#define     VWHF_GET1(A, b)     WHF_GET1(VWHF_ASTM(A), B)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    return  b
        ?   vget_lane_f16(v, 1)
        :   vget_lane_f16(v, 0);
}


INLINE(   _Bool,VDYU_GET1) (Vdyu a, Rc(0, 63) b)
{
#define      DYU_GET1(A, B)                 \
(                                           \
    (_Bool)                                 \
    (1&(vget_lane_u64((A),0)>>((B)%64)))    \
)

#define     VDYU_GET1(A, B)   DYU_GET1(VDYU_ASDU(A),B)
    return  VDYU_GET1(a, b);
}


INLINE( uint8_t,VDBU_GET1) (Vdbu a, Rc(0, 7) b)
{
#define     VDBU_GET1(A, B) vget_lane_u8(A,(7&B))
    return  vget_lane_u64(VDBU_ASDU(a), 0)>>(b*8);
}

INLINE(  int8_t,VDBI_GET1) (Vdbi a, Rc(0, 7) b)
{
#define     VDBI_GET1(A, B) vget_lane_s8(A,(7&B))
    return  vget_lane_u64(VDBI_ASDU(a), 0)>>(b*8);
}

INLINE(    char,VDBC_GET1) (Vdbc a, Rc(0, 7) b)
{
#if CHAR_MIN
#   define  VDBC_GET1(A, B) ((char) vget_lane_s8(VDBC_ASBI(A),B))
#else
#   define  VDBC_GET1(A, B) ((char) vget_lane_u8(VDBC_ASBU(A),B))
#endif
    return  vget_lane_u64(VDBC_ASDU(a), 0)>>(8*b);
}


INLINE(uint16_t,VDHU_GET1) (Vdhu a, Rc(0, 3) b)
{
#define     VDHU_GET1(A, B) vget_lane_u16(A,(3&B))
    return  vget_lane_u64(VDHU_ASDU(a),0)>>(16*b);
}

INLINE( int16_t,VDHI_GET1) (Vdhi a, Rc(0, 3) b)
{
#define     VDHI_GET1(A, B) vget_lane_s16(A,(3&B))
    return  vget_lane_u64(VDHI_ASDU(a),0)>>(16*b);
}

INLINE( flt16_t,VDHF_GET1) (Vdhf a, Rc(0, 3) b)
{
#define     VDHF_GET1(A, B) vget_lane_f16(A,(3&B))
    uint64x1_t t = vreinterpret_u64_f16(a);
    t = vshl_u64(t, vdup_n_s64((-16*b)));
    return  vget_lane_f16(VDDU_ASHF(t), 0);
}


INLINE(uint32_t,VDWU_GET1) (Vdwu a, Rc(0, 1) b)
{
#define     VDWU_GET1(A, B) vget_lane_u32(A,(1&B))
    return  vget_lane_u64(VDWU_ASDU(a),0)>>(32*b);
}

INLINE( int32_t,VDWI_GET1) (Vdwi a, Rc(0, 1) b)
{
#define     VDWI_GET1(A, B) vget_lane_s32(A,(1&B))
    return  vget_lane_u64(VDWI_ASDU(a),0)>>(32*b);
}

INLINE(   float,VDWF_GET1) (Vdwf a, Rc(0, 1) b)
{
#define     VDWF_GET1(A, B) vget_lane_f32(A,(1&B))
    uint64x1_t t = vreinterpret_u64_f32(a);
    t = vshl_u64(t, vdup_n_s64((-32*b)));
    return  vget_lane_f32(VDDU_ASWF(t), 0);
}


INLINE(   _Bool,VQYU_GET1) (Vqyu a, Rc(0, 127) b)
{
#define     QYU_GET1(A, B) \
vget_lane_u64(\
    vand_u64(\
        (\
            (B ==  0) ? vget_low_u64(A) :\
            (B == 64) ? vget_high_u64(A) :\
            vshr_n_u64(\
                vreinterpret_u64_u8(\
                    vqtbl1_u8(\
                        vreinterpretq_u8_u64(A),\
                        vadd_u8(\
                            vcreate_u8(0x0706050403020100ull)\
                            vdup_n_u8(((63<B)<<3))\
                        )\
                    )\
                ),\
                ((63&B)+((63&B)==0))\
            )\
        ),\
        vdup_n_u64(1)\
    ),\
    1\
)

#define VQYU_GET1(A, B) ((_Bool) QYU_GET1(VQYU_ASDU(A),B))
    uint8x16_t  t = VQYU_ASBU(a);
    uint8x8_t   i = vcreate_u8(0x0706050403020100ull);
    uint8x8_t   j = vdup_n_u8(b);
    uint8x8_t   k = vshr_n_u8(j, 6); // k = {0}*8 or {1}*8
    uint8x8_t   n = vshl_n_u8(k, 3); // n = {0}*8 or {8}*8
    i = vadd_u8(i, n);
    i = vqtbl1_u8(t, i);
    uint64x1_t  v = vreinterpret_u64_u8(i);
    int64x1_t   r = vdup_n_s64(b);
    r = vand_s64(r, vdup_n_s64(63));
    r = vneg_s64(r);
    v = vshl_u64(v, r);
    v = vand_u64(v, vdup_n_u64(1));
    return  vget_lane_u64(v, 0);
}

INLINE( uint8_t,VQBU_GET1) (Vqbu v, Rc(0, 15) k)
{
#define     VQBU_GET1(V, K)     vgetq_lane_u8(V,(15&K))
    return  vget_lane_u8(vqtbl1_u8(v, vdup_n_u8(k)), 0);
}

INLINE(  int8_t,VQBI_GET1) (Vqbi v, Rc(0, 15) k)
{
#define     VQBI_GET1(V, K)     vgetq_lane_s8(V,(15&K))
    return  vget_lane_s8(vqtbl1_s8(v, vdup_n_u8(k)), 0);
}

INLINE(    char,VQBC_GET1) (Vqbc v, Rc(0, 15) k)
{
#if CHAR_MIN
#   define  VQBC_GET1(V, K) ((char) vgetq_lane_s8(VQBC_ASBI(V),(15&K)))
    return  vget_lane_s8(vqtbl1_s8(VQBC_ASBI(v),vdup_n_u8(k)),0);
#else
#   define  VQBC_GET1(V, K) ((char) vgetq_lane_u8(VQBC_ASBU(V),(15&K)))
    return  vget_lane_u8(vqtbl1_u8(VQBC_ASBU(v),vdup_n_u8(k)),0);
#endif
}


INLINE(uint16_t,VQHU_GET1) (Vqhu v, Rc(0, 7) k)
{
#define     VQHU_GET1(V, K)     vgetq_lane_u16(V,(7&K))
    uint8x16_t  t = vreinterpretq_u8_u16(v);
    uint8x8_t   i = vdup_n_u8((2*k));
    i = vadd_u8(i, vcreate_u8(0x0100ull));
    i = vqtbl1_u8(t, i);
    uint16x4_t  m = vreinterpret_u16_u8(i);
    return  vget_lane_u16(m, 0);
}

INLINE( int16_t,VQHI_GET1) (Vqhi v, Rc(0, 7) k)
{
#define     VQHI_GET1(V, K)     vgetq_lane_s16(V,(7&K))
    uint8x16_t  t = vreinterpretq_u8_s16(v);
    uint8x8_t   i = vdup_n_u8((2*k));
    i = vadd_u8(i, vcreate_u8(0x0100ull));
    i = vqtbl1_u8(t, i);
    int16x4_t   m = vreinterpret_u16_u8(i);
    return  vget_lane_s16(m, 0);
}

INLINE( flt16_t,VQHF_GET1) (Vqhf v, Rc(0, 7) k)
{
#define     VQHF_GET1(V, K)     vgetq_lane_f16(V,(7&K))
    uint8x16_t  t = vreinterpretq_u8_f16(v);
    uint8x8_t   i = vdup_n_u8((2*k));
    i = vadd_u8(i, vcreate_u8(0x0100ull));
    i = vqtbl1_u8(t, i);
    float16x4_t m = vreinterpret_u16_u8(i);
    return  vget_lane_f16(m, 0);
}


INLINE(uint32_t,VQWU_GET1) (Vqwu v, Rc(0, 3) k)
{
#define     VQWU_GET1(A, B)     vgetq_lane_u32(A,(3&B))
    uint8x16_t  t = vreinterpretq_u8_s32(v);
    uint8x8_t   i = vdup_n_u8((4*k));
    i = vadd_u8(i, vcreate_u8(0x03020100ull));
    i = vqtbl1_u8(t, i);
    uint32x2_t  m = vreinterpret_u32_u8(i);
    return  vget_lane_u32(m, 0);
}

INLINE( int32_t,VQWI_GET1) (Vqwi v, Rc(0, 3) k)
{
#define     VQWI_GET1(V, K)     vgetq_lane_s32(V,(3&K))
    uint8x16_t  t = vreinterpretq_u8_s32(v);
    uint8x8_t   i = vdup_n_u8((4*k));
    i = vadd_u8(i, vcreate_u8(0x03020100ull));
    i = vqtbl1_u8(t, i);
    int32x2_t   m = vreinterpret_s32_u8(i);
    return  vget_lane_s32(m, 0);
}

INLINE(   float,VQWF_GET1) (Vqwf v, Rc(0, 3) k)
{
#define     VQWF_GET1(V, K)     vgetq_lane_f32(V,(3&K))
    uint8x16_t  t = vreinterpretq_u8_f32(v);
    uint8x8_t   i = vdup_n_u8((4*k));
    i = vadd_u8(i, vcreate_u8(0x03020100ull));
    i = vqtbl1_u8(t, i);
    float32x2_t m = vreinterpret_f32_u8(i);
    return  vget_lane_f32(m, 0);
}


INLINE(uint64_t,VQDU_GET1) (Vqdu v, Rc(0, 1) k)
{
#define     VQDU_GET1(V, K)     vgetq_lane_u64(V,(1&K))
    uint8x16_t  t = vreinterpretq_u8_u64(v);
    uint8x8_t   i = vdup_n_u8((8*k));
    i = vadd_u8(i, vcreate_u8(0x0706050403020100ull));
    i = vqtbl1_u8(t, i);
    uint64x1_t  m = vreinterpret_u64_u8(i);
    return  vget_lane_u64(m, 0);
}

INLINE( int64_t,VQDI_GET1) (Vqdi v, Rc(0, 1) k)
{
#define     VQDI_GET1(V, K)     vgetq_lane_s64(V,(1&K))
    uint8x16_t  t = vreinterpretq_u8_s64(v);
    uint8x8_t   i = vdup_n_u8((8*k));
    i = vadd_u8(i, vcreate_u8(0x0706050403020100ull));
    i = vqtbl1_u8(t, i);
    int64x1_t   m = vreinterpret_s64_u8(i);
    return  vget_lane_s64(m, 0);
}

INLINE(  double,VQDF_GET1) (Vqdf v, Rc(0, 1) k)
{
#define     VQDF_GET1(V, K)     vgetq_lane_f64(V,(1&K))
    uint8x16_t  t = vreinterpretq_u8_f64(v);
    uint8x8_t   i = vdup_n_u8((8*k));
    i = vadd_u8(i, vcreate_u8(0x0706050403020100ull));
    i = vqtbl1_u8(t, i);
    float64x1_t m = vreinterpret_f64_u8(i);
    return  vget_lane_f64(m, 0);
}

#if 0 // _LEAVE_ARM_GET1
}
#endif

#if 0 // _ENTER_ARM_BFC1
{
#endif

INLINE( uchar, UCHAR_BFC1) 
(
    uchar                   a, 
    Rc(0, UCHAR_WIDTH-1)    b, 
    Rc(1, UCHAR_WIDTH)      c
)
{
#define     UCHAR_BFC1(A, B, C) \
((uchar) (A&~((UCHAR_MAX>>(UCHAR_WIDTH-C))<<B)))

    return  UCHAR_BFC1(a, b, c);
}

INLINE( schar, SCHAR_BFC1)
(
    schar                   a, 
    Rc(0, SCHAR_WIDTH-1)    b, 
    Rc(1, SCHAR_WIDTH)      c
)
{
#define     SCHAR_BFC1(A, B, C) \
((schar) (A&~((UCHAR_MAX>>(SCHAR_WIDTH-C))<<B)))

    return  SCHAR_BFC1(a, b, c);
}

INLINE(  char,  CHAR_BFC1)
(
    char                   a, 
    Rc(0, CHAR_WIDTH-1)    b, 
    Rc(1, CHAR_WIDTH)      c
)
{
#define     CHAR_BFC1(A, B, C) \
((char) (A&~((UCHAR_MAX>>(CHAR_WIDTH-C))<<B)))

    return  CHAR_BFC1(a, b, c);
}


INLINE(ushort, USHRT_BFC1) 
(
    ushort                  a, 
    Rc(0, USHRT_WIDTH-1)    b, 
    Rc(1, USHRT_WIDTH)      c
)
{
#define     USHRT_BFC1(A, B, C) \
((ushort) (A&~((USHRT_MAX>>(USHRT_WIDTH-C))<<B)))

    return  USHRT_BFC1(a, b, c);
}

INLINE( short,  SHRT_BFC1)
(
    short                   a, 
    Rc(0, SHRT_WIDTH-1)     b, 
    Rc(1, SHRT_WIDTH)       c
)
{
#define     SHRT_BFC1(A, B, C) \
((short) (A&~((USHRT_MAX>>(SHRT_WIDTH-C))<<B)))

    return  SHRT_BFC1(a, b, c);
}


INLINE(  uint,  UINT_BFC1) 
(
    uint                   a, 
    Rc(0, UINT_WIDTH-1)    b, 
    Rc(1, UINT_WIDTH)      c
)
{
#define     UINT_BFC1(A, B, C) \
((uint) (A&~((UINT_MAX>>(UINT_WIDTH-C))<<B)))

    return  UINT_BFC1(a, b, c);
}

INLINE(   int,   INT_BFC1)
(
    int                    a, 
    Rc(0, INT_WIDTH-1)     b, 
    Rc(1, INT_WIDTH)       c
)
{
#define     INT_BFC1(A, B, C) \
((int) (A&~((UINT_MAX>>(INT_WIDTH-C))<<B)))

    return  INT_BFC1(a, b, c);
}


INLINE( ulong, ULONG_BFC1) 
(
    ulong                   a, 
    Rc(0, ULONG_WIDTH-1)    b, 
    Rc(1, ULONG_WIDTH)      c
)
{
#define     ULONG_BFC1(A, B, C) \
((ulong) (A&~((ULONG_MAX>>(ULONG_WIDTH-C))<<B)))

    return  ULONG_BFC1(a, b, c);
}

INLINE(  long,  LONG_BFC1)
(
    long                    a, 
    Rc(0, LONG_WIDTH-1)     b, 
    Rc(1, LONG_WIDTH)       c
)
{
#define     LONG_BFC1(A, B, C) \
((long) (A&~((ULONG_MAX>>(LONG_WIDTH-C))<<B)))

    return  LONG_BFC1(a, b, c);
}


INLINE( ullong, ULLONG_BFC1) 
(
    ullong                  a, 
    Rc(0, ULLONG_WIDTH-1)   b, 
    Rc(1, ULLONG_WIDTH)     c
)
{
#define     ULLONG_BFC1(A, B, C) \
((ullong) (A&~((ULLONG_MAX>>(ULLONG_WIDTH-C))<<B)))

    return  ULLONG_BFC1(a, b, c);
}

INLINE(  llong,  LLONG_BFC1)
(
    llong                   a, 
    Rc(0, LLONG_WIDTH-1)    b, 
    Rc(1, LLONG_WIDTH)      c
)
{
#define     LLONG_BFC1(A, B, C) \
((llong) (A&~((ULLONG_MAX>>(LLONG_WIDTH-C))<<B)))

    return  LLONG_BFC1(a, b, c);
}


/*  TODO: limit bfc1_yu to single bit clear
*/
INLINE(float,WYU_BFC1)
(
    float       src,
    Rc(0, 31)   off,
    Rc(1, 32)   len
)
{
#define WYU_BFC1(SRC, OFF, LEN)                     \
(                                                   \
    (LEN >= 32)                                     \
    ?   0.0f                                        \
    :   vget_lane_f32(                              \
            vreinterpret_f32_u32(                   \
                vbic_u32(                           \
                    vreinterpret_u32_f32(           \
                        vdup_n_f32(SRC)             \
                    ),                              \
                    vshl_n_u32(                     \
                        vshr_n_u32(                 \
                            vdup_n_u32(UINT32_MAX), \
                            (32-LEN*(LEN<32))       \
                        ),                          \
                        (31&OFF)                    \
                    )                               \
                )                                   \
            ),                                      \
            0                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(src);
    uint32x2_t  a = vreinterpret_u32_f32(m);
    uint32x2_t  b = vdup_n_u32(UINT32_MAX);
    int32x2_t   c = vdup_n_s32((32-len));
    b = vshl_u32(b, vneg_s32(c));
    b = vshl_u32(b, vdup_n_s32(off));
    a = vbic_u32(a, b);
    m = vreinterpret_f32_u32(a);
    return vget_lane_f32(m, 0);
}

INLINE(Vwyu,VWYU_BFC1)
(
    Vwyu        src, 
    Rc(0, 31)   off,
    Rc(1, 32)   len
)
{
#define     VWYU_BFC1(A, B, C) WYU_ASTV(WYU_BFC1(VWYU_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWYU_ASTM(src), off, len);
    return WYU_ASTV(r);
}


INLINE(Vwbu,VWBU_BFC1)
(
    Vwbu        src, 
    Rc(0, 31)   off,
    Rc(1, 8)    len
)
{
#define     VWBU_BFC1(A, B, C) WBU_ASTV(WYU_BFC1(VWBU_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWBU_ASTM(src), off, len);
    return  WBU_ASTV(r);
}

INLINE(Vwbi,VWBI_BFC1)
(
    Vwbi        src, 
    Rc(0, 31)   off,
    Rc(1, 8)    len
)
{
#define     VWBI_BFC1(A, B, C) WBI_ASTV(WYU_BFC1(VWBI_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWBI_ASTM(src), off, len);
    return  WBI_ASTV(r);
}

INLINE(Vwbc,VWBC_BFC1)
(
    Vwbc        src, 
    Rc(0, 31)   off,
    Rc(1, 8)    len
)
{
#define     VWBC_BFC1(A, B, C) WBC_ASTV(WYU_BFC1(VWBC_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWBC_ASTM(src), off, len);
    return  WBC_ASTV(r);
}


INLINE(Vwhu,VWHU_BFC1)
(
    Vwhu        src, 
    Rc(0, 31)   off,
    Rc(1, 16)   len
)
{
#define     VWHU_BFC1(A, B, C) WHU_ASTV(WYU_BFC1(VWHU_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWHU_ASTM(src), off, len);
    return  WHU_ASTV(r);
}

INLINE(Vwhi,VWHI_BFC1)
(
    Vwhi        src, 
    Rc(0, 31)   off,
    Rc(1, 16)   len
)
{
#define     VWHI_BFC1(A, B, C) WHI_ASTV(WYU_BFC1(VWHI_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWHI_ASTM(src), off, len);
    return  WHI_ASTV(r);
}


INLINE(Vwwu,VWWU_BFC1)
(
    Vwwu        src, 
    Rc(0, 31)   off,
    Rc(1, 32)   len
)
{
#define     VWWU_BFC1(A, B, C) WWU_ASTV(WYU_BFC1(VWWU_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWWU_ASTM(src), off, len);
    return  WWU_ASTV(r);
}

INLINE(Vwwi,VWWI_BFC1)
(
    Vwwi        src, 
    Rc(0, 31)   off,
    Rc(1, 32)   len
)
{
#define     VWWI_BFC1(A, B, C) WWI_ASTV(WYU_BFC1(VWWI_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWWI_ASTM(src), off, len);
    return  WWI_ASTV(r);
}


INLINE(uint64x1_t, DDU_BFC1)
(
    uint64x1_t  src,
    Rc(0, 63)   off,
    Rc(1, 64)   len
)
{
#if 0
            vshl_n_u64(                             \
                vshr_n_u64(                         \
                    vdup_n_u64(UINT64_MAX),         \
                     (64-LEN*(LEN<64))              \
                ),                                  \
                OFF                                 \
            )                                       
#endif
#define DDU_BFC1(SRC, OFF, LEN)                     \
(                                                   \
    (LEN >= 64)                                     \
    ?   vdup_n_u64(0)                               \
    :   vbic_u64(                                   \
            SRC,                                    \
            vdup_n_u64(                             \
                ((UINT64_MAX>>(64-LEN))<<OFF)       \
            )                                       \
        )                                           \
)

    uint64x1_t  a = src;
    uint64x1_t  b = vdup_n_u64(UINT64_MAX);
    int64x1_t   c = vdup_n_s64((64-len));
    b = vshl_u64(b, vneg_s64(c));
    b = vshl_u64(b, vdup_n_s64(off));
    return  vbic_u64(a, b);
}


INLINE(Vdyu,VDYU_BFC1)
(
    Vdyu        src,
    Rc(0, 63)   off,
    Rc(1, 64)   len
)
{
#define     VDYU_BFC1(A, B, C) VDDU_ASYU(DDU_BFC1(VDYU_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDYU_ASDU(src), off, len);
    return  VDDU_ASYU(r);
}


INLINE(Vdbu,VDBU_BFC1)
(
    Vdbu        src,
    Rc(0, 63)   off,
    Rc(1,  8)   len
)
{
#define     VDBU_BFC1(A, B, C) VDDU_ASBU(DDU_BFC1(VDBU_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDBU_ASDU(src), off, len);
    return  VDDU_ASBU(r);
}

INLINE(Vdbi,VDBI_BFC1)
(
    Vdbi        src,
    Rc(0, 63)   off,
    Rc(1,  8)   len
)
{
#define     VDBI_BFC1(A, B, C) VDDU_ASBI(DDU_BFC1(VDBI_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDBI_ASDU(src), off, len);
    return  VDDU_ASBI(r);
}

INLINE(Vdbc,VDBC_BFC1)
(
    Vdbc        src,
    Rc(0, 63)   off,
    Rc(1,  8)   len
)
{
#define     VDBC_BFC1(A, B, C) VDDU_ASBC(DDU_BFC1(VDBC_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDBC_ASDU(src), off, len);
    return  VDDU_ASBC(r);
}


INLINE(Vdhu,VDHU_BFC1)
(
    Vdhu        src,
    Rc(0, 63)   off,
    Rc(1, 16)   len
)
{
#define     VDHU_BFC1(A, B, C) VDDU_ASHU(DDU_BFC1(VDHU_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDHU_ASDU(src), off, len);
    return  VDDU_ASHU(r);
}

INLINE(Vdhi,VDHI_BFC1)
(
    Vdhi        src,
    Rc(0, 63)   off,
    Rc(1, 16)   len
)
{
#define     VDHI_BFC1(A, B, C) VDDU_ASHI(DDU_BFC1(VDHI_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDHI_ASDU(src), off, len);
    return  VDDU_ASHI(r);
}


INLINE(Vdwu,VDWU_BFC1)
(
    Vdwu        src,
    Rc(0, 63)   off,
    Rc(1, 32)   len
)
{
#define     VDWU_BFC1(A, B, C) VDDU_ASWU(DDU_BFC1(VDWU_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDWU_ASDU(src), off, len);
    return  VDDU_ASWU(r);
}

INLINE(Vdwi,VDWI_BFC1)
(
    Vdwi        src,
    Rc(0, 63)   off,
    Rc(1, 32)   len
)
{
#define     VDWI_BFC1(A, B, C) VDDU_ASWI(DDU_BFC1(VDWI_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDWI_ASDU(src), off, len);
    return  VDDU_ASWI(r);
}


INLINE(Vddu,VDDU_BFC1)
(
    Vddu        src,
    Rc(0, 63)   off,
    Rc(1, 64)   len
)
{
#define     VDDU_BFC1(A, B, C) DDU_BFC1(A,B,C)
    return  (DDU_BFC1)(src, off, len);
}

INLINE(Vddi,VDDI_BFC1)
(
    Vddi        src,
    Rc(0, 63)   off,
    Rc(1, 64)   len
)
{
#define     VDDI_BFC1(A, B, C) VDDU_ASDI(DDU_BFC1(VDDI_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDDI_ASDU(src), off, len);
    return  VDDU_ASDI(r);
}


#if 0 // _LEAVE_ARM_BFC1
}
#endif

#if 0 // _ENTER_ARM_BFGL
{
#endif

INLINE( uchar, UCHAR_BFGL) 
(
    unsigned                src,
    Rc(0, UCHAR_WIDTH-1)    off,
    Rc(1, UCHAR_WIDTH)      len
)
{
#define     UCHAR_BFGL(SRC, OFF, LEN) \
((uchar)((((unsigned) SRC)>>OFF)&(UCHAR_MAX>>(UCHAR_WIDTH-LEN))))

    return  (src>>off)&(UCHAR_MAX>>(UCHAR_WIDTH-len));
}

INLINE( schar, SCHAR_BFGL)
(
    signed                  src, 
    Rc(0, SCHAR_WIDTH-1)    off, 
    Rc(1, SCHAR_WIDTH)      len
)
{
#define     SCHAR_BFGL(SRC, OFF, LEN) \
(\
    (schar)\
    (\
        (SRC&(1U<<(OFF+(LEN-1))))\
        ?  ~(((unsigned)~SRC>>OFF)&(UCHAR_MAX>>(SCHAR_WIDTH-LEN)))\
        :   (((unsigned) SRC>>OFF)&(UCHAR_MAX>>(SCHAR_WIDTH-LEN)))\
    )\
)

    return  SCHAR_BFGL(src, off, len);
}

INLINE(  char,  CHAR_BFGL)
(
    signed                 src, 
    Rc(0, CHAR_WIDTH-1)    off, 
    Rc(1, CHAR_WIDTH)      len
)
{
#if CHAR_MIN
#   define  CHAR_BFGL(SRC, OFF, LEN) \
(\
    (char)\
    (\
        (SRC&(1U<<(OFF+(LEN-1))))\
        ?  ~(((unsigned)~SRC>>OFF)&(UCHAR_MAX>>(CHAR_WIDTH-LEN)))\
        :   (((unsigned) SRC>>OFF)&(UCHAR_MAX>>(CHAR_WIDTH-LEN)))\
    )\
)

    return  CHAR_BFGL(src, off, len);
#else
#   define  CHAR_BFGL(SRC, OFF, LEN) \
((char)((((unsigned) SRC)>>OFF)&(UCHAR_MAX>>(CHAR_WIDTH-LEN))))

#endif
    return  CHAR_BFGL(src, off, len);

}


INLINE(ushort, USHRT_BFGL) 
(
    unsigned                src, 
    Rc(0, USHRT_WIDTH-1)    off, 
    Rc(1, USHRT_WIDTH)      len
)
{
#define     USHRT_BFGL(SRC, OFF, LEN) \
((ushort)((((unsigned) SRC)>>OFF)&(USHRT_MAX>>(USHRT_WIDTH-LEN))))

    return  (src>>off)&(USHRT_MAX>>(USHRT_WIDTH-len));
}

INLINE( short,  SHRT_BFGL)
(
    signed                  src, 
    Rc(0, SHRT_WIDTH-1)     off, 
    Rc(1, SHRT_WIDTH)       len
)
{
#define     SHRT_BFGL(SRC, OFF, LEN) \
(\
    (short)\
    (\
        ((unsigned) SRC&(1U<<(OFF+(LEN-1))))\
        ?  ~((unsigned)~SRC>>OFF)&(USHRT_MAX>>(SHRT_WIDTH-LEN))\
        :   ((unsigned) SRC>>OFF)&(USHRT_MAX>>(SHRT_WIDTH-LEN))\
    )\
)

    unsigned x = src;
    if (x&(1U<<(off+(len-1))))
        return ~((~x>>off)&(USHRT_MAX>>(SHRT_WIDTH-len)));
    return      (( x>>off)&(USHRT_MAX>>(SHRT_WIDTH-len)));
}


INLINE(  uint,  UINT_BFGL) 
(
    unsigned               src, 
    Rc(0, UINT_WIDTH-1)    off, 
    Rc(1, UINT_WIDTH)      len
)
{
#define     UINT_BFGL(SRC, OFF, LEN) \
(((unsigned) SRC>>OFF)&(UINT_MAX>>(UINT_WIDTH-LEN)))

    return  (src>>off)&(UINT_MAX>>(UINT_WIDTH-len));
}

INLINE(   int,   INT_BFGL)
(
    signed                 src, 
    Rc(0, INT_WIDTH-1)     off, 
    Rc(1, INT_WIDTH)       len
)
{
#define     INT_BFGL(SRC, OFF, LEN) \
(\
    (int)\
    (\
        (SRC&(1U<<(OFF+(LEN-1))))\
        ?  ~(((unsigned)~SRC>>OFF)&(UINT_MAX>>(INT_WIDTH-LEN)))\
        :   (((unsigned) SRC>>OFF)&(UINT_MAX>>(INT_WIDTH-LEN)))\
    )\
)

    return  INT_BFGL(src, off, len);
}


INLINE( ulong, ULONG_BFGL) 
(
    ulong                   src, 
    Rc(0, ULONG_WIDTH-1)    off, 
    Rc(1, ULONG_WIDTH)      len
)
{
#define     ULONG_BFGL(SRC, OFF, LEN) \
(((ulong) SRC>>OFF)&(ULONG_MAX>>(ULONG_WIDTH-LEN)))

    return (src>>off)&(ULONG_MAX>>(ULONG_WIDTH-len));
}

INLINE(  long,  LONG_BFGL)
(
    long                src, 
    Rc(0, LONG_WIDTH-1) off, 
    Rc(1, LONG_WIDTH)   len
)
{
#define     LONG_BFGL(SRC, OFF, LEN) \
(\
    (long)\
    (\
        (SRC&(1UL<<(OFF+(LEN-1))))\
        ?  ~(((ulong)~SRC>>OFF)&(ULONG_MAX>>(LONG_WIDTH-LEN)))\
        :   (((ulong) SRC>>OFF)&(ULONG_MAX>>(LONG_WIDTH-LEN)))\
    )\
)

    return  LONG_BFGL(src, off, len);
}


INLINE(ullong,ULLONG_BFGL) 
(
    ullong                  src, 
    Rc(0, ULLONG_WIDTH-1)   off, 
    Rc(1, ULLONG_WIDTH)     len
)
{
#define     ULLONG_BFGL(SRC, OFF, LEN) \
(((ullong) SRC>>OFF)&(ULLONG_MAX>>(ULLONG_WIDTH-LEN)))

    return (src>>off)&(ULLONG_MAX>>(ULLONG_WIDTH-len));
}

INLINE( llong, LLONG_BFGL)
(
    llong                    src, 
    Rc(0, LLONG_WIDTH-1)     off, 
    Rc(1, LLONG_WIDTH)       len
)
{
#define     LLONG_BFGL(SRC, OFF, LEN) \
(\
    (llong)\
    (\
        (SRC&(1ULL<<(OFF+(LEN-1))))\
        ?  ~(((ullong)~SRC>>OFF)&(ULLONG_MAX>>(LLONG_WIDTH-LEN)))\
        :   (((ullong) SRC>>OFF)&(ULLONG_MAX>>(LLONG_WIDTH-LEN)))\
    )\
)

    return  LLONG_BFGL(src, off, len);
}


INLINE(uint16_t, FLT16_BFGL) (flt16_t src, Rc(0, 15) off, Rc(1, 16) len)
{
#define     FLT16_BFGL(SRC, OFF, LEN) UINT16_BFGL(FLT16_ASTU(SRC),OFF,LEN)
    return  UINT16_BFGL(FLT16_ASTU(src), off, len);
}

INLINE(uint32_t, FLT_BFGL) (float src, Rc(0, 31) off, Rc(1, 32) len)
{
#define     FLT_BFGL(SRC, OFF, LEN) UINT32_BFGL(FLT_ASTU(SRC),OFF,LEN)
    return  UINT32_BFGL(FLT_ASTU(src), off, len);
}

INLINE(uint64_t, DBL_BFGL) (double src, Rc(0, 63) off, Rc(1, 64) len)
{
#define     DBL_BFGL(SRC, OFF, LEN) UINT64_BFGL(DBL_ASTU(SRC),OFF,LEN)
    return  UINT64_BFGL(DBL_ASTU(src), off, len);
}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,bfglqu) (QUAD_UTYPE src, Rc(0, 127) off, Rc(1, 128) len)
{
    QUAD_UTYPE m = -1;
    return (src>>off)&(m>>(128-len));
}

INLINE(QUAD_ITYPE,bfglqi) (QUAD_ITYPE src, Rc(0, 127) off, Rc(1, 128) len)
{
    QUAD_UTYPE m = -1;
    QUAD_UTYPE s = 1;
    s <<= off+(len-1);
    if (s&src)
        return ~(((QUAD_UTYPE)~src>>off)&(m>>(128-len)));
    return      (((QUAD_UTYPE) src>>off)&(m>>(128-len)));
}

#endif

INLINE(Vwyu,VWYU_BFGL) (Vwyu src, Rc(0, 31) off, Rc(1, 32) len)
{
#define VWYU_BFGL(SRC, OFF, LEN) \
((Vwyu){ ((WORD_TYPE){.U=UINT32_BFGL(FLT_ASWU(SRC.V0),OFF,LEN)}).F })

    WORD_TYPE v = {.F=src.V0};
    v.U = (UINT32_BFGL)(v.U, off, len);
    src.V0 = v.F;
    return  src;
}

#define MY_BFGLWZ(T, W, SRC, OFF, LEN) \
((T){ UINT32_ASWF(UINT32_BFGL(FLT_ASWU(SRC.V0),(W*OFF),(W*LEN))) })

INLINE(float,WBZ_BFGL) 
(
    float       src,
    Rc(0,3)     off,
    Rc(1,4)     len
)
{
    WORD_TYPE v = {.F=src};
    v.U = (UINT32_BFGL)(v.U, (8*off), (8*len));
    return v.F;
}

INLINE(float,WHZ_BFGL) 
(
    float       src,
    Rc(0,1)     off,
    Rc(1,2)     len
)
{
    WORD_TYPE v = {.F=src};
    v.U = (UINT32_BFGL)(v.U, (16*off), (16*len));
    return v.F;
}

INLINE(Vwbu,VWBU_BFGL) (Vwbu src, Rc(0, 3) off, Rc(1, 4) len)
{
#define     VWBU_BFGL(SRC, OFF, LEN) MY_BFGLWZ(Vwbu,8,SRC,OFF,LEN)
    src.V0 = WBZ_BFGL(src.V0, off, len);
    return  src;
}

INLINE(Vwbi,VWBI_BFGL) (Vwbi src, Rc(0, 3) off, Rc(1, 4) len)
{
#define     VWBI_BFGL(SRC, OFF, LEN) MY_BFGLWZ(Vwbi,8,SRC,OFF,LEN)
    src.V0 = WBZ_BFGL(src.V0, off, len);
    return  src;
}

INLINE(Vwbc,VWBC_BFGL) (Vwbc src, Rc(0, 3) off, Rc(1, 4) len)
{
#define     VWBC_BFGL(SRC, OFF, LEN) MY_BFGLWZ(Vwbc,8,SRC,OFF,LEN)
    src.V0 = WBZ_BFGL(src.V0, off, len);
    return  src;
}


INLINE(Vwhu,VWHU_BFGL) (Vwhu src, Rc(0, 1) off, Rc(1, 2) len)
{
#define     VWHU_BFGL(SRC, OFF, LEN) MY_BFGLWZ(Vwhu,16,SRC,OFF,LEN)
    src.V0 = WHZ_BFGL(src.V0, off, len);
    return  src;
}

INLINE(Vwhi,VWHI_BFGL) (Vwhi src, Rc(0, 1) off, Rc(1, 2) len)
{
#define     VWHI_BFGL(SRC, OFF, LEN) MY_BFGLWZ(Vwhi,16,SRC,OFF,LEN)
    src.V0 = WHZ_BFGL(src.V0, off, len);
    return  src;
}

INLINE(Vwhf,VWHF_BFGL) (Vwhf src, Rc(0, 1) off, Rc(1, 2) len)
{
#define     VWHF_BFGL(SRC, OFF, LEN) MY_BFGLWZ(Vwhf,16,SRC,OFF,LEN)
    src.V0 = WHZ_BFGL(src.V0, off, len);
    return  src;
}


INLINE(Vdyu,VDYU_BFGL) (Vdyu src, Rc(0, 63) off, Rc(1, 64) len)
{
#define     VDYU_BFGL(SRC, OFF, LEN) \
(\
    (Vdyu)\
    {vdup_n_u64(UINT64_BFGL(vget_lane_u64(SRC.V0),OFF,LEN))}\
)

    union {uint64x1_t V; uint64_t Z;} t = {src.V0};
    t.Z = (UINT64_BFGL)(t.Z, off, len);
    src.V0 = t.V;
    return  src;
}


INLINE(Vdbu,VDBU_BFGL) (Vdbu src, Rc(0, 7) off, Rc(1, 8) len)
{
#define     VDBU_BFGL(SRC, OFF, LEN) \
vreinterpret_u8_u64(\
    vand_u64(\
        vreinterpret_u64_u8(vext_u8(SRC,vdup_n_u8(0),OFF)),\
        vdup_n_u64((UINT64_MAX>>(64-(8*LEN)))) \
    )\
)

    uint64x1_t  v = vreinterpret_u64_u8(src);
    uint64_t    m = vget_lane_u64(v, 0);
    m = (UINT64_BFGL)(m, (8*off), (8*len));
    return vcreate_u8(m);
}

INLINE(Vdbi,VDBI_BFGL) (Vdbi src, Rc(0, 7) off, Rc(1, 8) len)
{
#define     VDBI_BFGL(SRC, OFF, LEN) \
vreinterpret_s8_u64(\
    vand_u64(\
        vreinterpret_u64_s8(vext_s8(SRC,vdup_n_s8(0),OFF)),\
        vdup_n_u64( (UINT64_MAX>>(64-(8*LEN))) ) \
    )\
)

    uint64x1_t  v = vreinterpret_u64_s8(src);
    uint64_t    m = vget_lane_u64(v, 0);
    m = (UINT64_BFGL)(m, (8*off), (8*len));
    return vcreate_s8(m);
}

INLINE(Vdbc,VDBC_BFGL) (Vdbc src, Rc(0, 7) off, Rc(1, 8) len)
{
#if CHAR_MIN
#   define  VDBC_BFGL(SRC, OFF, LEN) ((Vdbc){VDBI_BFGL(SRC.V0,OFF,LEN)})
    src.V0 = (VDBI_BFGL)(src.V0, off, len);
#else
#   define  VDBC_BFGL(SRC, OFF, LEN) ((Vdbc){VDBU_BFGL(SRC.V0,OFF,LEN)})
    src.V0 = (VDBU_BFGL)(src.V0, off, len);
#endif

    return src;
}


INLINE(Vdhu,VDHU_BFGL) (Vdhu src, Rc(0, 3) off, Rc(1, 4) len)
{
#define     VDHU_BFGL(SRC, OFF, LEN) \
vand_u16(                  \
    vext_u16(              \
        vdup_n_u16(0xffff),\
        vdup_n_u16(0x0000),\
        (4-LEN)             \
    ),                      \
    vreinterpret_u16_u8(       \
        vtbl1_u8(             \
            vreinterpret_u8_u16(SRC), \
            vadd_u8(           \
                vdup_n_u8((2*OFF)),\
                vcreate_u8(0x0706050403020100ULL)\
            )                   \
        )                       \
    )                       \
)

    uint64x1_t  v = vreinterpret_u64_u16(src);
    uint64_t    m = vget_lane_u64(v, 0);
    m = (UINT64_BFGL)(m, (16*off), (16*len));
    return vcreate_u16(m);
}

INLINE(Vdhi,VDHI_BFGL) (Vdhi src, Rc(0, 3) off, Rc(1, 4) len)
{
#define     VDHI_BFGL(SRC, OFF, LEN) \
vand_s16(                  \
    vext_s16(              \
        vdup_n_s16(0xffff),\
        vdup_n_s16(0x0000),\
        (4-LEN)             \
    ),                      \
    vreinterpret_s16_u8(       \
        vtbl1_u8(             \
            vreinterpret_u8_s16(SRC), \
            vadd_u8(           \
                vdup_n_u8((2*OFF)),\
                vcreate_u8(0x0706050403020100ULL)\
            )                   \
        )                       \
    )                       \
)

    uint64x1_t  v = vreinterpret_u64_s16(src);
    uint64_t    m = vget_lane_u64(v, 0);
    m = (UINT64_BFGL)(m, (16*off), (16*len));
    return vcreate_s16(m);
}

INLINE(Vdhf,VDHF_BFGL) (Vdhf src, Rc(0, 3) off, Rc(1, 4) len)
{
#define     VDHF_BFGL(SRC, OFF, LEN) \
vreinterpret_f16_s16(\
    vand_s16(                  \
        vext_s16(              \
            vdup_n_s16(0xffff),\
            vdup_n_s16(0x0000),\
            (4-LEN)             \
        ),                      \
        vreinterpret_s16_u8(       \
            vtbl1_u8(             \
                vreinterpret_u8_f16(SRC), \
                vadd_u8(           \
                    vdup_n_u8((2*OFF)),\
                    vcreate_u8(0x0706050403020100ULL)\
                )                   \
            )                       \
        )                       \
    )                       \
)

    uint64x1_t  v = vreinterpret_u64_f16(src);
    uint64_t    m = vget_lane_u64(v, 0);
    m = (UINT64_BFGL)(m, (16*off), (16*len));
    return vreinterpret_f16_u16(vcreate_u16(m));
}

 
INLINE(Vdwu,VDWU_BFGL) (Vdwu src, Rc(0, 1) off, Rc(1, 2) len)
{
#define     VDWU_BFGL(SRC, OFF, LEN) \
vand_u32(                  \
    vext_u32(              \
        vdup_n_u32(0xffffffffU),\
        vdup_n_u32(0x00000000U),\
        (2-LEN)             \
    ),                      \
    vreinterpret_u32_u8(       \
        vtbl1_u8(             \
            vreinterpret_u8_u32(SRC), \
            vadd_u8(           \
                vdup_n_u8((4*OFF)),\
                vcreate_u8(0x0706050403020100ULL)\
            )                   \
        )                       \
    )                       \
)

    uint64x1_t  v = vreinterpret_u64_u32(src);
    uint64_t    m = vget_lane_u64(v, 0);
    m = (UINT64_BFGL)(m, (32*off), (32*len));
    return vcreate_u32(m);
}
 
INLINE(Vdwi,VDWI_BFGL) (Vdwi src, Rc(0, 1) off, Rc(1, 2) len)
{
#define     VDWI_BFGL(SRC, OFF, LEN) \
vand_s32(                  \
    vext_s32(              \
        vdup_n_s32(-1), \
        vdup_n_s32(+0), \
        (2-LEN)             \
    ),                      \
    vreinterpret_s32_u8(       \
        vtbl1_u8(             \
            vreinterpret_u8_s32(SRC), \
            vadd_u8(           \
                vdup_n_u8((4*OFF)),\
                vcreate_u8(0x0706050403020100ULL)\
            )                   \
        )                       \
    )                       \
)

    uint64x1_t  v = vreinterpret_u64_s32(src);
    uint64_t    m = vget_lane_u64(v, 0);
    m = (UINT64_BFGL)(m, (32*off), (32*len));
    return vcreate_s32(m);
}
 
INLINE(Vdwf,VDWF_BFGL) (Vdwf src, Rc(0, 1) off, Rc(1, 2) len)
{
#define     VDWF_BFGL(SRC, OFF, LEN) \
vreinterpret_f32_s32(           \
    vand_s32(                  \
        vext_s32(              \
            vdup_n_s32(-1), \
            vdup_n_s32(+0), \
            (2-LEN)             \
        ),                      \
        vreinterpret_s32_u8(       \
            vtbl1_u8(             \
                vreinterpret_u8_f32(SRC), \
                vadd_u8(           \
                    vdup_n_u8((4*OFF)),\
                    vcreate_u8(0x0706050403020100ULL)\
                )                   \
            )                       \
        )                       \
    )                       \
)

    uint64x1_t  v = vreinterpret_u64_f32(src);
    uint64_t    m = vget_lane_u64(v, 0);
    m = (UINT64_BFGL)(m, (32*off), (32*len));
    return vcreate_f32(m);
}


INLINE(Vqyu,VQYU_BFGL) (Vqyu src, Rc(0, 127) off, Rc(1, 128) len)
{
    union {
        Vqyu        V;
        QUAD_UTYPE  U;
    } r = {src};
    r.U >>= off;
    r.U &= (~((QUAD_UTYPE) 0))>>(128-len);
    return  r.V;
}


INLINE(Vqbu,VQBU_BFGL) (Vqbu src, Rc(0, 15) off, Rc(1, 16) len)
{
#define     VQBU_BFGL(SRC, OFF, LEN) \
vandq_u8(                   \
    vextq_u8(               \
        vdupq_n_u8(0xff),   \
        vdupq_n_u8(0x00),   \
        (16-LEN)            \
    ),                      \
    vqtbl1q_u8(             \
        SRC,                \
        vaddq_u8(           \
            vdupq_n_u8(OFF),\
            vcombine_u8(    \
                vcreate_u8(0x0706050403020100ULL),\
                vcreate_u8(0x0f0e0d0c0b0a0908ULL)\
            )               \
        )                   \
    )                       \
)

    uint8x8_t   l = vcreate_u8(0x0706050403020100ULL);
    uint8x8_t   r = vcreate_u8(0x0f0e0d0c0b0a0908ULL);
    uint8x16_t  t = vcombine_u8(l, r);
    uint8x16_t  x = vdupq_n_u8(off);
    uint64_t    i;
    uint64x1_t  z;
    t = vaddq_u8(t, x);
    x = vqtbl1q_u8(src, t);
    if (len > 8)
    {
        l = vdup_n_u8(0xff);
        i = UINT64_MAX>>(64-(8*(len-8)));
        z = vdup_n_u64(i);
        r = vreinterpret_u8_u64(z);
    }
    else
    {
        r = vdup_n_u8(0);
        i = UINT64_MAX>>(64-(8*len));
        z = vdup_n_u64(i);
        l = vreinterpret_u8_u64(z);
    }
    t = vcombine_u8(l, r);
    return vandq_u8(t, x);
}

INLINE(Vqbi,VQBI_BFGL) (Vqbi src, Rc(0, 15) off, Rc(1, 16) len)
{
#define     VQBI_BFGL(SRC, OFF, LEN) \
vandq_s8(                       \
    vextq_s8(                   \
        vdupq_n_s8(0xff),       \
        vdupq_n_s8(0x00),       \
        (16-LEN)                \
    ),                          \
    vreinterpretq_s8_u8(        \
        vqtbl1q_s8(             \
            SRC,                \
            vaddq_u8(           \
                vdupq_n_u8(OFF),\
                vcombine_u8(    \
                    vcreate_u8(0x0706050403020100ULL),  \
                    vcreate_u8(0x0f0e0d0c0b0a0908ULL)   \
                )               \
            )                   \
        )                       \
    )                           \
)
    uint8x16_t z = vreinterpretq_u8_s8(src);
    z = (VQBU_BFGL)(z, off, len);
    return vreinterpretq_s8_u8(z);
}

INLINE(Vqbc,VQBC_BFGL) (Vqbc src, Rc(0, 15) off, Rc(1, 16) len)
{
#if CHAR_MIN
#   define  VQBC_BFGL(SRC, OFF, LEN) ((Vqbc){VQBI_BFGL(SRC.V0, OFF, LEN)})
#else
#   define  VQBC_BFGL(SRC, OFF, LEN) ((Vqbc){VQBU_BFGL(SRC.V0, OFF, LEN)})
#endif
    uint8x16_t z = VQBC_ASBU(src);
    z = (VQBU_BFGL)(z, off, len);
    return  VQBU_ASBC(z);
}


INLINE(Vqhu,VQHU_BFGL) (Vqhu src, Rc(0, 7) off, Rc(1, 8) len)
{
#define     VQHU_BFGL(SRC, OFF, LEN) \
vandq_u16(                  \
    vextq_u16(              \
        vdupq_n_u16(0xffff),\
        vdupq_n_u16(0x0000),\
        (8-LEN)             \
    ),                      \
    vreinterpretq_u16_u8(       \
        vqtbl1q_u8(             \
            vreinterpretq_u8_u16(SRC),                \
            vaddq_u8(           \
                vdupq_n_u8((2*OFF)),\
                vcombine_u8(    \
                    vcreate_u8(0x0706050403020100ULL),\
                    vcreate_u8(0x0f0e0d0c0b0a0908ULL)\
                )               \
            )                   \
        )                       \
    )                       \
)


    uint8x16_t  b = vreinterpretq_u8_u16(src);
    uint8x8_t   l = vcreate_u8(0x0706050403020100ULL);
    uint8x8_t   r = vcreate_u8(0x0f0e0d0c0b0a0908ULL);
    uint8x16_t  t = vcombine_u8(l, r);
    uint8x16_t  x = vdupq_n_u8((2*off));
    uint64_t    i;
    uint64x1_t  z;
    t = vaddq_u8(t, x);
    x = vqtbl1q_u8(b, t);
    if (len > 4)
    {
        l = vdup_n_u8(0xff);
        i = UINT64_MAX>>(64-(16*(len-4)));
        z = vdup_n_u64(i);
        r = vreinterpret_u8_u64(z);
    }
    else
    {
        r = vdup_n_u8(0x00);
        i = UINT64_MAX>>(64-(16*len));
        z = vdup_n_u64(i);
        l = vreinterpret_u8_u64(z);
    }
    t = vcombine_u8(l, r);
    t = vandq_u8(t, x);
    return  vreinterpretq_u16_u8(t);
}

INLINE(Vqhi,VQHI_BFGL) (Vqhi src, Rc(0, 7) off, Rc(1, 8) len)
{
#define     VQHI_BFGL(SRC, OFF, LEN) \
vandq_s16(                  \
    vextq_s16(              \
        vdupq_n_s16(0xffff),\
        vdupq_n_s16(0x0000),\
        (8-LEN)             \
    ),                      \
    vreinterpretq_s16_u8(       \
        vqtbl1q_u8(             \
            vreinterpretq_u8_s16(SRC),                \
            vaddq_u8(           \
                vdupq_n_u8((2*OFF)),\
                vcombine_u8(    \
                    vcreate_u8(0x0706050403020100ULL),\
                    vcreate_u8(0x0f0e0d0c0b0a0908ULL)\
                )               \
            )                   \
        )                       \
    )                       \
)

    uint16x8_t z = vreinterpretq_u16_s16(src);
    z = (VQHU_BFGL)(z, off, len);
    return  vreinterpretq_s16_u16(z);
}

INLINE(Vqhf,VQHF_BFGL) (Vqhf src, Rc(0, 7) off, Rc(1, 8) len)
{
#define     VQHF_BFGL(SRC, OFF, LEN) \
vreinterpretq_f16_u16(\
    vandq_u16(                  \
        vextq_u16(              \
            vdupq_n_u16(0xffff),\
            vdupq_n_u16(0x0000),\
            (8-LEN)             \
        ),                      \
        vreinterpretq_u16_u8(       \
            vqtbl1q_u8(             \
                vreinterpretq_u8_f16(SRC),                \
                vaddq_u8(           \
                    vdupq_n_u8((2*OFF)),\
                    vcombine_u8(    \
                        vcreate_u8(0x0706050403020100ULL),\
                        vcreate_u8(0x0f0e0d0c0b0a0908ULL)\
                    )               \
                )                   \
            )                       \
        )                       \
    )                       \
)

    uint16x8_t z = vreinterpretq_u16_f16(src);
    z = (VQHU_BFGL)(z, off, len);
    return  vreinterpretq_f16_u16(z);
}


INLINE(Vqwu,VQWU_BFGL) (Vqwu src, Rc(0, 3) off, Rc(1, 4) len)
{
#define     VQWU_BFGL(SRC, OFF, LEN) \
vandq_u32(                  \
    vextq_u32(              \
        vdupq_n_u32(0xffffffffU),\
        vdupq_n_u32(0x00000000U),\
        (4-LEN)             \
    ),                      \
    vreinterpretq_u32_u8(       \
        vqtbl1q_u8(             \
            vreinterpretq_u8_u32(SRC),                \
            vaddq_u8(           \
                vdupq_n_u8((4*OFF)),\
                vcombine_u8(    \
                    vcreate_u8(0x0706050403020100ULL),\
                    vcreate_u8(0x0f0e0d0c0b0a0908ULL)\
                )               \
            )                   \
        )                       \
    )                       \
)


    uint8x16_t  b = vreinterpretq_u8_u32(src);
    uint8x8_t   l = vcreate_u8(0x0706050403020100ULL);
    uint8x8_t   r = vcreate_u8(0x0f0e0d0c0b0a0908ULL);
    uint8x16_t  t = vcombine_u8(l, r);
    uint8x16_t  x = vdupq_n_u8((4*off));
    uint64_t    i;
    uint64x1_t  z;
    t = vaddq_u8(t, x);
    x = vqtbl1q_u8(b, t);
    if (len > 2)
    {
        l = vdup_n_u8(0xff);
        i = UINT64_MAX>>(64-(32*(len-2)));
        z = vdup_n_u64(i);
        r = vreinterpret_u8_u64(z);
    }
    else
    {
        r = vdup_n_u8(0x00);
        i = UINT64_MAX>>(64-(32*len));
        z = vdup_n_u64(i);
        l = vreinterpret_u8_u64(z);
    }
    t = vcombine_u8(l, r);
    t = vandq_u8(t, x);
    return  vreinterpretq_u32_u8(t);
}

INLINE(Vqwi,VQWI_BFGL) (Vqwi src, Rc(0, 3) off, Rc(1, 4) len)
{
#define     VQWI_BFGL(SRC, OFF, LEN) \
vandq_s32(                  \
    vextq_s32(              \
        vdupq_n_s32( -1),\
        vdupq_n_s32(0x0),\
        (4-LEN)             \
    ),                      \
    vreinterpretq_s32_u8(       \
        vqtbl1q_u8(             \
            vreinterpretq_u8_s32(SRC),                \
            vaddq_u8(           \
                vdupq_n_u8((4*OFF)),\
                vcombine_u8(    \
                    vcreate_u8(0x0706050403020100ULL),\
                    vcreate_u8(0x0f0e0d0c0b0a0908ULL)\
                )               \
            )                   \
        )                       \
    )                       \
)

    uint32x4_t z = vreinterpretq_u32_s32(src);
    z = (VQWU_BFGL)(z, off, len);
    return  vreinterpretq_s32_u32(z);
}

INLINE(Vqwf,VQWF_BFGL) (Vqwf src, Rc(0, 3) off, Rc(1, 4) len)
{
#define     VQWF_BFGL(SRC, OFF, LEN) \
vreinterpretq_f32_u32(\
    vandq_u32(                  \
        vextq_u32(              \
            vdupq_n_u32(0xffff),\
            vdupq_n_u32(0x0000),\
            (4-LEN)             \
        ),                      \
        vreinterpretq_u32_u8(       \
            vqtbl1q_u8(             \
                vreinterpretq_u8_f32(SRC),                \
                vaddq_u8(           \
                    vdupq_n_u8((4*OFF)),\
                    vcombine_u8(    \
                        vcreate_u8(0x0706050403020100ULL),\
                        vcreate_u8(0x0f0e0d0c0b0a0908ULL)\
                    )               \
                )                   \
            )                       \
        )                       \
    )                       \
)

    uint32x4_t z = vreinterpretq_u32_f32(src);
    z = (VQWU_BFGL)(z, off, len);
    return  vreinterpretq_f32_u32(z);
}


INLINE(Vqdu,VQDU_BFGL) (Vqdu src, Rc(0, 1) off, Rc(1, 2) len)
{
#define     VQDU_BFGL(SRC, OFF, LEN) \
vandq_u64(                  \
    vextq_u64(              \
        vdupq_n_u64(-1),\
        vdupq_n_u64(+0),\
        (2-LEN)             \
    ),                      \
    vreinterpretq_u64_u8(       \
        vqtbl1q_u8(             \
            vreinterpretq_u8_u64(SRC),                \
            vaddq_u8(           \
                vdupq_n_u8((2*OFF)),\
                vcombine_u8(    \
                    vcreate_u8(0x0706050403020100ULL),\
                    vcreate_u8(0x0f0e0d0c0b0a0908ULL)\
                )               \
            )                   \
        )                       \
    )                       \
)
    
    if (len == 2) 
        return src;
    uint64x1_t  l, r=vdup_n_u64(0);
    if (off)    l=vget_high_u64(src);
    else        l=vget_low_u64(src);
    return vcombine_u64(l, r);
}

INLINE(Vqdi,VQDI_BFGL) (Vqdi src, Rc(0, 1) off, Rc(1, 2) len)
{
#define     VQDI_BFGL(SRC, OFF, LEN) \
vandq_s64(                  \
    vextq_s64(              \
        vdupq_n_s64(-1),\
        vdupq_n_s64(+0),\
        (2-LEN)             \
    ),                      \
    vreinterpretq_s64_u8(       \
        vqtbl1q_u8(             \
            vreinterpretq_u8_s64(SRC),                \
            vaddq_u8(           \
                vdupq_n_u8((2*OFF)),\
                vcombine_u8(    \
                    vcreate_u8(0x0706050403020100ULL),\
                    vcreate_u8(0x0f0e0d0c0b0a0908ULL)\
                )               \
            )                   \
        )                       \
    )                       \
)
    
    if (len == 2) 
        return src;
    int64x1_t   l, r=vdup_n_s64(0);
    if (off)    l=vget_high_s64(src);
    else        l=vget_low_s64(src);
    return vcombine_s64(l, r);
}

INLINE(Vqdf,VQDF_BFGL) (Vqdf src, Rc(0, 1) off, Rc(1, 2) len)
{
#define     VQDF_BFGL(SRC, OFF, LEN) \
vreinterpretq_f64_s64(\
    vandq_s64(                  \
        vextq_s64(              \
            vdupq_n_s64(-1),\
            vdupq_n_s64(+0),\
            (2-LEN)             \
        ),                      \
        vreinterpretq_s64_u8(       \
            vqtbl1q_u8(             \
                vreinterpretq_u8_f64(SRC),                \
                vaddq_u8(           \
                    vdupq_n_u8((2*OFF)),\
                    vcombine_u8(    \
                        vcreate_u8(0x0706050403020100ULL),\
                        vcreate_u8(0x0f0e0d0c0b0a0908ULL)\
                    )               \
                )                   \
            )                       \
        )                       \
    )                       \
)
    
    if (len == 2) 
        return src;
    float64x1_t l, r=vdup_n_f64(0);
    if (off)    l=vget_high_f64(src);
    else        l=vget_low_f64(src);
    return vcombine_f64(l, r);
}

#if 0 // _LEAVE_ARM_BFGL
}
#endif

#if 0 // _ENTER_ARM_BFSL
{
#endif

#define MY_BFSLM(T, DST, OFF, MSK, SRC) \
((((T) DST)&~(((T) MSK)<<OFF))|((((T) SRC)&((T) MSK))<<OFF))

INLINE(uchar,UCHAR_BFSL)
(
    unsigned                dst, 
    Rc(0, UCHAR_WIDTH-1)    off,
    Rc(1, UCHAR_WIDTH)      len,
    unsigned                src
)
{
#define     UCHAR_BFSL(DST, OFF, LEN, SRC)  \
(\
    (((uint) DST)&~((UCHAR_MAX>>(UCHAR_WIDTH-LEN))<<OFF))\
|   ((((uint) SRC)<<OFF)&((UCHAR_MAX>>(UCHAR_WIDTH-LEN))<<OFF))\
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = UCHAR_MAX>>(UCHAR_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}

INLINE(schar,SCHAR_BFSL)
(
    signed                  dst, 
    Rc(0, SCHAR_WIDTH-1)    off,
    Rc(1, SCHAR_WIDTH)      len,
    signed                  src
)
{
#define SCHAR_BFSL(DST, OFF, LEN, SRC)  \
(                                           \
    (schar)                                 \
    MY_BFSLM(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (UCHAR_MAX>>(SCHAR_WIDTH-LEN)),     \
        SRC                                 \
    )                                       \
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = UCHAR_MAX>>(SCHAR_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    s <<= off;  // shift SRC to OFF
    m <<= off;  // shift mask to OFF 
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}

INLINE(char,CHAR_BFSL)
(
    int                     dst, 
    Rc(0, CHAR_WIDTH-1)     off,
    Rc(1, CHAR_WIDTH)       len,
    int                     src
)
{
#define CHAR_BFSL(DST, OFF, LEN, SRC)       \
(                                           \
    (char)                                  \
    MY_BFSLM(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (UCHAR_MAX>>(CHAR_WIDTH-LEN)),      \
        SRC                                 \
    )                                       \
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = UCHAR_MAX>>(CHAR_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}


INLINE(ushort,USHRT_BFSL)
(
    unsigned                dst, 
    Rc(0, USHRT_WIDTH-1)    off,
    Rc(1, USHRT_WIDTH)      len,
    unsigned                src
)
{
#define     USHRT_BFSL(DST, OFF, LEN, SRC)  \
(                                           \
    (ushort)                                \
    MY_BFSLM(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (USHRT_MAX>>(USHRT_WIDTH-LEN)),     \
        SRC                                 \
    )                                       \
)
    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = USHRT_MAX>>(USHRT_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}

INLINE(short,SHRT_BFSL)
(
    signed                  dst, 
    Rc(0, SHRT_WIDTH-1)     off,
    Rc(1, SHRT_WIDTH)       len,
    signed                  src
)
{
#define     SHRT_BFSL(DST, OFF, LEN, SRC)   \
(                                           \
    (short)                                 \
    MY_BFSLM(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (USHRT_MAX>>(SHRT_WIDTH-LEN)),      \
        SRC                                 \
    )                                       \
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = USHRT_MAX>>(USHRT_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}


INLINE(uint,UINT_BFSL)
(
    unsigned                dst, 
    Rc(0, UINT_WIDTH-1)     off,
    Rc(1, UINT_WIDTH)       len,
    unsigned                src
)
{
#define     UINT_BFSL(DST, OFF, LEN, SRC)  \
(\
    (((uint) DST)&~((UINT_MAX>>(UINT_WIDTH-LEN))<<OFF))\
|   ((((uint) SRC)<<OFF)&((UINT_MAX>>(UINT_WIDTH-LEN))<<OFF))\
)


    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = UINT_MAX>>(UINT_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}

INLINE(int,INT_BFSL)
(
    int                     dst, 
    Rc(0, INT_WIDTH-1)      off,
    Rc(1, INT_WIDTH)        len,
    int                     src
)
{
#define     INT_BFSL(DST, OFF, LEN, SRC)    \
(                                           \
    (int)                                   \
    MY_BFSLM(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (UINT_MAX>>(UINT_WIDTH-LEN)),       \
        SRC                                 \
    )                                       \
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = UINT_MAX>>(UINT_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}


INLINE(ulong,ULONG_BFSL)
(
    ulong                   dst, 
    Rc(0, ULONG_WIDTH-1)    off,
    Rc(1, ULONG_WIDTH)      len,
    ulong                   src
)
{

#define     ULONG_BFSL(DST, OFF, LEN, SRC) \
(\
    (((ulong) DST)&(~(ULONG_UNOL(LEN)<<OFF)))\
|   ((((ulong) SRC)&ULONG_UNOL(LEN))<<OFF)\
)

    ulong  d = dst;
    ulong  s = src;
    ulong  m = ULONG_UNOL(len);
    s &= m;
    m <<= off;
    s <<= off;
    d &= ~m;
    d |= s;
    return d;
}

INLINE(long,LONG_BFSL)
(
    long                    dst, 
    Rc(0, LONG_WIDTH-1)     off,
    Rc(1, LONG_WIDTH)       len,
    long                    src
)
{
#define     LONG_BFSL(DST, OFF, LEN, SRC)   \
(                                           \
    (ulong)                                 \
    MY_BFSLM(                               \
        ulong,                              \
        DST,                                \
        OFF,                                \
        (ULONG_MAX>>(LONG_WIDTH-LEN)),      \
        SRC                                 \
    )                                       \
)

    ulong  d = dst;
    ulong  s = src;
    ulong  m = ULONG_MAX>>(ULONG_WIDTH-len);
    s &= m;
    m <<= off;
    s <<= off;
    d &= ~m;
    d |= s;
    return d;
}


INLINE(ullong,ULLONG_BFSL)
(
    ullong                  dst, 
    Rc(0, ULLONG_WIDTH-1)   off,
    Rc(1, ULLONG_WIDTH)     len,
    ullong                  src
)
{
#define ULLONG_BFSL(DST, OFF, LEN, SRC)     \
(                                           \
    (ullong)                                \
    MY_BFSLM(                               \
        ullong,                             \
        DST,                                \
        OFF,                                \
        (ULLONG_MAX>>(ULLONG_WIDTH-LEN)),   \
        SRC                                 \
    )                                       \
)

    ullong  d = dst;
    ullong  s = src;
    ullong  m = ULLONG_MAX>>(ULLONG_WIDTH-len);
    s &= m;
    m <<= off;
    s <<= off;
    d &= ~m;
    d |= s;
    return d;
}

INLINE(llong,LLONG_BFSL)
(
    llong                   dst, 
    Rc(0, LLONG_WIDTH-1)    off,
    Rc(1, LLONG_WIDTH)      len,
    llong                   src
)
{
#define     LLONG_BFSL(DST, OFF, LEN, SRC)  \
(                                           \
    (llong)                                 \
    MY_BFSLM(                               \
        ullong,                             \
        DST,                                \
        OFF,                                \
        (ULLONG_MAX>>(LLONG_WIDTH-LEN)),    \
        SRC                                 \
    )                                       \
)


    ullong  d = dst;
    ullong  s = src;
    ullong  m = ULLONG_MAX>>(ULLONG_WIDTH-len);
    s &= m;
    m <<= off;
    s <<= off;
    d &= ~m;
    d |= s;
    return d;
}

INLINE(flt16_t,FLT16_BFSL)
(
    flt16_t     dst, 
    Rc(0, 15)   off,
    Rc(1, 16)   len,
    unsigned    src
)
{
#define FLT16_BFSL(DST, OFF, LEN, SRC)      \
(                                           \
    (HALF_TYPE)                             \
    {                                       \
        .U=UINT16_BFSL(                     \
            ((HALF_TYPE){.F=DST}).U,        \
            OFF,                            \
            LEN,                            \
            ((HALF_TYPE){.F=SRC}).U         \
        )                                   \
    }                                       \
).F

    HALF_TYPE   a = {.F=dst};
    a.U = (UINT16_BFSL)(a.U, off, len, src);
    return  a.F;
}

INLINE(float,FLT_BFSL)
(
    float       dst,
    Rc(0, 31)   off,
    Rc(1, 32)   len,
    uint32_t    src
)
{
#define     FLT_BFSL(DST, OFF, LEN, SRC)    \
(                                           \
    (WORD_TYPE)                             \
    {                                       \
        .U=UINT32_BFSL(                     \
            ((WORD_TYPE){.F=DST}).U,        \
            OFF,                            \
            LEN,                            \
            SRC                             \
        )                                   \
    }                                       \
).F

    WORD_TYPE   a = {.F=dst};
    a.U = (UINT32_BFSL)(a.U, off, len, src);
    return  a.F;
    
}

INLINE(double,DBL_BFSL)
(
    double      dst,
    Rc(0, 31)   off,
    Rc(1, 32)   len,
    uint64_t    src
)
{
#define     DBL_BFSL(DST, OFF, LEN, SRC)    \
(                                           \
    (DWRD_TYPE)                             \
    {                                       \
        .U=UINT64_BFSL(                     \
            ((DWRD_TYPE){.F=DST}).U,        \
            OFF,                            \
            LEN,                            \
            SRC                             \
        )                                   \
    }                                       \
).F

    DWRD_TYPE   a = {.F=dst};
    a.U = (UINT64_BFSL)(a.U, off, len, src);
    return  a.F;
    
}

INLINE(float,WYU_BFSL)
(
    float       dst,
    Rc(0, 31)   off,
    Rc(1, 32)   len,
    float       src
)
{
#define     WYU_BFSL(DST, OFF, LEN, SRC)    \
(                                           \
    (WORD_TYPE)                             \
    {                                       \
        .U=UINT32_BFSL(                     \
            ((WORD_TYPE){.F=DST}).U,        \
            OFF,                            \
            LEN,                            \
            ((WORD_TYPE){.F=SRC}).U         \
        )                                   \
    }                                       \
).F

    WORD_TYPE   a = {.F=dst};
    WORD_TYPE   b = {.F=src};
    a.U = (UINT32_BFSL)(a.U, off, len, b.U);
    return a.F;
    
}

INLINE(uint64x1_t,DYU_BFSL)
(
    uint64x1_t  d, 
    Rc(0, 63)   off,
    Rc(1, 64)   len,
    uint64x1_t  s
)
{
#define DYU_BFSL(DST, OFF, LEN, SRC) \
vdup_n_u64(\
    UINT64_BFSL(\
        vget_lane_u64(DST, 0),\
        OFF,\
        LEN,\
        vget_lane_u64(SRC, 0)\
    )\
)

    uint64x1_t m = VDDU_UNOL(len);
    int64x1_t  n = vdup_n_s64(off);
    m = vshl_u64(m, n);
    s = vshl_u64(s, n);
    return  vorr_u64(
        vbic_u64(d, m),
        vand_u64(s, m)
    );
}


INLINE(Vwyu,VWYU_BFSL)
(
    Vwyu        dst,
    Rc(0, 31)   off,
    Rc(1, 32)   len,
    Vwyu        src
)
{
#define     VWYU_BFSL(DST, OFF, LEN, SRC) \
((Vwyu){WYU_BFSL(DST.V0,OFF,LEN,SRC.V0)})

    dst.V0 = (WYU_BFSL)(dst.V0, off, len, src.V0);
    return  dst;
}


INLINE(Vwbu,VWBU_BFSL)
(
    Vwbu        dst, 
    Rc(0, 3)    off,
    Rc(1, 4)    len,
    Vwbu        src
)
{
#define     VWBU_BFSL(DST, OFF, LEN, SRC) \
((Vwbu){WYU_BFSL(DST.V0,(8*OFF),(8*LEN),SRC.V0)})

    dst.V0 = (WYU_BFSL)(dst.V0, (8*off), (8*len), src.V0);
    return  dst;
}


INLINE(Vwbi,VWBI_BFSL)
(
    Vwbi        dst, 
    Rc(0, 3)    off,
    Rc(1, 4)    len,
    Vwbi        src
)
{
#define     VWBI_BFSL(DST, OFF, LEN, SRC) \
((Vwbi){WYU_BFSL(DST.V0,(8*OFF),(8*LEN),SRC.V0)})

    dst.V0 = (WYU_BFSL)(dst.V0, (8*off), (8*len), src.V0);
    return  dst;
}

INLINE(Vwbc,VWBC_BFSL)
(
    Vwbc        dst, 
    Rc(0, 3)    off,
    Rc(1, 4)    len,
    Vwbc        src
)
{
#define     VWBC_BFSL(DST, OFF, LEN, SRC) \
((Vwbc){WYU_BFSL(DST.V0,(8*OFF),(8*LEN),SRC.V0)})

    dst.V0 = (WYU_BFSL)(dst.V0, (8*off), (8*len), src.V0);
    return  dst;
}


INLINE(Vwhu,VWHU_BFSL)
(
    Vwhu        dst, 
    Rc(0, 1)    off,
    Rc(1, 2)    len,
    Vwhu        src
)
{
#define     VWHU_BFSL(DST, OFF, LEN, SRC) \
((Vwhu){WYU_BFSL(DST.V0,(16*OFF),(16*LEN),SRC.V0)})

    dst.V0 = (WYU_BFSL)(dst.V0, (16*off), (16*len), src.V0);
    return  dst;
}

INLINE(Vwhi,VWHI_BFSL)
(
    Vwhi        dst, 
    Rc(0, 1)    off,
    Rc(1, 2)    len,
    Vwhi        src
)
{
#define     VWHI_BFSL(DST, OFF, LEN, SRC) \
((Vwhi){WYU_BFSL(DST.V0,(16*OFF),(16*LEN),SRC.V0)})

    dst.V0 = (WYU_BFSL)(dst.V0, (16*off), (16*len), src.V0);
    return  dst;
}

INLINE(Vwhf,VWHF_BFSL)
(
    Vwhf        dst, 
    Rc(0, 1)    off,
    Rc(1, 2)    len,
    Vwhf        src
)
{
#define     VWHF_BFSL(DST, OFF, LEN, SRC) \
((Vwhf){WYU_BFSL(DST.V0,(16*OFF),(16*LEN),SRC.V0)})

    dst.V0 = (WYU_BFSL)(dst.V0, (16*off), (16*len), src.V0);
    return  dst;
}


INLINE(Vdyu,VDYU_BFSL)
(
    Vdyu        dst, 
    Rc(0, 63)   off,
    Rc(1, 64)   len,
    Vdyu        src
)
{
#define VDYU_BFSL(DST, OFF, LEN, SRC) \
((Vdyu){ DYU_BFSL(DST.V0,OFF,LEN,SRC.V0)})

    dst.V0 = (DYU_BFSL)(dst.V0, off, len, src.V0);
    return  dst;
}


INLINE(Vdbu,VDBU_BFSL) 
(
    Vdbu        dst, 
    Rc(0, 7)    off,
    Rc(1, 8)    len,
    Vdbu        src
)
{
#define     VDBU_BFSL(DST, OFF, LEN, SRC)   \
vcreate_u8(\
    UINT64_BFSL(\
        vget_lane_u64(VDBU_ASDU(DST), 0),\
        (8*OFF),\
        (8*LEN),\
        vget_lane_u64(VDBU_ASDU(SRC), 0)\
    )\
)

    uint64x1_t d = VDBU_ASDU(dst);
    uint64x1_t s = VDBU_ASDU(src);
    d = (DYU_BFSL)(d, (8*off), (8*len), s);
    return  VDDU_ASBU(d);
/*

    In some cases, clang is capable of condensing
        (v0&~v2)|(v1&v2) 
    into a 'bic' instruction but it unsurprisingly, it looks
    like gcc never does.
    
    It also looks like clang also optimizes the BSL into the
    (D&~M)|(S&M) sequence, which is... interesting, but also
    not really surprising and probably actually for the best.
*/
/*
    unsigned i = off*8;
    unsigned n = len*8;
    uint64x1_t  v0 = vreinterpret_u64_u8(dst);
    uint64x1_t  v1 = vreinterpret_u64_u8(src);
    uint64x1_t  v2 = vdup_n_u64( ((UINT64_MAX>>(64-n))<<i) );
    int64x1_t   v3 = vdup_n_s64(i);
    v1 = vshl_u64(v1, v3);
#if 1
    v1 = vand_u64(v1, v2); // clear unused upper src bits
    v0 = vbic_u64(v0, v2); // clear field bits
    v0 = vorr_u64(v0, v1);
    return  vreinterpret_u8_u64(v0);
#elif 0
   __asm__ volatile(
       "bit %0.8B, %1.8B, %2.8B" 
       : 
       : "X"(v0), "X"(v1), "X"(v2) 
       : 
   );
    return vreinterpret_u8_u64(v0);
#else 
    v2 = vbsl_u64(v2, v1, v0);
    return  vreinterpret_u8_u64(v2);
#endif

*/

}

INLINE(Vdbi,VDBI_BFSL)
(
    Vdbi        dst, 
    Rc(0, 7)    off,
    Rc(1, 8)    len,
    Vdbi        src
)
{
#define     VDBI_BFSL(DST, OFF, LEN, SRC)   \
vcreate_s8(\
    UINT64_BFSL(\
        vget_lane_u64(VDBI_ASDU(DST), 0),\
        (8*OFF),\
        (8*LEN),\
        vget_lane_u64(VDBI_ASDU(SRC), 0)\
    )\
)

    uint64x1_t d = VDBI_ASDU(dst);
    uint64x1_t s = VDBI_ASDU(src);
    d = (DYU_BFSL)(d, (8*off), (8*len), s);
    return  VDDU_ASBI(d);
}
    

INLINE(Vdbc,VDBC_BFSL)
(
    Vdbc        dst, 
    Rc(0, 7)    off,
    Rc(1, 8)    len,
    Vdbc        src
)
{
#define     VDBC_BFSL(DST, OFF, LEN, SRC)   \
VDDU_ASBC(              \
    DYU_BFSL(           \
        VDBC_ASDU(DST), \
        (8*OFF),        \
        (8*LEN),        \
        VDBC_ASDU(SRC)  \
    )                   \
)

    uint64x1_t d = VDBC_ASDU(dst);
    uint64x1_t s = VDBC_ASDU(src);
    d = (DYU_BFSL)(d, (8*off), (8*len), s);
    return  VDDU_ASBC(d);
}


INLINE(Vdhu,VDHU_BFSL)
(
    Vdhu        dst, 
    Rc(0, 3)    off,
    Rc(1, 4)    len,
    Vdhu        src
)
{
#define     VDHU_BFSL(DST, OFF, LEN, SRC)   \
vcreate_u16(\
    UINT64_BFSL(\
        vget_lane_u64(VDHU_ASDU(DST),0),\
        (16*OFF),\
        (16*LEN),\
        vget_lane_u64(VDHU_ASDU(SRC),0)\
    )\
)

    uint64x1_t d = VDHU_ASDU(dst);
    uint64x1_t s = VDHU_ASDU(src);
    d = (DYU_BFSL)(d, (16*off), (16*len), s);
    return  VDDU_ASHU(d);
}

INLINE(Vdhi,VDHI_BFSL)
(
    Vdhi        dst, 
    Rc(0, 3)    off,
    Rc(1, 4)    len,
    Vdhi        src
)
{
#define     VDHI_BFSL(DST, OFF, LEN, SRC)   \
vcreate_s16(\
    UINT64_BFSL(\
        vget_lane_u64(VDHI_ASDU(DST),0),\
        (16*OFF),\
        (16*LEN),\
        vget_lane_u64(VDHI_ASDU(SRC),0)\
    )\
)

    uint64x1_t d = VDHI_ASDU(dst);
    uint64x1_t s = VDHI_ASDU(src);
    d = (DYU_BFSL)(d, (16*off), (16*len), s);
    return  VDDU_ASHI(d);
}

INLINE(Vdhf,VDHF_BFSL)
(
    Vdhf        dst, 
    Rc(0, 3)    off,
    Rc(1, 4)    len,
    Vdhf        src
)
{
#define     VDHF_BFSL(DST, OFF, LEN, SRC)   \
vcreate_f16(\
    UINT64_BFSL(\
        vget_lane_u64(VDHF_ASDU(DST),0),\
        (16*OFF),\
        (16*LEN),\
        vget_lane_u64(VDHF_ASDU(SRC),0)\
    )\
)

    uint64x1_t d = VDHI_ASDU(dst);
    uint64x1_t s = VDHI_ASDU(src);
    d = (DYU_BFSL)(d, (16*off), (16*len), s);
    return  VDDU_ASHI(d);
}

INLINE(Vdwu,VDWU_BFSL)
(
    Vdwu        dst, 
    Rc(0, 1)    off,
    Rc(1, 2)    len,
    Vdwu        src
)
{
#define     VDWU_BFSL(DST, OFF, LEN, SRC)   \
vcreate_u32(\
    UINT64_BFSL(\
        vget_lane_u64(VDWU_ASDU(DST),0),\
        (32*OFF),\
        (32*LEN),\
        vget_lane_u64(VDWU_ASDU(SRC),0)\
    )\
)

    uint64x1_t d = VDWU_ASDU(dst);
    uint64x1_t s = VDWU_ASDU(src);
    d = (DYU_BFSL)(d, (32*off), (32*len), s);
    return  VDDU_ASWU(d);
}

INLINE(Vdwi,VDWI_BFSL)
(
    Vdwi        dst, 
    Rc(0, 1)    off,
    Rc(1, 2)    len,
    Vdwi        src
)
{
#define     VDWI_BFSL(DST, OFF, LEN, SRC)   \
vcreate_s32(\
    UINT64_BFSL(\
        vget_lane_u64(VDWI_ASDU(DST),0),\
        (32*OFF),\
        (32*LEN),\
        vget_lane_u64(VDWI_ASDU(SRC),0)\
    )\
)

    uint64x1_t d = VDWI_ASDU(dst);
    uint64x1_t s = VDWI_ASDU(src);
    d = (DYU_BFSL)(d, (32*off), (32*len), s);
    return  VDDU_ASWI(d);
}

INLINE(Vdwf,VDWF_BFSL)
(
    Vdwf        dst, 
    Rc(0, 1)    off,
    Rc(1, 2)    len,
    Vdwf        src
)
{
#define     VDWF_BFSL(DST, OFF, LEN, SRC)   \
vcreate_f32(\
    UINT64_BFSL(\
        vget_lane_u64(VDWF_ASDU(DST),0),\
        (32*OFF),\
        (32*LEN),\
        vget_lane_u64(VDWF_ASDU(SRC),0)\
    )\
)

    uint64x1_t d = VDWF_ASDU(dst);
    uint64x1_t s = VDWF_ASDU(src);
    d = (DYU_BFSL)(d, (32*off), (32*len), s);
    return  VDDU_ASWF(d);
}

INLINE(uint8x16_t,QBU_BFSL)
(
    uint8x16_t  dst, 
    Rc(0, 15)   off,
    Rc(1, 16)   len,
    uint8x16_t  src
)
{
    //if (len == 16)  return src;
    unsigned i = off*8;
    unsigned w = len*8;
    if (off > 7)
    {
        uint64x2_t dd = vreinterpretq_u64_u8(dst);
        uint64x2_t ds = vreinterpretq_u64_u8(src);
        dd = vsetq_lane_u64(
            UINT64_BFSL(
                vgetq_lane_u64(dd, 1),
                (i-64),
                w,
                vgetq_lane_u64(ds, 0)
            ),
            dd,
            1
        );
        return  vreinterpretq_u8_u64(dd);
    }

    if (len > 8)
    {
        uint8x8_t   r = vcreate_u8(0x0f0e0d0c0b0a0908ULL);
        uint8x8_t   l = vcreate_u8(0x0706050403020100ULL);
        uint8x8_t   k = vdup_n_u8((16-off));
        uint8x8_t   v;
        v = vshl_u64(k, vdup_n_s64(i));
        l = vadd_u8(l, v);
        v = vshl_u64(k, vdup_n_s64((-(128-(i+w)))));
        r = vadd_u8(r, v);
        uint8x16_t  t = vcombine_u8(l, r);
        union {
            struct {uint8x16_t V0, V1;};
            uint8x16x2_t M;
        } x = {.V0=dst, .V1=src};
        return  vqtbl2q_u8(x.M, t);
    }

    uint64x2_t z = vsetq_lane_u64(
        UINT64_BFSL(
            vgetq_lane_u64(vreinterpretq_u64_u8(dst), 0),
            i,
            w,
            vgetq_lane_u64(vreinterpretq_u64_u8(src), 0)
        ),
        vreinterpretq_u64_u8(dst),
        0
    );
    return  vreinterpretq_u8_u64(z);
}

INLINE(uint16x8_t,QHU_BFSL)
(
    uint16x8_t  dst, 
    Rc(0, 7)   off,
    Rc(1, 8)   len,
    uint16x8_t  src
)
{
    //if (len == 16)  return src;
    unsigned i = off*16;
    unsigned w = len*16;
    if (off > 3)
    {
        uint64x2_t dd = vreinterpretq_u64_u16(dst);
        uint64x2_t ds = vreinterpretq_u64_u16(src);
        dd = vsetq_lane_u64(
            UINT64_BFSL(
                vgetq_lane_u64(dd, 1),
                (i-64),
                w,
                vgetq_lane_u64(ds, 0)
            ),
            dd,
            1
        );
        return  vreinterpretq_u16_u64(dd);
    }

    if (len > 4)
    {
        uint8x8_t   r = vcreate_u8(0x0f0e0d0c0b0a0908ULL);
        uint8x8_t   l = vcreate_u8(0x0706050403020100ULL);
        uint8x8_t   k = vdup_n_u8((16-(2*off)));
        uint8x8_t   v;
        v = vshl_u64(k, vdup_n_s64(i));
        l = vadd_u8(l, v);
        v = vshl_u64(k, vdup_n_s64((-(128-(i+w)))));
        r = vadd_u8(r, v);
        uint8x16_t  t = vcombine_u8(l, r);
        union {
            struct {uint8x16_t V0, V1;};
            uint8x16x2_t M;
        } x = {.V0=dst, .V1=src};
        t = vqtbl2q_u8(x.M, t);
        return  vreinterpretq_u16_u8(t);
    }

    uint64x2_t z = vsetq_lane_u64(
        UINT64_BFSL(
            vgetq_lane_u64(vreinterpretq_u64_u8(dst), 0),
            i,
            w,
            vgetq_lane_u64(vreinterpretq_u64_u8(src), 0)
        ),
        vreinterpretq_u64_u8(dst),
        0
    );
    return  vreinterpretq_u16_u64(z);
}

INLINE(uint32x4_t,QWU_BFSL)
(
    uint32x4_t  dst, 
    Rc(0, 3)   off,
    Rc(1, 4)   len,
    uint32x4_t  src
)
{
    //if (len == 16)  return src;
    unsigned i = off*32;
    unsigned w = len*32;
    if (off > 1)
    {
        uint64x2_t dd = vreinterpretq_u64_u32(dst);
        uint64x2_t ds = vreinterpretq_u64_u32(src);
        dd = vsetq_lane_u64(
            UINT64_BFSL(
                vgetq_lane_u64(dd, 1),
                (i-64),
                w,
                vgetq_lane_u64(ds, 0)
            ),
            dd,
            1
        );
        return  vreinterpretq_u16_u64(dd);
    }

    if (len > 4)
    {
        uint8x8_t   r = vcreate_u8(0x0f0e0d0c0b0a0908ULL);
        uint8x8_t   l = vcreate_u8(0x0706050403020100ULL);
        uint8x8_t   k = vdup_n_u8((16-(4*off)));
        uint8x8_t   v;
        v = vshl_u64(k, vdup_n_s64(i));
        l = vadd_u8(l, v);
        v = vshl_u64(k, vdup_n_s64((-(128-(i+w)))));
        r = vadd_u8(r, v);
        uint8x16_t  t = vcombine_u8(l, r);
        union {
            struct {uint8x16_t V0, V1;};
            uint8x16x2_t M;
        } x = {.V0=dst, .V1=src};
        t = vqtbl2q_u8(x.M, t);
        return  vreinterpretq_u32_u8(t);
    }

    uint64x2_t z = vsetq_lane_u64(
        UINT64_BFSL(
            vgetq_lane_u64(vreinterpretq_u64_u8(dst), 0),
            i,
            w,
            vgetq_lane_u64(vreinterpretq_u64_u8(src), 0)
        ),
        vreinterpretq_u64_u8(dst),
        0
    );
    return  vreinterpretq_u32_u64(z);
}

INLINE(uint64x2_t,QDU_BFSL)
(
    uint64x2_t  dst, 
    Rc(0, 1)    off,
    Rc(1, 2)    len,
    uint64x2_t  src
)
{
    return
        (len == 2) ? src : 
        (off == 1) 
        ?   vcopyq_laneq_u64(dst, 1, src, 0)
        :   vcopyq_laneq_u64(dst, 0, src, 0);
}

INLINE(Vqbu,VQBU_BFSL)
(
    Vqbu        dst, 
    Rc(0, 15)   off,
    Rc(1, 16)   len,
    Vqbu        src
)
{
    return QBU_BFSL(dst, off, len, src);
}

INLINE(Vqbi,VQBI_BFSL)
(
    Vqbi        dst, 
    Rc(0, 15)   off,
    Rc(1, 16)   len,
    Vqbi        src
)
{
    uint8x16_t ret = QBU_BFSL(
        VQBI_ASBU(dst),
        off,
        len, 
        VQBI_ASBU(src)
    );
    return  VQBU_ASBI(ret);
}

INLINE(Vqbc,VQBC_BFSL)
(
    Vqbc        dst, 
    Rc(0, 15)   off,
    Rc(1, 16)   len,
    Vqbc        src
)
{
    uint8x16_t ret = QBU_BFSL(
        VQBC_ASTU(dst),
        off,
        len, 
        VQBC_ASTU(src)
    );
    return  VQBU_ASBC(ret);
}

INLINE(Vqhu,VQHU_BFSL)
(
    Vqhu        dst, 
    Rc(0, 7)    off,
    Rc(1, 8)    len,
    Vqhu        src
)
{
    return QHU_BFSL(dst, off, len, src);
}

INLINE(Vqhi,VQHI_BFSL)
(
    Vqhi        dst, 
    Rc(0, 7)    off,
    Rc(1, 8)    len,
    Vqhi        src
)
{
    uint16x8_t ret = QHU_BFSL(
        VQHI_ASTU(dst),
        off,
        len, 
        VQHI_ASTU(src)
    );
    return  VQHU_ASHI(ret);
}

INLINE(Vqhf,VQHF_BFSL)
(
    Vqhf        dst, 
    Rc(0, 7)    off,
    Rc(1, 8)    len,
    Vqhf        src
)
{
    uint16x8_t ret = QHU_BFSL(
        VQHF_ASTU(dst),
        off,
        len, 
        VQHF_ASTU(src)
    );
    return  VQHU_ASHF(ret);
}

INLINE(Vqwu,VQWU_BFSL)
(
    Vqwu        dst, 
    Rc(0, 3)    off,
    Rc(1, 4)    len,
    Vqwu        src
)
{
    return QWU_BFSL(dst, off, len, src);
}

INLINE(Vqwi,VQWI_BFSL)
(
    Vqwi        dst, 
    Rc(0, 3)    off,
    Rc(1, 4)    len,
    Vqwi        src
)
{
    uint32x4_t ret = QWU_BFSL(
        VQWI_ASTU(dst),
        off,
        len, 
        VQWI_ASTU(src)
    );
    return  VQWU_ASWI(ret);
}

INLINE(Vqwf,VQWF_BFSL)
(
    Vqwf        dst, 
    Rc(0, 3)    off,
    Rc(1, 4)    len,
    Vqwf        src
)
{
    uint32x4_t ret = QWU_BFSL(
        VQWF_ASTU(dst),
        off,
        len, 
        VQWF_ASTU(src)
    );
    return  VQWU_ASWF(ret);
}

INLINE(Vqdu,VQDU_BFSL)
(
    Vqdu        dst, 
    Rc(0, 1)    off,
    Rc(1, 2)    len,
    Vqdu        src
)
{
    return QDU_BFSL(dst, off, len, src);
}

INLINE(Vqdi,VQDI_BFSL)
(
    Vqdi        dst, 
    Rc(0, 1)    off,
    Rc(1, 2)    len,
    Vqdi        src
)
{
    uint64x2_t ret = QDU_BFSL(
        VQDI_ASTU(dst),
        off,
        len, 
        VQDI_ASTU(src)
    );
    return  VQDU_ASDI(ret);
}

INLINE(Vqdf,VQDF_BFSL)
(
    Vqdf        dst, 
    Rc(0, 1)    off,
    Rc(1, 2)    len,
    Vqdf        src
)
{
    uint64x2_t ret = QDU_BFSL(
        VQDF_ASTU(dst),
        off,
        len, 
        VQDF_ASTU(src)
    );
    return  VQDU_ASDF(ret);
}

#if 0 // _LEAVE_ARM_BFSL
}
#endif

#if 0 // _ENTER_ARM_NEGL
{
#endif

INLINE(flt16_t, FLT16_NEGL) (flt16_t x) {return -x;}
INLINE(  float,   FLT_NEGL)   (float x) {return -x;}
INLINE( double,   DBL_NEGL)  (double x) {return -x;}

INLINE(float,WBU_NEGL) (float x)
{
/*  vneg_s(x) and vsub_u(0, x) ARE equivalent 
*/
    DWRD_VTYPE z={.W.F={x}};
    z.B.I = vneg_s8(z.B.I);
    return  vget_lane_f32(z.W.F, 0);
}

INLINE(float,WHU_NEGL) (float x)
{
    DWRD_VTYPE z={.W.F={x}};
    z.H.I = vneg_s16(z.H.I);
    return  vget_lane_f32(z.W.F, 0);
}

INLINE(float,WWU_NEGL) (float x)
{
    DWRD_VTYPE z={.W.F={x}};
    z.W.I = vneg_s32(z.W.I);
    return  vget_lane_f32(z.W.F, 0);
}

#if CHAR_MIN
#   define  DBC_NEGL vneg_s8
#   define  QBC_NEGL vnegq_s8
#else
#   define  DBC_NEGL(X) vsub_u8(vdup_n_u8(0),X)
#   define  QBC_NEGL(X) vsubq_u8(vdupq_n_u8(0),X)
#endif

INLINE(uint64x2_t,QQU_NEGL) (uint64x2_t x)
{
    QUAD_VTYPE  c = {.D.U=x};
    DWRD_VTYPE  n, r, l={.D.U=vget_low_u64(x)};
    n.D.U = vdup_n_u64(1);
    l.B.U = vmvn_u8(l.B.U);
    r.D.U = vadd_u64(n.D.U, l.D.U);
    if (r.U < l.U)
    {
        n.D.U = vsub_u64(vget_high_u64(c.D.U), n.D.U);
        c.D.U = vcopyq_lane_u64(c.D.U, 1, n.D.U, 0);
    }
    l.B.U = vget_high_u8(c.B.U);
    l.B.U = vmvn_u8(l.B.U);
    return  vcombine_u64(r.D.U, l.D.U);
/*
    QUAD_TYPE   c = {.U=x};
    uint64_t    r, l;
    l = ~c.Lo.U;
    r = 1+l;
    if (r < l)
        c.Hi.U--;
    c.Hi.U = ~c.Hi.U;
    c.Lo.U = r;
    return c.U;
*/
}

INLINE( int64x2_t,QQI_NEGL)  (int64x2_t x)
{
    QUAD_VTYPE z = {.D.I=x};
    z.D.U = QQU_NEGL(z.D.U);
    return  z.D.I;
}

INLINE(Vwbu,VWBU_NEGL) (Vwbu x) {x.V0=WBU_NEGL(x.V0); return x;}
INLINE(Vwbi,VWBI_NEGL) (Vwbi x) {x.V0=WBU_NEGL(x.V0); return x;}
INLINE(Vwbc,VWBC_NEGL) (Vwbc x) {x.V0=WBU_NEGL(x.V0); return x;}
INLINE(Vwhu,VWHU_NEGL) (Vwhu x) {x.V0=WHU_NEGL(x.V0); return x;}
INLINE(Vwhi,VWHI_NEGL) (Vwhi x) {x.V0=WHU_NEGL(x.V0); return x;}
INLINE(Vwwu,VWWU_NEGL) (Vwwu x) {x.V0=WWU_NEGL(x.V0); return x;}
INLINE(Vwwi,VWWI_NEGL) (Vwwi x) {x.V0=WWU_NEGL(x.V0); return x;}

INLINE(Vdbu,VDBU_NEGL) (Vdbu x) {return vsub_u8(vdup_n_u8(0), x);}
INLINE(Vdbi,VDBI_NEGL) (Vdbi x) {return vneg_s8(x);}
INLINE(Vdbc,VDBC_NEGL) (Vdbc x) {x.V0=DBC_NEGL(x.V0); return x;}
INLINE(Vdhu,VDHU_NEGL) (Vdhu x) {return vsub_u16(vdup_n_u16(0), x);}
INLINE(Vdhi,VDHI_NEGL) (Vdhi x) {return vneg_s16(x);}
INLINE(Vdwu,VDWU_NEGL) (Vdwu x) {return vsub_u32(vdup_n_u32(0), x);}
INLINE(Vdwi,VDWI_NEGL) (Vdwi x) {return vneg_s32(x);}
INLINE(Vddu,VDDU_NEGL) (Vddu x) {return vsub_u64(vdup_n_u64(0), x);}
INLINE(Vddi,VDDI_NEGL) (Vddi x) {return vneg_s64(x);}

INLINE(Vqbu,VQBU_NEGL) (Vqbu x) {return vsubq_u8(vdupq_n_u8(0), x);}
INLINE(Vqbi,VQBI_NEGL) (Vqbi x) {return vnegq_s8(x);}
INLINE(Vqbc,VQBC_NEGL) (Vqbc x) {x.V0=QBC_NEGL(x.V0); return x;}
INLINE(Vqhu,VQHU_NEGL) (Vqhu x) {return vsubq_u16(vdupq_n_u16(0), x);}
INLINE(Vqhi,VQHI_NEGL) (Vqhi x) {return vnegq_s16(x);}
INLINE(Vqwu,VQWU_NEGL) (Vqwu x) {return vsubq_u32(vdupq_n_u32(0), x);}
INLINE(Vqwi,VQWI_NEGL) (Vqwi x) {return vnegq_s32(x);}
INLINE(Vqdu,VQDU_NEGL) (Vqdu x) {return vsubq_u64(vdupq_n_u64(0), x);}
INLINE(Vqdi,VQDI_NEGL) (Vqdi x) {return vnegq_s64(x);}
INLINE(Vqqu,VQQU_NEGL) (Vqqu x) {x.V0=QQU_NEGL(x.V0); return x;}
INLINE(Vqqi,VQQI_NEGL) (Vqqi x) {x.V0=QQI_NEGL(x.V0); return x;}


#if 0 // _LEAVE_ARM_NEGL
}
#endif

#if 0 // _ENTER_ARM_ABSU
{
#endif

INLINE(uint16_t,FLT16_ABSU) (flt16_t a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvth_u16_f16(vabsh_f16(a));
#else
    uint32_t    z = vcvts_u32_f32((a <= -0.0f ? -a : a));
    return  vqmovns_u32(z);
#endif
}

INLINE(uint32_t,FLT_ABSU) (float a)
{
    return  vcvts_u32_f32((a <= -0.0f ? -a : a));
}

INLINE(uint64_t,DBL_ABSU) (double a)
{
    return  vcvtd_u64_f64((a <= -0.0 ? -a : a));
}


INLINE(Vwbu,VWBI_ABSU) (Vwbi a)
{
    float m = VWBI_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    int8x8_t    i = vreinterpret_s8_f32(f);
    int8x8_t    b = vshr_n_s8(i, 7);
    i = veor_s8(b, vadd_s8(b, i));
    f = vreinterpret_f32_s8(i);
    m = vget_lane_f32(f, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbu,VWBC_ABSU) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ABSU(VWBC_ASBI(a));
#else
    return  VWBC_ASBU(a);
#endif
}


INLINE(Vwhu,VWHI_ABSU) (Vwhi a)
{
    float m = VWHI_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    int16x4_t   i = vreinterpret_s16_f32(f);
    int16x4_t   b = vshr_n_s16(i, 15);
    i = veor_s16(b, vadd_s16(b, i));
    f = vreinterpret_f32_s16(i);
    m = vget_lane_f32(f, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhu,VWHF_ABSU) (Vwhf a)
{
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    f = vabs_f16(f);
    uint16x4_t  z = vcvt_u16_f16(f);
    v = vreinterpret_f32_u16(z);
    m = vget_lane_f32(v, 0);
    return WHU_ASTV(m);
#else
    float32x4_t r = vcvt_f32_f16(f);
    r = vabsq_f32(r);
    v = vget_low_f32(r);
    return VDWF_CVHZ(v);
#endif

}


INLINE(Vwwu,VWWI_ABSU) (Vwwi a)
{
    int64_t     z = vabsd_s64(VWWI_ASTV(a));
    return  UINT_ASTV(z);
}

INLINE(Vwwu,VWWF_ABSU) (Vwwf a)
{
    float f = VWWF_ASTM(a);
    f = f <= -0.0f ? -f : f;
    uint32_t z = vcvts_u32_f32(f);
    return WWU_ASTV(z);
}


INLINE(Vdbu,VDBI_ABSU) (Vdbi a)
{
    Vdbi b = vshr_n_s8(a, 7);
    return  vreinterpret_u8_s8(veor_s8(b, vadd_s8(a, b)));
}

INLINE(Vdbu,VDBC_ABSU) (Vdbc a)
{
#if CHAR_MIN
    return  VDBI_ABSU(VDBC_ASBI(a));
#else
    return  VDBC_ASBU(a);
#endif
}


INLINE(Vdhu,VDHI_ABSU) (Vdhi a)
{
    Vdhi b = vshr_n_s16(a, 15);
    return  vreinterpret_u16_s16(veor_s16(b, vadd_s16(a, b)));
}

INLINE(Vdhu,VDHF_ABSU) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvt_u16_f16(vabs_f16(a));
#else
    float32x4_t f = vcvt_f32_f16(a);
    f = vabsq_f32(f);
    return  VQWF_CVHZ(f);
#endif

}


INLINE(Vdwu,VDWI_ABSU) (Vdwi a)
{
    Vdwi b = vshr_n_s32(a, 31);
    return  vreinterpret_u32_s32(veor_s32(b, vadd_s32(a, b)));
}

INLINE(Vdwu,VDWF_ABSU) (Vdwf a)
{
    return  vcvt_u32_f32(vabs_f32(a));
}


INLINE(Vddu,VDDI_ABSU) (Vddi a)
{
    Vddi b = vshr_n_s64(a, 63);
    return  vreinterpret_u64_s64(veor_s64(b, vadd_s64(a, b)));
}

INLINE(Vddu,VDDF_ABSU) (Vddf a)
{
    return  vcvt_u64_f64(vabs_f64(a));
}


INLINE(Vqbu,VQBI_ABSU) (Vqbi a)
{
    Vqbi b = vshrq_n_s8(a, 7);
    return  vreinterpretq_u8_s8(veorq_s8(b, vaddq_s8(a, b)));
}

INLINE(Vqbu,VQBC_ABSU) (Vqbc a)
{
#if CHAR_MIN
    return  VQBI_ABSU(VQBC_ASBI(a));
#else
    return  VQBC_ASBU(a);
#endif
}


INLINE(Vqhu,VQHI_ABSU) (Vqhi a)
{
    Vqhi b = vshrq_n_s16(a, 15);
    return  vreinterpretq_u16_s16(veorq_s16(b, vaddq_s16(a, b)));
}

INLINE(Vqhu,VQHF_ABSU) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_u16_f16(vabsq_f16(a));
#else
    return  vcombine_u16(
        VDHF_ABSU(vget_low_f16(a)),
        VDHF_ABSU(vget_high_f16(a))
    );
#endif

}


INLINE(Vqwu,VQWI_ABSU) (Vqwi a)
{
    Vqwi b = vshrq_n_s32(a, 31);
    return  vreinterpretq_u32_s32(veorq_s32(b, vaddq_s32(a, b)));
}

INLINE(Vqwu,VQWF_ABSU) (Vqwf a)
{
    return  vcvtq_u32_f32(vabsq_f32(a));
}


INLINE(Vqdu,VQDI_ABSU) (Vqdi a)
{
    Vqdi b = vshrq_n_s64(a, 63);
    return  vreinterpretq_u64_s64(veorq_s64(b, vaddq_s64(a, b)));
}

INLINE(Vqdu,VQDF_ABSU) (Vqdf a)
{
    return  vcvtq_u64_f64(vabsq_f64(a));
}


#if 0 // _LEAVE_ARM_ABSU
}
#endif

#if 0 // _ENTER_ARM_ABSS
{
#endif

INLINE(  schar,SCHAR_ABSS)   (schar x) {return vqabsb_s8(x);}
INLINE(   char, CHAR_ABSS)    (char x)
{
#if CHAR_MIN
    return  vqabsb_s8(x);
#else
    return  x;
#endif
}

INLINE(  short, SHRT_ABSS)   (short x) {return vqabsh_s16(x);}
INLINE(    int,  INT_ABSS)     (int x) {return vqabss_s32(x);}

INLINE(   long, LONG_ABSS)    (long x)
{
#if DWRD_NLONG == 2
    return  vqabss_s32(x);
#else
    return  vqabsd_s64(x);
#endif
}

INLINE(  llong,LLONG_ABSS)   (llong x)
{
#if QUAD_NLLONG == 2
    return  vqabsd_s64(x);
#else

#endif
}


INLINE(Vwbi,VWBI_ABSS) (Vwbi x)
{
    float32x2_t m = vdup_n_f32(VWBI_ASTM(x));
    int8x8_t    v = vreinterpret_s8_f32(m);
    v = vqabs_s8(v);
    m = vreinterpret_f32_s8(v);
    return  WBI_ASTV(vget_lane_f32(m,0));
}

INLINE(Vwbc,VWBC_ABSS) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ABSS(VWBC_ASBI(x)));
#else
    return  x;
#endif
}


INLINE(Vwhi,VWHI_ABSS) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   v = vreinterpret_s16_f32(m);
    v = vqabs_s16(v);
    m = vreinterpret_f32_s16(v);
    return  WHI_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwwi,VWWI_ABSS) (Vwwi x)
{
    return  INT32_ASTV(vqabss_s32(VWWI_ASTV(x)));
}


INLINE(Vdbi,VDBI_ABSS) (Vdbi x) {return vqabs_s8(x);}
INLINE(Vdbc,VDBC_ABSS) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_ASBC(vqabs_s8(VDBC_ASBI(x)));
#else
    return  x;
#endif
}

INLINE(Vdhi,VDHI_ABSS) (Vdhi x) {return vqabs_s16(x);}
INLINE(Vdwi,VDWI_ABSS) (Vdwi x) {return vqabs_s32(x);}
INLINE(Vddi,VDDI_ABSS) (Vddi x) {return vqabs_s64(x);}

INLINE(Vqbi,VQBI_ABSS) (Vqbi x) {return vqabsq_s8(x);}
INLINE(Vqbc,VQBC_ABSS) (Vqbc x)
{
#if CHAR_MIN
    return  VQBI_ASBC(vqabsq_s8(VQBC_ASBI(x)));
#else
    return  x;
#endif
}

INLINE(Vqhi,VQHI_ABSS) (Vqhi x) {return vqabsq_s16(x);}
INLINE(Vqwi,VQWI_ABSS) (Vqwi x) {return vqabsq_s32(x);}
INLINE(Vqdi,VQDI_ABSS) (Vqdi x) {return vqabsq_s64(x);}

#if 0 // _LEAVE_ARM_ABSS
}
#endif

#if 0 // _ENTER_ARM_ABSL
{
#endif

#if CHAR_MIN
#   define  WBC_ABSL WBI_ABSL
#   define  DBC_ABSL vabs_s8
#   define  QBC_ABSL vabsq_s8
#else
#   define  WBC_ABSL(X) X
#   define  DBC_ABSL    vreinterpret_s8_u8
#   define  QBC_ABSL    vreinterpretq_s8_u8
#endif

INLINE(float,WBI_ABSL) (float x)
{
    DWRD_VTYPE a = {.W.F={x}};
    a.B.I = vabs_s8(a.B.I);
    return  vget_lane_f32(a.W.F, 0);
}

INLINE(float,WHI_ABSL) (float x)
{
    DWRD_VTYPE a = {.W.F={x}};
    a.H.I = vabs_s16(a.H.I);
    return  vget_lane_f32(a.W.F, 0);
}

INLINE(float,WWI_ABSL) (float x)
{
    DWRD_VTYPE a = {.W.F={x}};
    a.W.I = vabs_s32(a.W.I);
    return  vget_lane_f32(a.W.F, 0);
}

INLINE(Vwbi,VWBI_ABSL) (Vwbi x) {x.V0 = WBI_ABSL(x.V0); return x;}
INLINE(Vwbi,VWBC_ABSL) (Vwbc x) {return ((Vwbi){WBC_ABSL(x.V0)});}
INLINE(Vwhi,VWHI_ABSL) (Vwhi x) {x.V0 = WHI_ABSL(x.V0); return x;}
INLINE(Vwwi,VWWI_ABSL) (Vwwi x) {x.V0 = WWI_ABSL(x.V0); return x;}

INLINE(Vdbi,VDBI_ABSL) (Vdbi x) {return vabs_s8(x);}
INLINE(Vdbi,VDBC_ABSL) (Vdbc x) {return DBC_ABSL(x.V0);}
INLINE(Vdhi,VDHI_ABSL) (Vdhi x) {return vabs_s16(x);}
INLINE(Vdwi,VDWI_ABSL) (Vdwi x) {return vabs_s32(x);}
INLINE(Vddi,VDDI_ABSL) (Vddi x) {return vabs_s64(x);}

INLINE(Vqbi,VQBI_ABSL) (Vqbi x) {return vabsq_s8(x);}
INLINE(Vqbi,VQBC_ABSL) (Vqbc x) {return QBC_ABSL(x.V0);}
INLINE(Vqhi,VQHI_ABSL) (Vqhi x) {return vabsq_s16(x);}
INLINE(Vqwi,VQWI_ABSL) (Vqwi x) {return vabsq_s32(x);}
INLINE(Vqdi,VQDI_ABSL) (Vqdi x) {return vabsq_s64(x);}
INLINE(Vqqi,VQQI_ABSL) (Vqqi x) 
{
    if (vgetq_lane_s64(x.V0, 1) < 0)
        x.V0 = QQI_NEGL(x.V0);
    return  x;
}

#if 0 // _LEAVE_ARM_ABSL
}
#endif

#if 0 // _ENTER_ARM_ABSF
{
#endif

INLINE(flt16_t,FLT16_ABSF) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
#   define  FLT16_ABSF(X) vabsh_f16(X)
    return  vabsh_f16(x);
#else
    DWRD_VTYPE a={.H.F=x}, b={.H.I=0x7fff};
    a.H.U = vand_u16(a.H.U, b.H.U);
    return  vget_lane_f16(a.H.F, 0);
#endif
}

INLINE( float,FLT_ABSF)  (float x) 
{
/*  
The current implementation compiles to a single instruction:
    'BIC v0.2s, #128, lsl #24'

To get a single FABS instruction, asm is required. However, 
it would probably result in unexpected exceptions when the
contents of v.s[1] are invalid, which is probably why clang
insists on filing all 64 bits of the v.2s register.

*/
#if 0
    __asm__ volatile(
       "fabs %0.2s, %1.2s" 
       : 
       : "X"(x), "X"(x)
       : 
    );
#else
    DWRD_VTYPE s = {.W.F=x};
    s.W.U = vbic_u32(s.W.U, vcreate_u32(0x80000000U));
    return  vget_lane_f32(s.W.F, 0);
#endif
    
}

INLINE(double,DBL_ABSF) (double x) 
{
    DWRD_VTYPE s = {.F=x};
    s.D.F = vabs_f64(s.D.F);
    return  s.F;
}

INLINE(QUAD_FTYPE,absfqf) (QUAD_FTYPE x)
{
    QUAD_VTYPE a = {.F=x};
    uint64x1_t b = vget_high_u64(a.D.U);
    uint64x1_t c = vdup_n_u64((1ULL<<63));
    b = vbic_u64(b, c);
    a.D.U = vcopyq_lane_u64(a.D.U, 1, b, 0);
    return  a.F;
}

INLINE(float16x4_t, DHF_ABSF)  (float16x4_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return vabs_f16(x);
#else
    DWRD_VTYPE a={.H.F=x}, b={.H.U=vdup_n_u16(0x8000)};
    a.H.U = vbic_u16(a.H.U, b.H.U);
    return  a.H.F;
#endif
}

INLINE(float16x8_t, QHF_ABSF)  (float16x8_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return vabsq_f16(x);
#else
    QUAD_VTYPE a={.H.F=x}, b={.H.U=vdupq_n_u16(0x8000)};
    a.H.U = vbicq_u16(a.H.U, b.H.U);
    return  a.H.F;
#endif
}

INLINE(Vwhf,VWHF_ABSF) (Vwhf x)
{
    DWRD_VTYPE a={.W.F={x.V0}};
    a.H.F = DHF_ABSF(a.H.F);
    x.V0 = vget_lane_f32(a.W.F, 0);
    return x;
}

INLINE(Vwwf,VWWF_ABSF) (Vwwf x) {x.V0=FLT_ABSF(x.V0); return x;}


INLINE(Vdhf,VDHF_ABSF) (Vdhf x) {return DHF_ABSF(x);}
INLINE(Vdwf,VDWF_ABSF) (Vdwf x) {return vabs_f32(x);}
INLINE(Vddf,VDDF_ABSF) (Vddf x) {return vabs_f64(x);}

INLINE(Vqhf,VQHF_ABSF) (Vqhf x) {return QHF_ABSF(x);}
INLINE(Vqwf,VQWF_ABSF) (Vqwf x) {return vabsq_f32(x);}
INLINE(Vqdf,VQDF_ABSF) (Vqdf x) {return vabsq_f64(x);}
INLINE(Vqqf,VQQF_ABSF) (Vqqf x) {x.V0=absfqf(x.V0); return x;}

#if 0 // _LEAVE_ARM_ABSF
}
#endif

#if 0 // _ENTER_ARM_ABSW
{
#endif

INLINE(float,SCHAR_ABSW)  (schar a) 
{
    return a < 0 ? 0.0f-a : a;
}

INLINE(float, CHAR_ABSW)   (char a)
{
#if CHAR_MIN
    return  SCHAR_ABSW(a);
#else
    return  a;
#endif
}


INLINE(float,SHRT_ABSW)  (short a)
{
    return  a < 0 ? 0.0f-a : a;
}

INLINE(float, INT_ABSW)    (int a)
{
    return  a < 0 ? 0.0f-a : a;
}

INLINE(float, LONG_ABSW)  (long a)
{
    return  a < 0l ? 0.0f-a : a;
}

INLINE(float,LLONG_ABSW) (llong a)
{
    return  a < 0ll ? 0.0f-a : a;
}

#if QUAD_NLLONG == 2

INLINE(float, abswqi) (QUAD_ITYPE a)
{
    return  a < 0ll ? 0.0f-a : a;
}

#endif

INLINE(float,FLT16_ABSW) (flt16_t a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabsh_f16(a);
#else
    return a <= -0.0f16 ? 0.0f-a : a;
#endif
}

INLINE(float,FLT_ABSW) (float a)
{
    return a <= -0.0f ? -a : a;
}

INLINE(float,DBL_ABSW) (double a)
{
    float64x1_t v = vdup_n_f64(a);
    v = vabs_f64(v);
    return  vget_lane_f64(v, 0);
}


INLINE(Vqwf,VWBI_ABSW) (Vwbi a)
{
    return vabsq_f32(VWBI_CVWF(a));
}

INLINE(Vqwf,VWBC_ABSW) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ABSW(VWBC_ASBI(a));
#else
    return  VWBC_CVWF(a);
#endif
}


INLINE(Vdwf,VWHI_ABSW) (Vwhi a)
{
    return  vabs_f32(VWHI_CVWF(a));
}

INLINE(Vdwf,VWHF_ABSW) (Vwhf a)
{
    return  vabs_f32(VWHF_CVWF(a));
}


INLINE(Vwwf,VWWI_ABSW) (Vwwi a)
{
    float   m = VWWI_ASTM(a);
    int     r = FLT_ASTI(m);
    m = r < 0 ? 0.0f-r : r;
    return  WWF_ASTV(m);
}

INLINE(Vwwf,VWWF_ABSW) (Vwwf a)
{
    float   m = VWWF_ASTM(a);
    m = m <= -0.0f ? -m : m;
    return  WWF_ASTV(m);
}


INLINE(Vqwf,VDHI_ABSW) (Vdhi a)
{
    return  vabsq_f32(VDHI_CVWF(a));
}

INLINE(Vqwf,VDHF_ABSW) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    a = vabs_f16(a);
    return  vcvt_f32_f16(a);
#else
    return vabsq_f32(vcvt_f32_f16(a));
#endif

}


INLINE(Vdwf,VDWI_ABSW) (Vdwi a)
{
    return vabs_f32(vcvt_f32_s32(a));
}

INLINE(Vdwf,VDWF_ABSW) (Vdwf a)
{
    return vabs_f32(a);
}


INLINE(Vwwf,VDDI_ABSW) (Vddi a)
{
    float64x1_t f = vcvt_f64_s64(a);
    f = vabs_f64(f);
    float m = vget_lane_f64(f, 0);
    return  WWF_ASTV(m);
}

INLINE(Vwwf,VDDF_ABSW) (Vddf a)
{
    a = vabs_f64(a);
    float m = vget_lane_f64(a, 0);
    return  WWF_ASTV(m);
}


INLINE(Vqwf,VQWI_ABSW) (Vqwi a)
{
    return vabsq_f32(vcvtq_f32_s32(a));
}

INLINE(Vqwf,VQWF_ABSW) (Vqwf a)
{
    return vabsq_f32(a);
}


INLINE(Vdwf,VQDI_ABSW) (Vqdi a)
{
    float64x2_t q = vcvtq_f64_s64(a);
    float32x2_t f = vcvt_f32_f64(q);
    return vabs_f32(f);
}

INLINE(Vdwf,VQDF_ABSW) (Vqdf a)
{
    float32x2_t f = vcvt_f32_f64(a);
    return vabs_f32(f);
}

#if 0 // _LEAVE_ARM_ABSW
}
#endif

#if 0 // _ENTER_ARM_ABSD
{
#endif

INLINE(double,SCHAR_ABSD)  (schar a) 
{
    return a < 0 ? 0.0-a : a;
}

INLINE(double, CHAR_ABSD)   (char a)
{
#if CHAR_MIN
    return  SCHAR_ABSD(a);
#else
    return  a;
#endif
}


INLINE(double,SHRT_ABSD)  (short a)
{
    return  a < 0 ? 0.0-a : a;
}

INLINE(double, INT_ABSD)    (int a)
{
    return  a < 0 ? 0.0-a : a;
}

INLINE(double, LONG_ABSD)  (long a)
{
    return  a < 0l ? 0.0-a : a;
}

INLINE(double,LLONG_ABSD) (llong a)
{
    return  a < 0ll ? 0.0-a : a;
}

#if QUAD_NLLONG == 2

INLINE(double, absdqi) (QUAD_ITYPE a)
{
    return  a < 0ll ? 0.0-a : a;
}

#endif

INLINE(double,FLT16_ABSD) (flt16_t a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabsh_f16(a);
#else
    return a <= -0.0f16 ? 0.0-a : a;
#endif
}

INLINE(double,FLT_ABSD) (float a)
{
    return a <= -0.0f ? -a : a;
}

INLINE(double,DBL_ABSD) (double a)
{
    float64x1_t v = vdup_n_f64(a);
    v = vabs_f64(v);
    return  vget_lane_f64(v, 0);
}



INLINE(Vqdf,VWHI_ABSD) (Vwhi a)
{
    return  vabsq_f64(VWHI_CVDF(a));
}

INLINE(Vqdf,VWHF_ABSD) (Vwhf a)
{
    return  vabsq_f64(VWHF_CVDF(a));
}


INLINE(Vddf,VWWI_ABSD) (Vwwi a)
{
    float       m = VWWI_ASTM(a);
    int64x1_t   z = vdup_n_s64(FLT_ASTI(m));
    float64x1_t f = vcvt_f64_s64(z);
    return  vabs_f64(f);
}

INLINE(Vddf,VWWF_ABSD) (Vwwf a)
{
    float   m = VWWF_ASTM(a);
    float64x1_t f = vdup_n_f32(m);
    return  vabs_f64(f);
}



INLINE(Vqdf,VDWI_ABSD) (Vdwi a)
{
    return vabsq_f64(VDWI_CVDF(a));
}

INLINE(Vqdf,VDWF_ABSD) (Vdwf a)
{
    return vabsq_f64(vcvt_f64_f32(a));
}


INLINE(Vddf,VDDI_ABSD) (Vddi a)
{
    float64x1_t f = vcvt_f64_s64(a);
    return vabs_f64(f);
}

INLINE(Vddf,VDDF_ABSD) (Vddf a)
{
    return vabs_f64(a);
}



INLINE(Vqdf,VQDI_ABSD) (Vqdi a)
{
    float64x2_t q = vcvtq_f64_s64(a);
    return  vabsq_f64(q);
}

INLINE(Vqdf,VQDF_ABSD) (Vqdf a)
{
    return  vabsq_f64(a);
}

#if 0 // _LEAVE_ARM_ABSD
}
#endif

#if 0 // _ENTER_ARM_NEGS
{
#endif

INLINE(schar, UCHAR_NEGS)  (unsigned a)
{
    a = (uchar) a;
    return  (a > SCHAR_MAX) ? SCHAR_MIN : -a;
}

INLINE(schar, SCHAR_NEGS)    (signed a) {return vqnegb_s8(a);}
INLINE(schar,  CHAR_NEGS)       (int a) 
{
#if CHAR_MIN
    return  SCHAR_NEGS(a);
#else
    return  UCHAR_NEGS(a);
#endif
}

INLINE(short, USHRT_NEGS)  (unsigned a)
{
    return  (a > SHRT_MAX) ? SHRT_MIN : -a;
}

INLINE(short,  SHRT_NEGS)    (signed a) {return vqnegh_s16(a);}

INLINE(  int,  UINT_NEGS)      (uint a)
{
    return  (a > INT_MAX) ? INT_MIN : -a;
}

INLINE(  int,   INT_NEGS)       (int a) {return vqnegs_s32(a);}

INLINE( long, ULONG_NEGS)     (ulong a)
{
    return  (a > LONG_MAX) ? LONG_MIN : -a;
}

INLINE( long,  LONG_NEGS)      (long a) 
{
#if DWRD_NLONG == 2
    return  vqnegs_s32(a);
#else
    return  vqnegd_s64(a);
#endif
}


INLINE(llong,ULLONG_NEGS)    (ullong a)
{
    return  (a > LLONG_MAX) ? LLONG_MIN : -a;
}

INLINE(llong, LLONG_NEGS)     (llong a) 
{
#if QUAD_NLLONG == 2
    return  vqnegd_s64(a);
#else

#endif
}


#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE,negsqu) (QUAD_UTYPE a)
{
    QUAD_TYPE x={.U=a};
    if (x.Hi.I < 0)
    {
        x.Hi.U =  INT64_MAX;
        x.Lo.U = UINT64_MAX;
        return  x.U;
    }
    x.U = ((QUAD_UTYPE){0});
    x.U = sublqu(x.U, a);
    return  x.I;
}

INLINE(QUAD_ITYPE,negsqi) (QUAD_ITYPE a) 
{
    union {
        QUAD_ITYPE I;
        struct {uint64_t Lo, Hi;};
    } v = {a};
    if ((v.Hi>>63) && (!v.Lo && !(v.Hi&0x7fffffffffffffffull)))
    {
        v.Lo = UINT64_MAX;
        v.Hi =  INT64_MAX;
    }
    else 
    {
        v.I = -v.I;
    }
    return v.I;
}

#endif

#define     WBI_NEGS(X)     \
vget_lane_f32(vqneg_s8(vreinterpret_s8_f32(vdup_n_f32(X))),0)

#if CHAR_MIN

#   define  WBC_NEGS(X) \
vget_lane_f32(vqneg_s8(vreinterpret_s8_f32(vdup_n_f32(X))),0)

#   define  DBC_NEGS(X) vqneg_s8(X)
#   define  QBC_NEGS(X) vqnegq_s8(X)

#else

#   define  WBC_NEGS(X) 
#   define  DBC_NEGS(X) vdup_n_u8(0)
#   define  QBC_NEGS(X) vdupq_n_u8(0)

#endif

#define     WHI_NEGS(X)     \
vget_lane_f32(vqneg_s16(vreinterpret_s16_f32(vdup_n_f32(X))),0)


#define     WWI_NEGS(X)     \
vget_lane_f32(vqneg_s32(vreinterpret_s32_f32(vdup_n_f32(X))),0)


INLINE(Vwbi,VWBU_NEGS) (Vwbu a) 
{
    float       f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  u = vmovl_u8(d);
    int16x8_t   i = vreinterpretq_s16_u16(u);
    int8x8_t    b = vqmovn_s16(vnegq_s16(i));
    m = vreinterpret_f32_s8(b);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbi,VWBI_NEGS) (Vwbi a) {return WBI_ASTV(WBI_NEGS(VWBI_ASTM(a)));}

INLINE(Vwbi,VWBC_NEGS) (Vwbc a) 
{
#if CHAR_MIN
    return  VWBI_NEGS(VWBC_ASBI(a));
#else
    return  VWBU_NEGS(VWBC_ASBU(a));
#endif
}


INLINE(Vwhi,VWHU_NEGS) (Vwhu a) 
{
    float       f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  d = vreinterpret_u16_f32(m);
    uint32x4_t  u = vmovl_u16(d);
    int32x4_t   i = vreinterpretq_s32_u32(u);
    int16x4_t   b = vqmovn_s32(vnegq_s32(i));
    m = vreinterpret_f32_s16(b);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHI_NEGS) (Vwhi a) {return WHI_ASTV(WHI_NEGS(VWHI_ASTM(a)));}


INLINE(Vwwi,VWWU_NEGS) (Vwwu a) 
{
    float       f = VWWU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  d = vreinterpret_u32_f32(m);
    uint64x2_t  u = vmovl_u32(d);
    int64x2_t   i = vreinterpretq_s64_u64(u);
    int32x2_t   b = vqmovn_s64(vnegq_s64(i));
    m = vreinterpret_f32_s32(b);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWI_NEGS) (Vwwi a) {return WWI_ASTV(WWI_NEGS(VWWI_ASTM(a)));}


INLINE(Vdbi,VDBU_NEGS) (Vdbu a)
{
    uint16x8_t  u = vmovl_u8(a);
    int16x8_t   i = vreinterpretq_s16_u16(u);
    i = vnegq_s16(i);
    return vqmovn_s16(i);
}

INLINE(Vdbi,VDBI_NEGS) (Vdbi a) {return vqneg_s8(a);}

INLINE(Vdbi,VDBC_NEGS) (Vdbc a) 
{
#if CHAR_MIN
    return  VDBI_NEGS(VDBC_ASBI(a));
#else
    return  VDBU_NEGS(VDBC_ASBU(a));
#endif
}


INLINE(Vdhi,VDHU_NEGS) (Vdhu a)
{
    uint32x4_t  u = vmovl_u16(a);
    int32x4_t   i = vreinterpretq_s32_u32(u);
    i = vnegq_s32(i);
    return  vqmovn_s32(i);
}

INLINE(Vdhi,VDHI_NEGS) (Vdhi a) {return vqneg_s16(a);}


INLINE(Vdwi,VDWU_NEGS) (Vdwu a)
{
    uint64x2_t  u = vmovl_u32(a);
    int64x2_t   i = vreinterpretq_s64_u64(u);
    i = vnegq_s64(i);
    return  vqmovn_s64(i);
}

INLINE(Vdwi,VDWI_NEGS) (Vdwi a) {return vqneg_s32(a);}


INLINE(Vddi,VDDU_NEGS) (Vddu a) 
{
    uint64_t b = vget_lane_u64(a, 0);
    return  (b>>63) 
    ?   vdup_n_s64(INT64_MIN)
    :   vneg_s64(vreinterpret_s64_u64(a));
}

INLINE(Vddi,VDDI_NEGS) (Vddi a) {return vqneg_s64(a);}



INLINE(Vqbi,VQBU_NEGS) (Vqbu a) 
{
    return vcombine_s8(
        VDBU_NEGS(vget_low_u8(a)),
        VDBU_NEGS(vget_high_u8(a))
    );
}

INLINE(Vqbi,VQBI_NEGS) (Vqbi a) {return vqnegq_s8(a);}

INLINE(Vqbi,VQBC_NEGS) (Vqbc a) 
{
#if CHAR_MIN
    return  VQBI_NEGS(VQBC_ASBI(a));
#else
    return  VQBU_NEGS(VQBC_ASBU(a));
#endif
}


INLINE(Vqhi,VQHU_NEGS) (Vqhu a) 
{
    return vcombine_s16(
        VDHU_NEGS(vget_low_u16(a)),
        VDHU_NEGS(vget_high_u16(a))
    );
}

INLINE(Vqhi,VQHI_NEGS) (Vqhi a) {return vqnegq_s16(a);}

INLINE(Vqwi,VQWU_NEGS) (Vqwu a) 
{
    return vcombine_s32(
        VDWU_NEGS(vget_low_u32(a)),
        VDWU_NEGS(vget_high_u32(a))
    );
}

INLINE(Vqwi,VQWI_NEGS) (Vqwi a) {return vqnegq_s32(a);}


INLINE(Vqdi,VQDU_NEGS) (Vqdu a) 
{
    return vcombine_s64(
        VDDU_NEGS(vget_low_u64(a)),
        VDDU_NEGS(vget_high_u64(a))
    );
}

INLINE(Vqdi,VQDI_NEGS) (Vqdi a) {return vqnegq_s64(a);}

#if 0 // _LEAVE_ARM_NEGS
}
#endif

#if 0 // _ENTER_ARM_NEGH
{
#endif

INLINE(flt16_t,  BOOL_NEGH)  (_Bool x) {return 0.0f16-x;}

INLINE(flt16_t, UCHAR_NEGH)  (uchar x) {return 0.0f16-x;}
INLINE(flt16_t, SCHAR_NEGH)  (schar x) {return 0.0f16-x;}
INLINE(flt16_t,  CHAR_NEGH)   (char x) {return 0.0f16-x;}

INLINE(flt16_t, USHRT_NEGH) (ushort x) {return 0.0f16-x;}
INLINE(flt16_t,  SHRT_NEGH)  (short x) {return 0.0f16-x;}

INLINE(flt16_t,  UINT_NEGH)   (uint x) {return 0.0f16-x;}
INLINE(flt16_t,   INT_NEGH)    (int x) {return 0.0f16-x;}

INLINE(flt16_t, ULONG_NEGH)  (ulong x) {return 0.0f16-x;}
INLINE(flt16_t,  LONG_NEGH)   (long x) {return 0.0f16-x;}

INLINE(flt16_t,ULLONG_NEGH) (ullong x) {return 0.0f16-x;}
INLINE(flt16_t, LLONG_NEGH)  (llong x) {return 0.0f16-x;}

#if QUAD_NLLONG == 2
INLINE(flt16_t,neghqu) (QUAD_UTYPE x) {return 0.0f16-x;}
INLINE(flt16_t,neghqi) (QUAD_ITYPE x) {return 0.0f16-x;}
INLINE(flt16_t,neghqf) (QUAD_FTYPE x) {return -x;}
#else
INLINE(flt16_t,LDBL_NEGH) (long double x) {return -x;}
#endif

INLINE(flt16_t, FLT16_NEGH) (flt16_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegh_f16(x);
#else
    return  0.0f-x;
#endif
}

INLINE(flt16_t, FLT_NEGH) (float x) {return  -x;}

INLINE(flt16_t, DBL_NEGH) (double x) {return  -x;}


INLINE(Vdhf,VWBU_NEGH) (Vwbu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vneg_f16(VWBU_CVHF(x));
#else
    float32x4_t f = VWBU_CVWF(x);
    f = vnegq_f32(f);
    return  vcvt_f16_f32(f);
#endif
}

INLINE(Vdhf,VWBI_NEGH) (Vwbi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vneg_f16(VWBI_CVHF(x));
#else
    float32x4_t f = VWBI_CVWF(x);
    f = vnegq_f32(f);
    return  vcvt_f16_f32(f);
#endif
}

INLINE(Vdhf,VWBC_NEGH) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_NEGH(VWBC_ASBI(x));
#else
    return  VWBU_NEGH(VWBC_ASBU(x));
#endif
}


INLINE(Vwhf,VWHU_NEGH) (Vwhu x)
{
    float       m = VWHU_ASTM(x);
    float32x2_t f = vdup_n_f32(m);
    uint16x4_t  z = vreinterpret_u16_f32(f);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t r = vcvt_f16_u16(z);
    r = vneg_f16(r);
#else
    float32x4_t y = vcvtq_f32_u32(vmovl_u16(z));
    y = vnegq_f32(y);
    float16x4_t r = vcvt_f16_f32(y);
#endif
    f = vreinterpret_f32_f16(r);
    m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHI_NEGH) (Vwhi x)
{
    float           m = VWHI_ASTM(x);
    float32x2_t     f = vdup_n_f32(m);
    int16x4_t       z = vreinterpret_s16_f32(f);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t     r = vcvt_f16_s16(z);
    r = vneg_f16(r);
#else
    float32x4_t     y = vcvtq_f32_s32(vmovl_s16(z));
    y = vnegq_f32(y);
    float16x4_t     r = vcvt_f16_f32(y);
#endif
    f = vreinterpret_f32_f16(r);
    m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_NEGH) (Vwhf x)
{
    float           m = VWHF_ASTM(x);
    float32x2_t     f = vdup_n_f32(m);
    float16x4_t     z = vreinterpret_f16_f32(f);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t     r = vneg_f16(z);
#else
    float32x4_t     y = vcvt_f32_f16(z);
    y = vnegq_f32(y);
    float16x4_t     r = vcvt_f16_f32(y);
#endif
    f = vreinterpret_f32_f16(r);
    m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}


INLINE(Vqhf,VDBU_NEGH) (Vdbu x)
{
    uint16x8_t  m = vmovl_u8(x);
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegq_f16(vcvtq_f16_u16(m));
#else
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(vget_low_u16(m)));
    float32x4_t r = vcvtq_f32_u32(vmovl_u16(vget_high_u16(m)));
    return  vcombine_f16(
        vcvt_f16_f32(vnegq_f32(l)),
        vcvt_f16_f32(vnegq_f32(r))
    );
#endif

}

INLINE(Vqhf,VDBI_NEGH) (Vdbi x)
{
    int16x8_t   m = vmovl_s8(x);
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegq_f16(vcvtq_f16_s16(m));
#else
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(vget_low_s16(m)));
    float32x4_t r = vcvtq_f32_s32(vmovl_s16(vget_high_s16(m)));
    return  vcombine_f16(
        vcvt_f16_f32(vnegq_f32(l)),
        vcvt_f16_f32(vnegq_f32(r))
    );
#endif

}

INLINE(Vqhf,VDBC_NEGH) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_NEGH(VDBC_ASBI(x));
#else
    return  VDBU_NEGH(VDBC_ASBU(x));
#endif
}


INLINE(Vdhf,VDHU_NEGH) (Vdhu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vneg_f16(vcvt_f16_u16(x));
#else
    uint32x4_t  z = vmovl_u16(x);
    float32x4_t f = vcvtq_f32_u32(z);
    f = vnegq_f32(f);
    return  vcvt_f16_f32(f);
#endif

}

INLINE(Vdhf,VDHI_NEGH) (Vdhi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vneg_f16(vcvt_f16_s16(x));
#else
    int32x4_t  z = vmovl_s16(x);
    float32x4_t f = vcvtq_f32_s32(z);
    f = vnegq_f32(f);
    return  vcvt_f16_f32(f);
#endif

}

INLINE(Vdhf,VDHF_NEGH) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vneg_f16(x);
#else
    float32x4_t f = vcvtq_f32_s32(vcvt_f32_f16(x));
    f = vnegq_f32(f);
    return  vcvt_f16_f32(f);
#endif

}


INLINE(Vwhf,VDWU_NEGH) (Vdwu x)
{
    float32x2_t     f = vcvt_f32_u32(x);
    f = vneg_f32(f);
    float32x4_t     c = vcombine_f32(f, f);
    float16x4_t     r = vcvt_f16_f32(c);
    f = vreinterpret_f16_f32(f);
    float m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VDWI_NEGH) (Vdwi x)
{
    float32x2_t     f = vcvt_f32_s32(x);
    f = vneg_f32(f);
    float32x4_t     c = vcombine_f32(f, f);
    float16x4_t     r = vcvt_f16_f32(c);
    f = vreinterpret_f16_f32(f);
    float m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VDWF_NEGH) (Vdwf x)
{
    float32x2_t f = vneg_f32(x);
    float32x4_t     c = vcombine_f32(f, f);
    float16x4_t     r = vcvt_f16_f32(c);
    f = vreinterpret_f16_f32(f);
    float m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}


INLINE(Vqhf,VQHU_NEGH) (Vqhu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegq_f16(vcvtq_f16_u16(x));
#else
    uint32x4_t  lz = vmovl_u16(vget_low_u16(x));
    uint32x4_t  rz = vmovl_u16(vget_high_u16(x));
    float32x4_t lf = vcvtq_f32_u32(lz);
    float32x4_t rf = vcvtq_f32_u32(rz);
    lf = vnegq_f32(lf);
    rf = vnegq_f32(rf);
    return  vcombine_f16(
        vcvt_f16_f32(lf),
        vcvt_f16_f32(rf)
    );
#endif
}

INLINE(Vqhf,VQHI_NEGH) (Vqhi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegq_f16(vcvtq_f16_s16(x));
#else
    int32x4_t   lz = vmovl_s16(vget_low_s16(x));
    int32x4_t   rz = vmovl_s16(vget_high_s16(x));
    float32x4_t lf = vcvtq_f32_s32(lz);
    float32x4_t rf = vcvtq_f32_s32(rz);
    lf = vnegq_f32(lf);
    rf = vnegq_f32(rf);
    return  vcombine_f16(
        vcvt_f16_f32(lf),
        vcvt_f16_f32(rf)
    );
#endif
}

INLINE(Vqhf,VQHF_NEGH) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegq_f16(x);
#else
    float16x4_t     lr = vget_low_f16(x);
    float16x4_t     rr = vget_high_f16(x);
    float32x4_t     lf = vcvt_f32_f16(lr);
    float32x4_t     rf = vcvt_f32_f16(rr);
    lf = vnegq_f32(lf);
    rf = vnegq_f32(rf);
    return  vcombine_f16(
        vcvt_f16_f32(lf),
        vcvt_f16_f32(rf)
    );
#endif
}


INLINE(Vdhf,VQWU_NEGH) (Vqwu x)
{
    float32x4_t f = vcvtq_f32_u32(x);
    f = vnegq_f32(f);
    return vcvt_f16_f32(f);
}

INLINE(Vdhf,VQWI_NEGH) (Vqwi x)
{
    float32x4_t f = vcvtq_f32_s32(x);
    f = vnegq_f32(f);
    return vcvt_f16_f32(f);
}

INLINE(Vdhf,VQWF_NEGH) (Vqwf x)
{
    x = vnegq_f32(x);
    return  vcvt_f16_f32(x);
}


INLINE(Vwhf,VQDU_NEGH) (Vqdu x)
{
    float64x2_t d = vcvtq_f64_u64(x);
    d = vnegq_f64(d);
    float32x2_t w = vcvt_f32_f64(d);
    float16x4_t h = vcvt_f16_f32(vcombine_f32(w, w));
    w = vreinterpret_f32_f16(h);
    float m = vget_lane_f32(w, 0);
    return WHF_ASTV(m);
}

INLINE(Vwhf,VQDI_NEGH) (Vqdi x)
{
    float64x2_t d = vcvtq_f64_s64(x);
    d = vnegq_f64(d);
    float32x2_t w = vcvt_f32_f64(d);
    float16x4_t h = vcvt_f16_f32(vcombine_f32(w, w));
    w = vreinterpret_f32_f16(h);
    float m = vget_lane_f32(w, 0);
    return WHF_ASTV(m);
}

INLINE(Vwhf,VQDF_NEGH) (Vqdf x)
{
    x = vnegq_f64(x);
    float32x2_t w = vcvt_f32_f64(x);
    float16x4_t h = vcvt_f16_f32(vcombine_f32(w, w));
    w = vreinterpret_f32_f16(h);
    float m = vget_lane_f32(w, 0);
    return  WHF_ASTV(m);
}


#if 0 // _LEAVE_ARM_NEGH
}
#endif

#if 0 // _ENTER_ARM_NEGW
{
#endif

INLINE(float,  BOOL_NEGW)  (_Bool x) {return 0.0f-x;}

INLINE(float, UCHAR_NEGW)  (uchar x) {return 0.0f-x;}
INLINE(float, SCHAR_NEGW)  (schar x) {return 0.0f-x;}
INLINE(float,  CHAR_NEGW)   (char x) {return 0.0f-x;}

INLINE(float, USHRT_NEGW) (ushort x) {return 0.0f-x;}
INLINE(float,  SHRT_NEGW)  (short x) {return 0.0f-x;}

INLINE(float,  UINT_NEGW)   (uint x) {return 0.0f-x;}
INLINE(float,   INT_NEGW)    (int x) {return 0.0f-x;}

INLINE(float, ULONG_NEGW)  (ulong x) {return 0.0f-x;}
INLINE(float,  LONG_NEGW)   (long x) {return 0.0f-x;}

INLINE(float,ULLONG_NEGW) (ullong x) {return 0.0f-x;}
INLINE(float, LLONG_NEGW)  (llong x) {return 0.0f-x;}

#if QUAD_NLLONG == 2
INLINE(float,negwqu) (QUAD_UTYPE x) {return 0.0f-x;}
INLINE(float,negwqi) (QUAD_ITYPE x) {return 0.0f-x;}
INLINE(float,negwqf) (QUAD_FTYPE x) {return -x;}
#else
INLINE(float,LDBL_NEGW) (long double x) {return -x;}
#endif

INLINE(float, FLT16_NEGW) (flt16_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegh_f16(x);
#else
    return  0.0f-x;
#endif
}

INLINE(float, FLT_NEGW) (float x) {return  -x;}

INLINE(float, DBL_NEGW) (double x) {return  -x;}


INLINE(Vqwf,VWBU_NEGW) (Vwbu x)
{
    return  vnegq_f32(VWBU_CVWF(x));
}

INLINE(Vqwf,VWBI_NEGW) (Vwbi x)
{
    return  vnegq_f32(VWBI_CVWF(x));
}

INLINE(Vqwf,VWBC_NEGW) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_NEGW(VWBC_ASBI(x));
#else
    return  VWBU_NEGW(VWBC_ASBU(x));
#endif
}


INLINE(Vdwf,VWHU_NEGW) (Vwhu x)
{
    return  vneg_f32(VWHU_CVWF(x));
}

INLINE(Vdwf,VWHI_NEGW) (Vwhi x)
{
    return  vneg_f32(VWHI_CVWF(x));
}

INLINE(Vdwf,VWHF_NEGW) (Vwhf x)
{
    return  vneg_f32(VWHF_CVWF(x));
}


INLINE(Vwwf,VWWU_NEGW) (Vwwu x)
{
    float m = VWWU_ASTM(x);
    m = FLT_ASWU(m);
    return WWF_ASTV((-m));
}

INLINE(Vwwf,VWWI_NEGW) (Vwwi x)
{
    float m = VWWI_ASTM(x);
    m = FLT_ASWI(m);
    return WWF_ASTV((-m));
}

INLINE(Vwwf,VWWF_NEGW) (Vwwf x)
{
    return  WWF_ASTV((-VWWF_ASTM(x)));
}



INLINE(Vqwf,VDHU_NEGW) (Vdhu x)
{
    return  vnegq_f32(VDHU_CVWF(x));
}

INLINE(Vqwf,VDHI_NEGW) (Vdhi x)
{
    return  vnegq_f32(VDHI_CVWF(x));
}

INLINE(Vqwf,VDHF_NEGW) (Vdhf x)
{
    return  vnegq_f32(VDHF_CVWF(x));
}


INLINE(Vdwf,VDWU_NEGW) (Vdwu x)
{
    return  vneg_f32(vcvt_f32_u32(x));
}

INLINE(Vdwf,VDWI_NEGW) (Vdwi x)
{
    return  vneg_f32(vcvt_f32_s32(x));
}

INLINE(Vdwf,VDWF_NEGW) (Vdwf x)
{
    return  vneg_f32(x);
}


INLINE(Vwwf,VDDU_NEGW) (Vddu x)
{
    return  WWF_ASTV((0.0f-vget_lane_u64(x, 0)));
}

INLINE(Vwwf,VDDI_NEGW) (Vddi x)
{
    return  WWF_ASTV((0.0f-vget_lane_u64(x, 0)));
}

INLINE(Vwwf,VDDF_NEGW) (Vddf x)
{
    x = vneg_f64(x);
    return  WWF_ASTV((vget_lane_f64(x, 0)));
}



INLINE(Vqwf,VQWU_NEGW) (Vqwu x)
{
    return  vnegq_f32(vcvtq_f32_u32(x));
}

INLINE(Vqwf,VQWI_NEGW) (Vqwi x)
{
    return  vnegq_f32(vcvtq_f32_s32(x));
}

INLINE(Vqwf,VQWF_NEGW) (Vqwf x)
{
    return  vnegq_f32(x);
}


INLINE(Vdwf,VQDU_NEGW) (Vqdu x)
{
    float64x2_t f = vcvtq_f64_u64(x);
    float32x2_t m = vcvt_f32_f64(f);
    return  vneg_f32(m);
}

INLINE(Vdwf,VQDI_NEGW) (Vqdi x)
{
    float64x2_t f = vcvtq_f64_s64(x);
    float32x2_t m = vcvt_f32_f64(f);
    return  vneg_f32(m);
}

INLINE(Vdwf,VQDF_NEGW) (Vqdf x)
{
    float32x2_t m = vcvt_f32_f64(x);
    return  vneg_f32(m);
}

#if 0 // _LEAVE_ARM_NEGW
}
#endif

#if 0 // _ENTER_ARM_NEGD
{
#endif

INLINE(double,  BOOL_NEGD)  (_Bool x) {return 0.0-x;}

INLINE(double, UCHAR_NEGD)  (uchar x) {return 0.0-x;}
INLINE(double, SCHAR_NEGD)  (schar x) {return 0.0-x;}
INLINE(double,  CHAR_NEGD)   (char x) {return 0.0-x;}

INLINE(double, USHRT_NEGD) (ushort x) {return 0.0-x;}
INLINE(double,  SHRT_NEGD)  (short x) {return 0.0-x;}

INLINE(double,  UINT_NEGD)   (uint x) {return 0.0-x;}
INLINE(double,   INT_NEGD)    (int x) {return 0.0-x;}

INLINE(double, ULONG_NEGD)  (ulong x) {return 0.0-x;}
INLINE(double,  LONG_NEGD)   (long x) {return 0.0-x;}

INLINE(double,ULLONG_NEGD) (ullong x) {return 0.0-x;}
INLINE(double, LLONG_NEGD)  (llong x) {return 0.0-x;}

#if QUAD_NLLONG == 2
INLINE(double,negdqu) (QUAD_UTYPE x) {return 0.0-x;}
INLINE(double,negdqi) (QUAD_ITYPE x) {return 0.0-x;}
INLINE(double,negdqf) (QUAD_FTYPE x) {return -x;}
#else
INLINE(double,LDBL_NEGD) (long double x) {return -x;}
#endif

INLINE(double, FLT16_NEGD) (flt16_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegh_f16(x);
#else
    return  0.0-x;
#endif
}

INLINE(double, FLT_NEGD) (float x) {return  -x;}

INLINE(double, DBL_NEGD) (double x) {return -x;}



INLINE(Vqdf,VWHU_NEGD) (Vwhu x)
{
    return  vnegq_f64(VWHU_CVDF(x));
}

INLINE(Vqdf,VWHI_NEGD) (Vwhi x)
{
    return  vnegq_f64(VWHI_CVDF(x));
}

INLINE(Vqdf,VWHF_NEGD) (Vwhf x)
{
    return  vnegq_f64(VWHF_CVDF(x));
}


INLINE(Vddf,VWWU_NEGD) (Vwwu x)
{
    return  vneg_f64(vdup_n_f64(FLT_ASWU(VWWU_ASTM(x))));
}

INLINE(Vddf,VWWI_NEGD) (Vwwi x)
{
    return  vneg_f64(vdup_n_f64(FLT_ASWI(VWWI_ASTM(x))));
}

INLINE(Vddf,VWWF_NEGD) (Vwwf x)
{
    return  vneg_f64(vdup_n_f64(VWWF_ASTM(x)));
}



INLINE(Vqdf,VDWU_NEGD) (Vdwu x)
{
    return  vnegq_f64(vcvtq_f64_u64(vmovl_u32(x)));
}

INLINE(Vqdf,VDWI_NEGD) (Vdwi x)
{
    return  vnegq_f64(vcvtq_f64_s64(vmovl_s32(x)));
}

INLINE(Vqdf,VDWF_NEGD) (Vdwf x)
{
    return  vcvt_f64_f32(vneg_f32(x));
}


INLINE(Vddf,VDDU_NEGD) (Vddu x)
{
    return  vneg_f64(vcvt_f64_u64(x));
}

INLINE(Vddf,VDDI_NEGD) (Vddi x)
{
    return  vneg_f64(vcvt_f64_s64(x));
}

INLINE(Vddf,VDDF_NEGD) (Vddf x)
{
    return  vneg_f64(x);
}



INLINE(Vqdf,VQDU_NEGD) (Vqdu x)
{
    return vnegq_f64(vcvtq_f64_u64(x));
}

INLINE(Vqwf,VQDI_NEGD) (Vqdi x)
{
    return vnegq_f64(vcvtq_f64_s64(x));
}

INLINE(Vqdf,VQDF_NEGD) (Vqdf x)
{
    return  vnegq_f64(x);
}

#if 0 // _LEAVE_ARM_NEGD
}
#endif

#if 0 // _ENTER_ARM_INVS
{
#endif


#if CHAR_MIN
#   define  DBC_INVS             vmvn_s8
#   define  QBC_INVS             vmvnq_s8
#else
#   define  DBC_INVS             vmvn_u8
#   define  QBC_INVS             vmvnq_u8
#endif


INLINE(float16x4_t,DHF_INVS) (float16x4_t x)
{
    uint8x8_t b = vreinterpret_u8_f16(x);
    b = vmvn_u8(b);
    return  vreinterpret_f16_u8(b);
}

INLINE(float32x2_t,DWF_INVS) (float32x2_t x)
{
    uint8x8_t b = vreinterpret_u8_f32(x);
    b = vmvn_u8(b);
    return  vreinterpret_f32_u8(b);
}

INLINE(uint64x1_t,DDU_INVS) (uint64x1_t x)
{
    uint8x8_t b = vreinterpret_u8_u64(x);
    b = vmvn_u8(b);
    return  vreinterpret_u64_u8(b);
}

INLINE(int64x1_t,DDI_INVS) (int64x1_t x)
{
    uint8x8_t b = vreinterpret_u8_s64(x);
    b = vmvn_u8(b);
    return  vreinterpret_s64_u8(b);
}

INLINE(float64x1_t,DDF_INVS) (float64x1_t x)
{
    uint8x8_t b = vreinterpret_u8_f64(x);
    b = vmvn_u8(b);
    return  vreinterpret_f64_u8(b);
}


INLINE(float16x8_t,QHF_INVS) (float16x8_t x)
{
    uint8x16_t b = vreinterpretq_u8_f16(x);
    b = vmvnq_u8(b);
    return  vreinterpretq_f16_u8(b);
}

INLINE(float32x4_t,QWF_INVS) (float32x4_t x)
{
    uint8x16_t b = vreinterpretq_u8_f32(x);
    b = vmvnq_u8(b);
    return  vreinterpretq_f32_u8(b);
}

INLINE(uint64x2_t,QDU_INVS) (uint64x2_t x)
{
    uint8x16_t b = vreinterpretq_u8_u64(x);
    b = vmvnq_u8(b);
    return  vreinterpretq_u64_u8(b);
}

INLINE(int64x2_t,QDI_INVS) (int64x2_t x)
{
    uint8x16_t b = vreinterpretq_u8_s64(x);
    b = vmvnq_u8(b);
    return  vreinterpretq_s64_u8(b);
}

INLINE(float64x2_t,QDF_INVS) (float64x2_t x)
{
    uint8x16_t b = vreinterpretq_u8_f64(x);
    b = vmvnq_u8(b);
    return  vreinterpretq_f64_u8(b);
}

INLINE(flt16_t, FLT16_INVS) (flt16_t x)
{
    HALF_TYPE z = {.F=x};
    z.U = ~z.U;
    return  z.F;
}

INLINE(  float,   FLT_INVS)   (float x)
{
    float32x2_t f = {x};
    f = DWF_INVS(f);
    return  vget_lane_f32(f, 0);
}

INLINE( double,   DBL_INVS)  (double x)
{
    float64x1_t f = {x};
    f = DDF_INVS(f);
    return  vget_lane_f64(f, 0);
}

INLINE(QUAD_FTYPE,invsqf) (QUAD_FTYPE x)
{
    QUAD_VTYPE q = {.F=x};
    q.B.U = vmvnq_u8(q.B.U);
    return  q.F;
}

INLINE(Vwyu,VWYU_INVS) (Vwyu x) {return ((Vwyu){FLT_INVS(x.V0)});}
INLINE(Vwbu,VWBU_INVS) (Vwbu x) {return ((Vwbu){FLT_INVS(x.V0)});}
INLINE(Vwbi,VWBI_INVS) (Vwbi x) {return ((Vwbi){FLT_INVS(x.V0)});}
INLINE(Vwbc,VWBC_INVS) (Vwbc x) {return ((Vwbc){FLT_INVS(x.V0)});}
INLINE(Vwhu,VWHU_INVS) (Vwhu x) {return ((Vwhu){FLT_INVS(x.V0)});}
INLINE(Vwhi,VWHI_INVS) (Vwhi x) {return ((Vwhi){FLT_INVS(x.V0)});}
INLINE(Vwhf,VWHF_INVS) (Vwhf x) {return ((Vwhf){FLT_INVS(x.V0)});}
INLINE(Vwwu,VWWU_INVS) (Vwwu x) {return ((Vwwu){FLT_INVS(x.V0)});}
INLINE(Vwwi,VWWI_INVS) (Vwwi x) {return ((Vwwi){FLT_INVS(x.V0)});}
INLINE(Vwwf,VWWF_INVS) (Vwwf x) {return ((Vwwf){FLT_INVS(x.V0)});}

INLINE(Vdyu,VDYU_INVS) (Vdyu x) {return ((Vdyu){DDU_INVS(x.V0)});}
INLINE(Vdbu,VDBU_INVS) (Vdbu x) {return vmvn_u8(x);}
INLINE(Vdbi,VDBI_INVS) (Vdbi x) {return vmvn_s8(x);}
INLINE(Vdbc,VDBC_INVS) (Vdbc x) {return ((Vdbc){DBC_INVS(x.V0)});}
INLINE(Vdhu,VDHU_INVS) (Vdhu x) {return vmvn_u16(x);}
INLINE(Vdhi,VDHI_INVS) (Vdhi x) {return vmvn_s16(x);}
INLINE(Vdhf,VDHF_INVS) (Vdhf x) {return DHF_INVS(x);}
INLINE(Vdwu,VDWU_INVS) (Vdwu x) {return vmvn_u32(x);}
INLINE(Vdwi,VDWI_INVS) (Vdwi x) {return vmvn_s32(x);}
INLINE(Vdwi,VDWF_INVS) (Vdwi x) {return DWF_INVS(x);}
INLINE(Vddu,VDDU_INVS) (Vddu x) {return DDU_INVS(x);}
INLINE(Vddi,VDDI_INVS) (Vddi x) {return DDI_INVS(x);}
INLINE(Vddf,VDDF_INVS) (Vddf x) {return DDF_INVS(x);}

INLINE(Vqyu,VQYU_INVS) (Vqyu x) {x.V0=QDU_INVS(x.V0); return x;}
INLINE(Vqbu,VQBU_INVS) (Vqbu x) {return vmvnq_u8(x);}
INLINE(Vqbi,VQBI_INVS) (Vqbi x) {return vmvnq_s8(x);}
INLINE(Vqbc,VQBC_INVS) (Vqbc x) {x.V0=QBC_INVS(x.V0); return x;}
INLINE(Vqhu,VQHU_INVS) (Vqhu x) {return vmvnq_u16(x);}
INLINE(Vqhi,VQHI_INVS) (Vqhi x) {return vmvnq_s16(x);}
INLINE(Vqhf,VQHF_INVS) (Vqhf x) {return QHF_INVS(x);}
INLINE(Vqwu,VQWU_INVS) (Vqwu x) {return vmvnq_u32(x);}
INLINE(Vqwi,VQWI_INVS) (Vqwi x) {return vmvnq_s32(x);}
INLINE(Vqwf,VQWF_INVS) (Vqwf x) {return QWF_INVS(x);}
INLINE(Vqdu,VQDU_INVS) (Vqdu x) {return QDU_INVS(x);}
INLINE(Vqdi,VQDI_INVS) (Vqdi x) {return QDI_INVS(x);}
INLINE(Vqdf,VQDF_INVS) (Vqdf x) {return QDF_INVS(x);}
INLINE(Vqqu,VQQU_INVS) (Vqqu x) {x.V0=QDU_INVS(x.V0); return x;}
INLINE(Vqqi,VQQI_INVS) (Vqqi x) {x.V0=QDI_INVS(x.V0); return x;}
INLINE(Vqqf,VQQF_INVS) (Vqqf x) {x.V0=  invsqf(x.V0); return x;}

#if 0 // _LEAVE_ARM_INVS
}
#endif

#if 0 // _ENTER_ARM_ANDS
{
#endif

INLINE(flt16_t, FLT16_ANDS)  (flt16_t a,  flt16_t b)
{
    DWRD_VTYPE x={.H.F={a}}, y={.H.F={b}};
    x.H.U = vand_u16(x.H.U, y.H.U);
    return  vget_lane_f16(x.H.F, 0);
}

INLINE(  float,   FLT_ANDS)   (float a,   float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = vand_u32(x.W.U, y.W.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE( double,   DBL_ANDS)  (double a,  double b)
{
    DWRD_VTYPE x={.D.F={a}}, y={.D.F={b}};
    x.D.U = vand_u64(x.D.U, y.D.U);
    return  vget_lane_f64(x.D.F, 0);
}

INLINE(QUAD_FTYPE,andsqf)  (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE x={.F=a}, y={.F=b};
    x.D.U = vandq_u64(x.D.U, y.D.U);
    return  x.F;
}

#if CHAR_MIN
#   define  DBC_ANDS vand_s8
#   define  QBC_ANDS vandq_s8
#else
#   define  DBC_ANDS vand_u8
#   define  QBC_ANDS vandq_u8
#endif

INLINE(float16x4_t,DHF_ANDS) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    z.H.U = vand_u16(x.H.U, y.H.U);
    return  z.H.F;
}

INLINE(float32x2_t,DWF_ANDS) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.W.U = vand_u32(x.W.U, y.W.U);
    return  z.W.F;
}

INLINE(float64x1_t,DDF_ANDS) (float64x1_t a, float64x1_t b)
{
    DWRD_VTYPE x={.D.F=a}, y={.D.F=b}, z;
    z.D.U = vand_u32(x.D.U, y.D.U);
    return  z.D.F;
}


INLINE(float16x8_t,QHF_ANDS) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    z.H.U = vandq_u16(x.H.U, y.H.U);
    return  z.H.F;
}

INLINE(float32x4_t,QWF_ANDS) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.W.U = vandq_u32(x.W.U, y.W.U);
    return  z.W.F;
}

#define QDU_ANDS vandq_u64
#define QDI_ANDS vandq_s64
INLINE(float64x2_t,QDF_ANDS) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE x={.D.F=a}, y={.D.F=b}, z;
    z.D.U = vandq_u64(x.D.U, y.D.U);
    return  z.D.F;
}

INLINE(Vwyu,VWYU_ANDS) (Vwyu a, Vwyu b) {return ((Vwyu){FLT_ANDS(a.V0,b.V0)});}
INLINE(Vwbu,VWBU_ANDS) (Vwbu a, Vwbu b) {return ((Vwbu){FLT_ANDS(a.V0,b.V0)});}
INLINE(Vwbi,VWBI_ANDS) (Vwbi a, Vwbi b) {return ((Vwbi){FLT_ANDS(a.V0,b.V0)});}
INLINE(Vwbc,VWBC_ANDS) (Vwbc a, Vwbc b) {return ((Vwbc){FLT_ANDS(a.V0,b.V0)});}
INLINE(Vwhu,VWHU_ANDS) (Vwhu a, Vwhu b) {return ((Vwhu){FLT_ANDS(a.V0,b.V0)});}
INLINE(Vwhi,VWHI_ANDS) (Vwhi a, Vwhi b) {return ((Vwhi){FLT_ANDS(a.V0,b.V0)});}
INLINE(Vwhf,VWHF_ANDS) (Vwhf a, Vwhf b) {return ((Vwhf){FLT_ANDS(a.V0,b.V0)});}
INLINE(Vwwu,VWWU_ANDS) (Vwwu a, Vwwu b) {return ((Vwwu){FLT_ANDS(a.V0,b.V0)});}
INLINE(Vwwi,VWWI_ANDS) (Vwwi a, Vwwi b) {return ((Vwwi){FLT_ANDS(a.V0,b.V0)});}
INLINE(Vwwf,VWWF_ANDS) (Vwwf a, Vwwf b) {return ((Vwwf){FLT_ANDS(a.V0,b.V0)});}

INLINE(Vdyu,VDYU_ANDS) (Vdyu a, Vdyu b) {return ((Vdyu){vand_u64(a.V0, b.V0)});}
INLINE(Vdbu,VDBU_ANDS) (Vdbu a, Vdbu b) {return  vand_u8(a, b);}
INLINE(Vdbi,VDBI_ANDS) (Vdbi a, Vdbi b) {return  vand_s8(a, b);}
INLINE(Vdbc,VDBC_ANDS) (Vdbc a, Vdyu b) {return ((Vdbc){DBC_ANDS(a.V0, b.V0)});}
INLINE(Vdhu,VDHU_ANDS) (Vdhu a, Vdhu b) {return  vand_u16(a, b);}
INLINE(Vdhi,VDHI_ANDS) (Vdhi a, Vdhi b) {return  vand_s16(a, b);}
INLINE(Vdhf,VDHF_ANDS) (Vdhf a, Vdhf b) {return  DHF_ANDS(a, b);}
INLINE(Vdwu,VDWU_ANDS) (Vdwu a, Vdwu b) {return  vand_u32(a, b);}
INLINE(Vdwi,VDWI_ANDS) (Vdwi a, Vdwi b) {return  vand_s32(a, b);}
INLINE(Vdwf,VDWF_ANDS) (Vdwf a, Vdwf b) {return  DWF_ANDS(a, b);}
INLINE(Vddu,VDDU_ANDS) (Vddu a, Vddu b) {return  vand_u64(a, b);}
INLINE(Vddi,VDDI_ANDS) (Vddi a, Vddi b) {return  vand_s64(a, b);}
INLINE(Vddf,VDDF_ANDS) (Vddf a, Vddf b) {return  DDF_ANDS(a, b);}

INLINE(Vqyu,VQYU_ANDS) (Vqyu a, Vqyu b) {return ((Vqyu){QDU_ANDS(a.V0,b.V0)});}
INLINE(Vqbu,VQBU_ANDS) (Vqbu a, Vqbu b) {return vandq_u8(a, b);}
INLINE(Vqbi,VQBI_ANDS) (Vqbi a, Vqbi b) {return vandq_s8(a, b);}
INLINE(Vqbc,VQBC_ANDS) (Vqbc a, Vqyu b) {return ((Vqbc){QBC_ANDS(a.V0,b.V0)});}
INLINE(Vqhu,VQHU_ANDS) (Vqhu a, Vqhu b) {return vandq_u16(a, b);}
INLINE(Vqhi,VQHI_ANDS) (Vqhi a, Vqhi b) {return vandq_s16(a, b);}
INLINE(Vqhf,VQHF_ANDS) (Vqhf a, Vqhf b) {return  QHF_ANDS(a, b);}
INLINE(Vqwu,VQWU_ANDS) (Vqwu a, Vqwu b) {return vandq_u32(a, b);}
INLINE(Vqwi,VQWI_ANDS) (Vqwi a, Vqwi b) {return vandq_s32(a, b);}
INLINE(Vqwf,VQWF_ANDS) (Vqwf a, Vqwf b) {return  QWF_ANDS(a, b);}
INLINE(Vqdu,VQDU_ANDS) (Vqdu a, Vqdu b) {return vandq_u64(a, b);}
INLINE(Vqdi,VQDI_ANDS) (Vqdi a, Vqdi b) {return vandq_s64(a, b);}
INLINE(Vqdf,VQDF_ANDS) (Vqdf a, Vqdf b) {return QDF_ANDS(a, b);}
INLINE(Vqqu,VQQU_ANDS) (Vqqu a, Vqqu b) {return ((Vqqu){QDU_ANDS(a.V0,b.V0)});}
INLINE(Vqqi,VQQI_ANDS) (Vqqi a, Vqqi b) {return ((Vqqi){QDI_ANDS(a.V0,b.V0)});}
INLINE(Vqqf,VQQF_ANDS) (Vqqf a, Vqqf b) {return ((Vqqf){andsqf(a.V0,b.V0)});}

#if 0 // _LEAVE_ARM_ANDS
}
#endif

#if 0 // _ENTER_ARM_ANDN
{
#endif

INLINE(flt16_t, FLT16_ANDN)  (flt16_t a,  flt16_t b)
{
    DWRD_VTYPE x={.H.F={a}}, y={.H.F={b}};
    x.H.U = vbic_u16(x.H.U, y.H.U);
    return  vget_lane_f16(x.H.F, 0);
}

INLINE(  float,   FLT_ANDN)   (float a,   float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = vbic_u32(x.W.U, y.W.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE( double,   DBL_ANDN)  (double a,  double b)
{
    DWRD_VTYPE x={.D.F={a}}, y={.D.F={b}};
    x.D.U = vbic_u64(x.D.U, y.D.U);
    return  vget_lane_f64(x.D.F, 0);
}

INLINE(QUAD_FTYPE,andnqf)  (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE x={.F=a}, y={.F=b};
    x.D.U = vbicq_u64(x.D.U, y.D.U);
    return  x.F;
}

#if CHAR_MIN
#   define  DBC_ANDN vbic_s8
#   define  QBC_ANDN vbicq_s8
#else
#   define  DBC_ANDN vbic_u8
#   define  QBC_ANDN vbicq_u8
#endif

INLINE(float16x4_t,DHF_ANDN) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    z.H.U = vbic_u16(x.H.U, y.H.U);
    return  z.H.F;
}

INLINE(float32x2_t,DWF_ANDN) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.W.U = vbic_u32(x.W.U, y.W.U);
    return  z.W.F;
}

INLINE(float64x1_t,DDF_ANDN) (float64x1_t a, float64x1_t b)
{
    DWRD_VTYPE x={.D.F=a}, y={.D.F=b}, z;
    z.D.U = vbic_u32(x.D.U, y.D.U);
    return  z.D.F;
}


INLINE(float16x8_t,QHF_ANDN) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    z.H.U = vbicq_u16(x.H.U, y.H.U);
    return  z.H.F;
}

INLINE(float32x4_t,QWF_ANDN) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.W.U = vbicq_u32(x.W.U, y.W.U);
    return  z.W.F;
}

#define QDU_ANDN vbicq_u64
#define QDI_ANDN vbicq_s64
INLINE(float64x2_t,QDF_ANDN) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE x={.D.F=a}, y={.D.F=b}, z;
    z.D.U = vbicq_u64(x.D.U, y.D.U);
    return  z.D.F;
}

INLINE(Vwyu,VWYU_ANDN) (Vwyu a, Vwyu b) {return ((Vwyu){FLT_ANDN(a.V0,b.V0)});}
INLINE(Vwbu,VWBU_ANDN) (Vwbu a, Vwbu b) {return ((Vwbu){FLT_ANDN(a.V0,b.V0)});}
INLINE(Vwbi,VWBI_ANDN) (Vwbi a, Vwbi b) {return ((Vwbi){FLT_ANDN(a.V0,b.V0)});}
INLINE(Vwbc,VWBC_ANDN) (Vwbc a, Vwbc b) {return ((Vwbc){FLT_ANDN(a.V0,b.V0)});}
INLINE(Vwhu,VWHU_ANDN) (Vwhu a, Vwhu b) {return ((Vwhu){FLT_ANDN(a.V0,b.V0)});}
INLINE(Vwhi,VWHI_ANDN) (Vwhi a, Vwhi b) {return ((Vwhi){FLT_ANDN(a.V0,b.V0)});}
INLINE(Vwhf,VWHF_ANDN) (Vwhf a, Vwhf b) {return ((Vwhf){FLT_ANDN(a.V0,b.V0)});}
INLINE(Vwwu,VWWU_ANDN) (Vwwu a, Vwwu b) {return ((Vwwu){FLT_ANDN(a.V0,b.V0)});}
INLINE(Vwwi,VWWI_ANDN) (Vwwi a, Vwwi b) {return ((Vwwi){FLT_ANDN(a.V0,b.V0)});}
INLINE(Vwwf,VWWF_ANDN) (Vwwf a, Vwwf b) {return ((Vwwf){FLT_ANDN(a.V0,b.V0)});}

INLINE(Vdyu,VDYU_ANDN) (Vdyu a, Vdyu b) {return ((Vdyu){vbic_u64(a.V0, b.V0)});}
INLINE(Vdbu,VDBU_ANDN) (Vdbu a, Vdbu b) {return  vbic_u8(a, b);}
INLINE(Vdbi,VDBI_ANDN) (Vdbi a, Vdbi b) {return  vbic_s8(a, b);}
INLINE(Vdbc,VDBC_ANDN) (Vdbc a, Vdyu b) {return ((Vdbc){DBC_ANDN(a.V0, b.V0)});}
INLINE(Vdhu,VDHU_ANDN) (Vdhu a, Vdhu b) {return  vbic_u16(a, b);}
INLINE(Vdhi,VDHI_ANDN) (Vdhi a, Vdhi b) {return  vbic_s16(a, b);}
INLINE(Vdhf,VDHF_ANDN) (Vdhf a, Vdhf b) {return  DHF_ANDN(a, b);}
INLINE(Vdwu,VDWU_ANDN) (Vdwu a, Vdwu b) {return  vbic_u32(a, b);}
INLINE(Vdwi,VDWI_ANDN) (Vdwi a, Vdwi b) {return  vbic_s32(a, b);}
INLINE(Vdwf,VDWF_ANDN) (Vdwf a, Vdwf b) {return  DWF_ANDN(a, b);}
INLINE(Vddu,VDDU_ANDN) (Vddu a, Vddu b) {return  vbic_u64(a, b);}
INLINE(Vddi,VDDI_ANDN) (Vddi a, Vddi b) {return  vbic_s64(a, b);}
INLINE(Vddf,VDDF_ANDN) (Vddf a, Vddf b) {return  DDF_ANDN(a, b);}

INLINE(Vqyu,VQYU_ANDN) (Vqyu a, Vqyu b) {return ((Vqyu){QDU_ANDN(a.V0,b.V0)});}
INLINE(Vqbu,VQBU_ANDN) (Vqbu a, Vqbu b) {return vbicq_u8(a, b);}
INLINE(Vqbi,VQBI_ANDN) (Vqbi a, Vqbi b) {return vbicq_s8(a, b);}
INLINE(Vqbc,VQBC_ANDN) (Vqbc a, Vqyu b) {return ((Vqbc){QBC_ANDN(a.V0,b.V0)});}
INLINE(Vqhu,VQHU_ANDN) (Vqhu a, Vqhu b) {return vbicq_u16(a, b);}
INLINE(Vqhi,VQHI_ANDN) (Vqhi a, Vqhi b) {return vbicq_s16(a, b);}
INLINE(Vqhf,VQHF_ANDN) (Vqhf a, Vqhf b) {return  QHF_ANDN(a, b);}
INLINE(Vqwu,VQWU_ANDN) (Vqwu a, Vqwu b) {return vbicq_u32(a, b);}
INLINE(Vqwi,VQWI_ANDN) (Vqwi a, Vqwi b) {return vbicq_s32(a, b);}
INLINE(Vqwf,VQWF_ANDN) (Vqwf a, Vqwf b) {return  QWF_ANDN(a, b);}
INLINE(Vqdu,VQDU_ANDN) (Vqdu a, Vqdu b) {return vbicq_u64(a, b);}
INLINE(Vqdi,VQDI_ANDN) (Vqdi a, Vqdi b) {return vbicq_s64(a, b);}
INLINE(Vqdf,VQDF_ANDN) (Vqdf a, Vqdf b) {return QDF_ANDN(a, b);}
INLINE(Vqqu,VQQU_ANDN) (Vqqu a, Vqqu b) {return ((Vqqu){QDU_ANDN(a.V0,b.V0)});}
INLINE(Vqqi,VQQI_ANDN) (Vqqi a, Vqqi b) {return ((Vqqi){QDI_ANDN(a.V0,b.V0)});}
INLINE(Vqqf,VQQF_ANDN) (Vqqf a, Vqqf b) {return ((Vqqf){andnqf(a.V0,b.V0)});}

#if 0 // _LEAVE_ARM_ANDN
}
#endif

#if 0 // _ENTER_ARM_ANDV
{
#endif

INLINE(_Bool,VWYU_ANDV) (Vwyu a)
{
    uint32_t v = FLT_ASTU(VWYU_ASTM(a));
    return v == UINT32_MAX;
}


INLINE(uint8_t,VWBU_ANDV) (Vwbu a)
{
    float   m = VWBU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint32x2_t  x = vreinterpret_u32_f32(f);
    uint32x2_t  y = vshr_n_u32(x, 16);
    x = vand_u32(x, y);
    y = vshr_n_u32(x, 8);
    x = vand_u32(x, y);
    return  vget_lane_u32(x, 0);
}

INLINE(int8_t, VWBI_ANDV) (Vwbi a)
{
    return  VWBU_ANDV(VWBI_ASBU(a));
}

INLINE(char, VWBC_ANDV) (Vwbc a)
{
    return  VWBU_ANDV(VWBC_ASBU(a));
}


INLINE(uint16_t,VWHU_ANDV) (Vwhu a)
{
    float       m = VWHU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint16x4_t  x = vreinterpret_u16_f32(f);
    return  vget_lane_u16(x, 0)&vget_lane_u16(x, 1);
}

INLINE( int16_t,VWHI_ANDV) (Vwhi a)
{
    return  VWHU_ANDV(VWHI_ASTU(a));
}


INLINE(_Bool,VDYU_ANDV) (Vdyu a)
{
    uint64x1_t l = VDYU_ASDU(a);
    l = vceq_u64(l, vdup_n_u64(UINT64_MAX));
    return vget_lane_u64(l, 0);
}

INLINE( uint8_t,VDBU_ANDV) (Vdbu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    r = l&r;
    l = r>>16;
    r = l&r;
    l = r>>8;
    return l&r;
}

INLINE(  int8_t,VDBI_ANDV) (Vdbi a)
{
    return  VDBU_ANDV(VDBI_ASTU(a));
}

INLINE(   char,VDBC_ANDV) (Vdbc a)
{
    return  VDBU_ANDV(VDBC_ASTU(a));
}


INLINE(uint16_t,VDHU_ANDV) (Vdhu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    l = l&r;
    r = l>>16;
    return l&r;
}

INLINE( int16_t,VDHI_ANDV) (Vdhi a)
{
    return  VDHU_ANDV(VDHI_ASTU(a));
}


INLINE(uint32_t,VDWU_ANDV) (Vdwu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    return  l&r;
}

INLINE( int32_t,VDWI_ANDV) (Vdwi a)
{
    return  VDWU_ANDV(VDWI_ASTU(a));
}


INLINE(   _Bool,VQYU_ANDV) (Vqyu a)
{
    uint64x2_t q = VQYU_ASDU(a);
    q = vceqq_u64(q, vdupq_n_u64(UINT64_MAX));
    uint64x1_t l = vget_low_u64(q);
    uint64x1_t r = vget_high_u64(q);
    l = vand_u64(l, r);
    return vget_lane_u64(l, 0);
}


INLINE( uint8_t,VQBU_ANDV) (Vqbu a)
{
    uint64x2_t q = vreinterpretq_u64_u8(a);
    uint64x1_t d = vand_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDBU_ANDV(VDDU_ASBU(d));
}

INLINE(  int8_t,VQBI_ANDV) (Vqbi a)
{
    return VQBU_ANDV(VQBI_ASTU(a));
}

INLINE(    char,VQBC_ANDV) (Vqbc a)
{
    return VQBU_ANDV(VQBC_ASTU(a));
}


INLINE(uint16_t,VQHU_ANDV) (Vqhu a)
{
    uint64x2_t q = vreinterpretq_u64_u16(a);
    uint64x1_t d = vand_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDHU_ANDV(VDDU_ASHU(d));
}

INLINE( int16_t,VQHI_ANDV) (Vqhi a)
{
    return VQHU_ANDV(VQHI_ASTU(a));
}


INLINE(uint32_t,VQWU_ANDV) (Vqwu a)
{
    uint64x2_t q = vreinterpretq_u64_u32(a);
    uint64x1_t d = vand_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDWU_ANDV(VDDU_ASWU(d));
}

INLINE( int32_t,VQWI_ANDV) (Vqwi a)
{
    return VQWU_ANDV(VQWI_ASTU(a));
}


INLINE(uint64_t,VQDU_ANDV) (Vqdu a)
{
    uint64x1_t d = vand_u64(
        vget_low_u64(a),
        vget_high_u64(a)
    );
    return  vget_lane_u64(d, 0);
}

INLINE( int64_t,VQDI_ANDV) (Vqdi a)
{
    int64x1_t d = vand_s64(
        vget_low_s64(a),
        vget_high_s64(a)
    );
    return  vget_lane_s64(d, 0);
}


#if 0 // _LEAVE_ARM_ANDV
}
#endif

#if 0 // _ENTER_ARM_ORRS
{
#endif

INLINE(flt16_t, FLT16_ORRS)  (flt16_t a,  flt16_t b)
{
    DWRD_VTYPE x={.H.F={a}}, y={.H.F={b}};
    x.H.U = vorr_u16(x.H.U, y.H.U);
    return  vget_lane_f16(x.H.F, 0);
}

INLINE(  float,   FLT_ORRS)   (float a,   float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = vorr_u32(x.W.U, y.W.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE( double,   DBL_ORRS)  (double a,  double b)
{
    DWRD_VTYPE x={.D.F={a}}, y={.D.F={b}};
    x.D.U = vorr_u64(x.D.U, y.D.U);
    return  vget_lane_f64(x.D.F, 0);
}

INLINE(QUAD_FTYPE,orrsqf)  (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE x={.F=a}, y={.F=b};
    x.D.U = vorrq_u64(x.D.U, y.D.U);
    return  x.F;
}

#if CHAR_MIN
#   define  DBC_ORRS vorr_s8
#   define  QBC_ORRS vorrq_s8
#else
#   define  DBC_ORRS vorr_u8
#   define  QBC_ORRS vorrq_u8
#endif

INLINE(float16x4_t,DHF_ORRS) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    z.H.U = vorr_u16(x.H.U, y.H.U);
    return  z.H.F;
}

INLINE(float32x2_t,DWF_ORRS) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.W.U = vorr_u32(x.W.U, y.W.U);
    return  z.W.F;
}

INLINE(float64x1_t,DDF_ORRS) (float64x1_t a, float64x1_t b)
{
    DWRD_VTYPE x={.D.F=a}, y={.D.F=b}, z;
    z.D.U = vorr_u32(x.D.U, y.D.U);
    return  z.D.F;
}


INLINE(float16x8_t,QHF_ORRS) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    z.H.U = vorrq_u16(x.H.U, y.H.U);
    return  z.H.F;
}

INLINE(float32x4_t,QWF_ORRS) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.W.U = vorrq_u32(x.W.U, y.W.U);
    return  z.W.F;
}

#define QDU_ORRS vorrq_u64
#define QDI_ORRS vorrq_s64
INLINE(float64x2_t,QDF_ORRS) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE x={.D.F=a}, y={.D.F=b}, z;
    z.D.U = vorrq_u64(x.D.U, y.D.U);
    return  z.D.F;
}

INLINE(Vwyu,VWYU_ORRS) (Vwyu a, Vwyu b) {return ((Vwyu){FLT_ORRS(a.V0,b.V0)});}
INLINE(Vwbu,VWBU_ORRS) (Vwbu a, Vwbu b) {return ((Vwbu){FLT_ORRS(a.V0,b.V0)});}
INLINE(Vwbi,VWBI_ORRS) (Vwbi a, Vwbi b) {return ((Vwbi){FLT_ORRS(a.V0,b.V0)});}
INLINE(Vwbc,VWBC_ORRS) (Vwbc a, Vwbc b) {return ((Vwbc){FLT_ORRS(a.V0,b.V0)});}
INLINE(Vwhu,VWHU_ORRS) (Vwhu a, Vwhu b) {return ((Vwhu){FLT_ORRS(a.V0,b.V0)});}
INLINE(Vwhi,VWHI_ORRS) (Vwhi a, Vwhi b) {return ((Vwhi){FLT_ORRS(a.V0,b.V0)});}
INLINE(Vwhf,VWHF_ORRS) (Vwhf a, Vwhf b) {return ((Vwhf){FLT_ORRS(a.V0,b.V0)});}
INLINE(Vwwu,VWWU_ORRS) (Vwwu a, Vwwu b) {return ((Vwwu){FLT_ORRS(a.V0,b.V0)});}
INLINE(Vwwi,VWWI_ORRS) (Vwwi a, Vwwi b) {return ((Vwwi){FLT_ORRS(a.V0,b.V0)});}
INLINE(Vwwf,VWWF_ORRS) (Vwwf a, Vwwf b) {return ((Vwwf){FLT_ORRS(a.V0,b.V0)});}

INLINE(Vdyu,VDYU_ORRS) (Vdyu a, Vdyu b) {return ((Vdyu){vorr_u64(a.V0, b.V0)});}
INLINE(Vdbu,VDBU_ORRS) (Vdbu a, Vdbu b) {return  vorr_u8(a, b);}
INLINE(Vdbi,VDBI_ORRS) (Vdbi a, Vdbi b) {return  vorr_s8(a, b);}
INLINE(Vdbc,VDBC_ORRS) (Vdbc a, Vdyu b) {return ((Vdbc){DBC_ORRS(a.V0, b.V0)});}
INLINE(Vdhu,VDHU_ORRS) (Vdhu a, Vdhu b) {return  vorr_u16(a, b);}
INLINE(Vdhi,VDHI_ORRS) (Vdhi a, Vdhi b) {return  vorr_s16(a, b);}
INLINE(Vdhf,VDHF_ORRS) (Vdhf a, Vdhf b) {return  DHF_ORRS(a, b);}
INLINE(Vdwu,VDWU_ORRS) (Vdwu a, Vdwu b) {return  vorr_u32(a, b);}
INLINE(Vdwi,VDWI_ORRS) (Vdwi a, Vdwi b) {return  vorr_s32(a, b);}
INLINE(Vdwf,VDWF_ORRS) (Vdwf a, Vdwf b) {return  DWF_ORRS(a, b);}
INLINE(Vddu,VDDU_ORRS) (Vddu a, Vddu b) {return  vorr_u64(a, b);}
INLINE(Vddi,VDDI_ORRS) (Vddi a, Vddi b) {return  vorr_s64(a, b);}
INLINE(Vddf,VDDF_ORRS) (Vddf a, Vddf b) {return  DDF_ORRS(a, b);}

INLINE(Vqyu,VQYU_ORRS) (Vqyu a, Vqyu b) {return ((Vqyu){QDU_ORRS(a.V0,b.V0)});}
INLINE(Vqbu,VQBU_ORRS) (Vqbu a, Vqbu b) {return vorrq_u8(a, b);}
INLINE(Vqbi,VQBI_ORRS) (Vqbi a, Vqbi b) {return vorrq_s8(a, b);}
INLINE(Vqbc,VQBC_ORRS) (Vqbc a, Vqyu b) {return ((Vqbc){QBC_ORRS(a.V0,b.V0)});}
INLINE(Vqhu,VQHU_ORRS) (Vqhu a, Vqhu b) {return vorrq_u16(a, b);}
INLINE(Vqhi,VQHI_ORRS) (Vqhi a, Vqhi b) {return vorrq_s16(a, b);}
INLINE(Vqhf,VQHF_ORRS) (Vqhf a, Vqhf b) {return  QHF_ORRS(a, b);}
INLINE(Vqwu,VQWU_ORRS) (Vqwu a, Vqwu b) {return vorrq_u32(a, b);}
INLINE(Vqwi,VQWI_ORRS) (Vqwi a, Vqwi b) {return vorrq_s32(a, b);}
INLINE(Vqwf,VQWF_ORRS) (Vqwf a, Vqwf b) {return  QWF_ORRS(a, b);}
INLINE(Vqdu,VQDU_ORRS) (Vqdu a, Vqdu b) {return vorrq_u64(a, b);}
INLINE(Vqdi,VQDI_ORRS) (Vqdi a, Vqdi b) {return vorrq_s64(a, b);}
INLINE(Vqdf,VQDF_ORRS) (Vqdf a, Vqdf b) {return QDF_ORRS(a, b);}
INLINE(Vqqu,VQQU_ORRS) (Vqqu a, Vqqu b) {return ((Vqqu){QDU_ORRS(a.V0,b.V0)});}
INLINE(Vqqi,VQQI_ORRS) (Vqqi a, Vqqi b) {return ((Vqqi){QDI_ORRS(a.V0,b.V0)});}
INLINE(Vqqf,VQQF_ORRS) (Vqqf a, Vqqf b) {return ((Vqqf){orrsqf(a.V0,b.V0)});}

#if 0 // _LEAVE_ARM_ORRS
}
#endif

#if 0 // _ENTER_ARM_ORRN
{
#endif

INLINE(flt16_t, FLT16_ORRN)  (flt16_t a,  flt16_t b)
{
    DWRD_VTYPE x={.H.F={a}}, y={.H.F={b}};
    x.H.U = vorn_u16(x.H.U, y.H.U);
    return  vget_lane_f16(x.H.F, 0);
}

INLINE(  float,   FLT_ORRN)   (float a,   float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = vorn_u32(x.W.U, y.W.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE( double,   DBL_ORRN)  (double a,  double b)
{
    DWRD_VTYPE x={.D.F={a}}, y={.D.F={b}};
    x.D.U = vorn_u64(x.D.U, y.D.U);
    return  vget_lane_f64(x.D.F, 0);
}

INLINE(QUAD_FTYPE,orrnqf)  (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE x={.F=a}, y={.F=b};
    x.D.U = vornq_u64(x.D.U, y.D.U);
    return  x.F;
}

#if CHAR_MIN
#   define  DBC_ORRN vorn_s8
#   define  QBC_ORRN vornq_s8
#else
#   define  DBC_ORRN vorn_u8
#   define  QBC_ORRN vornq_u8
#endif

INLINE(float16x4_t,DHF_ORRN) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    z.H.U = vorn_u16(x.H.U, y.H.U);
    return  z.H.F;
}

INLINE(float32x2_t,DWF_ORRN) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.W.U = vorn_u32(x.W.U, y.W.U);
    return  z.W.F;
}

INLINE(float64x1_t,DDF_ORRN) (float64x1_t a, float64x1_t b)
{
    DWRD_VTYPE x={.D.F=a}, y={.D.F=b}, z;
    z.D.U = vorn_u32(x.D.U, y.D.U);
    return  z.D.F;
}


INLINE(float16x8_t,QHF_ORRN) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    z.H.U = vornq_u16(x.H.U, y.H.U);
    return  z.H.F;
}

INLINE(float32x4_t,QWF_ORRN) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.W.U = vornq_u32(x.W.U, y.W.U);
    return  z.W.F;
}

#define QDU_ORRN vornq_u64
#define QDI_ORRN vornq_s64
INLINE(float64x2_t,QDF_ORRN) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE x={.D.F=a}, y={.D.F=b}, z;
    z.D.U = vornq_u64(x.D.U, y.D.U);
    return  z.D.F;
}

INLINE(Vwyu,VWYU_ORRN) (Vwyu a, Vwyu b) {return ((Vwyu){FLT_ORRN(a.V0,b.V0)});}
INLINE(Vwbu,VWBU_ORRN) (Vwbu a, Vwbu b) {return ((Vwbu){FLT_ORRN(a.V0,b.V0)});}
INLINE(Vwbi,VWBI_ORRN) (Vwbi a, Vwbi b) {return ((Vwbi){FLT_ORRN(a.V0,b.V0)});}
INLINE(Vwbc,VWBC_ORRN) (Vwbc a, Vwbc b) {return ((Vwbc){FLT_ORRN(a.V0,b.V0)});}
INLINE(Vwhu,VWHU_ORRN) (Vwhu a, Vwhu b) {return ((Vwhu){FLT_ORRN(a.V0,b.V0)});}
INLINE(Vwhi,VWHI_ORRN) (Vwhi a, Vwhi b) {return ((Vwhi){FLT_ORRN(a.V0,b.V0)});}
INLINE(Vwhf,VWHF_ORRN) (Vwhf a, Vwhf b) {return ((Vwhf){FLT_ORRN(a.V0,b.V0)});}
INLINE(Vwwu,VWWU_ORRN) (Vwwu a, Vwwu b) {return ((Vwwu){FLT_ORRN(a.V0,b.V0)});}
INLINE(Vwwi,VWWI_ORRN) (Vwwi a, Vwwi b) {return ((Vwwi){FLT_ORRN(a.V0,b.V0)});}
INLINE(Vwwf,VWWF_ORRN) (Vwwf a, Vwwf b) {return ((Vwwf){FLT_ORRN(a.V0,b.V0)});}

INLINE(Vdyu,VDYU_ORRN) (Vdyu a, Vdyu b) {return ((Vdyu){vorn_u64(a.V0, b.V0)});}
INLINE(Vdbu,VDBU_ORRN) (Vdbu a, Vdbu b) {return  vorn_u8(a, b);}
INLINE(Vdbi,VDBI_ORRN) (Vdbi a, Vdbi b) {return  vorn_s8(a, b);}
INLINE(Vdbc,VDBC_ORRN) (Vdbc a, Vdyu b) {return ((Vdbc){DBC_ORRN(a.V0, b.V0)});}
INLINE(Vdhu,VDHU_ORRN) (Vdhu a, Vdhu b) {return  vorn_u16(a, b);}
INLINE(Vdhi,VDHI_ORRN) (Vdhi a, Vdhi b) {return  vorn_s16(a, b);}
INLINE(Vdhf,VDHF_ORRN) (Vdhf a, Vdhf b) {return  DHF_ORRN(a, b);}
INLINE(Vdwu,VDWU_ORRN) (Vdwu a, Vdwu b) {return  vorn_u32(a, b);}
INLINE(Vdwi,VDWI_ORRN) (Vdwi a, Vdwi b) {return  vorn_s32(a, b);}
INLINE(Vdwf,VDWF_ORRN) (Vdwf a, Vdwf b) {return  DWF_ORRN(a, b);}
INLINE(Vddu,VDDU_ORRN) (Vddu a, Vddu b) {return  vorn_u64(a, b);}
INLINE(Vddi,VDDI_ORRN) (Vddi a, Vddi b) {return  vorn_s64(a, b);}
INLINE(Vddf,VDDF_ORRN) (Vddf a, Vddf b) {return  DDF_ORRN(a, b);}

INLINE(Vqyu,VQYU_ORRN) (Vqyu a, Vqyu b) {return ((Vqyu){QDU_ORRN(a.V0,b.V0)});}
INLINE(Vqbu,VQBU_ORRN) (Vqbu a, Vqbu b) {return vornq_u8(a, b);}
INLINE(Vqbi,VQBI_ORRN) (Vqbi a, Vqbi b) {return vornq_s8(a, b);}
INLINE(Vqbc,VQBC_ORRN) (Vqbc a, Vqyu b) {return ((Vqbc){QBC_ORRN(a.V0,b.V0)});}
INLINE(Vqhu,VQHU_ORRN) (Vqhu a, Vqhu b) {return vornq_u16(a, b);}
INLINE(Vqhi,VQHI_ORRN) (Vqhi a, Vqhi b) {return vornq_s16(a, b);}
INLINE(Vqhf,VQHF_ORRN) (Vqhf a, Vqhf b) {return  QHF_ORRN(a, b);}
INLINE(Vqwu,VQWU_ORRN) (Vqwu a, Vqwu b) {return vornq_u32(a, b);}
INLINE(Vqwi,VQWI_ORRN) (Vqwi a, Vqwi b) {return vornq_s32(a, b);}
INLINE(Vqwf,VQWF_ORRN) (Vqwf a, Vqwf b) {return  QWF_ORRN(a, b);}
INLINE(Vqdu,VQDU_ORRN) (Vqdu a, Vqdu b) {return vornq_u64(a, b);}
INLINE(Vqdi,VQDI_ORRN) (Vqdi a, Vqdi b) {return vornq_s64(a, b);}
INLINE(Vqdf,VQDF_ORRN) (Vqdf a, Vqdf b) {return QDF_ORRN(a, b);}
INLINE(Vqqu,VQQU_ORRN) (Vqqu a, Vqqu b) {return ((Vqqu){QDU_ORRN(a.V0,b.V0)});}
INLINE(Vqqi,VQQI_ORRN) (Vqqi a, Vqqi b) {return ((Vqqi){QDI_ORRN(a.V0,b.V0)});}
INLINE(Vqqf,VQQF_ORRN) (Vqqf a, Vqqf b) {return ((Vqqf){orrnqf(a.V0,b.V0)});}

#if 0 // _LEAVE_ARM_ORRN
}
#endif

#if 0 // _ENTER_ARM_ORRV
{
#endif

INLINE(_Bool,VWYU_ORRV) (Vwyu a)
{
    float f = VWYU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vcgt_u32(v, vdup_n_u32(0));
    return  vget_lane_u32(v, 0);
}

INLINE( uint8_t,VWBU_ORRV) (Vwbu a)
{
    float f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    uint32_t    l = vget_lane_u32(v, 0);
    return ((l>>24)|(l>>16)|(l>>8)|l)&UINT8_MAX;
}

INLINE(int8_t, VWBI_ORRV) (Vwbi a)
{
    return  VWBU_ORRV(VWBI_ASTU(a));
}

INLINE(char, VWBC_ORRV) (Vwbc a)
{
    return  VWBU_ORRV(VWBC_ASTU(a));
}


INLINE( uint16_t,VWHU_ORRV) (Vwhu a)
{
    float f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    uint32_t    l = vget_lane_u32(v, 0);
    return  ((l>>16)|l)&UINT16_MAX;
}

INLINE(int16_t, VWHI_ORRV) (Vwhi a)
{
    return  VWHU_ORRV(VWHI_ASTU(a));
}


INLINE(_Bool,VDYU_ORRV) (Vdyu a)
{
    uint64x1_t l = VDYU_ASDU(a);
    l = vtst_u64(l, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(l, 0);
}

INLINE( uint8_t,VDBU_ORRV) (Vdbu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    r = l|r;
    l = r>>16;
    r = l|r;
    l = r>>8;
    return l|r;
}

INLINE(  int8_t,VDBI_ORRV) (Vdbi a)
{
    return  VDBU_ORRV(VDBI_ASTU(a));
}

INLINE(   char,VDBC_ORRV) (Vdbc a)
{
    return  VDBU_ORRV(VDBC_ASTU(a));
}


INLINE(uint16_t,VDHU_ORRV) (Vdhu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    l = l|r;
    r = l>>16;
    return l|r;
}

INLINE( int16_t,VDHI_ORRV) (Vdhi a)
{
    return  VDHU_ORRV(VDHI_ASTU(a));
}


INLINE(uint32_t,VDWU_ORRV) (Vdwu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    return  l|r;
}

INLINE( int32_t,VDWI_ORRV) (Vdwi a)
{
    return  VDWU_ORRV(VDWI_ASTU(a));
}


INLINE(   _Bool,VQYU_ORRV) (Vqyu a)
{
    uint64x2_t q = VQYU_ASDU(a);
    q = vtstq_u64(q, vdupq_n_u64(UINT64_MAX));
    uint64x1_t l = vget_low_u64(q);
    uint64x1_t r = vget_high_u64(q);
    l = vorr_u64(l, r);
    return  vget_lane_u64(l, 0);
}


INLINE( uint8_t,VQBU_ORRV) (Vqbu a)
{
    uint64x2_t q = vreinterpretq_u64_u8(a);
    uint64x1_t d = vorr_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDBU_ORRV(VDDU_ASBU(d));
}

INLINE(  int8_t,VQBI_ORRV) (Vqbi a)
{
    return VQBU_ORRV(VQBI_ASTU(a));
}

INLINE(    char,VQBC_ORRV) (Vqbc a)
{
    return VQBU_ORRV(VQBC_ASTU(a));
}


INLINE(uint16_t,VQHU_ORRV) (Vqhu a)
{
    uint64x2_t q = vreinterpretq_u64_u16(a);
    uint64x1_t d = vand_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDHU_ORRV(VDDU_ASHU(d));
}

INLINE( int16_t,VQHI_ORRV) (Vqhi a)
{
    return VQHU_ORRV(VQHI_ASTU(a));
}


INLINE(uint32_t,VQWU_ORRV) (Vqwu a)
{
    uint64x2_t q = vreinterpretq_u64_u32(a);
    uint64x1_t d = vand_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDWU_ORRV(VDDU_ASWU(d));
}

INLINE( int32_t,VQWI_ORRV) (Vqwi a)
{
    return VQWU_ORRV(VQWI_ASTU(a));
}


INLINE(uint64_t,VQDU_ORRV) (Vqdu a)
{
    uint64x1_t d = vand_u64(
        vget_low_u64(a),
        vget_high_u64(a)
    );
    return  vget_lane_u64(d, 0);
}

INLINE( int64_t,VQDI_ORRV) (Vqdi a)
{
    int64x1_t d = vand_s64(
        vget_low_s64(a),
        vget_high_s64(a)
    );
    return  vget_lane_s64(d, 0);
}

#if 0 // _LEAVE_ARM_ORRV
}
#endif

#if 0 // _ENTER_ARM_XORS
{
#endif

INLINE(flt16_t, FLT16_XORS)  (flt16_t a,  flt16_t b)
{
    DWRD_VTYPE x={.H.F={a}}, y={.H.F={b}};
    x.H.U = veor_u16(x.H.U, y.H.U);
    return  vget_lane_f16(x.H.F, 0);
}

INLINE(  float,   FLT_XORS)   (float a,   float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = veor_u32(x.W.U, y.W.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE( double,   DBL_XORS)  (double a,  double b)
{
    DWRD_VTYPE x={.D.F={a}}, y={.D.F={b}};
    x.D.U = veor_u64(x.D.U, y.D.U);
    return  vget_lane_f64(x.D.F, 0);
}

INLINE(QUAD_FTYPE,xorsqf)  (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE x={.F=a}, y={.F=b};
    x.D.U = veorq_u64(x.D.U, y.D.U);
    return  x.F;
}

#if CHAR_MIN
#   define  DBC_XORS veor_s8
#   define  QBC_XORS veorq_s8
#else
#   define  DBC_XORS veor_u8
#   define  QBC_XORS veorq_u8
#endif

INLINE(float16x4_t,DHF_XORS) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    z.H.U = veor_u16(x.H.U, y.H.U);
    return  z.H.F;
}

INLINE(float32x2_t,DWF_XORS) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.W.U = veor_u32(x.W.U, y.W.U);
    return  z.W.F;
}

INLINE(float64x1_t,DDF_XORS) (float64x1_t a, float64x1_t b)
{
    DWRD_VTYPE x={.D.F=a}, y={.D.F=b}, z;
    z.D.U = veor_u32(x.D.U, y.D.U);
    return  z.D.F;
}


INLINE(float16x8_t,QHF_XORS) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    z.H.U = veorq_u16(x.H.U, y.H.U);
    return  z.H.F;
}

INLINE(float32x4_t,QWF_XORS) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.W.U = veorq_u32(x.W.U, y.W.U);
    return  z.W.F;
}

#define QDU_XORS veorq_u64
#define QDI_XORS veorq_s64
INLINE(float64x2_t,QDF_XORS) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE x={.D.F=a}, y={.D.F=b}, z;
    z.D.U = veorq_u64(x.D.U, y.D.U);
    return  z.D.F;
}

INLINE(Vwyu,VWYU_XORS) (Vwyu a, Vwyu b) {return ((Vwyu){FLT_XORS(a.V0,b.V0)});}
INLINE(Vwbu,VWBU_XORS) (Vwbu a, Vwbu b) {return ((Vwbu){FLT_XORS(a.V0,b.V0)});}
INLINE(Vwbi,VWBI_XORS) (Vwbi a, Vwbi b) {return ((Vwbi){FLT_XORS(a.V0,b.V0)});}
INLINE(Vwbc,VWBC_XORS) (Vwbc a, Vwbc b) {return ((Vwbc){FLT_XORS(a.V0,b.V0)});}
INLINE(Vwhu,VWHU_XORS) (Vwhu a, Vwhu b) {return ((Vwhu){FLT_XORS(a.V0,b.V0)});}
INLINE(Vwhi,VWHI_XORS) (Vwhi a, Vwhi b) {return ((Vwhi){FLT_XORS(a.V0,b.V0)});}
INLINE(Vwhf,VWHF_XORS) (Vwhf a, Vwhf b) {return ((Vwhf){FLT_XORS(a.V0,b.V0)});}
INLINE(Vwwu,VWWU_XORS) (Vwwu a, Vwwu b) {return ((Vwwu){FLT_XORS(a.V0,b.V0)});}
INLINE(Vwwi,VWWI_XORS) (Vwwi a, Vwwi b) {return ((Vwwi){FLT_XORS(a.V0,b.V0)});}
INLINE(Vwwf,VWWF_XORS) (Vwwf a, Vwwf b) {return ((Vwwf){FLT_XORS(a.V0,b.V0)});}

INLINE(Vdyu,VDYU_XORS) (Vdyu a, Vdyu b) {return ((Vdyu){veor_u64(a.V0, b.V0)});}
INLINE(Vdbu,VDBU_XORS) (Vdbu a, Vdbu b) {return  veor_u8(a, b);}
INLINE(Vdbi,VDBI_XORS) (Vdbi a, Vdbi b) {return  veor_s8(a, b);}
INLINE(Vdbc,VDBC_XORS) (Vdbc a, Vdyu b) {return ((Vdbc){DBC_XORS(a.V0, b.V0)});}
INLINE(Vdhu,VDHU_XORS) (Vdhu a, Vdhu b) {return  veor_u16(a, b);}
INLINE(Vdhi,VDHI_XORS) (Vdhi a, Vdhi b) {return  veor_s16(a, b);}
INLINE(Vdhf,VDHF_XORS) (Vdhf a, Vdhf b) {return  DHF_XORS(a, b);}
INLINE(Vdwu,VDWU_XORS) (Vdwu a, Vdwu b) {return  veor_u32(a, b);}
INLINE(Vdwi,VDWI_XORS) (Vdwi a, Vdwi b) {return  veor_s32(a, b);}
INLINE(Vdwf,VDWF_XORS) (Vdwf a, Vdwf b) {return  DWF_XORS(a, b);}
INLINE(Vddu,VDDU_XORS) (Vddu a, Vddu b) {return  veor_u64(a, b);}
INLINE(Vddi,VDDI_XORS) (Vddi a, Vddi b) {return  veor_s64(a, b);}
INLINE(Vddf,VDDF_XORS) (Vddf a, Vddf b) {return  DDF_XORS(a, b);}

INLINE(Vqyu,VQYU_XORS) (Vqyu a, Vqyu b) {return ((Vqyu){QDU_XORS(a.V0,b.V0)});}
INLINE(Vqbu,VQBU_XORS) (Vqbu a, Vqbu b) {return veorq_u8(a, b);}
INLINE(Vqbi,VQBI_XORS) (Vqbi a, Vqbi b) {return veorq_s8(a, b);}
INLINE(Vqbc,VQBC_XORS) (Vqbc a, Vqyu b) {return ((Vqbc){QBC_XORS(a.V0,b.V0)});}
INLINE(Vqhu,VQHU_XORS) (Vqhu a, Vqhu b) {return veorq_u16(a, b);}
INLINE(Vqhi,VQHI_XORS) (Vqhi a, Vqhi b) {return veorq_s16(a, b);}
INLINE(Vqhf,VQHF_XORS) (Vqhf a, Vqhf b) {return  QHF_XORS(a, b);}
INLINE(Vqwu,VQWU_XORS) (Vqwu a, Vqwu b) {return veorq_u32(a, b);}
INLINE(Vqwi,VQWI_XORS) (Vqwi a, Vqwi b) {return veorq_s32(a, b);}
INLINE(Vqwf,VQWF_XORS) (Vqwf a, Vqwf b) {return  QWF_XORS(a, b);}
INLINE(Vqdu,VQDU_XORS) (Vqdu a, Vqdu b) {return veorq_u64(a, b);}
INLINE(Vqdi,VQDI_XORS) (Vqdi a, Vqdi b) {return veorq_s64(a, b);}
INLINE(Vqdf,VQDF_XORS) (Vqdf a, Vqdf b) {return QDF_XORS(a, b);}
INLINE(Vqqu,VQQU_XORS) (Vqqu a, Vqqu b) {return ((Vqqu){QDU_XORS(a.V0,b.V0)});}
INLINE(Vqqi,VQQI_XORS) (Vqqi a, Vqqi b) {return ((Vqqi){QDI_XORS(a.V0,b.V0)});}
INLINE(Vqqf,VQQF_XORS) (Vqqf a, Vqqf b) {return ((Vqqf){xorsqf(a.V0,b.V0)});}

#if 0 // _LEAVE_ARM_XORS
}
#endif

#if 0 // _ENTER_ARM_XORN
{
#endif
/*  clang reorders 'a XOR NOT b' to 'NOT (a XOR b)', probably
    since it limits modifications to a single register. 
    However, since this is C and xorn is technically defined
    as equivalent to the C expression 'a^~b', we aren't 
    going to try to affect the ordering. However, the naive
    implementation of the float variants is predictably bad,
    although the difference isn't as significant for XORN 
    compared to the other 5 bitwise logical ops:

        STG_TYPE x={.F=a}, y={.F=b};
        x.U ^= ~y.U;
        return x.F
    
    that results in the following:

        fmov w8, s0
        fmov w9, s1
        eon  w8, w8, w9
        fmov s0, w8 
    
    Our "complicated" implementation is much more efficient:
        eor v0.8B, v0.8B, v1.8B
        mvn v0.8B, v0.8B

*/
INLINE( uint8x8_t,DBU_XORN)(uint8x8_t a, uint8x8_t b)
{
    b = vmvn_u8(b);
    return  veor_u8(a, b);
}

INLINE(  int8x8_t,DBI_XORN)(int8x8_t a, int8x8_t b)
{
    b = vmvn_s8(b);
    return  veor_s8(a, b);
}

#if CHAR_MIN
#   define DBC_XORN DBI_XORN
#else
#   define DBC_XORN DBU_XORN
#endif

INLINE( uint16x4_t,DHU_XORN)  (uint16x4_t a,  uint16x4_t b)
{
    DWRD_VTYPE x={.H.U=a}, y={.H.U=b};
    x.B.U = DBU_XORN(x.B.U, y.B.U);
    return  x.H.U;
}

INLINE(  int16x4_t,DHI_XORN)   (int16x4_t a,   int16x4_t b)
{
    DWRD_VTYPE x={.H.I=a}, y={.H.I=b};
    x.B.U = DBU_XORN(x.B.U, y.B.U);
    return  x.H.I;
}

INLINE(float16x4_t,DHF_XORN) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE x={.H.F=a}, y={.H.F=b};
    x.B.U = DBU_XORN(x.B.U, y.B.U);
    return  x.H.F;
}

INLINE( uint32x2_t,DWU_XORN)  (uint32x2_t a,  uint32x2_t b)
{
    DWRD_VTYPE x={.W.U=a}, y={.W.U=b};
    x.B.U = DBU_XORN(x.B.U, y.B.U);
    return  x.W.U;
}

INLINE(  int32x2_t,DWI_XORN)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE x={.W.I=a}, y={.W.I=b};
    x.B.U = DBU_XORN(x.B.U, y.B.U);
    return  x.W.I;
}

INLINE(float32x2_t,DWF_XORN) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.B.U = DBU_XORN(x.B.U, y.B.U);
    return  x.W.F;
}

INLINE( uint64x1_t,DDU_XORN)  (uint64x1_t a,  uint64x1_t b)
{
    DWRD_VTYPE x={.D.U=a}, y={.D.U=b};
    x.B.U = DBU_XORN(x.B.U, y.B.U);
    return  x.D.U;
}

INLINE(  int64x1_t,DDI_XORN)   (int64x1_t a,   int64x1_t b)
{
    DWRD_VTYPE x={.D.I=a}, y={.D.I=b};
    x.B.U = DBU_XORN(x.B.U, y.B.U);
    return  x.D.I;
}

INLINE(float64x1_t,DDF_XORN) (float64x1_t a, float64x1_t b)
{
    DWRD_VTYPE x={.D.F=a}, y={.D.F=b};
    x.B.U = DBU_XORN(x.B.U, y.B.U);
    return  x.D.F;
}


INLINE( uint8x16_t,QBU_XORN)  (uint8x16_t a,  uint8x16_t b)
{
    b = vmvnq_u8(b);
    return  veorq_u8(a, b);
}

INLINE(int8x16_t,QBI_XORN)(int8x16_t a, int8x16_t b)
{
    b = vmvnq_u8(b);
    return  veorq_u8(a, b);
}

#if CHAR_MIN
#   define QBC_XORN QBI_XORN
#else
#   define QBC_XORN QBU_XORN
#endif

INLINE(uint16x8_t,QHU_XORN)(uint16x8_t a, uint16x8_t b)
{
    QUAD_VTYPE x={.H.U=a}, y={.H.U=b};
    x.B.U = QBU_XORN(x.B.U, y.B.U);
    return  x.H.U;
}

INLINE(int16x8_t,QHI_XORN)(int16x8_t a, int16x8_t b)
{
    QUAD_VTYPE x={.H.I=a}, y={.H.I=b};
    x.B.U = QBU_XORN(x.B.U, y.B.U);
    return  x.H.I;
}

INLINE(float16x8_t,QHF_XORN) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE x={.H.F=a}, y={.H.F=b};
    x.B.U = QBU_XORN(x.B.U, y.B.U);
    return  x.H.F;
}

INLINE(uint32x4_t,QWU_XORN)(uint32x4_t a, uint32x4_t b)
{
    QUAD_VTYPE x={.W.U=a}, y={.W.U=b};
    x.B.U = QBU_XORN(x.B.U, y.B.U);
    return  x.W.U;
}

INLINE(int32x4_t,QWI_XORN)(int32x4_t a, int32x4_t b)
{
    QUAD_VTYPE x={.W.I=a}, y={.W.I=b};
    x.B.U = QBU_XORN(x.B.U, y.B.U);
    return  x.W.I;
}

INLINE(float32x4_t,QWF_XORN) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.B.U = QBU_XORN(x.B.U, y.B.U);
    return  x.W.F;
}

INLINE(uint64x2_t,QDU_XORN) (uint64x2_t a, uint64x2_t b)
{
    QUAD_VTYPE x={.D.U=a}, y={.D.U=b};
    x.B.U = QBU_XORN(x.B.U, y.B.U);
    return  x.D.U;
}

INLINE(int64x2_t,QDI_XORN) (int64x2_t a, int64x2_t b)
{
    QUAD_VTYPE x={.D.I=a}, y={.D.I=b};
    x.B.U = QBU_XORN(x.B.U, y.B.U);
    return  x.D.I;
}

INLINE(float64x2_t,QDF_XORN) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE x={.D.F=a}, y={.D.F=b};
    x.B.U = QBU_XORN(x.B.U, y.B.U);
    return  x.D.F;
}

INLINE(flt16_t, FLT16_XORN)  (flt16_t a,  flt16_t b)
{
    float16x4_t x={a}, y={b};
    x = DHF_XORN(x, y);
    return  vget_lane_f16(x, 0);
}

INLINE(  float,   FLT_XORN)   (float a,   float b)
{    
    float32x2_t x={a}, y={b};
    x = DWF_XORN(x, y);
    return  vget_lane_f32(x, 0);
}

INLINE( double,   DBL_XORN)  (double a,  double b)
{
    float64x1_t x={a}, y={b};
    x = DDF_XORN(x, y);
    return  vget_lane_f64(x, 0);
}

INLINE(QUAD_FTYPE,xornqf)  (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE x={.F=a}, y={.F=b};
    x.B.U = QBU_XORN(x.B.U, y.B.U);
    return  x.F;
}

INLINE(Vwyu,VWYU_XORN) (Vwyu a, Vwyu b) {return ((Vwyu){FLT_XORN(a.V0,b.V0)});}
INLINE(Vwbu,VWBU_XORN) (Vwbu a, Vwbu b) {return ((Vwbu){FLT_XORN(a.V0,b.V0)});}
INLINE(Vwbi,VWBI_XORN) (Vwbi a, Vwbi b) {return ((Vwbi){FLT_XORN(a.V0,b.V0)});}
INLINE(Vwbc,VWBC_XORN) (Vwbc a, Vwbc b) {return ((Vwbc){FLT_XORN(a.V0,b.V0)});}
INLINE(Vwhu,VWHU_XORN) (Vwhu a, Vwhu b) {return ((Vwhu){FLT_XORN(a.V0,b.V0)});}
INLINE(Vwhi,VWHI_XORN) (Vwhi a, Vwhi b) {return ((Vwhi){FLT_XORN(a.V0,b.V0)});}
INLINE(Vwhf,VWHF_XORN) (Vwhf a, Vwhf b) {return ((Vwhf){FLT_XORN(a.V0,b.V0)});}
INLINE(Vwwu,VWWU_XORN) (Vwwu a, Vwwu b) {return ((Vwwu){FLT_XORN(a.V0,b.V0)});}
INLINE(Vwwi,VWWI_XORN) (Vwwi a, Vwwi b) {return ((Vwwi){FLT_XORN(a.V0,b.V0)});}
INLINE(Vwwf,VWWF_XORN) (Vwwf a, Vwwf b) {return ((Vwwf){FLT_XORN(a.V0,b.V0)});}

INLINE(Vdyu,VDYU_XORN) (Vdyu a, Vdyu b) {return ((Vdyu){DDU_XORN(a.V0, b.V0)});}
INLINE(Vdbu,VDBU_XORN) (Vdbu a, Vdbu b) {return  DBU_XORN(a, b);}
INLINE(Vdbi,VDBI_XORN) (Vdbi a, Vdbi b) {return  DBI_XORN(a, b);}
INLINE(Vdbc,VDBC_XORN) (Vdbc a, Vdbc b) {return ((Vdbc){DBC_XORN(a.V0, b.V0)});}
INLINE(Vdhu,VDHU_XORN) (Vdhu a, Vdhu b) {return  DHU_XORN(a, b);}
INLINE(Vdhi,VDHI_XORN) (Vdhi a, Vdhi b) {return  DHI_XORN(a, b);}
INLINE(Vdhf,VDHF_XORN) (Vdhf a, Vdhf b) {return  DHF_XORN(a, b);}
INLINE(Vdwu,VDWU_XORN) (Vdwu a, Vdwu b) {return  DWU_XORN(a, b);}
INLINE(Vdwi,VDWI_XORN) (Vdwi a, Vdwi b) {return  DWI_XORN(a, b);}
INLINE(Vdwf,VDWF_XORN) (Vdwf a, Vdwf b) {return  DWF_XORN(a, b);}
INLINE(Vddu,VDDU_XORN) (Vddu a, Vddu b) {return  DDU_XORN(a, b);}
INLINE(Vddi,VDDI_XORN) (Vddi a, Vddi b) {return  DDI_XORN(a, b);}
INLINE(Vddf,VDDF_XORN) (Vddf a, Vddf b) {return  DDF_XORN(a, b);}

INLINE(Vqyu,VQYU_XORN) (Vqyu a, Vqyu b) {return ((Vqyu){QDU_XORN(a.V0,b.V0)});}
INLINE(Vqbu,VQBU_XORN) (Vqbu a, Vqbu b) {return QBU_XORN(a, b);}
INLINE(Vqbi,VQBI_XORN) (Vqbi a, Vqbi b) {return QBI_XORN(a, b);}
INLINE(Vqbc,VQBC_XORN) (Vqbc a, Vqbc b) {return ((Vqbc){QBC_XORN(a.V0,b.V0)});}
INLINE(Vqhu,VQHU_XORN) (Vqhu a, Vqhu b) {return QHU_XORN(a, b);}
INLINE(Vqhi,VQHI_XORN) (Vqhi a, Vqhi b) {return QHI_XORN(a, b);}
INLINE(Vqhf,VQHF_XORN) (Vqhf a, Vqhf b) {return QHF_XORN(a, b);}
INLINE(Vqwu,VQWU_XORN) (Vqwu a, Vqwu b) {return QWU_XORN(a, b);}
INLINE(Vqwi,VQWI_XORN) (Vqwi a, Vqwi b) {return QWI_XORN(a, b);}
INLINE(Vqwf,VQWF_XORN) (Vqwf a, Vqwf b) {return QWF_XORN(a, b);}
INLINE(Vqdu,VQDU_XORN) (Vqdu a, Vqdu b) {return QDU_XORN(a, b);}
INLINE(Vqdi,VQDI_XORN) (Vqdi a, Vqdi b) {return QDI_XORN(a, b);}
INLINE(Vqdf,VQDF_XORN) (Vqdf a, Vqdf b) {return QDF_XORN(a, b);}
INLINE(Vqqu,VQQU_XORN) (Vqqu a, Vqqu b) {return ((Vqqu){QDU_XORN(a.V0,b.V0)});}
INLINE(Vqqi,VQQI_XORN) (Vqqi a, Vqqi b) {return ((Vqqi){QDI_XORN(a.V0,b.V0)});}
INLINE(Vqqf,VQQF_XORN) (Vqqf a, Vqqf b) {return ((Vqqf){xornqf(a.V0,b.V0)});}

#if 0 // _LEAVE_ARM_XORN
}
#endif

#if 0 // _ENTER_ARM_XORV
{
#endif

INLINE(_Bool,VWYU_XORV) (Vwyu a)
{
    uint32_t v = VWWU_ASTV(VWYU_ASWU(a));
    v = v^(v>>16);
    v = v^(v>>8);
    v = v^(v>>4);
    v = v^(v>>2);
    return 1&(v^(v>>1));
}


INLINE( uint8_t,VWBU_XORV) (Vwbu a)
{
    uint32_t  v = VWWU_ASTV(VWBU_ASWU(a));
    v = v^(v>>16);
    v = v^(v>>8);
    return 0xff&v;
}

INLINE(  int8_t,VWBI_XORV) (Vwbi a)
{
    return VWBU_XORV(VWBI_ASTU(a));
}

INLINE(uint16_t,VWHU_XORV) (Vwhu a)
{
    uint32_t  v = VWWU_ASTV(VWHU_ASWU(a));
    v = v^(v>>16);
    return 0xffff&v;
}

INLINE( int16_t,VWHI_XORV) (Vwhi a)
{
    return VWHU_XORV(VWHI_ASTU(a));
}

INLINE(_Bool,VDYU_XORV) (Vdyu a)
{
    uint32x2_t l = VDYU_ASWU(a);
    uint32_t   x = vget_lane_u32(l, 0);
    uint32_t   y = vget_lane_u32(l, 1);
    x = x^y;
    x = (x>>16)^x;
    x = (x>>8)^x;
    x = (x>>4)^x;
    x = (x>>2)^x;
    return 1&((x>>1)^x);
}

INLINE( uint8_t,VDBU_XORV) (Vdbu a)
{
    uint32x2_t  v = vreinterpret_u32_u8(a);
    uint32_t    x = vget_lane_u32(v, 0)^vget_lane_u32(v, 1);
    x ^= (x>>16);
    return 0xff&(x^(x>>8));
}

INLINE(  int8_t,VDBI_XORV) (Vdbi a)
{
    return  VDBU_XORV(VDBI_ASTU(a));
}

INLINE(   char,VDBC_XORV) (Vdbc a)
{
    return  VDBU_XORV(VDBC_ASTU(a));
}

INLINE(uint16_t,VDHU_XORV) (Vdhu a)
{
    uint32x2_t  v = vreinterpret_u32_u16(a);
    uint32_t    x = vget_lane_u32(v, 0)^vget_lane_u32(v, 1);
    return 0xffff&((x>>16)^x);
}

INLINE( int16_t,VDHI_XORV) (Vdhi a)
{
    return  VDHU_XORV(VDHI_ASTU(a));
}

INLINE(uint32_t,VDWU_XORV) (Vdwu a)
{
    return vget_lane_u32(a, 0)^vget_lane_u32(a, 1);
}

INLINE( int32_t,VDWI_XORV) (Vdwi a)
{
    return vget_lane_s32(a, 0)^vget_lane_s32(a, 1);
}


INLINE(   _Bool,VQYU_XORV) (Vqyu a)
{
    uint64x2_t v = VQYU_ASDU(a);
    uint64_t   x = vgetq_lane_u64(v, 0)^vgetq_lane_u64(v, 1);
    x ^= x>>32;
    x ^= x>>16;
    x ^= x>>8;
    x ^= x>>4;
    x ^= x>>2;
    x ^= x>>1;
    return x&1;
}


INLINE( uint8_t,VQBU_XORV) (Vqbu a)
{
    uint64x2_t q = vreinterpretq_u64_u8(a);
    uint64x1_t d = veor_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDBU_XORV(VDDU_ASBU(d));
}

INLINE(  int8_t,VQBI_XORV) (Vqbi a)
{
    return VQBU_XORV(VQBI_ASTU(a));
}

INLINE(    char,VQBC_XORV) (Vqbc a)
{
    return VQBU_XORV(VQBC_ASTU(a));
}


INLINE(uint16_t,VQHU_XORV) (Vqhu a)
{
    uint64x2_t q = vreinterpretq_u64_u16(a);
    uint64x1_t d = veor_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDHU_XORV(VDDU_ASHU(d));
}

INLINE( int16_t,VQHI_XORV) (Vqhi a)
{
    return VQHU_XORV(VQHI_ASTU(a));
}


INLINE(uint32_t,VQWU_XORV) (Vqwu a)
{
    uint64x2_t q = vreinterpretq_u64_u32(a);
    uint64x1_t d = veor_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDWU_XORV(VDDU_ASWU(d));
}

INLINE( int32_t,VQWI_XORV) (Vqwi a)
{
    return VQWU_XORV(VQWI_ASTU(a));
}


INLINE(uint64_t,VQDU_XORV) (Vqdu a)
{
    uint64x1_t d = veor_u64(
        vget_low_u64(a),
        vget_high_u64(a)
    );
    return  vget_lane_u64(d, 0);
}

INLINE( int64_t,VQDI_XORV) (Vqdi a)
{
    int64x1_t d = veor_s64(
        vget_low_s64(a),
        vget_high_s64(a)
    );
    return  vget_lane_s64(d, 0);
}


#if 0 // _LEAVE_ARM_XORV
}
#endif

#if 0 // _ENTER_ARM_SHLL
{
#endif

INLINE(float,WBU_SHLL) (float a, Rc(0, 8) b)
{
#define     WBU_SHLL(A, B)              \
(                                       \
    (B < 8)                             \
    ?   vget_lane_f32(                  \
            vreinterpret_f32_u8(        \
                vshl_n_u8(              \
                    vreinterpret_u8_f32(\
                        vdup_n_f32(A)   \
                    ),                  \
                    (7&B)               \
                )                       \
            ),                          \
            0                           \
        )                               \
    :   0.0f                            \
)
    float32x2_t f = vdup_n_f32(a);
    uint8x8_t   z = vreinterpret_u8_f32(f);
    z = vshl_u8(z, vdup_n_u8(b));
    f = vreinterpret_f32_u8(z);
    return  vget_lane_f32(f, 0);
}

INLINE(float,WBI_SHLL) (float a, Rc(0, 8) b)
{
    float32x2_t f = vdup_n_f32(a);
    int8x8_t    z = vreinterpret_s8_f32(f);
    z = vshl_s8(z, vdup_n_s8(b));
    f = vreinterpret_f32_s8(z);
    return  vget_lane_f32(f, 0);
}

INLINE(float,WHU_SHLL) (float a, Rc(0, 16) b)
{
#define     WHU_SHLL(A, B)                  \
(                                           \
    (B < 16)                                \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u16(           \
                vshl_n_u16(                 \
                    vreinterpret_u16_f32(   \
                        vdup_n_f32(A)       \
                    ),                      \
                    (15&B)                  \
                )                           \
            ),                              \
            0                               \
        )                                   \
    :   0.0f                                \
)

    float32x2_t f = vdup_n_f32(a);
    uint16x4_t  z = vreinterpret_u16_f32(f);
    z = vshl_u16(z, vdup_n_s8(b));
    f = vreinterpret_f32_u16(z);
    return  vget_lane_f32(f, 0);
}

INLINE(float,WHI_SHLL) (float a, Rc(0, 16) b)
{
    float32x2_t f = {a};
    int16x4_t   z = vreinterpret_s16_f32(f);
    z = vshl_s16(z, vdup_n_s16(b));
    f = vreinterpret_f32_s16(z);
    return  vget_lane_f32(f, 0);
}

INLINE(float,WWU_SHLL) (float a, Rc(0, 32) b)
{
#define     WWU_SHLL(A, B)                  \
(                                           \
    (B < 32)                                \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u32(           \
                vshl_n_u32(                 \
                    vreinterpret_u32_f32(   \
                        vdup_n_f32(A)       \
                    ),                      \
                    (31&B)                  \
                )                           \
            ),                              \
            0                               \
        )                                   \
    :   0.0f                                \
)

    float32x2_t f = vdup_n_f32(a);
    uint8x8_t   z = vreinterpret_u8_f32(f);
    z = vshl_u32(z, vdup_n_s32(b));
    f = vreinterpret_f32_u32(z);
    return  vget_lane_f32(f, 0);

}

INLINE(float,WWI_SHLL) (float a, Rc(0, 32) b)
{
    float32x2_t f = vdup_n_f32(a);
    int32x2_t   z = vreinterpret_s32_f32(f);
    z = vshl_s32(z, vdup_n_s32(b));
    f = vreinterpret_f32_s32(z);
    return  vget_lane_f32(f, 0);
}

#define     DBU_SHLL(A, B) vshl_u8( A,vdup_n_s8( B))
#define     DHU_SHLL(A, B) vshl_u16(A,vdup_n_s16(B))
#define     DWU_SHLL(A, B) vshl_u32(A,vdup_n_s32(B))
#define     DDU_SHLL(A, B) vshl_u64(A,vdup_n_s64(B))

#define     QBU_SHLL(A, B) vshlq_u8( A,vdupq_n_s8( B))
#define     QHU_SHLL(A, B) vshlq_u16(A,vdupq_n_s16(B))
#define     QWU_SHLL(A, B) vshlq_u32(A,vdupq_n_s32(B))
#define     QDU_SHLL(A, B) vshlq_u64(A,vdupq_n_s64(B))


INLINE(Vwyu,VWYU_SHLL) (Vwyu a, Rc(0, 1) b)
{
    float       f = VWYU_ASTM(a);
    float32x2_t v = vdup_n_f32(f);
    uint32x2_t  l = vreinterpret_u32_f32(v);
    uint32x2_t  r = vdup_n_u32(0u-(b==0));
    l = vand_u32(l, r);
    v = vreinterpret_f32_u32(l);
    f = vget_lane_f32(v, 0);
    return  WYU_ASTV(f);
}


INLINE(Vwbu,VWBU_SHLL) (Vwbu a, Rc(0,  8) b)
{
#define     VWBU_SHLL(A, B)     ((VWBU_TYPE){WBU_SHLL(A.V0,B)})
    a.V0 = (WBU_SHLL)(a.V0, b);
    return  a;
}

INLINE(Vwbi,VWBI_SHLL) (Vwbi a, Rc(0,  8) b)
{
    a.V0 = (WBI_SHLL)(a.V0, b);
    return  a;
}

INLINE(Vwbc,VWBC_SHLL) (Vwbc a, Rc(0,  8) b)
{
#if CHAR_MIN
#   define  VWBC_SHLL(A, B) ((VWBC_TYPE){WBI_SHLL(A.V0,B)})
    a.V0 = (WBI_SHLL)(a.V0, b);
#else
#   define  VWBC_SHLL(A, B) ((VWBC_TYPE){WBU_SHLL(A.V0,B)})
    a.V0 = (WBU_SHLL)(a.V0, b);
#endif

    return  a;
}


INLINE(Vwhu,VWHU_SHLL) (Vwhu a, Rc(0, 16) b)
{
#define     VWHU_SHLL(A, B)     ((VWHU_TYPE){WHU_SHLL(A.V0,B)})
    a.V0 = (WHU_SHLL)(a.V0, b);
    return  a;
}

INLINE(Vwhi,VWHI_SHLL) (Vwhi a, Rc(0, 16) b)
{
    a.V0 = (WHI_SHLL)(a.V0, b);
    return  a;
}


INLINE(Vwwu,VWWU_SHLL) (Vwwu a, Rc(0, 32) b)
{
#define     VWWU_SHLL(A, B)     ((VWWU_TYPE){WWU_SHLL(A.V0,B)})
    a.V0 = (WWU_SHLL)(a.V0, b);
    return  a;
}

INLINE(Vwwi,VWWI_SHLL) (Vwwi a, Rc(0, 32) b)
{
    a.V0 = (WWI_SHLL)(a.V0, b);
    return  a;
}


INLINE(Vdyu,VDYU_SHLL) (Vdyu a, Rc(0, 1) b)
{
    return  ((Vdyu){vand_u64(a.V0, vdup_n_u64(0ull-!b))});
}


INLINE(Vdbu,VDBU_SHLL) (Vdbu a, Rc(0, 8) b)
{
#define     VDBU_SHLL(A, B) \
(                           \
    (7 < B)                 \
    ?   vdup_n_u8(0)        \
    :   vshl_n_u8(A, (7&B)) \
)

    return  vshl_u8(a, vdup_n_s8(b));
}

INLINE(Vdbi,VDBI_SHLL) (Vdbi a, Rc(0, 8) b)
{
    return  vshl_s8(a, vdup_n_s8(b));
}

INLINE(Vdbc,VDBC_SHLL) (Vdbc a, Rc(0, 8) b)
{
#if CHAR_MIN
    return  ((Vdbc){vshl_s8(a.V0, vdup_n_s8(b))});
#else
#   define  VDBU_SHLL(A, B) \
(                           \
    (7 < B)                 \
    ?   ((Vdbc){0})         \
    :   ((Vdbc){vshl_n_u8(A.V0,(7&B))}) \
)

    return  ((Vdbc){vshl_u8(a.V0, vdup_n_s8(b))});
#endif

}


INLINE(Vdhu,VDHU_SHLL) (Vdhu a, Rc(0, 16) b)
{
#define     VDHU_SHLL(A, B) \
(                           \
    (15 < B)                \
    ?   vdup_n_u16(0)       \
    :   vshl_n_u16(A,(15&B))\
)

    return  vshl_u16(a, vdup_n_s16(b));
}

INLINE(Vdhi,VDHI_SHLL) (Vdhi a, Rc(0, 16) b)
{
    return  vshl_s16(a, vdup_n_s16(b));
}


INLINE(Vdwu,VDWU_SHLL) (Vdwu a, Rc(0, 32) b)
{
#define     VDWU_SHLL(A, B) \
(                           \
    (31 < B)                \
    ?   vdup_n_u32(0)       \
    :   vshl_n_u32(A,(31&B))\
)

    return  vshl_u32(a, vdup_n_s32(b));
}

INLINE(Vdwi,VDWI_SHLL) (Vdwi a, Rc(0, 32) b)
{
    return  vshl_s32(a, vdup_n_s32(b));
}


INLINE(Vddu,VDDU_SHLL) (Vddu a, Rc(0, 64) b)
{
#define     VDDU_SHLL(A, B) \
(                           \
    (63 < B)                \
    ?   vdup_n_u64(0)       \
    :   vshl_n_u64(A,(63&B))\
)

    return  vshl_u64(a, vdup_n_s64(b));
}

INLINE(Vddi,VDDI_SHLL) (Vddi a, Rc(0, 64) b)
{
    return  vshl_s64(a, vdup_n_s64(b));
}


INLINE(Vqyu,VQYU_SHLL) (Vqyu a, Rc(0, 1) b)
{
    return  VQDU_ASYU(
        vandq_u64(
            VQYU_ASDU(a),
            vdupq_n_u64(0ull-(b==0))
        )
    );
        
}


INLINE(Vqbu,VQBU_SHLL) (Vqbu a, Rc(0, 8) b)
{
#define     VQBU_SHLL(A, B) \
(                           \
    (7 < B)                 \
    ?   vdupq_n_u8(0)       \
    :   vshlq_n_u8(A,(7&B)) \
)

    return  vshlq_u8(a, vdupq_n_s8(b));
}

INLINE(Vqbi,VQBI_SHLL) (Vqbi a, Rc(0, 8) b)
{
    return  vshlq_s8(a, vdupq_n_s8(b));
}

INLINE(Vqbc,VQBC_SHLL) (Vqbc a, Rc(0, 8) b)
{
#if CHAR_MIN
    return  ((Vqbc){vshlq_s8(a.V0, vdupq_n_s8(b))});
#else
#   define  VQBU_SHLL(A, B)             \
(                                       \
    (7 < B)                             \
    ?   ((Vqbc){0})                     \
    :   ((Vqbc){vshlq_n_u8(A.V0,(7&B))})\
)

    return  ((Vqbc){vshlq_u8(a.V0,vdupq_n_s8(b))});
#endif

}


INLINE(Vqhu,VQHU_SHLL) (Vqhu a, Rc(0, 16) b)
{
#define     VQHU_SHLL(A, B)     \
(                               \
    (15 < B)                    \
    ?   vdupq_n_u16(0)          \
    :   vshlq_n_u16(A,(15&B))   \
)

    return  vshlq_u16(a, vdupq_n_s16(b));
}

INLINE(Vqhi,VQHI_SHLL) (Vqhi a, Rc(0, 16) b)
{
    return  vshlq_s16(a, vdupq_n_s16(b));
}


INLINE(Vqwu,VQWU_SHLL) (Vqwu a, Rc(0, 32) b)
{
#define     VQWU_SHLL(A, B)     \
(                               \
    (31 < B)                    \
    ?   vdupq_n_u32(0)          \
    :   vshlq_n_u32(A,(31&B))   \
)

    return  vshlq_u32(a, vdupq_n_s32(b));
}

INLINE(Vqwi,VQWI_SHLL) (Vqwi a, Rc(0, 32) b)
{
    return  vshlq_s32(a, vdupq_n_s32(b));
}


INLINE(Vqdu,VQDU_SHLL) (Vqdu a, Rc(0, 64) b)
{
#define     VQDU_SHLL(A, B)     \
(                               \
    (63 < B)                    \
    ?   vdupq_n_u64(0)          \
    :   vshlq_n_u64(A,(63&B))   \
)

    return  vshlq_u64(a, vdupq_n_s64(b));
}

INLINE(Vqdi,VQDI_SHLL) (Vqdi a, Rc(0, 64) b)
{
    return  vshlq_s64(a, vdupq_n_s64(b));
}

#if 0 // _LEAVE_ARM_SHLL
}
#endif

#if 0 // _ENTER_ARM_SHLS
{
#endif

INLINE( _Bool,  BOOL_SHLS)  (_Bool a, Rc(0, 1) b) {return a;}

INLINE( uchar, UCHAR_SHLS)  (uchar a, Rc(0,  UCHAR_WIDTH) b)
{
#define     UCHAR_SHLS(A, B) vqshlb_n_u8(A,((UCHAR_WIDTH-1)&B))
    return  vqshlb_u8(a, b);
}

INLINE( schar, SCHAR_SHLS)  (schar a, Rc(0,  SCHAR_WIDTH) b)
{
#define     SCHAR_SHLS(A, B) vqshlb_n_s8(A,((SCHAR_WIDTH-1)&B))
    return  vqshlb_s8(a, b);
}

INLINE(  char,  CHAR_SHLS)   (char a, Rc(0,   CHAR_WIDTH) b)
{
#if CHAR_MIN
#   define  CHAR_SHLS(A, B) ((char)vqshlb_n_s8(A,((CHAR_WIDTH-1)&B)))
    return  vqshlb_s8(a, b);
#else
#   define  CHAR_SHLS(A, B) ((char)vqshlb_n_u8(A,((CHAR_WIDTH-1)&B)))
    return  vqshlb_u8(a, b);
#endif
}

INLINE(ushort, USHRT_SHLS) (ushort a, Rc(0,  USHRT_WIDTH) b)
{
#define     USHRT_SHLS(A, B) vqshlh_n_s16(A,((USHRT_WIDTH-1)&B))
    return  vqshlh_u16(a, b);
}

INLINE( short,  SHRT_SHLS)  (short a, Rc(0,   SHRT_WIDTH) b)
{
#define     SHRT_SHLS(A, B) vqshlh_n_s16(A,((SHRT_WIDTH-1)&B))
    return  vqshlh_s16(a, b);
}

INLINE(  uint,  UINT_SHLS)   (uint a, Rc(0,   UINT_WIDTH) b)
{
#define     UINT_SHLS(A, B) vqshls_n_u32(A,((UINT_WIDTH-1)&B))
    return  vqshls_u32(a, b);
}

INLINE(   int,   INT_SHLS)    (int a, Rc(0,    INT_WIDTH) b)
{
#define     INT_SHLS(A, B) vqshls_n_s32(A,((INT_WIDTH-1)&B))
    return  vqshls_s32(a, b);
}

INLINE( ulong, ULONG_SHLS)  (ulong a, Rc(0,  ULONG_WIDTH) b)
{
#if DWRD_NLONG == 2
#   define  ULONG_SHLS(A, B) ((ulong) vqshls_n_u32(A, ((ULONG_WIDTH-1)&B) ))
    return  vqshls_u32(a, b);
#else
#   define  ULONG_SHLS(A, B) vqshld_n_u64(A, ((ULONG_WIDTH-1)&B) )
    return  vqshld_u64(a, b);
#endif
}

INLINE( long,   LONG_SHLS)   (long a, Rc(0,   LONG_WIDTH) b)
{
#if DWRD_NLONG == 2
#   define  LONG_SHLS(A, B) ((long) vqshls_n_s32(A, ((LONG_WIDTH-1)&B) ))
    return  vqshls_u32(a, b);
#else
#   define  LONG_SHLS(A, B) vqshld_n_s64(A, ((LONG_WIDTH-1)&B) )
    return  vqshld_s64(a, b);
#endif
}

INLINE(ullong,ULLONG_SHLS) (ullong a, Rc(0, ULLONG_WIDTH) b)
{
#if QUAD_NLLONG == 2
#   define  ULLONG_SHLS(A, B) \
(\
    (B >= 64)\
    ?   0xffffffffffffffffULL\
    :   ((ullong) vqshld_n_u64(A, ((ULLONG_WIDTH-1)&B) ))\
)
    return  vqshld_u64(a, b);
#else

#endif

}

INLINE( llong, LLONG_SHLS)  (llong a, Rc(0,  LLONG_WIDTH) b)
{
#if QUAD_NLLONG == 2
#   define  LLONG_SHLS(A, B)        \
(                                   \
    (B >= 64)                       \
    ?   (a < 0)                     \
        ?   -0x8000000000000000LL   \
        :   +0x7fffffffffffffffLL   \
    :   ((llong) vqshld_n_u64(A, ((LLONG_WIDTH-1)&B) ))\
)
    return  vqshld_s64(a, b);
#else

#endif

}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,shlsqu) (QUAD_UTYPE a, Rc(0, 128) b) 
{
    unsigned z;
    z = (a > UINT64_MAX) 
    ?   __clzll(a>>64)
    :   __clzll(a)+64;
    return  (z < b) ? (~(QUAD_UTYPE){0}) : (a<<b);
}

INLINE(QUAD_ITYPE,shlsqi) (QUAD_ITYPE a, Rc(0, 128) b) 
{
    unsigned    z;
    QUAD_TYPE   r = {.I=a};
    if (a < 0)
    {
        z = __clzll(~r.Hi.U);
        if (z == 64)
            z += __clzll(~r.Lo.U);
        if (b >= z) 
        {
            r.Hi.I = ~INT64_MAX;
            r.Lo.U = UINT64_MAX;
        }
        else 
        {
            r.I <<= b;
        }
    }
    else
    {
        z = __clzll(r.Hi.U);
        if (z == 64)
            z += __clzll(r.Lo.U);
        if (b >= z)
        {
            r.Hi.U =  INT64_MAX;
            r.Lo.U = UINT64_MAX;
        }
        else 
        {
            r.I <<= b;
        }
    }
    return  r.I;
}

#endif

INLINE(float,WBU_SHLS) (float a, Rc(0, 8) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vqshl_u8(
                vreinterpret_u8_f32(vdup_n_f32(a)),
                vdup_n_s8(b)
            )
        ),
        0
    );
}

INLINE(float,WBI_SHLS) (float a, Rc(0, 8) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_s8(
            vqshl_s8(
                vreinterpret_s8_f32(vdup_n_f32(a)),
                vdup_n_s8(b)
            )
        ),
        0
    );
}

INLINE(float,WHU_SHLS) (float a, Rc(0, 16) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vqshl_u16(
                vreinterpret_u16_f32(vdup_n_f32(a)),
                vdup_n_s16(b)
            )
        ),
        0
    );
}

INLINE(float,WHI_SHLS) (float a, Rc(0, 16) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_s16(
            vqshl_s16(
                vreinterpret_s16_f32(vdup_n_f32(a)),
                vdup_n_s16(b)
            )
        ),
        0
    );
}

INLINE(float,WWU_SHLS) (float a, Rc(0, 32) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vqshl_u32(
                vreinterpret_u32_f32(vdup_n_f32(a)),
                vdup_n_s32(b)
            )
        ),
        0
    );
}

INLINE(float,WWI_SHLS) (float a, Rc(0, 32) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_s32(
            vqshl_s32(
                vreinterpret_s32_f32(vdup_n_f32(a)),
                vdup_n_s32(b)
            )
        ),
        0
    );
}

#define     DBU_SHLS(A, B) vqshl_u8( A,vdup_n_s8( B))
#define     DBI_SHLS(A, B) vqshl_s8( A,vdup_n_s8( B))
#define     DHU_SHLS(A, B) vqshl_u16(A,vdup_n_s16(B))
#define     DHI_SHLS(A, B) vqshl_s16(A,vdup_n_s16(B))
#define     DWU_SHLS(A, B) vqshl_u32(A,vdup_n_s32(B))
#define     DWI_SHLS(A, B) vqshl_s32(A,vdup_n_s32(B))
#define     DDU_SHLS(A, B) vqshl_u64(A,vdup_n_s64(B))
#define     DDI_SHLS(A, B) vqshl_s64(A,vdup_n_s64(B))

#define     QBU_SHLS(A, B) vqshlq_u8( A,vdupq_n_s8( B))
#define     QBI_SHLS(A, B) vqshlq_s8( A,vdupq_n_s8( B))
#define     QHU_SHLS(A, B) vqshlq_u16(A,vdupq_n_s16(B))
#define     QHI_SHLS(A, B) vqshlq_s16(A,vdupq_n_s16(B))
#define     QWU_SHLS(A, B) vqshlq_u32(A,vdupq_n_s32(B))
#define     QWI_SHLS(A, B) vqshlq_s32(A,vdupq_n_s32(B))
#define     QDU_SHLS(A, B) vqshlq_u64(A,vdupq_n_s64(B))
#define     QDI_SHLS(A, B) vqshlq_s64(A,vdupq_n_s64(B))

INLINE(Vwbu,VWBU_SHLS) (Vwbu a, Rc(0, 8) b)
{
#define     VWBU_SHLS(A, B)     \
(                               \
    (B > 7)                     \
    ?   VWWU_ASBU(UINT_ASTV(0xffffffffu))\
    :   WBU_ASTV(               \
            vget_lane_f32(      \
                VDBU_ASWF(      \
                    vqshl_n_u8( \
                        VDWF_ASBU(vdup_n_f32(VWBU_ASTM(A))),\
                        (7&B)   \
                    )           \
                ),              \
                0           \
            )                   \
        )                       \
)
    return  WBU_ASTV(WBU_SHLS(VWBU_ASTM(a), b));
}

INLINE(Vwbi,VWBI_SHLS) (Vwbi a, Rc(0, 8) b)
{
    return  WBI_ASTV(WBU_SHLS(VWBI_ASTM(a), b));
}

INLINE(Vwbc,VWBC_SHLS) (Vwbc a, Rc(0, 8) b)
{
#if CHAR_MIN
#   define  VWBC_SHLS(A, B) VWBI_ASBC(VWBI_SHLS(VWBC_ASBI(A), B))
    return  WBC_ASTV(WBI_SHLS(VWBC_ASTM(a), b));
#else
#   define  VWBC_SHLS(A, B) VWBU_ASBC(VWBU_SHLS(VWBC_ASBU(A), B))
    return  WBC_ASTV(WBU_SHLS(VWBC_ASTM(a), b));
#endif
}


INLINE(Vwhu,VWHU_SHLS) (Vwhu a, Rc(0, 16) b)
{
#define     VWHU_SHLS(A, B)     \
(                               \
    (B > 15)                    \
    ?   VWWU_ASHU(UINT_ASTV(0xffffffffu))\
    :   WHU_ASTV(               \
            vget_lane_f32(      \
                VDHU_ASWF(      \
                    vqshl_n_u16( \
                        VDWF_ASHU(\
                            vdup_n_f32(VWHU_ASTM(A))\
                        ),      \
                        (15&B)   \
                    )           \
                ),              \
                0           \
            )                   \
        )                       \
)
    return  WHU_ASTV(WHU_SHLS(VWHU_ASTM(a), b));
}

INLINE(Vwhi,VWHI_SHLS) (Vwhi a, Rc(0, 16) b)
{
    return  WHI_ASTV(WHI_SHLS(VWHI_ASTM(a), b));
}


INLINE(Vwwu,VWWU_SHLS) (Vwwu a, Rc(0, 32) b)
{
#define     VWWU_SHLS(A, B)     \
(                               \
    (B > 31)                    \
    ?   WWU_ASTV(0.0f)          \
    :   WWU_ASTV(               \
            vget_lane_f32(      \
                VDWU_ASWF(      \
                    vqshl_n_u32(\
                        VDWF_ASWU(\
                            vdup_n_f32(VWWU_ASTM(A))\
                        ),      \
                        (31&B)  \
                    )           \
                ),              \
                0           \
            )                   \
        )                       \
)
    return  WWU_ASTV(WWU_SHLS(VWWU_ASTM(a), b));
}

INLINE(Vwwi,VWWI_SHLS) (Vwwi a, Rc(0, 32) b)
{
#define     VWWI_SHLS(A, B)     \
(                               \
    (B > 31)                    \
    ?   WWI_ASTV(0.0f)          \
    :   WWI_ASTV(               \
            vget_lane_f32(      \
                VDWU_ASWF(      \
                    vqshl_n_s32(\
                        VDWF_ASWI(\
                            vdup_n_f32(VWWI_ASTM(A))\
                        ),      \
                        (31&B)   \
                    )           \
                ),              \
                0           \
            )                   \
        )                       \
)
    return  WWI_ASTV(WWU_SHLS(VWWI_ASTM(a), b));
}

INLINE(Vdyu,VDYU_SHLS) (Vdyu a, Rc(0, 1) b) {return a;}

INLINE(Vdbu,VDBU_SHLS) (Vdbu a, Rc(0, 8) b)
{
#define     VDBU_SHLS(A, B)     \
(                               \
    (B >= 8)                    \
    ?   vqshl_u8(A, B)          \
    :   vqshl_n_u8(A, (7&B))    \
)
    return  DBU_SHLS(a, b);
}

INLINE(Vdbi,VDBI_SHLS) (Vdbi a, Rc(0, 8) b)
{
#define     VDBI_SHLS(A, B)     \
(                               \
    (B >= 8)                    \
    ?   vqshl_s8(A, B)          \
    :   vqshl_n_s8(A, (7&B))    \
)
    return  DBI_SHLS(a, b);
}

INLINE(Vdbc,VDBC_SHLS) (Vdbc a, Rc(0, 8) b)
{
#if CHAR_MIN
#   define  VDBC_SHLS(A, B) VDBI_ASBC(VDBI_SHLS(VDBC_ASBI(A),B))
    return  VDBI_ASBC(DBI_SHLS(VDBC_ASTM(a), b));
#else
#   define  VDBC_SHLS(A, B) VDBU_ASBC(VDBU_SHLS(VDBC_ASBU(A),B))
    return  VDBU_ASBC(DBU_SHLS(VDBC_ASTM(a), b));
#endif
}


INLINE(Vdhu,VDHU_SHLS) (Vdhu a, Rc(0, 16) b)
{
#define     VDHU_SHLS(A, B)     \
(                               \
    (B >= 16)                   \
    ?   vqshl_u16(A, B)         \
    :   vqshl_n_u16(A, (15&B))  \
)

    return  DHU_SHLS(a, b);
}

INLINE(Vdhi,VDHI_SHLS) (Vdhi a, Rc(0, 16) b)
{
#define     VDHI_SHLS(A, B)     \
(                               \
    (B >= 16)                   \
    ?   vqshl_s16(A, B)         \
    :   vqshl_n_s16(A, (15&B))  \
)

    return  DHI_SHLS(a, b);
}


INLINE(Vdwu,VDWU_SHLS) (Vdwu a, Rc(0, 32) b)
{
#define     VDWU_SHLS(A, B)     \
(                               \
    (B >= 32)                   \
    ?   vqshl_u32(A, B)         \
    :   vqshl_n_u32(A, (31&B))  \
)

    return  DWU_SHLS(a, b);
}

INLINE(Vdwi,VDWI_SHLS) (Vdwi a, Rc(0, 32) b)
{
#define     VDWI_SHLS(A, B)     \
(                               \
    (B >= 32)                   \
    ?   vqshl_s32(A, B)         \
    :   vqshl_n_s32(A, (31&B))  \
)

    return  DWI_SHLS(a, b);
}

INLINE(Vddu,VDDU_SHLS) (Vddu a, Rc(0, 64) b)
{
#define     VDDU_SHLS(A, B)     \
(                               \
    (B >= 64)                   \
    ?   vqshl_u64(A, B)         \
    :   vqshl_n_u64(A, (63&B))  \
)
    return  DDU_SHLS(a, b);
}

INLINE(Vddi,VDDI_SHLS) (Vddi a, Rc(0, 64) b)
{
#define     VDDI_SHLS(A, B)     \
(                               \
    (B >= 64)                   \
    ?   vqshl_s64(A, B)         \
    :   vqshl_n_u64(A, (63&B))  \
)
    return  DDI_SHLS(a, b);
}

// TODO: update arm's shlsqz to same method as shlsdz
INLINE(Vqbu,VQBU_SHLS) (Vqbu a, Rc(0, 8) b)
{
#define     VQBU_SHLS(A, B) vqshlq_n_u8(A, (7&B))
    return  QBU_SHLS(a, b);
}

INLINE(Vqbi,VQBI_SHLS) (Vqbi a, Rc(0, 8) b)
{
#define     VQBI_SHLS(A, B) vqshlq_n_s8(A, (7&B))
    return  QBI_SHLS(a, b);
}

INLINE(Vqbc,VQBC_SHLS) (Vqbc a, Rc(0, CHAR_WIDTH-1) b)
{
#if CHAR_MIN
#   define  VQBC_SHLS(A, B) VQBI_ASBC(VQBI_SHLS(VQBC_ASBI(A),B))
    return  VQBI_ASBC(QBI_SHLS(VQBC_ASTM(a), b));
#else
#   define  VQBC_SHLS(A, B) VQBU_ASBC(VQBU_SHLS(VQBC_ASBU(A),B))
    return  VQBU_ASBC(QBU_SHLS(VQBC_ASTM(a), b));
#endif
}


INLINE(Vqhu,VQHU_SHLS) (Vqhu a, Rc(0, 16) b)
{
#define     VQHU_SHLS(A, B) vqshlq_n_u16(A, (15&B))
    return  QHU_SHLS(a, b);
}

INLINE(Vqhi,VQHI_SHLS) (Vqhi a, Rc(0, 16) b)
{
#define     VQHI_SHLS(A, B) vqshlq_n_s16(A, (15&B))
    return  QHI_SHLS(a, b);
}


INLINE(Vqwu,VQWU_SHLS) (Vqwu a, Rc(0, 32) b)
{
#define     VQWU_SHLS(A, B) vqshlq_n_u32(A, (31&B))
    return  QWU_SHLS(a, b);
}

INLINE(Vqwi,VQWI_SHLS) (Vqwi a, Rc(0, 32) b)
{
#define     VQWI_SHLS(A, B) vqshlq_n_s32(A, (31&B))
    return  QWI_SHLS(a, b);
}


INLINE(Vqdu,VQDU_SHLS) (Vqdu a, Rc(0, 64) b)
{
#define     VQDU_SHLS(A, B) vqshlq_n_u64(A, (63&B))
    return  QDU_SHLS(a, b);
}

INLINE(Vqdi,VQDI_SHLS) (Vqdi a, Rc(0, 64) b)
{
#define     VQDI_SHLS(A, B) vqshlq_n_s64(A, (63&B))
    return  QDI_SHLS(a, b);
}

#if 0 // _LEAVE_ARM_SHLS
}
#endif

#if 0 // _ENTER_ARM_SHL2
{
#endif

/*
    NOTE: vshll_n_u8(v, n) has two entries in arm's NEON
    docs; one for when 0 <= n <= 7 and another for n == 8.
    Some implementations allow specifying 9..15, implicitly
    converting the call to:
        r = vshll_n_u8(v, 8);
        r = vshlq_n_u16(r,n-8);
    However, we're not going to use this.
*/

#define WBU_SHL2(A, B)              \
(                                   \
    (B > 7)                         \
    ?   vzip1_u8(                   \
            vdup_n_u8(0),           \
            vreinterpret_u8_f32(    \
                vdup_n_f32(A)       \
            )                       \
        )                           \
    :   vshl_n_u16(                 \
            vzip1_u8(               \
                vreinterpret_u8_f32(\
                    vdup_n_f32(A)   \
                )                   \
            ),                      \
            (7&B)                   \
        )                           \
)

#define WBI_SHL2(A, B)              \
(                                   \
    (B > 7)                         \
    ?   vget_low_s16(               \
            vshll_n_s8(             \
                vreinterpret_s8_f32(\
                    vdup_n_f32(A)   \
                ),                  \
                8                   \
            )                       \
        )                           \
    :   vget_low_s16(               \
            vshll_n_s8(             \
                vreinterpret_s8_f32(\
                    vdup_n_f32(A)   \
                ),                  \
                (7&B)               \
            )                       \
        )                           \
)


#define WHU_SHL2(A, B)                      \
(                                           \
    (B > 15)                                \
    ?   vreinterpret_u32_u16(               \
            vzip1_u16(                      \
                vdup_n_u16(0),              \
                vreinterpret_u16_f32(       \
                    vdup_n_f32(A)           \
                )                           \
            )                               \
        )                                   \
    :   vshl_n_u32(                         \
            vreinterpret_u32_u16(           \
                vzip1_u16(                  \
                    vreinterpret_u16_f32(   \
                        vdup_n_f32(A)       \
                    ),                      \
                    vdup_n_u16(0)           \
                )                           \
            ),                              \
            (15&B)                          \
        )                                   \
)

#define WHI_SHL2(A, B)                      \
(                                           \
    (B > 15)                                \
    ?   vget_low_s32(                       \
            vmovl_s16(                      \
                vreinterpret_s16_f32(       \
                    vdup_n_f32(A)           \
                )                           \
            )                               \
        )                                   \
    :   vget_low_s32(                       \
            vshll_n_s16(                    \
                vreinterpret_s16_f32(       \
                    vdup_n_f32(A)           \
                ),                          \
                (15&B)                      \
            )                               \
        )                                   \
)

#define WWU_SHL2(A, B)                      \
(                                           \
    (B > 31)                                \
    ?   vreinterpret_u64_u32(               \
            vzip1_u32(                      \
                vdup_n_u32(0),              \
                vreinterpret_u32_f32(       \
                    vdup_n_f32(A)           \
                )                           \
            )                               \
        )                                   \
    :   vshl_n_u64(                         \
            vreinterpret_u64_u32(           \
                vzip1_u32(                  \
                    vreinterpret_u32_f32(   \
                        vdup_n_f32(A)       \
                    ),                      \
                    vdup_n_u32(0)           \
                )                           \
            ),                              \
            (31&B)                          \
        )                                   \
)

#define WWI_SHL2(A, B)                      \
(                                           \
    (B > 31)                                \
    ?   vget_low_s64(                       \
            vmovl_s32(                      \
                vreinterpret_s32_f32(       \
                    vdup_n_f32(A)           \
                )                           \
            )                               \
        )                                   \
    :   vget_low_s64(                       \
            vshll_n_s32(                    \
                vreinterpret_s32_f32(       \
                    vdup_n_f32(A)           \
                ),                          \
                (31&B)                      \
            )                               \
        )                                   \
)
#if 0
#define     VWBU_SHL2(A, B) \
(                           \
    (B > 7)                 \
    ?   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u8_f32(vdup_n_f32(VWBU_ASTM(A))),\
                8           \
            )               \
        )                   \
    :   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u8_f32(vdup_n_f32(VWBU_ASTM(A))),\
                7&B         \
            )               \
        )                   \
)
#define     VWBI_SHL2(A, B) \
(                           \
    (B > 7)                 \
    ?   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s8_f32(vdup_n_f32(VWBI_ASTM(A))),\
                8           \
            )               \
        )                   \
    :   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s8_f32(vdup_n_f32(VWBI_ASTM(A))),\
                7&B         \
            )               \
        )                   \
)
#define     VWBC_SHL2(A, B) \
(                           \
    (B > 7)                 \
    ?   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s8_f32(vdup_n_f32(VWBC_ASTM(A))),\
                8           \
            )               \
        )                   \
    :   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s8_f32(vdup_n_f32(VWBC_ASTM(A))),\
                7&B         \
            )               \
        )                   \
)
#define     VWBC_SHL2(A, B) \
(                           \
    (B > 7)                 \
    ?   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u8_f32(vdup_n_f32(VWBC_ASTM(A))),\
                8           \
            )               \
        )                   \
    :   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u8_f32(vdup_n_f32(VWBC_ASTM(A))),\
                7&B         \
            )               \
        )                   \
)
#define     VWHU_SHL2(A, B) \
(                           \
    (B > 15)                \
    ?   vget_low_u32(       \
            vshll_n_u16(    \
                vreinterpret_u16_f32(vdup_n_f32(VWHU_ASTM(A))),\
                16          \
            )               \
        )                   \
    :   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u16_f32(vdup_n_f32(VWHU_ASTM(A))),\
                15&B        \
            )               \
        )                   \
)

#define     VWWI_SHL2(A, B) \
(                           \
    (B > 31)                \
    ?   vget_low_s32(       \
            vshll_n_s32(    \
                vreinterpret_s32_f32(vdup_n_f32(VWWI_ASTM(A))),\
                32          \
            )               \
        )                   \
    :   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s32_f32(vdup_n_f32(VWWI_ASTM(A))),\
                31&B        \
            )               \
        )                   \
)
#define     VWWU_SHL2(A, B) \
(                           \
    (B > 31)                \
    ?   vget_low_u32(       \
            vshll_n_u32(    \
                vreinterpret_u32_f32(vdup_n_f32(VWWU_ASTM(A))),\
                32          \
            )               \
        )                   \
    :   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u32_f32(vdup_n_f32(VWWU_ASTM(A))),\
                31&B        \
            )               \
        )                   \
)
#define     VWHI_SHL2(A, B) \
(                           \
    (B > 15)                \
    ?   vget_low_s32(       \
            vshll_n_s16(    \
                vreinterpret_s16_f32(vdup_n_f32(VWHI_ASTM(A))),\
                16          \
            )               \
        )                   \
    :   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s16_f32(vdup_n_f32(VWHI_ASTM(A))),\
                15&B        \
            )               \
        )                   \
)
#endif

INLINE(Vdhu,VWBU_SHL2) (Vwbu a, Rc(0,  8) b)
{
#define     VWBU_SHL2(A, B) WBU_SHL2(VWBU_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWBU_ASTM(a));
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  q = vmovl_u8(d);
    q = vshlq_u16(q, vdupq_n_s16(b));
    return  vget_low_u16(q);
}

INLINE(Vdhi,VWBI_SHL2) (Vwbi a, Rc(0,  8) b)
{
#define     VWBI_SHL2(A, B) WBI_SHL2(VWBI_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWBI_ASTM(a));
    int8x8_t    d = vreinterpret_s8_f32(m);
    int16x8_t   q = vmovl_s8(d);
    q = vshlq_s16(q, vdupq_n_s16(b));
    return  vget_low_s16(q);
}

#if CHAR_MIN

INLINE(Vdhi,VWBC_SHL2) (Vwbc a, Rc(0,  8) b)
{

#define     VWBC_SHL2(A, B) WBI_SHL2(VWBC_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWBC_ASTM(a));
    int8x8_t    d = vreinterpret_s8_f32(m);
    int16x8_t   q = vmovl_s8(d);
    q = vshlq_s16(q, vdupq_n_s16(b));
    return  vget_low_s16(q);
}

#else

INLINE(Vdhu,VWBC_SHL2) (Vwbc a, Rc(0,  8) b)
{
#define     VWBC_SHL2(A, B) WBU_SHL2(VWBC_ASTM(A),B)

    float32x2_t m = vdup_n_f32(VWBC_ASTM(a));
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  q = vmovl_u8(d);
    q = vshlq_u16(q, vdupq_n_s16(b));
    return  vget_low_u16(q);
}

#endif


INLINE(Vdwu,VWHU_SHL2) (Vwhu a, Rc(0, 16) b)
{
#define     VWHU_SHL2(A, B) WHU_SHL2(VWHU_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWHU_ASTM(a));
    uint16x4_t  d = vreinterpret_u16_f32(m);
    uint32x4_t  q = vmovl_u16(d);
    q = vshlq_u32(q, vdupq_n_s32(b));
    return  vget_low_u32(q);
}

INLINE(Vdwi,VWHI_SHL2) (Vwhi a, Rc(0, 16) b)
{
#define     VWHI_SHL2(A, B) WHI_SHL2(VWHI_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWHI_ASTM(a));
    uint16x4_t  d = vreinterpret_s16_f32(m);
    uint32x4_t  q = vmovl_s16(d);
    q = vshlq_s32(q, vdupq_n_s32(b));
    return  vget_low_s32(q);
}


INLINE(Vddu,VWWU_SHL2) (Vwwu a, Rc(0, 32) b)
{
#define     VWWU_SHL2(A, B) WWU_SHL2(VWWU_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWWU_ASTM(a));
    uint32x2_t  d = vreinterpret_u32_f32(m);
    uint64x2_t  q = vmovl_u32(d);
    q = vshlq_u64(q, vdupq_n_s64(b));
    return  vget_low_u64(q);
}

INLINE(Vddi,VWWI_SHL2) (Vwwi a, Rc(0, 32) b)
{
#define     VWWI_SHL2(A, B) WWI_SHL2(VWWI_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWWI_ASTM(a));
    uint32x2_t  d = vreinterpret_s32_f32(m);
    uint64x2_t  q = vmovl_s32(d);
    q = vshlq_s64(q, vdupq_n_s64(b));
    return  vget_low_s64(q);
}


INLINE(Vqhu,VDBU_SHL2) (Vdbu a, Rc(0,  8) b)
{
#define     VDBU_SHL2(A, B)     \
(                               \
    (B > 7)                     \
    ?   vshll_n_u8(A, (8))      \
    :   vshll_n_u8(A, (7&B))    \
)
    return  vshlq_u16(vmovl_u8(a), vdupq_n_s16(b));
}

INLINE(Vqhi,VDBI_SHL2) (Vdbi a, Rc(0,  8) b)
{
#define     VDBI_SHL2(A, B)     \
(                               \
    (B > 7)                     \
    ?   vshll_n_s8(A, (8))      \
    :   vshll_n_s8(A, (7&B))    \
)
    return  vshlq_s16(vmovl_s8(a), vdupq_n_s16(b));
}

#if CHAR_MIN

INLINE(Vqhi,VDBC_SHL2) (Vdbc a, Rc(0,  8) b)
{
#define     VDBC_SHL2(A, B)             \
(                                       \
    (B > 7)                             \
    ?   vshll_n_s8(VDBC_ASBI(A),(8))    \
    :   vshll_n_s8(VDBC_ASBI(A),(7&B))  \
)
    int16x8_t   v = vmovl_s8(VDBC_ASTM(a));
    return  vshlq_s16(v, vdupq_n_s16(b));
}

#else

INLINE(Vqhu,VDBC_SHL2) (Vdbc a, Rc(0,  8) b)
{
#define     VDBC_SHL2(A, B)             \
(                                       \
    (B > 7)                             \
    ?   vshll_n_u8(VDBC_ASBU(A),(8))    \
    :   vshll_n_u8(VDBC_ASBU(A),(7&B))  \
)
    uint16x8_t  v = vmovl_u8(VDBC_ASTM(a));
    return  vshlq_u16(v, vdupq_n_s16(b));
}

#endif


INLINE(Vqwu,VDHU_SHL2) (Vdhu a, Rc(0, 16) b)
{
#define     VDHU_SHL2(A, B)     \
(                               \
    (B > 15)                    \
    ?   vshll_n_u16(A,(16))     \
    :   vshll_n_u16(A,(15&B))   \
)
    return  vshlq_u32(vmovl_u16(a), vdupq_n_s32(b));
}

INLINE(Vqwi,VDHI_SHL2) (Vdhi a, Rc(0, 16) b)
{
#define     VDHI_SHL2(A, B)     \
(                               \
    (B > 15)                    \
    ?   vshll_n_s16(A,(16))     \
    :   vshll_n_s16(A,(15&B))   \
)
    return  vshlq_s32(vmovl_s16(a), vdupq_n_s32(b));
}


INLINE(Vqdu,VDWU_SHL2) (Vdwu a, Rc(0, 32) b)
{
#define     VDWU_SHL2(A, B)     \
(                               \
    (B > 31)                    \
    ?   vshll_n_u32(A,(32))     \
    :   vshll_n_u32(A,(31&B))   \
)
    return  vshlq_u64(vmovl_u32(a), vdupq_n_s64(b));
}

INLINE(Vqdi,VDWI_SHL2) (Vdwi a, Rc(0, 32) b)
{
#define     VDWI_SHL2(A, B)     \
(                               \
    (B > 63)                    \
    ?   vshll_n_s64(A,(64))     \
    :   vshll_n_u64(A,(63&B))   \
)
    return  vshlq_s64(vmovl_s32(a), vdupq_n_s64(b));
}


#if 0 // _LEAVE_ARM_SHL2
}
#endif

#if 0 // _ENTER_ARM_SHLR
{
#endif

INLINE(Vwyu,VWYU_SHLR) (Vwyu a, Rc(0, 1) b)
{
#define     VWYU_SHLR(A, B) ((B==1)?A:((Vwyu){0}))
    return  (b==1) ? a : ((Vwyu){0});
}

INLINE(Vwbu,VWBU_SHLR) (Vwbu a, Rc(0, 8) b)
{
#define     VWBU_SHLR(A, B)                         \
(                                                   \
    (B > 7)                                         \
    ?   A                                           \
    :   WBU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vshr_n_u8(                      \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBU_ASTM(A))\
                        ),                          \
                        (8-(B*(B<8)))               \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWBU_ASTM(a));
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  q = vmovl_u8(d);
    q = vshlq_u16(q, vdupq_n_s16(b));
    uint8x16_t  r = vreinterpretq_u8_u16(q);
    d = vget_low_u8(r);
    d = vuzp2_u8(d, d);
    m = vreinterpret_f32_u8(d);
    return  WBU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwbi,VWBI_SHLR) (Vwbi a, Rc(0, 8) b)
{
#define     VWBI_SHLR(A, B)                         \
(                                                   \
    (B > 7)                                         \
    ?   A                                           \
    :   WBI_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_s8(                \
                    vshr_n_s8(                      \
                        vreinterpret_s8_f32(        \
                            vdup_n_f32(VWBI_ASTM(A))\
                        ),                          \
                        (8-(B*(B<8)))               \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWBI_ASTM(a));
    int8x8_t    d = vreinterpret_s8_f32(m);
    int16x8_t   q = vmovl_s8(d);
    q = vshlq_s16(q, vdupq_n_s16(b));
    uint8x16_t  r = vreinterpretq_s8_s16(q);
    d = vget_low_s8(r);
    d = vuzp2_s8(d, d);
    m = vreinterpret_f32_s8(d);
    return  WBI_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwbc,VWBC_SHLR) (Vwbc a, Rc(0, 8) b)
{
    float32x2_t m = vdup_n_f32(VWBC_ASTM(a));
#if CHAR_MIN
#   define  VWBC_SHLR(A, B)                         \
(                                                   \
    (B > 7)                                         \
    ?   A                                           \
    :   WBC_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_s8(                \
                    vshr_n_s8(                      \
                        vreinterpret_s8_f32(        \
                            vdup_n_f32(VWBC_ASTM(A))\
                        ),                          \
                        (8-(B*(B<8)))               \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)


    int8x8_t    d = vreinterpret_s8_f32(m);
    int16x8_t   q = vmovl_s8(d);
    q = vshlq_s16(q, vdupq_n_s16(b));
    uint8x16_t  r = vreinterpretq_s8_s16(q);
    d = vget_low_s8(r);
    d = vuzp2_s8(d, d);
    m = vreinterpret_f32_s8(d);

#else

#define     VWBC_SHLR(A, B)                         \
(                                                   \
    (B > 7)                                         \
    ?   A                                           \
    :   WBU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vshr_n_u8(                      \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBC_ASTM(A))\
                        ),                          \
                        (8-(B*(B<8)))               \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  q = vmovl_u8(d);
    q = vshlq_u16(q, vdupq_n_s16(b));
    uint8x16_t  r = vreinterpretq_u8_u16(q);
    d = vget_low_u8(r);
    d = vuzp2_u8(d, d);
    m = vreinterpret_f32_u8(d);

#endif

    return  WBC_ASTV(vget_lane_f32(m, 0));
    
}


INLINE(Vwhu,VWHU_SHLR) (Vwhu a, Rc(0, 16) b)
{
#define     VWHU_SHLR(A, B)                         \
(                                                   \
    (B > 15)                                        \
    ?   A                                           \
    :   WHU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u16(               \
                    vshr_n_u16(                     \
                        vreinterpret_u16_f32(       \
                            vdup_n_f32(VWHU_ASTM(A))\
                        ),                          \
                        (16-(B*(B<16)))             \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWHU_ASTM(a));
    uint16x4_t  d = vreinterpret_u16_f32(m);
    uint32x4_t  q = vmovl_u16(d);
    q = vshlq_u32(q, vdupq_n_s32(b));
    uint16x8_t  r = vreinterpretq_u16_u32(q);
    d = vget_low_u16(r);
    d = vuzp2_u16(d, d);
    m = vreinterpret_f32_u16(d);
    return  WHU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwhi,VWHI_SHLR) (Vwhi a, Rc(0, 16) b)
{
#define     VWHI_SHLR(A, B)                         \
(                                                   \
    (B > 15)                                        \
    ?   A                                           \
    :   WHI_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_s16(               \
                    vshr_n_s16(                     \
                        vreinterpret_s16_f32(       \
                            vdup_n_f32(VWHI_ASTM(A))\
                        ),                          \
                        (16-(B*(B<16)))             \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWHI_ASTM(a));
    int16x4_t   d = vreinterpret_s16_f32(m);
    int32x4_t   q = vmovl_s16(d);
    q = vshlq_s32(q, vdupq_n_s32(b));
    int16x8_t   r = vreinterpretq_s16_s32(q);
    d = vget_low_s16(r);
    d = vuzp2_s16(d, d);
    m = vreinterpret_f32_s16(d);
    return  WHI_ASTV(vget_lane_f32(m, 0));
}


INLINE(Vwwu,VWWU_SHLR) (Vwwu a, Rc(0, 32) b)
{
#define     VWWU_SHLR(A, B)                         \
(                                                   \
    (B > 31)                                        \
    ?   A                                           \
    :   WWU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u32(               \
                    vshr_n_u32(                     \
                        vreinterpret_u32_f32(       \
                            vdup_n_f32(VWWU_ASTM(A))\
                        ),                          \
                        (32-(B*(B<32)))             \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWWU_ASTM(a));
    uint32x2_t  d = vreinterpret_u32_f32(m);
    uint64x2_t  q = vmovl_u32(d);
    q = vshlq_u64(q, vdupq_n_s64(b));
    uint32x4_t  r = vreinterpretq_u32_u64(q);
    d = vget_low_u32(r);
    d = vuzp2_u32(d, d);
    m = vreinterpret_f32_u32(d);
    return  WWU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwwi,VWWI_SHLR) (Vwwi a, Rc(0, 32) b)
{
#define     VWWI_SHLR(A, B)                         \
(                                                   \
    (B > 31)                                        \
    ?   A                                           \
    :   WWI_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_s32(               \
                    vshr_n_s32(                     \
                        vreinterpret_s32_f32(       \
                            vdup_n_f32(VWWI_ASTM(A))\
                        ),                          \
                        (32-(B*(B<32)))             \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWWI_ASTM(a));
    int32x2_t   d = vreinterpret_s32_f32(m);
    int64x2_t   q = vmovl_s32(d);
    q = vshlq_s64(q, vdupq_n_s64(b));
    uint32x4_t  r = vreinterpretq_s32_s64(q);
    d = vget_low_s32(r);
    d = vuzp2_s32(d, d);
    m = vreinterpret_f32_s32(d);
    return  WWI_ASTV(vget_lane_f32(m, 0));
}



INLINE(Vdyu,VDYU_SHLR) (Vdyu a, Rc(0, 1) b)
{
#define     VDYU_SHLR(A, B) ((B==1)?A:((Vdyu){0}))
    return  (b==1) ? a : ((Vdyu){0});
}


INLINE(Vdbu,VDBU_SHLR) (Vdbu a, Rc(0, 8) b)
{
#define     VDBU_SHLR(A, B) \
(                           \
    (B > 7)                 \
    ?   A                   \
    :   vshr_n_u8(          \
            A,              \
            (8-B*(B<8))     \
        )                   \
)
    uint16x8_t  q = vmovl_u8(a);
    q = vshlq_u16(q, vdupq_n_s16(b));
    uint8x16_t  v = vreinterpretq_u8_u16(q);
    return  vqtbl1_u8(
        v,
        vcreate_u8(0x0f0d0b0907050301ULL)
    );
}

INLINE(Vdbi,VDBI_SHLR) (Vdbi a, Rc(0, 8) b)
{
#define     VDBI_SHLR(A, B) \
(                           \
    (B > 7)                 \
    ?   A                   \
    :   vshr_n_s8(          \
            A,              \
            (8-B*(B<8))     \
        )                   \
)
    int16x8_t   q = vmovl_s8(a);
    q = vshlq_s16(q, vdupq_n_s16(b));
    int8x16_t   v = vreinterpretq_s8_s16(q);
    return  vqtbl1_s8(
        v,
        vcreate_u8(0x0f0d0b0907050301ULL)
    );
}

INLINE(Vdbc,VDBC_SHLR) (Vdbc a, Rc(0, 8) b)
{
#if CHAR_MIN
#   define  VDBC_SHLR(A, B)     \
(                               \
    (B > 7)                     \
    ?   A                       \
    :   VDBI_ASBC(              \
            vshr_n_s8(          \
                VDBC_ASBI(A),   \
                (8-B*(B<8))     \
            )                   \
        )                       \
)
    int16x8_t   q = vmovl_s8(VDBC_ASBI(a));
    q = vshlq_s16(q, vdupq_n_s16(b));
    int8x16_t   v = vreinterpretq_s8_s16(q);
    return  VDBI_ASBC(
        vqtbl1_s8(
            v,
            vcreate_u8(0x0f0d0b0907050301ULL)
        )
    );
#else
#   define  VDBC_SHLR(A, B)     \
(                               \
    (B > 7)                     \
    ?   A                       \
    :   VDBU_ASBC(              \
            vshr_n_u8(          \
                VDBC_ASBU(A),   \
                (8-B*(B<8))     \
            )                   \
        )                       \
)
    uint16x8_t  q = vmovl_u8(VDBC_ASBU(a));
    q = vshlq_u16(q, vdupq_n_s16(b));
    uint8x16_t  v = vreinterpretq_u8_u16(q);
    return  VDBU_ASBC(
        vqtbl1_u8(
            v,
            vcreate_u8(0x0f0d0b0907050301ULL)
        )
    );

#endif

}



INLINE(Vdhu,VDHU_SHLR) (Vdhu a, Rc(0, 16) b)
{
#define     VDHU_SHLR(A, B) \
(                           \
    (B > 15)                \
    ?   A                   \
    :   vshr_n_u16(         \
            A,              \
            (16-B*(B<16))   \
        )                   \
)
    uint32x4_t  q = vmovl_u16(a);
    q = vshlq_u32(q, vdupq_n_s32(b));
    uint8x16_t  v = vreinterpretq_u8_u32(q);
    return  vreinterpret_u16_u8(
        vqtbl1_u8(
            v,
            vcreate_u8(0x0f0e0b0a07060302ULL)
        )
    );
}

INLINE(Vdhi,VDHI_SHLR) (Vdhi a, Rc(0, 16) b)
{
#define     VDHI_SHLR(A, B) \
(                           \
    (B > 15)                \
    ?   A                   \
    :   vshr_n_s16(         \
            A,              \
            (16-B*(B<16))   \
        )                   \
)
    int32x4_t   q = vmovl_s16(a);
    q = vshlq_s32(q, vdupq_n_s32(b));
    uint8x16_t  v = vreinterpretq_u8_s32(q);
    return  vreinterpret_s16_u8(
        vqtbl1_u8(
            v,
            vcreate_u8(0x0f0e0b0a07060302ULL)
        )
    );
}


INLINE(Vdwu,VDWU_SHLR) (Vdwu a, Rc(0, 32) b)
{
#define     VDWU_SHLR(A, B) \
(                           \
    (B > 31)                \
    ?   A                   \
    :   vshr_n_u32(         \
            A,              \
            (32-B*(B<32))   \
        )                   \
)
    uint64x2_t  q = vmovl_u32(a);
    q = vshlq_u64(q, vdupq_n_s64(b));
    uint8x16_t  v = vreinterpretq_u8_u64(q);
    return  vreinterpret_u32_u8(
        vqtbl1_u8(
            v,
            vcreate_u8(0x0f0e0d0c07060504ULL)
        )
    );
}

INLINE(Vdwi,VDWI_SHLR) (Vdwi a, Rc(0, 32) b)
{
#define     VDWI_SHLR(A, B) \
(                           \
    (B > 31)                \
    ?   A                   \
    :   vshr_n_s32(         \
            A,              \
            (32-B*(B<32))   \
        )                   \
)
    int64x2_t   q = vmovl_s32(a);
    q = vshlq_s64(q, vdupq_n_s64(b));
    uint8x16_t  v = vreinterpretq_u8_s64(q);
    return  vreinterpret_s32_u8(
        vqtbl1_u8(
            v,
            vcreate_u8(0x0f0e0d0c07060504ULL)
        )
    );
}


INLINE(Vddu,VDDU_SHLR) (Vddu a, Rc(0, 64) b)
{
#define     VDDU_SHLR(A, B) \
(                           \
    (B > 63)                \
    ?   A                   \
    :   vshr_n_u64(         \
            A,              \
            (64-B*(B<64))   \
        )                   \
)
    return  vshl_u64(a, vdup_n_s64(b-64));
}

INLINE(Vddi,VDDI_SHLR) (Vddi a, Rc(0, 64) b)
{
#define     VDDI_SHLR(A, B) \
(                           \
    (B > 63)                \
    ?   A                   \
    :   vshr_n_s64(         \
            A,              \
            (64-B*(B<64))   \
        )                   \
)
    return  vshl_s64(a, vdup_n_s64(b-64));
}


INLINE(Vqyu,VQYU_SHLR) (Vqyu a, Rc(0, 1) b)
{
#define     VQYU_SHLR(A, B) ((B==1)?A:((Vqyu){0}))
    return  (b==1) ? a : ((Vqyu){0});
}

INLINE(Vqbu,VQBU_SHLR) (Vqbu a, Rc(0, 8) b)
{
#define     VQBU_SHLR(A, B) \
(                           \
    (B > 7)                 \
    ?   A                   \
    :   vshrq_n_u8(         \
            A,              \
            (8-B*(B<8))     \
        )                   \
)
    return  vshlq_u8(a, vdupq_n_s8(b-8));
}

INLINE(Vqbi,VQBI_SHLR) (Vqbi a, Rc(0, 8) b)
{
#define     VQBI_SHLR(A, B) \
(                           \
    (B > 7)                 \
    ?   A                   \
    :   vshrq_n_s8(         \
            A,              \
            (8-B*(B<8))     \
        )                   \
)
    return  vshlq_s8(a, vdupq_n_s8(b-8));
}

INLINE(Vqbc,VQBC_SHLR) (Vqbc a, Rc(0, 8) b)
{
#if CHAR_MIN
#   define  VQBC_SHLR(A, B)     \
(                               \
    (B > 7)                     \
    ?   A                       \
    :   VQBI_ASBC(              \
            vshrq_n_s8(         \
                VQBC_ASBI(A),   \
                (8-B*(B<8))     \
            )                   \
        )                       \
)
    return  VQBI_ASBC(
        vshlq_s8(
            VQBC_ASBI(a), 
            vdupq_n_s8(b-8)
        )
    );
#else
#   define  VQBC_SHLR(A, B)     \
(                               \
    (B > 7)                     \
    ?   A                       \
    :   VQBU_ASBC(              \
            vshrq_n_u8(         \
                VQBC_ASBU(A),   \
                (8-B*(B<8))     \
            )                   \
        )                       \
)
    return  VQBU_ASBC(
        vshlq_u8(
            VQBC_ASBU(a), 
            vdupq_n_u8(b-8)
        )
    );
#endif

}


INLINE(Vqhu,VQHU_SHLR) (Vqhu a, Rc(0, 16) b)
{
#define     VQHU_SHLR(A, B) \
(                           \
    (B > 15)                \
    ?   A                   \
    :   vshrq_n_u16(        \
            A,              \
            (16-B*(B<16))   \
        )                   \
)
    return  vshlq_u16(a, vdupq_n_s16(b-16));
}

INLINE(Vqhi,VQHI_SHLR) (Vqhi a, Rc(0, 16) b)
{
#define     VQHI_SHLR(A, B) \
(                           \
    (B > 15)                \
    ?   A                   \
    :   vshrq_n_s16(        \
            A,              \
            (16-B*(B<16))   \
        )                   \
)
    return  vshlq_s16(a, vdupq_n_s16(b-16));
}


INLINE(Vqwu,VQWU_SHLR) (Vqwu a, Rc(0, 32) b)
{
#define     VQWU_SHLR(A, B) \
(                           \
    (B > 31)                \
    ?   A                   \
    :   vshrq_n_u32(        \
            A,              \
            (32-B*(B<32))   \
        )                   \
)
    return  vshlq_u32(a, vdupq_n_s32(b-32));
}

INLINE(Vqwi,VQWI_SHLR) (Vqwi a, Rc(0, 32) b)
{
#define     VQWI_SHLR(A, B) \
(                           \
    (B > 31)                \
    ?   A                   \
    :   vshrq_n_s32(        \
            A,              \
            (32-B*(B<32))   \
        )                   \
)
    return  vshlq_s32(a, vdupq_n_s32(b-32));
}


INLINE(Vqdu,VQDU_SHLR) (Vqdu a, Rc(0, 64) b)
{
#define     VQDU_SHLR(A, B) \
(                           \
    (B > 63)                \
    ?   A                   \
    :   vshrq_n_u64(        \
            A,              \
            (64-B*(B<64))   \
        )                   \
)
    return  vshlq_u64(a, vdupq_n_s64(b-64));
}

INLINE(Vqdi,VQDI_SHLR) (Vqdi a, Rc(0, 64) b)
{
#define     VQDI_SHLR(A, B) \
(                           \
    (B > 63)                \
    ?   A                   \
    :   vshrq_n_s64(        \
            A,              \
            (64-B*(B<64))   \
        )                   \
)
    return  vshlq_s64(a, vdupq_n_s64(b-64));
}

#if 0 // _LEAVE_ARM_SHLR
}
#endif

#if 0 // _ENTER_ARM_SHRS
{
#endif

#ifndef SCHAR_SHRS
INLINE( schar, SCHAR_SHRS)  (signed a, Rc(0, SCHAR_WIDTH) b)
{
#define  MY_SCHAR_SHRS(A, B, ...) ((SCHAR_SHRS)(A,B))
#define     SCHAR_SHRS(...) MY_SCHAR_SHRS(__VA_ARGS__,1)
    return  vqshlb_s8(a, -b);
}
#endif

#ifndef SHRT_SHRS

INLINE( short, SHRT_SHRS)  (signed a, Rc(0, SHRT_WIDTH) b)
{
#define  MY_SHRT_SHRS(A, B, ...) ((SHRT_SHRS)(A,B))
#define     SHRT_SHRS(...) MY_SHRT_SHRS(__VA_ARGS__,1)
    return  vqshlh_s16(a, -b);
}

#endif

#ifndef INT_SHRS

INLINE( int, INT_SHRS)  (int a, Rc(0, INT_WIDTH) b)
{
#define  MY_INT_SHRS(A, B, ...) ((INT_SHRS)(A,B))
#define     INT_SHRS(...) MY_INT_SHRS(__VA_ARGS__,1)
    return  vqshls_s32(a, -b);
}

#endif

#ifndef LONG_SHRS
INLINE( long, LONG_SHRS)  (long a, Rc(0, LONG_WIDTH) b)
{
#define  MY_LONG_SHRS(A, B, ...) ((LONG_SHRS)(A,B))
#define     LONG_SHRS(...) MY_LONG_SHRS(__VA_ARGS__,1)
#if DWRD_NLONG == 2
    return  vqshls_s32(a,-b);
#else
    return  vqshld_s64(a,-b);
#endif
}

#endif

#ifndef LLONG_SHRS
INLINE(llong, LLONG_SHRS)  (llong a, Rc(0, LLONG_WIDTH) b)
{
#define  MY_LLONG_SHRS(A, B, ...) ((LLONG_SHRS)(A,B))
#define     LLONG_SHRS(...) MY_LLONG_SHRS(__VA_ARGS__,1)
#if QUAD_NLLONG == 2
    return  vqshld_s64(a, -b);
#else
#error "128 bit shoulda had LLONG_SHRS defined in <allop.h>"
#endif
}

#endif


INLINE(Vwyu,VWYU_SHRS) (Vwyu a, Rc(0, 1) b)
{
    float       f = VWYU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  l = vreinterpret_u32_f32(m);
    uint32x2_t  r = vdup_n_u32(0u-(b==0));
    l = vand_u32(l, r);
    m = vreinterpret_u32_f32(l);
    f = vget_lane_f32(m, 0);
    return WYU_ASTV(f);
}


INLINE(Vwbu,VWBU_SHRS) (Vwbu a, Rc(0, 8) b)
{
#define     WBU_SHRS(A, B)      \
vget_lane_f32(                  \
    VDBU_ASWF(                  \
        vshl_u8(                \
            VDWF_ASBU(          \
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s8(0-B)      \
        )                       \
    ),                          \
    0                       \
)

#define     VWBU_SHRS(A, B) WBU_ASTV(WBU_SHRS(VWBU_ASTM(A), B))
    return  VWBU_SHRS(a, b);
}

INLINE(Vwbi,VWBI_SHRS) (Vwbi a, Rc(0, 8) b)
{
#define     WBI_SHRS(A, B)      \
vget_lane_f32(                  \
    vreinterpret_f32_s8(        \
        vshl_s8(                \
            vreinterpret_s8_f32(\
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s8(0-B)      \
        )                       \
    ),                          \
    0                       \
)
#define     VWBI_SHRS(A, B) WBI_ASTV(WBI_SHRS(VWBI_ASTM(A), B))
    return  VWBI_SHRS(a, b);
}

INLINE(Vwbc,VWBC_SHRS) (Vwbc a, Rc(0, CHAR_WIDTH) b)
{
#if CHAR_MIN
#   define  VWBC_SHRS(A, B) WBC_ASTV(WBI_SHRS(VWBC_ASTM(A), B))
#else
#   define  VWBC_SHRS(A, B) WBC_ASTV(WBU_SHRS(VWBC_ASTM(A), B))
#endif
    return  VWBC_SHRS(a, b);
}


INLINE(Vwhu,VWHU_SHRS) (Vwhu a, Rc(0, 16) b)
{
#define     WHU_SHRS(A, B)      \
vget_lane_f32(                  \
    vreinterpret_f32_u16(       \
        vshl_u16(               \
            VDWF_ASHU(          \
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s16(0-B)     \
        )                       \
    ),                          \
    0                       \
)

#define     VWHU_SHRS(A, B) WHU_ASTV(WHU_SHRS(VWHU_ASTM(A), B))
    return  VWHU_SHRS(a, b);
}

INLINE(Vwhi,VWHI_SHRS) (Vwhi a, Rc(0, 16) b)
{
#define     WHI_SHRS(A, B)      \
vget_lane_f32(                  \
    vreinterpret_f32_s16(       \
        vshl_s16(               \
            VDWF_ASHI(          \
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s16(0-B)     \
        )                       \
    ),                          \
    0                       \
)

#define     VWHI_SHRS(A, B) WHI_ASTV(WHI_SHRS(VWHI_ASTM(A), B))
    return  VWHI_SHRS(a, b);
}


INLINE(Vwwu,VWWU_SHRS) (Vwwu a, Rc(0, 32) b)
{
#define     WWU_SHRS(A, B)      \
vget_lane_f32(                  \
    vreinterpret_f32_u32(       \
        vshl_u32(               \
            VDWF_ASWU(          \
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s32(0-B)     \
        )                       \
    ),                          \
    0                       \
)

#define     VWWU_SHRS(A, B) WWU_ASTV(WWU_SHRS(VWWU_ASTM(A), B))
    return  VWWU_SHRS(a, b);
}

INLINE(Vwwi,VWWI_SHRS) (Vwwi a, Rc(0, 32) b)
{
#define     WWI_SHRS(A, B)      \
vget_lane_f32(                  \
    vreinterpret_f32_s32(       \
        vshl_s32(               \
            VDWF_ASWI(          \
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s32(0-B)     \
        )                       \
    ),                          \
    0                       \
)

#define     VWWI_SHRS(A, B) WWI_ASTV(WWI_SHRS(VWWI_ASTM(A), B))
    return  VWWI_SHRS(a, b);
}


INLINE(Vdyu,VDYU_SHRS) (Vdyu a, Rc(0, 1) b)
{
    uint64x1_t l = VDYU_ASDU(a);
    uint64x1_t r = vdup_n_u64(UINT64_C(0)-(b==0));
    l = vand_u64(l, r);
    return  DYU_ASTV(l);
}


INLINE(Vdbu,VDBU_SHRS) (Vdbu a, Rc(0, 8) b)
{
#define     VDBU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   vshr_n_u8(A, 16)                \
    :   vshr_n_u8(A,((B&7)+(B==0)))     \
)
    return  vshl_u8(a, vneg_s8(vdup_n_s8(b)));
}

INLINE(Vdbi,VDBI_SHRS) (Vdbi a, Rc(0, 8) b)
{
#define     VDBI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   vshr_n_s8(A, 8)                 \
    :   vshr_n_s8(A,((B&7)+(B==0)))     \
)
    return  vshl_s8(a, vneg_s8(vdup_n_s8(b)));

}

INLINE(Vdbc,VDBC_SHRS) (Vdbc a, Rc(0, CHAR_WIDTH) b)
{
#if CHAR_MIN
#   define  VDBC_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   VDBI_ASBC(vshr_n_s8(VDBC_ASBI(A), 8))               \
    :   VDBI_ASBC(vshr_n_s8(VDBC_ASBI(A),((B&7)+(B==0))))   \
)

    return  VDBI_ASBC(vshl_s8(VDBC_ASBI(a), vneg_s8(vdup_n_s8(b))));
    
#else
#   define  VDBC_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   VDBU_ASBC(vshr_n_u8(VDBC_ASBU(A), 8))               \
    :   VDBU_ASBC(vshr_n_u8(VDBC_ASBU(A),((B&7)+(B==0))))   \
)

    return  VDBU_ASBC(vshl_u8(VDBC_ASBU(a), vneg_s8(vdup_n_s8(b))));

#endif

}


INLINE(Vdhu,VDHU_SHRS) (Vdhu a, Rc(0, 16) b)
{
#define     VDHU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==16)                  \
    ?   vshr_n_u16(A, 16)               \
    :   vshr_n_u16(A,((B&15)+(B==0)))   \
)
    return  vshl_u16(a, vneg_s16(vdup_n_s16(b)));
}

INLINE(Vdhi,VDHI_SHRS) (Vdhi a, Rc(0, 16) b)
{
#define     VDHI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==16)                  \
    ?   vshr_n_s16(A, 16)               \
    :   vshr_n_s16(A,((B&15)+(B==0)))   \
)
    return  vshl_s16(a, vneg_s16(vdup_n_s16(b)));

}


INLINE(Vdwu,VDWU_SHRS) (Vdwu a, Rc(0, 32) b)
{
#define     VDWU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==32)                  \
    ?   vshr_n_u32(A,32)                \
    :   vshr_n_u32(A,((B&31)+(B==0)))   \
)
    return  vshl_u32(a, vneg_s32(vdup_n_s32(b)));
}

INLINE(Vdwi,VDWI_SHRS) (Vdwi a, Rc(0, 32) b)
{
#define     VDWI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==32)                  \
    ?   vshr_n_s32(A,32)                \
    :   vshr_n_s32(A,((B&31)+(B==0)))   \
)
    return  vshl_s32(a, vneg_s32(vdup_n_s32(b)));

}


INLINE(Vddu,VDDU_SHRS) (Vddu a, Rc(0, 64) b)
{
#define     VDDU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==64)                  \
    ?   vshr_n_u64(A,64)                \
    :   vshr_n_u64(A,((B&63)+(B==0)))   \
)
    return  vshl_u64(a, vneg_s64(vdup_n_s64(b)));
}

INLINE(Vddi,VDDI_SHRS) (Vddi a, Rc(0, 64) b)
{
#define     VDDI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==64)                  \
    ?   vshr_n_s64(A,64)                \
    :   vshr_n_s64(A,((B&63)+(B==0)))   \
)
    return  vshl_s64(a, vneg_s64(vdup_n_s64(b)));

}


INLINE(Vqyu,VQYU_SHRS) (Vqyu a, Rc(0, 1) b)
{
    uint64x2_t l = VQYU_ASDU(a);
    uint64x2_t r = vdupq_n_u64(UINT64_C(0)-(b==0));
    l = vandq_u64(l, r);
    return  QYU_ASTV(l);
}


INLINE(Vqbu,VQBU_SHRS) (Vqbu a, Rc(0, 8) b)
{
#define     VQBU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   vshrq_n_u8(A, 16)               \
    :   vshrq_n_u8(A,((B&7)+(B==0)))    \
)
    return  vshlq_u8(a, vnegq_s8(vdupq_n_s8(b)));
}

INLINE(Vqbi,VQBI_SHRS) (Vqbi a, Rc(0, 8) b)
{
#define     VQBI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   vshrq_n_s8(A, 8)                \
    :   vshrq_n_s8(A,((B&7)+(B==0)))    \
)
    return  vshlq_s8(a, vnegq_s8(vdupq_n_s8(b)));

}

INLINE(Vqbc,VQBC_SHRS) (Vqbc a, Rc(0, 8) b)
{
#if CHAR_MIN
#   define  VQBC_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   VQBI_ASBC(vshrq_n_s8(VQBC_ASBI(A), 8))               \
    :   VQBI_ASBC(vshrq_n_s8(VQBC_ASBI(A),((B&7)+(B==0))))   \
)

    return  VQBI_ASBC(vshlq_s8(VQBC_ASBI(a), vnegq_s8(vdupq_n_s8(b))));
    
#else
#   define  VQBC_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   VQBU_ASBC(vshrq_n_u8(VQBC_ASBU(A), 8))               \
    :   VQBU_ASBC(vshrq_n_u8(VQBC_ASBU(A),((B&7)+(B==0))))   \
)

    return  VQBU_ASBC(vshlq_u8(VQBC_ASBU(a), vnegq_s8(vdupq_n_s8(b))));

#endif

}


INLINE(Vqhu,VQHU_SHRS) (Vqhu a, Rc(0, 16) b)
{
#define     VQHU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==16)                  \
    ?   vshrq_n_u16(A, 16)              \
    :   vshrq_n_u16(A,((B&15)+(B==0)))  \
)
    return  vshlq_u16(a, vnegq_s16(vdupq_n_s16(b)));
}

INLINE(Vqhi,VQHI_SHRS) (Vqhi a, Rc(0, 16) b)
{
#define     VQHI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==16)                  \
    ?   vshrq_n_s16(A, 16)              \
    :   vshrq_n_s16(A,((B&15)+(B==0)))  \
)
    return  vshlq_s16(a, vnegq_s16(vdupq_n_s16(b)));

}


INLINE(Vqwu,VQWU_SHRS) (Vqwu a, Rc(0, 32) b)
{
#define     VQWU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==32)                  \
    ?   vshrq_n_u32(A,32)               \
    :   vshrq_n_u32(A,((B&31)+(B==0)))  \
)
    return  vshlq_u32(a, vnegq_s32(vdupq_n_s32(b)));
}

INLINE(Vqwi,VQWI_SHRS) (Vqwi a, Rc(0, 32) b)
{
#define     VQWI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==32)                  \
    ?   vshrq_n_s32(A,32)               \
    :   vshrq_n_s32(A,((B&31)+(B==0)))  \
)
    return  vshlq_s32(a, vnegq_s32(vdupq_n_s32(b)));

}


INLINE(Vqdu,VQDU_SHRS) (Vqdu a, Rc(0, 64) b)
{
#define     VQDU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==64)                  \
    ?   vshrq_n_u64(A,64)               \
    :   vshrq_n_u64(A,((B&63)+(B==0)))  \
)
    return  vshlq_u64(a, vnegq_s64(vdupq_n_s64(b)));
}

INLINE(Vqdi,VQDI_SHRS) (Vqdi a, Rc(0, 64) b)
{
#define     VQDI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==64)                  \
    ?   vshrq_n_s64(A,64)               \
    :   vshrq_n_s64(A,((B&63)+(B==0)))  \
)
    return  vshlq_s64(a, vnegq_s64(vdupq_n_s64(b)));

}

#if 0 // _LEAVE_ARM_SHRS
}
#endif

#if 0 // _ENTER_ARM_SILL
{
#endif

INLINE(Vwyu,VWYU_SILL) (Vwyu a, Vwyu b, Rc(0, 1) c)
{
    return c == 0 ? a : b;
}

INLINE(Vwbu,VWBU_SILL) (Vwbu a, Vwbu b, Rc(0, 8) c)
{
#define     VWBU_SILL(A, B, C)                      \
(                                                   \
    (C > 7)                                         \
    ?   B                                           \
    :   WBU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vsli_n_u8(                      \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBU_ASTM(B))\
                        ),                          \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBU_ASTM(A))\
                        ),                          \
                        (7&C)                       \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float       s0 = VWBU_ASTM(a);
    uint8x8_t   v0 = vreinterpret_u8_f32(vdup_n_f32(s0));

    float       s1 = VWBU_ASTM(b);
    uint8x8_t   v1 = vreinterpret_u8_f32(vdup_n_f32(s1));

    uint8x8_t   v2 = vdup_n_u8(UINT8_MAX);
    v2 = vshl_u8(v2, vdup_n_s8(c-8));
    v1 = vand_u8(v1, v2);
    v0 = vshl_u8(v0, vdup_n_s8(c));
    v0 = vorr_u8(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u8(v0), 0);
    return  WBU_ASTV(s0);
}

INLINE(Vwbc,VWBC_SILL) (Vwbc a, Vwbc b, Rc(0, 8) c)
{
#define     VWBC_SILL(A, B, C)                      \
(                                                   \
    (C > 7)                                         \
    ?   B                                           \
    :   WBU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vsli_n_u8(                      \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBC_ASTM(B))\
                        ),                          \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBC_ASTM(A))\
                        ),                          \
                        (7&C)                       \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float       s0 = VWBC_ASTM(a);
    uint8x8_t   v0 = vreinterpret_u8_f32(vdup_n_f32(s0));

    float       s1 = VWBC_ASTM(b);
    uint8x8_t   v1 = vreinterpret_u8_f32(vdup_n_f32(s1));

    uint8x8_t   v2 = vdup_n_u8(UINT8_MAX);
    v2 = vshl_u8(v2, vdup_n_s8(c-8));
    v1 = vand_u8(v1, v2);
    v0 = vshl_u8(v0, vdup_n_s8(c));
    v0 = vorr_u8(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u8(v0), 0);
    return  WBC_ASTV(s0);
}


INLINE(Vwhu,VWHU_SILL) (Vwhu a, Vwhu b, Rc(0, 16) c)
{
#define     VWHU_SILL(A, B, C)                          \
(                                                       \
    (C > 15)                                            \
    ?   B                                               \
    :   WHU_ASTV(                                       \
            vget_lane_f32(                              \
                vreinterpret_f32_u16(                   \
                    vsli_n_u16(                         \
                        vreinterpret_u16_f32(           \
                            vdup_n_f32(VWHU_ASTM(B))    \
                        ),                              \
                        vreinterpret_u16_f32(           \
                            vdup_n_f32(VWHU_ASTM(A))    \
                        ),                              \
                        (15&C)                          \
                    )                                   \
                ),                                      \
                0                                       \
            )                                           \
        )                                               \
)

    float       s0 = VWHU_ASTM(a);
    uint16x4_t  v0 = vreinterpret_u16_f32(vdup_n_f32(s0));

    float       s1 = VWHU_ASTM(b);
    uint16x4_t  v1 = vreinterpret_u16_f32(vdup_n_f32(s1));

    uint16x4_t  v2 = vdup_n_u16(UINT16_MAX);
    v2 = vshl_u16(v2, vdup_n_s16(c-16));
    v1 = vand_u16(v1, v2);
    v0 = vshl_u16(v0, vdup_n_s16(c));
    v0 = vorr_u16(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u16(v0), 0);
    return  WHU_ASTV(s0);
}


INLINE(Vwwu,VWWU_SILL) (Vwwu a, Vwwu b, Rc(0, 32) c)
{
#define     VWWU_SILL(A, B, C)                          \
(                                                       \
    (C > 31)                                            \
    ?   B                                               \
    :   WWU_ASTV(                                       \
            vget_lane_f32(                              \
                vreinterpret_f32_u32(                   \
                    vsli_n_u32(                         \
                        vreinterpret_u32_f32(           \
                            vdup_n_f32(VWWU_ASTM(B))    \
                        ),                              \
                        vreinterpret_u32_f32(           \
                            vdup_n_f32(VWWU_ASTM(A))    \
                        ),                              \
                        (31&C)                          \
                    )                                   \
                ),                                      \
                0                                       \
            )                                           \
        )                                               \
)

    float       s0 = VWWU_ASTM(a);
    uint32x2_t  v0 = vreinterpret_u32_f32(vdup_n_f32(s0));

    float       s1 = VWWU_ASTM(b);
    uint32x2_t  v1 = vreinterpret_u32_f32(vdup_n_f32(s1));

    uint32x2_t  v2 = vdup_n_u32(UINT32_MAX);
    v2 = vshl_u32(v2, vdup_n_s32(c-16));
    v1 = vand_u32(v1, v2);
    v0 = vshl_u32(v0, vdup_n_s32(c));
    v0 = vorr_u32(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u32(v0), 0);
    return  WWU_ASTV(s0);
}



INLINE(Vdyu,VDYU_SILL) (Vdyu a, Vdyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vdbu,VDBU_SILL) (Vdbu a, Vdbu b, Rc(0, 8) c)
{
#define     VDBU_SILL(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsli_n_u8(B,A,(7&C))    \
)
    uint8x8_t   m = vdup_n_u8(UINT8_MAX);
    m = vshl_u8(m, vdup_n_s8(c-8));
    b = vand_u8(b, m);
    a = vshl_u8(a, vdup_n_s8(c));
    return  vorr_u8(a, b);
}

INLINE(Vdbc,VDBC_SILL) (Vdbc a, Vdbc b, Rc(0, 8) c)
{
#define     VDBC_SILL(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   VDBU_ASBC(              \
            vsli_n_u8(          \
                VDBC_ASBU(B),   \
                VDBC_ASBU(A),   \
                (7&C)           \
            )                   \
        )                       \
)
    return  VDBU_ASBC((VDBU_SILL)(VDBC_ASBU(a), VDBC_ASBU(b), c));
}

INLINE(Vdhu,VDHU_SILL) (Vdhu a, Vdhu b, Rc(0, 16) c)
{
#define     VDHU_SILL(A, B, C)  \
(                               \
    (C > 15)                    \
    ?   B                       \
    :   vsli_n_u16(B,A,(15&C))  \
)
    uint16x4_t   m = vdup_n_u16(UINT16_MAX);
    m = vshl_u16(m, vdup_n_s16(c-16));
    b = vand_u16(b, m);
    a = vshl_u16(a, vdup_n_s16(c));
    return  vorr_u16(a, b);
}

INLINE(Vdwu,VDWU_SILL) (Vdwu a, Vdwu b, Rc(0, 32) c)
{
#define     VDWU_SILL(A, B, C)  \
(                               \
    (C > 31)                    \
    ?   B                       \
    :   vsli_n_u32(B,A,(31&C))  \
)
    uint32x2_t   m = vdup_n_u32(UINT32_MAX);
    m = vshl_u32(m, vdup_n_s32(c-32));
    b = vand_u32(b, m);
    a = vshl_u32(a, vdup_n_s32(c));
    return  vorr_u32(a, b);
}

INLINE(Vddu,VDDU_SILL) (Vddu a, Vddu b, Rc(0, 64) c)
{
#define     VDDU_SILL(A, B, C)  \
(                               \
    (C > 63)                    \
    ?   B                       \
    :   vsli_n_u64(B,A,(63&C))  \
)
    uint64x1_t   m = vdup_n_u64(UINT64_MAX);
    m = vshl_u64(m, vdup_n_s64(c-64));
    b = vand_u64(b, m);
    a = vshl_u64(a, vdup_n_s64(c));
    return  vorr_u64(a, b);
}


INLINE(Vqyu,VQYU_SILL) (Vqyu a, Vqyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vqbu,VQBU_SILL) (Vqbu a, Vqbu b, Rc(0, 8) c)
{
#define     VQBU_SILL(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsliq_n_u8(B,A,(7&C))    \
)
    uint8x16_t  m = vdupq_n_u8(UINT8_MAX);
    m = vshlq_u8(m, vdupq_n_s8(c-8));
    b = vandq_u8(b, m);
    a = vshlq_u8(a, vdupq_n_s8(c));
    return  vorrq_u8(a, b);
}

INLINE(Vqbc,VQBC_SILL) (Vqbc a, Vqbc b, Rc(0, 8) c)
{
#define     VQBC_SILL(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   VQBU_ASBC(              \
            vsliq_n_u8(         \
                VQBC_ASBU(B),   \
                VQBC_ASBU(A),   \
                (7&C)           \
            )                   \
        )                       \
)
    return  VQBU_ASBC((VQBU_SILL)(VQBC_ASBU(a), VQBC_ASBU(b), c));
}

INLINE(Vqhu,VQHU_SILL) (Vqhu a, Vqhu b, Rc(0, 16) c)
{
#define     VQHU_SILL(A, B, C)  \
(                               \
    (C > 15)                    \
    ?   B                       \
    :   vsliq_n_u16(B,A,(15&C)) \
)
    uint16x8_t   m = vdupq_n_u16(UINT16_MAX);
    m = vshlq_u16(m, vdupq_n_s16(c-16));
    b = vandq_u16(b, m);
    a = vshlq_u16(a, vdupq_n_s16(c));
    return  vorrq_u16(a, b);
}

INLINE(Vqwu,VQWU_SILL) (Vqwu a, Vqwu b, Rc(0, 32) c)
{
#define     VQWU_SILL(A, B, C)  \
(                               \
    (C > 31)                    \
    ?   B                       \
    :   vsliq_n_u32(B,A,(31&C)) \
)
    uint32x4_t   m = vdupq_n_u32(UINT32_MAX);
    m = vshlq_u32(m, vdupq_n_s32(c-32));
    b = vandq_u32(b, m);
    a = vshlq_u32(a, vdupq_n_s32(c));
    return  vorrq_u32(a, b);
}

INLINE(Vqdu,VQDU_SILL) (Vqdu a, Vqdu b, Rc(0, 64) c)
{
#define     VQDU_SILL(A, B, C)  \
(                               \
    (C > 63)                    \
    ?   B                       \
    :   vsliq_n_u64(B,A,(63&C)) \
)
    uint64x2_t   m = vdupq_n_u64(UINT64_MAX);
    m = vshlq_u64(m, vdupq_n_s64(c-64));
    b = vandq_u64(b, m);
    a = vshlq_u64(a, vdupq_n_s64(c));
    return  vorrq_u64(a, b);
}

#if 0 // _LEAVE_ARM_SILL
}
#endif

#if 0 // _ENTER_ARM_SILR
{
#endif


INLINE(Vwyu,VWYU_SILR) (Vwyu a, Vwyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vwbu,VWBU_SILR) (Vwbu a, Vwbu b, Rc(0, 8) c)
{
    uint8x8_t   p = VDWF_ASBU(vdup_n_f32(VWBU_ASTM(a)));
    uint8x8_t   q = VDWF_ASBU(vdup_n_f32(VWBU_ASTM(b)));
    p = vshl_u8(p, vdup_n_s8(c));
    q = vshl_u8(q, vdup_n_s8((8-c)-8));
    p = vorr_u8(p, q);
    float32x2_t r = vreinterpret_u8_f32(p);
    float       m = vget_lane_f32(r, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbc,VWBC_SILR) (Vwbc a, Vwbc b, Rc(0, 8) c)
{
    uint8x8_t   p = VDWF_ASBU(vdup_n_f32(VWBC_ASTM(a)));
    uint8x8_t   q = VDWF_ASBU(vdup_n_f32(VWBC_ASTM(b)));
    p = vshl_u8(p, vdup_n_s8(c));
    q = vshl_u8(q, vdup_n_s8((8-c)-8));
    p = vorr_u8(p, q);
    float32x2_t r = vreinterpret_u8_f32(p);
    float       m = vget_lane_f32(r, 0);
    return  WBC_ASTV(m);
}

INLINE(Vwhu,VWHU_SILR) (Vwhu a, Vwhu b, Rc(0, 16) c)
{
    uint16x4_t p = VDWF_ASHU(vdup_n_f32(VWHU_ASTM(a)));
    uint16x4_t q = VDWF_ASHU(vdup_n_f32(VWHU_ASTM(b)));
    p = vshl_u16(p, vdup_n_s16(c));
    q = vshl_u16(q, vdup_n_s16((16-c)-16));
    p = vorr_u16(p, q);
    float32x2_t r = vreinterpret_u16_f32(p);
    float       m = vget_lane_f32(r, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwwu,VWWU_SILR) (Vwwu a, Vwwu b, Rc(0, 32) c)
{
    uint32x2_t p = VDWF_ASWU(vdup_n_f32(VWWU_ASTM(a)));
    uint32x2_t q = VDWF_ASWU(vdup_n_f32(VWWU_ASTM(b)));
    p = vshl_u32(p, vdup_n_s32(c));
    q = vshl_u32(q, vdup_n_s32((32-c)-32));
    p = vorr_u32(p, q);
    float32x2_t r = vreinterpret_u32_f32(p);
    float       m = vget_lane_f32(r, 0);
    return  WWU_ASTV(m);
}


INLINE(Vdyu,VDYU_SILR) (Vdyu a, Vdyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vdbu,VDBU_SILR) (Vdbu a, Vdbu b, Rc(0, 8) c)
{
#define     VDBU_SILR(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsli_n_u8(              \
            vshr_n_u8(          \
                B,              \
                (8-(7&C))       \
            ),                  \
            A,                  \
            (7&C)               \
        )                       \
)
    a = vshl_u8(a, vdup_n_s8(c));
    b = vshl_u8(b, vdup_n_s8((8-c)-8));
    return  vorr_u8(a, b);
}

INLINE(Vdbc,VDBC_SILR) (Vdbc a, Vdbc b, Rc(0, 8) c)
{
#define     VDBC_SILR(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsli_n_u8(              \
            vshr_n_u8(          \
                VDBC_ASBU(B),   \
                8-(7&C)         \
            ),                  \
            VDBC_ASBU(A),       \
            (7&C)               \
        )                       \
)
    return  VDBU_ASBC(((VDBU_SILR)(VDBC_ASBU(a), VDBC_ASBU(b), c)));
}

INLINE(Vdhu,VDHU_SILR) (Vdhu a, Vdhu b, Rc(0, 16) c)
{
#define     VDHU_SILR(A, B, C)  \
(                               \
    (C > 15)                    \
    ?   B                       \
    :   vsli_n_u16(             \
            vshr_n_u16(         \
                B,              \
                16-(15&C)       \
            ),                  \
            A,                  \
            (15&C)              \
        )                       \
)
    a = vshl_u16(a, vdup_n_s16(c));
    b = vshl_u16(b, vdup_n_s16((16-c)-16));
    return  vorr_u16(a, b);
}

INLINE(Vdwu,VDWU_SILR) (Vdwu a, Vdwu b, Rc(0, 32) c)
{
#define     VDWU_SILR(A, B, C)  \
(                               \
    (C > 31)                    \
    ?   B                       \
    :   vsli_n_u32(             \
            vshr_n_u32(         \
                B,              \
                32-(31&C)       \
            ),                  \
            A,                  \
            (31&C)              \
        )                       \
)
    a = vshl_u32(a, vdup_n_s32(c));
    b = vshl_u32(b, vdup_n_s32((32-c)-32));
    return  vorr_u32(a, b);
}

INLINE(Vddu,VDDU_SILR) (Vddu a, Vddu b, Rc(0, 64) c)
{
#define     VDDU_SILR(A, B, C)  \
(                               \
    (C > 63)                    \
    ?   B                       \
    :   vsli_n_u64(             \
            vshr_n_u64(         \
                B,              \
                64-(63&C)       \
            ),                  \
            A,                  \
            (63&C)              \
        )                       \
)
    a = vshl_u64(a, vdup_n_s64(c));
    b = vshl_u64(b, vdup_n_s64((64-c)-64));
    return  vorr_u64(a, b);
}


INLINE(Vqyu,VQYU_SILR) (Vqyu a, Vqyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vqbu,VQBU_SILR) (Vqbu a, Vqbu b, Rc(0, 8) c)
{
#define     VQBU_SILR(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsliq_n_u8(             \
            vshrq_n_u8(         \
                B,              \
                8-(7&C)         \
            ),                  \
            A,                  \
            (7&C)               \
        )                       \
)
    a = vshlq_u8(a, vdupq_n_s8(c));
    b = vshlq_u8(b, vdupq_n_s8((8-c)-8));
    return  vorrq_u8(a, b);
}

INLINE(Vqbc,VQBC_SILR) (Vqbc a, Vqbc b, Rc(0, 8) c)
{
#define     VQBC_SILR(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsliq_n_u8(             \
            vshrq_n_u8(         \
                VQBC_ASBU(B),   \
                8-(7&C)         \
            ),                  \
            VQBC_ASBU(A),       \
            (7&C)               \
        )                       \
)
    return  VQBU_ASBC(((VQBU_SILR)(VQBC_ASBU(a), VQBC_ASBU(b), c)));
}

INLINE(Vqhu,VQHU_SILR) (Vqhu a, Vqhu b, Rc(0, 16) c)
{
#define     VQHU_SILR(A, B, C)  \
(                               \
    (C > 15)                    \
    ?   B                       \
    :   vsliq_n_u16(            \
            vshrq_n_u16(        \
                B,              \
                16-(15&C)       \
            ),                  \
            A,                  \
            (15&C)              \
        )                       \
)
    a = vshlq_u16(a, vdupq_n_s16(c));
    b = vshlq_u16(b, vdupq_n_s16((16-c)-16));
    return  vorrq_u16(a, b);
}

INLINE(Vqwu,VQWU_SILR) (Vqwu a, Vqwu b, Rc(0, 32) c)
{
#define     VQWU_SILR(A, B, C)  \
(                               \
    (C > 31)                    \
    ?   B                       \
    :   vsliq_n_u32(            \
            vshrq_n_u32(        \
                B,              \
                32-(31&C)       \
            ),                  \
            A,                  \
            (31&C)              \
        )                       \
)
    a = vshlq_u32(a, vdupq_n_s32(c));
    b = vshlq_u32(b, vdupq_n_s32((32-c)-32));
    return  vorrq_u32(a, b);
}

INLINE(Vqdu,VQDU_SILR) (Vqdu a, Vqdu b, Rc(0, 64) c)
{
#define     VQDU_SILR(A, B, C)  \
(                               \
    (C > 63)                    \
    ?   B                       \
    :   vsliq_n_u64(            \
            vshrq_n_u64(        \
                B,              \
                64-(63&C)       \
            ),                  \
            A,                  \
            (63&C)              \
        )                       \
)
    a = vshlq_u64(a, vdupq_n_s64(c));
    b = vshlq_u64(b, vdupq_n_s64((64-c)-64));
    return  vorrq_u64(a, b);
}

#if 0 // _LEAVE_ARM_SILR
}
#endif

#if 0 // _ENTER_ARM_SIRR
{
#endif

INLINE(Vwyu,VWYU_SIRR) (Vwyu a, Vwyu b, Rc(0, 1) c)
{
    return c == 0 ? a : b;
}


INLINE(Vwbu,VWBU_SIRR) (Vwbu a, Vwbu b, Rc(0, 8) c)
{
#define     VWBU_SIRR(A, B, C)                      \
(                                                   \
    (C > 7) ? B :                                   \
    (C < 1) ? A                                     \
    :   WBU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vsri_n_u8(                      \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBU_ASTM(B))\
                        ),                          \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBU_ASTM(A))\
                        ),                          \
                        ((7&C)+(!(7&C)))            \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float       s0 = VWBU_ASTM(a);
    uint8x8_t   v0 = vreinterpret_u8_f32(vdup_n_f32(s0));

    float       s1 = VWBU_ASTM(b);
    uint8x8_t   v1 = vreinterpret_u8_f32(vdup_n_f32(s1));

    uint8x8_t   v2 = vdup_n_u8(UINT8_MAX);

    int8x8_t    n = vdup_n_s8((-c));
    v0 = vshl_u8(v0, n);
    v2 = vshl_u8(v2, n);
    v1 = vbic_u8(v1, v2);
    v0 = vorr_u8(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u8(v0), 0);
    return  WBU_ASTV(s0);
}

INLINE(Vwbc,VWBC_SIRR) (Vwbc a, Vwbc b, Rc(0, 8) c)
{
#define     VWBC_SIRR(A, B, C)                  \
(                                               \
    (C > 7) ? B :                               \
    (C < 1) ? A :                               \
    WBC_ASTV(                                   \
        vget_lane_f32(                          \
            vreinterpret_f32_u8(                \
                vsri_n_u8(                      \
                    vreinterpret_u8_f32(        \
                        vdup_n_f32(VWBC_ASTM(B))\
                    ),                          \
                    vreinterpret_u8_f32(        \
                        vdup_n_f32(VWBC_ASTM(A))\
                    ),                          \
                    ((7&C)+(!(7&C)))            \
                )                               \
            ),                                  \
            0                                   \
        )                                       \
    )                                           \
)

    float       s0 = VWBC_ASTM(a);
    uint8x8_t   v0 = vreinterpret_u8_f32(vdup_n_f32(s0));

    float       s1 = VWBC_ASTM(b);
    uint8x8_t   v1 = vreinterpret_u8_f32(vdup_n_f32(s1));

    uint8x8_t   v2 = vdup_n_u8(UINT8_MAX);

    int8x8_t    n = vdup_n_s8((-c));
    v0 = vshl_u8(v0, n);
    v2 = vshl_u8(v2, n);
    v1 = vbic_u8(v1, v2);
    v0 = vorr_u8(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u8(v0), 0);
    return  WBC_ASTV(s0);
}


INLINE(Vwhu,VWHU_SIRR) (Vwhu a, Vwhu b, Rc(0, 16) c)
{
#define     VWHU_SIRR(A, B, C)                      \
(                                                   \
    (C > 15) ? B :                                  \
    (C <  1) ? A                                    \
    :   WHU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u16(               \
                    vsri_n_u16(                     \
                        vreinterpret_u16_f32(       \
                            vdup_n_f32(VWHU_ASTM(B))\
                        ),                          \
                        vreinterpret_u16_f32(       \
                            vdup_n_f32(VWHU_ASTM(A))\
                        ),                          \
                        ((15&C)+(!(15&C)))          \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float       s0 = VWHU_ASTM(a);
    uint16x4_t  v0 = vreinterpret_u16_f32(vdup_n_f32(s0));

    float       s1 = VWHU_ASTM(b);
    uint16x4_t  v1 = vreinterpret_u16_f32(vdup_n_f32(s1));

    uint16x4_t   v2 = vdup_n_u16(UINT16_MAX);

    int16x4_t    n = vdup_n_s16((-c));
    v0 = vshl_u16(v0, n);
    v2 = vshl_u16(v2, n);
    v1 = vbic_u16(v1, v2);
    v0 = vorr_u16(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u16(v0), 0);
    return  WHU_ASTV(s0);
}


INLINE(Vwwu,VWWU_SIRR) (Vwwu a, Vwwu b, Rc(0, 32) c)
{
#define     VWWU_SIRR(A, B, C)                      \
(                                                   \
    (C > 31) ? B :                                  \
    (C <  1) ? A                                    \
    :   WWU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u32(               \
                    vsri_n_u32(                     \
                        vreinterpret_u32_f32(       \
                            vdup_n_f32(VWWU_ASTM(B))\
                        ),                          \
                        vreinterpret_u32_f32(       \
                            vdup_n_f32(VWWU_ASTM(A))\
                        ),                          \
                        ((31&C)+(!(31&C)))          \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float       s0 = VWWU_ASTM(a);
    uint32x2_t  v0 = vreinterpret_u32_f32(vdup_n_f32(s0));

    float       s1 = VWWU_ASTM(b);
    uint32x2_t  v1 = vreinterpret_u32_f32(vdup_n_f32(s1));

    uint32x2_t   v2 = vdup_n_u32(UINT32_MAX);

    int32x2_t    n = vdup_n_s32((-c));
    v0 = vshl_u32(v0, n);
    v2 = vshl_u32(v2, n);
    v1 = vbic_u32(v1, v2);
    v0 = vorr_u32(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u32(v0), 0);
    return  WWU_ASTV(s0);
}



INLINE(Vdyu,VDYU_SIRR) (Vdyu a, Vdyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vdbu,VDBU_SIRR) (Vdbu a, Vdbu b, Rc(0, 8) c)
{
#define     VDBU_SIRR(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C >  7) ? B :                      \
    vsri_n_u8(B,A,((7&C)+(!(7&C))))     \
)
    uint8x8_t   m = vdup_n_u8(UINT8_MAX);
    int8x8_t    n = vdup_n_s8((-c));
    m = vshl_u8(m, n);
    a = vshl_u8(a, n);
    b = vbic_u8(b, m);
    return  vorr_u8(a, b);
}

INLINE(Vdbc,VDBC_SIRR) (Vdbc a, Vdbc b, Rc(0, 8) c)
{
#define     VDBC_SIRR(A, B, C)      \
(                                   \
    (C <  1) ? A :                  \
    (C >  7) ? B :                  \
    VDBU_ASBC(                      \
        vsri_n_u8(                  \
            VDBC_ASBU(B),           \
            VDBC_ASBU(A),           \
            ((7&C)+(!(7&C)))        \
        )                           \
    )                               \
)

    return  VDBU_ASBC((VDBU_SIRR)(VDBC_ASBU(a), VDBC_ASBU(b), c));
}

INLINE(Vdhu,VDHU_SIRR) (Vdhu a, Vdhu b, Rc(0, 16) c)
{
#define     VDHU_SIRR(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 15) ? B :                      \
    vsri_n_u16(B,A,((15&C)+(!(15&C))))  \
)
    uint16x4_t  m = vdup_n_u16(UINT16_MAX);
    int16x4_t   n = vdup_n_u16((-c));
    m = vshl_u16(m, n);
    a = vshl_u16(a, n);
    b = vbic_u16(b, m);
    return  vorr_u16(a, b);
}

INLINE(Vdwu,VDWU_SIRR) (Vdwu a, Vdwu b, Rc(0, 32) c)
{
#define     VDWU_SIRR(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 31) ? B :                      \
    vsri_n_u32(B,A,((31&C)+(!(31&C))))  \
)
    uint32x2_t  m = vdup_n_u32(UINT32_MAX);
    int32x2_t   n = vdup_n_u32((-c));
    m = vshl_u32(m, n);
    a = vshl_u32(a, n);
    b = vbic_u32(b, m);
    return  vorr_u32(a, b);
}

INLINE(Vddu,VDDU_SIRR) (Vddu a, Vddu b, Rc(0, 64) c)
{
#define     VDDU_SIRR(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 63) ? B :                      \
    vsri_n_u64(B,A,((63&C)+(!(63&C))))  \
)
    uint64x1_t  m = vdup_n_u64(UINT64_MAX);
    int64x1_t   n = vdup_n_u64((-c));
    m = vshl_u64(m, n);
    a = vshl_u64(a, n);
    b = vbic_u64(b, m);
    return  vorr_u64(a, b);
}

INLINE(Vqyu,VQYU_SIRR) (Vqyu a, Vqyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vqbu,VQBU_SIRR) (Vqbu a, Vqbu b, Rc(0, 8) c)
{
#define     VQBU_SIRR(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C >  7) ? B :                      \
    vsriq_n_u8(B,A,((7&C)+(!(7&C))))    \
)
    uint8x16_t  m = vdupq_n_u8(UINT8_MAX);
    int8x16_t   n = vdupq_n_s8((-c));
    m = vshlq_u8(m, n);
    a = vshlq_u8(a, n);
    b = vbicq_u8(b, m);
    return  vorrq_u8(a, b);
}

INLINE(Vqbc,VQBC_SIRR) (Vqbc a, Vqbc b, Rc(0, 8) c)
{
#define     VQBC_SIRR(A, B, C)      \
(                                   \
    (C <  1) ? A :                  \
    (C >  7) ? B :                  \
    VQBU_ASBC(                      \
        vsriq_n_u8(                 \
            VQBC_ASBU(B),           \
            VQBC_ASBU(A),           \
            ((7&C)+(!(7&C)))        \
        )                           \
    )                               \
)

    return  VQBU_ASBC((VQBU_SIRR)(VQBC_ASBU(a), VQBC_ASBU(b), c));
}

INLINE(Vqhu,VQHU_SIRR) (Vqhu a, Vqhu b, Rc(0, 16) c)
{
#define     VQHU_SIRR(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 15) ? B :                      \
    vsriq_n_u16(B,A,((15&C)+(!(15&C)))) \
)
    uint16x8_t  m = vdupq_n_u16(UINT16_MAX);
    int16x8_t   n = vdupq_n_u16((-c));
    m = vshlq_u16(m, n);
    a = vshlq_u16(a, n);
    b = vbicq_u16(b, m);
    return  vorrq_u16(a, b);
}

INLINE(Vqwu,VQWU_SIRR) (Vqwu a, Vqwu b, Rc(0, 32) c)
{
#define     VQWU_SIRR(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 31) ? B :                      \
    vsriq_n_u32(B,A,((31&C)+(!(31&C)))) \
)
    uint32x4_t  m = vdupq_n_u32(UINT32_MAX);
    int32x4_t   n = vdupq_n_u32((-c));
    m = vshlq_u32(m, n);
    a = vshlq_u32(a, n);
    b = vbicq_u32(b, m);
    return  vorrq_u32(a, b);
}

INLINE(Vqdu,VQDU_SIRR) (Vqdu a, Vqdu b, Rc(0, 64) c)
{
#define     VQDU_SIRR(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 63) ? B :                      \
    vsriq_n_u64(B,A,((63&C)+(!(63&C)))) \
)
    uint64x2_t  m = vdupq_n_u64(UINT64_MAX);
    int64x2_t   n = vdupq_n_u64((-c));
    m = vshlq_u64(m, n);
    a = vshlq_u64(a, n);
    b = vbicq_u64(b, m);
    return  vorrq_u64(a, b);
}

#if 0 // _LEAVE_ARM_SIRR
}
#endif

#if 0 // _ENTER_ARM_SIRL
{
#endif

INLINE(Vwyu,VWYU_SIRL) (Vwyu a, Vwyu b, Rc(0, 1) c)
{
    return c == 0 ? a : b;
}


INLINE(Vwbu,VWBU_SIRL) (Vwbu a, Vwbu b, Rc(0, 8) c)
{
#define     VWBU_SIRL(A, B, C)                      \
(                                                   \
    (C < 1) ? A :                                   \
    (C > 7) ? B :                                   \
    WBU_ASTV(                                       \
        vget_lane_f32(                              \
            vreinterpret_f32_u8(                    \
                vsri_n_u8(                          \
                    vshl_n_u8(                      \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBU_ASTM(B))\
                        ),                          \
                        (8-((7&C)+(!(7&C))))        \
                    ),                              \
                    vreinterpret_u8_f32(            \
                        vdup_n_f32(VWBU_ASTM(A))    \
                    ),                              \
                    ((7&C)+(!(7&C)))                \
                )                                   \
            ),                                      \
            0                                       \
        )                                           \
    )                                               \
)

    float       s0 = VWBU_ASTM(a);
    uint8x8_t   v0 = vreinterpret_u8_f32(vdup_n_f32(s0));

    float       s1 = VWBU_ASTM(b);
    uint8x8_t   v1 = vreinterpret_u8_f32(vdup_n_f32(s1));

    int8x8_t    v2 = vdup_n_s8(c);
    int8x8_t    v3 = vdup_n_s8(-8);
    v3 = vadd_s8(v2, v3);
    v2 = vneg_s8(v2);
    v0 = vshl_u8(v0, v2);
    v1 = vshl_u8(v1, v3);
    v0 = vorr_u8(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u8(v0), 0);
    return  WBU_ASTV(s0);
}

INLINE(Vwbc,VWBC_SIRL) (Vwbc a, Vwbc b, Rc(0, 8) c)
{
#define     VWBC_SIRL(A, B, C)                      \
(                                                   \
    (C < 1) ? A :                                   \
    (C > 7) ? B :                                   \
    WBC_ASTV(                                       \
        vget_lane_f32(                              \
            vreinterpret_f32_u8(                    \
                vsri_n_u8(                          \
                    vshl_n_u8(                      \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBC_ASTM(B))\
                        ),                          \
                        (8-((7&C)+(!(7&C))))        \
                    ),                              \
                    vreinterpret_u8_f32(            \
                        vdup_n_f32(VWBC_ASTM(A))    \
                    ),                              \
                    ((7&C)+(!(7&C)))                \
                )                                   \
            ),                                      \
            0                                       \
        )                                           \
    )                                               \
)

    float       s0 = VWBC_ASTM(a);
    uint8x8_t   v0 = vreinterpret_u8_f32(vdup_n_f32(s0));

    float       s1 = VWBC_ASTM(b);
    uint8x8_t   v1 = vreinterpret_u8_f32(vdup_n_f32(s1));

    int8x8_t    v2 = vdup_n_s8(c);
    int8x8_t    v3 = vdup_n_s8(-8);
    v3 = vadd_s8(v2, v3);
    v2 = vneg_s8(v2);
    v0 = vshl_u8(v0, v2);
    v1 = vshl_u8(v1, v3);
    v0 = vorr_u8(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u8(v0), 0);
    return  WBC_ASTV(s0);
}


INLINE(Vwhu,VWHU_SIRL) (Vwhu a, Vwhu b, Rc(0, 16) c)
{
#define     VWHU_SIRL(A, B, C)                      \
(                                                   \
    (C <  1) ? A :                                  \
    (C > 15) ? B :                                  \
    WHU_ASTV(                                       \
        vget_lane_f32(                              \
            vreinterpret_f32_u16(                   \
                vsri_n_u16(                         \
                    vshl_n_u16(                     \
                        vreinterpret_u16_f32(       \
                            vdup_n_f32(VWHU_ASTM(B))\
                        ),                          \
                        (16-((15&C)+(!(15&C))))     \
                    ),                              \
                    vreinterpret_u16_f32(           \
                        vdup_n_f32(VWHU_ASTM(A))    \
                    ),                              \
                    ((15&C)+(!(15&C)))              \
                )                                   \
            ),                                      \
            0                                       \
        )                                           \
    )                                               \
)

    float       s0 = VWHU_ASTM(a);
    uint16x4_t  v0 = vreinterpret_u16_f32(vdup_n_f32(s0));

    float       s1 = VWHU_ASTM(b);
    uint16x4_t  v1 = vreinterpret_u16_f32(vdup_n_f32(s1));

    int16x4_t   v2 = vdup_n_s16(c);
    int16x4_t   v3 = vdup_n_s16(-16);
    v3 = vadd_s16(v2, v3);
    v2 = vneg_s16(v2);
    v0 = vshl_u16(v0, v2);
    v1 = vshl_u16(v1, v3);
    v0 = vorr_u16(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u16(v0), 0);
    return  WHU_ASTV(s0);
}


INLINE(Vwwu,VWWU_SIRL) (Vwwu a, Vwwu b, Rc(0, 32) c)
{
#define     VWWU_SIRL(A, B, C)                      \
(                                                   \
    (C <  1) ? A :                                  \
    (C > 31) ? B :                                  \
    WWU_ASTV(                                       \
        vget_lane_f32(                              \
            vreinterpret_f32_u32(                   \
                vsri_n_u32(                         \
                    vshl_n_u32(                     \
                        vreinterpret_u32_f32(       \
                            vdup_n_f32(VWWU_ASTM(B))\
                        ),                          \
                        (32-((31&C)+(!(31&C))))     \
                    ),                              \
                    vreinterpret_u32_f32(           \
                        vdup_n_f32(VWWU_ASTM(A))    \
                    ),                              \
                    ((31&C)+(!(31&C)))              \
                )                                   \
            ),                                      \
            0                                       \
        )                                           \
    )                                               \
)

    float       s0 = VWWU_ASTM(a);
    uint32x2_t  v0 = vreinterpret_u32_f32(vdup_n_f32(s0));

    float       s1 = VWWU_ASTM(b);
    uint32x2_t  v1 = vreinterpret_u32_f32(vdup_n_f32(s1));

    int32x2_t   v2 = vdup_n_s32(c);
    int32x2_t   v3 = vdup_n_s32(-32);
    v3 = vadd_s32(v2, v3);
    v2 = vneg_s32(v2);
    v0 = vshl_u32(v0, v2);
    v1 = vshl_u32(v1, v3);
    v0 = vorr_u32(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u32(v0), 0);
    return  WWU_ASTV(s0);
}



INLINE(Vdyu,VDYU_SIRL) (Vdyu a, Vdyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vdbu,VDBU_SIRL) (Vdbu a, Vdbu b, Rc(0, 8) c)
{
#define     VDBU_SIRL(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C >  7) ? B :                      \
    vsri_n_u8(                          \
        vshl_n_u8(                      \
            B,                          \
            (8-((7&C)+(!(7&C))))        \
        ),                              \
        A,                              \
        ((7&C)+(!(7&C)))                \
    )                                   \
)

    int8x8_t    l = vdup_n_s8(c);
    int8x8_t    r = vdup_n_s8(-8);
    r = vadd_s8(r, l);
    l = vneg_s8(l);
    a = vshl_u8(a, l);
    b = vshl_u8(b, r);
    return  vorr_u8(a, b);
}

INLINE(Vdbc,VDBC_SIRL) (Vdbc a, Vdbc b, Rc(0, 8) c)
{
#define     VDBC_SIRL(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C >  7) ? B :                      \
    vsri_n_u8(                          \
        vshl_n_u8(                      \
            VDBC_ASBU(B),               \
            (8-((7&C)+(!(7&C))))        \
        ),                              \
        VDBC_ASBU(A),                   \
        ((7&C)+(!(7&C)))                \
    )                                   \
)

    return  VDBU_ASBC((VDBU_SIRL)(VDBC_ASBU(a), VDBC_ASBU(b), c));
}


INLINE(Vdhu,VDHU_SIRL) (Vdhu a, Vdhu b, Rc(0, 16) c)
{
#define     VDHU_SIRL(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 15) ? B :                      \
    vsri_n_u16(                         \
        vshl_n_u16(                     \
            B,                          \
            (16-((15&C)+(!(15&C))))     \
        ),                              \
        A,                              \
        ((15&C)+(!(15&C)))              \
    )                                   \
)

    int16x4_t   l = vdup_n_s16(c);
    int16x4_t   r = vdup_n_s16(-16);
    r = vadd_s16(r, l);
    l = vneg_s16(l);
    a = vshl_u16(a, l);
    b = vshl_u16(b, r);
    return  vorr_u16(a, b);
}

INLINE(Vdwu,VDWU_SIRL) (Vdwu a, Vdwu b, Rc(0, 32) c)
{
#define     VDWU_SIRL(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 31) ? B :                      \
    vsri_n_u32(                         \
        vshl_n_u32(                     \
            B,                          \
            (32-((31&C)+(!(31&C))))     \
        ),                              \
        A,                              \
        ((31&C)+(!(31&C)))              \
    )                                   \
)

    int32x2_t   l = vdup_n_s32(c);
    int32x2_t   r = vdup_n_s32(-32);
    r = vadd_s32(r, l);
    l = vneg_s32(l);
    a = vshl_u32(a, l);
    b = vshl_u32(b, r);
    return  vorr_u32(a, b);
}

INLINE(Vddu,VDDU_SIRL) (Vddu a, Vddu b, Rc(0, 64) c)
{
#define     VDDU_SIRL(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 63) ? B :                      \
    vsri_n_u64(                         \
        vshl_n_u64(                     \
            B,                          \
            (64-((63&C)+(!(63&C))))     \
        ),                              \
        A,                              \
        ((63&C)+(!(63&C)))              \
    )                                   \
)

    int64x1_t   l = vdup_n_s64(c);
    int64x1_t   r = vdup_n_s64(-64);
    r = vadd_s64(r, l);
    l = vneg_s64(l);
    a = vshl_u64(a, l);
    b = vshl_u64(b, r);
    return  vorr_u64(a, b);
}


INLINE(Vqyu,VQYU_SIRL) (Vqyu a, Vqyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vqbu,VQBU_SIRL) (Vqbu a, Vqbu b, Rc(0, 8) c)
{
#define     VQBU_SIRL(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C >  7) ? B :                      \
    vsriq_n_u8(                         \
        vshlq_n_u8(                     \
            B,                          \
            (8-((7&C)+(!(7&C))))        \
        )                               \
        A,                              \
        ((7&C)+(!(7&C)))                \
    )                                   \
)

    int8x16_t    l = vdupq_n_s8(c);
    int8x16_t    r = vdupq_n_s8(-8);
    r = vaddq_s8(r, l);
    l = vnegq_s8(l);
    a = vshlq_u8(a, l);
    b = vshlq_u8(b, r);
    return  vorrq_u8(a, b);
}

INLINE(Vqbc,VQBC_SIRL) (Vqbc a, Vqbc b, Rc(0, 8) c)
{
#define     VQBC_SIRL(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C >  7) ? B :                      \
    vsriq_n_u8(                         \
        vshlq_n_u8(                     \
            VQBC_ASBU(B),               \
            (8-((7&C)+(!(7&C))))        \
        )                               \
        VQBC_ASBU(A),                   \
        ((7&C)+(!(7&C)))                \
    )                                   \
)

    return  VQBU_ASBC((VQBU_SIRL)(VQBC_ASBU(a), VQBC_ASBU(b), c));
}


INLINE(Vqhu,VQHU_SIRL) (Vqhu a, Vqhu b, Rc(0, 16) c)
{
#define     VQHU_SIRL(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 15) ? B :                      \
    vsriq_n_u16(                        \
        vshlq_n_u16(                    \
            B,                          \
            (16-((15&C)+(!(15&C))))     \
        )                               \
        A,                              \
        ((15&C)+(!(15&C)))              \
    )                                   \
)

    int16x8_t   l = vdupq_n_s16(c);
    int16x8_t   r = vdupq_n_s16(-16);
    r = vaddq_s16(r, l);
    l = vnegq_s16(l);
    a = vshlq_u16(a, l);
    b = vshlq_u16(b, r);
    return  vorrq_u16(a, b);
}

INLINE(Vqwu,VQWU_SIRL) (Vqwu a, Vqwu b, Rc(0, 32) c)
{
#define     VQWU_SIRL(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 31) ? B :                      \
    vsriq_n_u32(                        \
        vshlq_n_u32(                    \
            B,                          \
            (32-((31&C)+(!(31&C))))     \
        )                               \
        A,                              \
        ((31&C)+(!(31&C)))              \
    )                                   \
)

    int32x4_t   l = vdupq_n_s32(c);
    int32x4_t   r = vdupq_n_s32(-32);
    r = vaddq_s32(r, l);
    l = vnegq_s32(l);
    a = vshlq_u32(a, l);
    b = vshlq_u32(b, r);
    return  vorrq_u32(a, b);
}

INLINE(Vqdu,VQDU_SIRL) (Vqdu a, Vqdu b, Rc(0, 64) c)
{
#define     VQDU_SIRL(A, B, C)          \
(                                       \
    (C <  1) ? A :                      \
    (C > 63) ? B :                      \
    vsriq_n_u64(                        \
        vshlq_n_u64(                    \
            B,                          \
            (64-((63&C)+(!(63&C))))     \
        )                               \
        A,                              \
        ((63&C)+(!(63&C)))              \
    )                                   \
)

    int64x2_t   l = vdupq_n_s64(c);
    int64x2_t   r = vdupq_n_s64(-64);
    r = vaddq_s64(r, l);
    l = vnegq_s64(l);
    a = vshlq_u64(a, l);
    b = vshlq_u64(b, r);
    return  vorrq_u64(a, b);
}

#if 0 // _LEAVE_ARM_SIRL
}
#endif

#if 0 // _ENTER_ARM_SPRL
{
#endif

INLINE(Vwyu,VWYU_SPRL) (Vwyu l, Vwyu r, Rc(0, 32) n)
{
#define     WYU_SPRL(L, R, N) \
((WORD_TYPE){.U=((DWRD_TYPE){.Lo.F=L,.Hi.F=R}).U>>N}).F

#define     VWYU_SPRL(L, R, N) ((Vwyu){WYU_SPRL(L.V0,R.V0,N)})
    DWRD_TYPE c = {.Lo.F=l.V0, .Hi.F=r.V0};
    c.U >>= n;
    return  ((Vwyu){c.W0.F});
}

INLINE(float,WBR_SPRL) (float l, float r, Rc(0, 4) n)
{
    DWRD_TYPE   c = {.Lo.F=l, .Hi.F=r};
    float64x1_t m = vdup_n_f64(c.F);
    uint8x8_t   v = vreinterpret_u8_f64(m);
    uint8x8_t   t = vcreate_u8(0x0706050403020100ULL);
    t = vadd_u8(t, vdup_n_u8(n));
    v = vtbl1_u8(v, t);
    float32x2_t f = vreinterpret_f32_u8(v);
    return  vget_lane_f32(f, 0);
}

INLINE(Vwbu,VWBU_SPRL) (Vwbu l, Vwbu r, Rc(0, 4) n)
{
    return ((Vwbu){WBR_SPRL(l.V0, r.V0, n)});
}

INLINE(Vwbi,VWBI_SPRL) (Vwbi l, Vwbi r, Rc(0, 4) n)
{
    return ((Vwbi){WBR_SPRL(l.V0, r.V0, n)});
}   

INLINE(Vwbc,VWBC_SPRL) (Vwbc l, Vwbc r, Rc(0, 4) n)
{
    return ((Vwbc){WBR_SPRL(l.V0, r.V0, n)});
}


INLINE(float,WHR_SPRL) (float l, float r, Rc(0, 2) n)
{
    DWRD_TYPE   c = {.Lo.F=l, .Hi.F=l};
    float64x1_t m = vdup_n_f64(c.F);
    uint8x8_t   v = vreinterpret_u8_f64(m);
    uint8x8_t   t = vcreate_u8(0x0706050403020100ULL);
    t = vadd_u8(t, vdup_n_u8((2*n)));
    v = vtbl1_u8(v, t);
    float32x2_t f = vreinterpret_f32_u8(v);
    return  vget_lane_f32(f, 0);
}

INLINE(Vwhu,VWHU_SPRL) (Vwhu l, Vwhu r, Rc(0, 2) n)
{
    return ((Vwhu){WHR_SPRL(l.V0, r.V0, n)});
}

INLINE(Vwhi,VWHI_SPRL) (Vwhi l, Vwhi r, Rc(0, 2) n)
{
    return ((Vwhi){WHR_SPRL(l.V0, r.V0, n)});
}

INLINE(Vwhf,VWHF_SPRL) (Vwhf l, Vwhf r, Rc(0, 2) n)
{
    return ((Vwhf){WHR_SPRL(l.V0, r.V0, n)});
}


INLINE(Vdyu,VDYU_SPRL) (Vdyu l, Vdyu r, Rc(0, 64) n)
{
#define     DYU_SPRL(L, R, N)   \
(                               \
    (N > 63) ? R :              \
    (N == 0) ? L :              \
    vorr_u64(                   \
        vshr_n_u64(             \
            L,                  \
            (63&(N+(N==0))),    \
        ),                      \
        vshl_n_u64(             \
            R,                  \
            (63&(64-N))         \
        )                       \
    )                           \
)

#define     VDYU_SPRL(L, R, N)  ((Vdyu){DYU_SPRL(L.V0,R.V0,N)})
    int64x1_t   x = vdup_n_s64(n);
    int64x1_t   y = vdup_n_s64(64);
    x = vneg_s64(x);
    y = vadd_s64(x, y);
    l.V0 = vshl_u64(l.V0, x);
    r.V0 = vshl_u64(r.V0, y);
    l.V0 = vorr_u64(l.V0, r.V0);
    return  l;
}


INLINE(Vdbu,VDBU_SPRL) (Vdbu l, Vdbu r, Rc(0, 8) n)
{
#define     VDBU_SPRL(L, R, N)  \
(                               \
    (N==8)                      \
    ?   R                       \
    :   vext_u8(L,R,(7&N))      \
)

    return  vqtbl1_u8(
        vcombine_u8(l, r),
        vadd_u8(
            vcreate_u8(0x0706050403020100ULL),
            vdup_n_u8(n)
        )
    );
}

INLINE(Vdbi,VDBI_SPRL) (Vdbi l, Vdbi r, Rc(0, 8) n)
{
#define     VDBI_SPRL(L, R, N)  \
(                               \
    (N==8)                      \
    ?   R                       \
    :   vext_s8(L,R,(7&N))      \
)

    return  vqtbl1_s8(
        vcombine_s8(l, r),
        vadd_u8(
            vcreate_u8(0x0706050403020100ULL),
            vdup_n_u8(n)
        )
    );
}

INLINE(Vdbc,VDBC_SPRL) (Vdbc l, Vdbc r, Rc(0, 8) n)
{
#if CHAR_MIN
#   define  VDBC_SPRL(L, R, N)  \
(                               \
    (N==8)                      \
    ?   R                       \
    :   ((Vdbc){vext_s8(L.V0,R.V0,(7&N))})      \
)

#else
#   define  VDBC_SPRL(L, R, N)  \
(                               \
    (N==8)                      \
    ?   R                       \
    :   ((Vdbc){vext_u8(L.V0,R.V0,(7&N))})      \
)

#endif

    return VDBU_ASBC((VDBU_SPRL)(VDBC_ASBU(l), VDBC_ASBU(r), n));
}


INLINE(Vdhu,VDHU_SPRL) (Vdhu l, Vdhu r, Rc(0, 4) n)
{
#define     VDHU_SPRL(L, R, N)  \
(                               \
    (N==4)                      \
    ?   R                       \
    :   vext_u16(L,R,(3&N))     \
)

    uint16x8_t c = vcombine_u16(l, r);
    return  vreinterpret_u16_u8(
        vqtbl1_u8(
            vreinterpretq_u8_u16(c),
            vadd_u8(
                vcreate_u8(0x0706050403020100ULL),
                vdup_n_u8((2*n))
            )
        )
    );
}

INLINE(Vdhi,VDHI_SPRL) (Vdhi l, Vdhi r, Rc(0, 4) n)
{
#define     VDHI_SPRL(L, R, N)  \
(                               \
    (N==4)                      \
    ?   R                       \
    :   vext_s16(L,R,(3&N))     \
)

    int16x8_t c = vcombine_s16(l, r);
    return  vreinterpret_s16_u8(
        vqtbl1_u8(
            vreinterpretq_u8_s16(c),
            vadd_u8(
                vcreate_u8(0x0706050403020100ULL),
                vdup_n_u8((2*n))
            )
        )
    );
}

INLINE(Vdhf,VDHF_SPRL) (Vdhf l, Vdhf r, Rc(0, 4) n)
{
#define     VDHF_SPRL(L, R, N) \
VDHU_ASHF(VDHU_SPRL(VDHF_ASHU(L),VDHF_ASHU(R),N))

    return  VDHU_ASHF((VDHU_SPRL)(VDHF_ASHU(l), VDHF_ASHU(r), n));
}


INLINE(Vdwu,VDWU_SPRL) (Vdwu l, Vdwu r, Rc(0, 2) n)
{
#define     VDWU_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vext_u32(L,R,(1&N))     \
)

    uint32x4_t c = vcombine_u32(l, r);
    return  vreinterpret_u32_u8(
        vqtbl1_u8(
            vreinterpretq_u8_u32(c),
            vadd_u8(
                vcreate_u8(0x0706050403020100ULL),
                vdup_n_u8(n<<2)
            )
        )
    );
}

INLINE(Vdwi,VDWI_SPRL) (Vdwi l, Vdwi r, Rc(0, 2) n)
{
#define     VDWI_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vext_s32(L,R,(1&N))     \
)

    int32x4_t c = vcombine_s32(l, r);
    return  vreinterpret_s32_u8(
        vqtbl1_u8(
            vreinterpretq_u8_s32(c),
            vadd_u8(
                vcreate_u8(0x0706050403020100ULL),
                vdup_n_u8(n<<2)
            )
        )
    );
}

INLINE(Vdwf,VDWF_SPRL) (Vdwf l, Vdwf r, Rc(0, 2) n)
{
#define     VDWF_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vext_f32(L,R,(1&N))     \
)

    float32x4_t c = vcombine_f32(l, r);
    return  vreinterpret_f32_u8(
        vqtbl1_u8(
            vreinterpretq_u8_f32(c),
            vadd_u8(
                vcreate_u8(0x0706050403020100ULL),
                vdup_n_u8(n<<2)
            )
        )
    );
}


INLINE(Vqyu,VQYU_SPRL) (Vqyu l, Vqyu r, Rc(0, 128) n)
{
    QUAD_TYPE a;
    a.Lo.U = vgetq_lane_u64(l.V0, 0);
    a.Hi.U = vgetq_lane_u64(l.V0, 1);
    QUAD_TYPE b;
    b.Lo.U = vgetq_lane_u64(r.V0, 0);
    b.Hi.U = vgetq_lane_u64(r.V0, 1);
    QUAD_TYPE c = {.U=(a.U>>n)|(b.U<<(128-n))};
    uint64x1_t p = vdup_n_u64(c.Lo.U);
    uint64x1_t q = vdup_n_u64(c.Hi.U);
    l.V0 = vcombine_u64(p, q);
    return l;
#if 0
    uint64x1_t  a, m, z, x, y;
    uint64x2_t  p = VQYU_ASDU(l);
    uint64x2_t  q = VQYU_ASDU(r);
    int v;
    if (v < 64)
    {
        a = vget_low_u64(p);
        m = vget_high_u64(p);
        z = vget_low_u64(q);
        v = n;
    }
    else 
    {
        x = vget_high_u64(p);
        y = vget_low_u64(q);
        z = vget_high_u64(q);
        v = n-64;
    }
    int s = 64-v;
    x = (VDDU_SHRS)(a, v);
    y = (VDDU_SHLL)(m, s);
    a = vorr_u64(x, y);
    x = (VDDU_SHRS)(m, v);
    y = (VDDU_SHLL)(z, s);
    z = vorr_u64(x, y);
    p = vcombine_u64(a, z);
    return VQDU_ASYU(p);
#endif
}

INLINE(Vqbu,VQBU_SPRL) (Vqbu l, Vqbu r, Rc(0, 16) n)
{
#define     VQBU_SPRL(L, R, N)  \
(                               \
    (N==16)                     \
    ?   R                       \
    :   vextq_u8(L,R,(15&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    t = vaddq_u8(t, vdupq_n_u8(n));
    return  vqtbl2q_u8(
        ((uint8x16x2_t){l,r}),
        t
    );
}

INLINE(Vqbi,VQBI_SPRL) (Vqbi l, Vqbi r, Rc(0, 16) n)
{
#define     VQBI_SPRL(L, R, N)  \
(                               \
    (N==16)                     \
    ?   R                       \
    :   vextq_s8(L,R,(15&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    t = vaddq_u8(t, vdupq_n_u8(n));
    return  vqtbl2q_s8(
        ((int8x16x2_t){l, r}),
        t
    );
}

INLINE(Vqbc,VQBC_SPRL) (Vqbc l, Vqbc r, Rc(0, 16) n)
{
#define     VQBC_SPRL(L, R, N)  \
(                               \
    (N==16)                     \
    ?   R                       \
    :   VQBU_ASBC(              \
            vextq_u8(           \
                VQBC_ASBU(L),   \
                VQBC_ASBU(R),   \
                (15&N)          \
            )                   \
        )                       \
)

    return VQBU_ASBC(((VQBU_SPRL)(VQBC_ASBU(l), VQBC_ASBU(r), n)));

}


INLINE(Vqhu,VQHU_SPRL) (Vqhu l, Vqhu r, Rc(0, 8) n)
{
#define     VQHU_SPRL(L, R, N)  \
(                               \
    (N==8)                      \
    ?   R                       \
    :   vextq_u16(L,R,(7&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_u16(l);
    uint8x16_t b = vreinterpretq_u8_u16(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<1));
    return  vreinterpretq_u16_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}

INLINE(Vqhi,VQHI_SPRL) (Vqhi l, Vqhi r, Rc(0, 8) n)
{
#define     VQHI_SPRL(L, R, N)  \
(                               \
    (N==8)                      \
    ?   R                       \
    :   vextq_s16(L,R,(7&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_s16(l);
    uint8x16_t b = vreinterpretq_u8_s16(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<1));
    return  vreinterpretq_s16_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
}
   
INLINE(Vqhf,VQHF_SPRL) (Vqhf l, Vqhf r, Rc(0, 8) n)
{
#define     VQHF_SPRL(L, R, N)              \
(                                           \
    (N==8)                                  \
    ?   R                                   \
    :   vreinterpretq_f16_u16(              \
            vextq_u16(                      \
                vreinterpretq_u16_f16(L),   \
                vreinterpretq_u16_f16(R),   \
                (7&N)                       \
            )                               \
        )                                   \
)
    return VQHU_ASHF(((VQHF_SPRL)(VQHF_ASHU(l), VQHF_ASHU(r), n)));
}


INLINE(Vqwu,VQWU_SPRL) (Vqwu l, Vqwu r, Rc(0, 4) n)
{
#define     VQWU_SPRL(L, R, N)  \
(                               \
    (N==4)                      \
    ?   R                       \
    :   vextq_u32(L,R,(3&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_u32(l);
    uint8x16_t b = vreinterpretq_u8_u32(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<2));
    return  vreinterpretq_u32_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}

INLINE(Vqwi,VQWI_SPRL) (Vqwi l, Vqwi r, Rc(0, 4) n)
{
#define     VQWI_SPRL(L, R, N)  \
(                               \
    (N==4)                      \
    ?   R                       \
    :   vextq_s32(L,R,(3&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_s32(l);
    uint8x16_t b = vreinterpretq_u8_s32(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<2));
    return  vreinterpretq_s32_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}

INLINE(Vqwf,VQWF_SPRL) (Vqwf l, Vqwf r, Rc(0, 4) n)
{
#define     VQWF_SPRL(L, R, N)  \
(                               \
    (N==4)                      \
    ?   R                       \
    :   vextq_f32(L,R,(3&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_f32(l);
    uint8x16_t b = vreinterpretq_u8_f32(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<2));
    return  vreinterpretq_f32_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}


INLINE(Vqdu,VQDU_SPRL) (Vqdu l, Vqdu r, Rc(0, 2) n)
{
#define     VQDU_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vextq_u64(L,R,(1&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_u64(l);
    uint8x16_t b = vreinterpretq_u8_u64(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<3));
    return  vreinterpretq_u64_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}

INLINE(Vqdi,VQDI_SPRL) (Vqdi l, Vqdi r, Rc(0, 2) n)
{
#define     VQDI_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vextq_s64(L,R,(1&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_s64(l);
    uint8x16_t b = vreinterpretq_u8_s64(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<3));
    return  vreinterpretq_s64_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}


INLINE(Vqdf,VQDF_SPRL) (Vqdf l, Vqdf r, Rc(0, 2) n)
{
#define     VQDF_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vextq_f64(L,R,(1&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_f64(l);
    uint8x16_t b = vreinterpretq_u8_f64(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<3));
    return  vreinterpretq_f64_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}


#if 0 // _LEAVE_ARM_SPRL
}
#endif

#if 0 // _ENTER_ARM_SVLL
{
#endif
/*
0,0=0, 0,1=0, 1,0=1, 1,1=0
*/
#define     DYU_SVLL        vbic_u64
#define     DBU_SVLL(A,B)   vshl_u8(A,vreinterpret_s8_u8(B))
#define     DBI_SVLL(A,B)   vshl_s8(A,vreinterpret_s8_u8(B))
#if CHAR_MIN
#   define  DBC_SVLL    DBI_SVLL
#else
#   define  DBC_SVLL    DBU_SVLL
#endif

#define     DHU_SVLL(A, B)  vshl_u16(A,vreinterpret_s16_u16(B))
#define     DHI_SVLL(A, B)  vshl_s16(A,vreinterpret_s16_u16(B))
#define     DWU_SVLL(A, B)  vshl_u32(A,vreinterpret_s32_u32(B))
#define     DWI_SVLL(A, B)  vshl_s32(A,vreinterpret_s32_u32(B))
#define     DDU_SVLL(A, B)  vshl_u64(A,vreinterpret_s64_u64(B))
#define     DDI_SVLL(A, B)  vshl_s64(A,vreinterpret_s64_u64(B))

#define     QYU_SVLL        vbicq_u64
#define     QBU_SVLL(A, B)  vshlq_u8(A,vreinterpretq_s8_u8(B))
#define     QBI_SVLL(A, B)  vshlq_s8(A,vreinterpretq_s8_u8(B))
#if CHAR_MIN
#   define  QBC_SVLL    QBI_SVLL
#else
#   define  QBC_SVLL    QBU_SVLL
#endif

#define     QHU_SVLL(A, B)  vshlq_u16(A,vreinterpretq_s16_u16(B))
#define     QHI_SVLL(A, B)  vshlq_s16(A,vreinterpretq_s16_u16(B))
#define     QWU_SVLL(A, B)  vshlq_u32(A,vreinterpretq_s32_u32(B))
#define     QWI_SVLL(A, B)  vshlq_s32(A,vreinterpretq_s32_u32(B))
#define     QDU_SVLL(A, B)  vshlq_u64(A,vreinterpretq_s64_u64(B))
#define     QDI_SVLL(A, B)  vshlq_s64(A,vreinterpretq_s64_u64(B))

INLINE(float,WYU_SVLL) (float a, float b)
{
    float32x2_t p = vdup_n_f32(a);
    float32x2_t q = vdup_n_f32(b);
    uint32x2_t  l = vreinterpret_u32_f32(p);
    uint32x2_t  r = vreinterpret_u32_f32(q);
    l = vbic_u32(l, r);
    p = vreinterpret_f32_u32(l);
    return  vget_lane_f32(p, 0);
}


INLINE(Vwyu,VWYU_SVLL) (Vwyu a, Vwyu b)
{
#define     VWYU_SVLL(A, B) ((Vwyu){WYU_SVLL(a.V0,b.V0)})
    return  VWYU_SVLL(a, b);
}

INLINE(Vwbu,VWBU_SVLL) (Vwbu a, Vwbu b)
{
    DWRD_VTYPE x = {.W.F={a.V0}};
    DWRD_VTYPE y = {.W.F={b.V0}};
    x.B.U = vshl_u8(x.B.U, y.B.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwbi,VWBI_SVLL) (Vwbi a, Vwbu b)
{
    DWRD_VTYPE x = {.W.F={a.V0}};
    DWRD_VTYPE y = {.W.F={b.V0}};
    x.B.U = vshl_u8(x.B.I, y.B.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwbc,VWBC_SVLL) (Vwbc a, Vwbu b)
{
    DWRD_VTYPE x = {.W.F={a.V0}};
    DWRD_VTYPE y = {.W.F={b.V0}};
#if CHAR_MIN
    x.B.I = vshl_s8(x.B.I, y.B.I);
#else
    x.B.U = vshl_u8(x.B.U, y.B.I);
#endif
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwhu,VWHU_SVLL) (Vwhu a, Vwhu b)
{
    DWRD_VTYPE x = {.W.F={a.V0}};
    DWRD_VTYPE y = {.W.F={b.V0}};
    x.H.U = vshl_u16(x.H.U, y.H.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwhi,VWHI_SVLL) (Vwhi a, Vwhu b)
{
    DWRD_VTYPE x = {.W.F={a.V0}};
    DWRD_VTYPE y = {.W.F={b.V0}};
    x.H.I = vshl_s16(x.H.I, y.H.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwwu,VWWU_SVLL) (Vwwu a, Vwwu b)
{
    DWRD_VTYPE x = {.W.F={a.V0}};
    DWRD_VTYPE y = {.W.F={b.V0}};
    x.W.U = vshl_u32(x.W.U, y.W.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwwi,VWWI_SVLL) (Vwwi a, Vwwu b)
{
    DWRD_VTYPE x = {.W.F={a.V0}};
    DWRD_VTYPE y = {.W.F={b.V0}};
    x.W.I = vshl_s32(x.W.U, y.W.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}


INLINE(Vdyu,VDYU_SVLL) (Vdyu a, Vdyu b) {return ((Vdyu){DYU_SVLL(a.V0,b.V0)});}
INLINE(Vdbu,VDBU_SVLL) (Vdbu a, Vdbu b) {return DBU_SVLL(a, b);}
INLINE(Vdbi,VDBI_SVLL) (Vdbi a, Vdbu b) {return DBI_SVLL(a, b);}
INLINE(Vdbc,VDBC_SVLL) (Vdbc a, Vdbu b) {return ((Vdbc){DBC_SVLL(a.V0,b)});}
INLINE(Vdhu,VDHU_SVLL) (Vdhu a, Vdhu b) {return DHU_SVLL(a, b);}
INLINE(Vdhi,VDHI_SVLL) (Vdhi a, Vdhu b) {return DHI_SVLL(a, b);}
INLINE(Vdwu,VDWU_SVLL) (Vdwu a, Vdwu b) {return DWU_SVLL(a, b);}
INLINE(Vdwi,VDWI_SVLL) (Vdwi a, Vdwu b) {return DWI_SVLL(a, b);}
INLINE(Vddu,VDDU_SVLL) (Vddu a, Vddu b) {return DDU_SVLL(a, b);}
INLINE(Vddi,VDDI_SVLL) (Vddi a, Vddu b) {return DDI_SVLL(a, b);}

INLINE(Vqyu,VQYU_SVLL) (Vqyu a, Vqyu b) {return ((Vqyu){QYU_SVLL(a.V0,b.V0)});}
INLINE(Vqbu,VQBU_SVLL) (Vqbu a, Vqbu b) {return QBU_SVLL(a, b);}
INLINE(Vqbi,VQBI_SVLL) (Vqbi a, Vqbu b) {return QBI_SVLL(a, b);}
INLINE(Vqbc,VQBC_SVLL) (Vqbc a, Vqbu b) {return ((Vqbc){QBC_SVLL(a.V0,b)});}
INLINE(Vqhu,VQHU_SVLL) (Vqhu a, Vqhu b) {return QHU_SVLL(a, b);}
INLINE(Vqhi,VQHI_SVLL) (Vqhi a, Vqhu b) {return QHI_SVLL(a, b);}
INLINE(Vqwu,VQWU_SVLL) (Vqwu a, Vqwu b) {return QWU_SVLL(a, b);}
INLINE(Vqwi,VQWI_SVLL) (Vqwi a, Vqwu b) {return QWI_SVLL(a, b);}
INLINE(Vqdu,VQDU_SVLL) (Vqdu a, Vqdu b) {return QDU_SVLL(a, b);}
INLINE(Vqdi,VQDI_SVLL) (Vqdi a, Vqdu b) {return QDI_SVLL(a, b);}

#if 0 // _LEAVE_ARM_SVLL
}
#endif

#if 0 // _ENTER_ARM_SVLS
{
#endif

#define     DBU_SVLS(A,B)   vqshl_u8(A, vreinterpret_s8_u8(B))
#define     DBI_SVLS(A,B)   vqshl_s8(A, vreinterpret_s8_u8(B))
#if CHAR_MIN
#   define  DBC_SVLS    DBI_SVLS
#else
#   define  DBC_SVLS    DBU_SVLS
#endif

#define     DHU_SVLS(A, B)  vqshl_u16(A, vreinterpret_s16_u16(B))
#define     DHI_SVLS(A, B)  vqshl_s16(A, vreinterpret_s16_u16(B))
#define     DWU_SVLS(A, B)  vqshl_u32(A, vreinterpret_s32_u32(B))
#define     DWI_SVLS(A, B)  vqshl_s32(A, vreinterpret_s32_u32(B))
#define     DDU_SVLS(A, B)  vqshl_u64(A, vreinterpret_s64_u64(B))
#define     DDI_SVLS(A, B)  vqshl_s64(A, vreinterpret_s64_u64(B))

#define     QBU_SVLS(A, B)  vqshlq_u8(A, vreinterpretq_s8_u8(B))
#define     QBI_SVLS(A, B)  vqshlq_s8(A, vreinterpretq_s8_u8(B))
#if CHAR_MIN
#   define  QBC_SVLS    QBI_SVLS
#else
#   define  QBC_SVLS    QBU_SVLS
#endif

#define     QHU_SVLS(A, B)  vqshlq_u16(A, vreinterpretq_s16_u16(B))
#define     QHI_SVLS(A, B)  vqshlq_s16(A, vreinterpretq_s16_u16(B))
#define     QWU_SVLS(A, B)  vqshlq_u32(A, vreinterpretq_s32_u32(B))
#define     QWI_SVLS(A, B)  vqshlq_s32(A, vreinterpretq_s32_u32(B))
#define     QDU_SVLS(A, B)  vqshlq_u64(A, vreinterpretq_s64_u64(B))
#define     QDI_SVLS(A, B)  vqshlq_s64(A, vreinterpretq_s64_u64(B))


INLINE(Vwyu,VWYU_SVLS) (Vwyu a, Vwyu b) {return a;}

INLINE(Vwbu,VWBU_SVLS) (Vwbu a, Vwbu b)
{
    float32x2_t l = vdup_n_f32(VWBU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBU_ASTM(b));
    uint8x8_t   v = vqshl_u8(
        vreinterpret_u8_f32(l),
        vreinterpret_s8_f32(r)
    );
    l = vreinterpret_f32_u8(v);
    return  WBU_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwbi,VWBI_SVLS) (Vwbi a, Vwbu b)
{
    float32x2_t l = vdup_n_f32(VWBI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBU_ASTM(b));
    int8x8_t    v = vqshl_s8(
        vreinterpret_s8_f32(l),
        vreinterpret_s8_f32(r)
    );
    l = vreinterpret_f32_s8(v);
    return  WBI_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwbc,VWBC_SVLS) (Vwbc a, Vwbu b)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_SVLS(VWBC_ASBI(a), b));
#else
    return  VWBU_ASBC(VWBU_SVLS(VWBC_ASBU(a), b));
#endif

}

INLINE(Vwhu,VWHU_SVLS) (Vwhu a, Vwhu b)
{
    float32x2_t l = vdup_n_f32(VWHU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHU_ASTM(b));
    uint16x4_t   v = vqshl_u16(
        vreinterpret_u16_f32(l),
        vreinterpret_s16_f32(r)
    );
    l = vreinterpret_f32_u16(v);
    return  WHU_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwhi,VWHI_SVLS) (Vwhi a, Vwhu b)
{
    float32x2_t l = vdup_n_f32(VWHI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHU_ASTM(b));
    int16x4_t   v = vqshl_s16(
        vreinterpret_s16_f32(l),
        vreinterpret_s16_f32(r)
    );
    l = vreinterpret_f32_s16(v);
    return  WHI_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwwu,VWWU_SVLS) (Vwwu a, Vwwu b)
{
    float32x2_t l = vdup_n_f32(VWWU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWWU_ASTM(b));
    uint32x2_t  v = vqshl_u32(
        vreinterpret_u32_f32(l),
        vreinterpret_s32_f32(r)
    );
    l = vreinterpret_f32_u32(v);
    return  WWU_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwwi,VWWI_SVLS) (Vwwi a, Vwwu b)
{
    float32x2_t l = vdup_n_f32(VWWI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWWU_ASTM(b));
    int32x2_t   v = vqshl_s32(
        vreinterpret_s32_f32(l),
        vreinterpret_s32_f32(r)
    );
    l = vreinterpret_f32_s32(v);
    return  WWI_ASTV(vget_lane_f32(l,0));
}


INLINE(Vdyu,VDYU_SVLS) (Vdyu a, Vdyu b) {return a;}
INLINE(Vdbu,VDBU_SVLS) (Vdbu a, Vdbu b) {return DBU_SVLS(a, b);}
INLINE(Vdbi,VDBI_SVLS) (Vdbi a, Vdbu b) {return DBI_SVLS(a, b);}
INLINE(Vdbc,VDBC_SVLS) (Vdbc a, Vdbu b)
{
    return  DBC_ASTV(DBC_SVLS(VDBC_ASTM(a), b));
}

INLINE(Vdhu,VDHU_SVLS) (Vdhu a, Vdhu b) {return DHU_SVLS(a, b);}
INLINE(Vdhi,VDHI_SVLS) (Vdhi a, Vdhu b) {return DHI_SVLS(a, b);}
INLINE(Vdwu,VDWU_SVLS) (Vdwu a, Vdwu b) {return DWU_SVLS(a, b);}
INLINE(Vdwi,VDWI_SVLS) (Vdwi a, Vdwu b) {return DWI_SVLS(a, b);}
INLINE(Vddu,VDDU_SVLS) (Vddu a, Vddu b) {return DDU_SVLS(a, b);}
INLINE(Vddi,VDDI_SVLS) (Vddi a, Vddu b) {return DDI_SVLS(a, b);}


INLINE(Vqyu,VQYU_SVLS) (Vqyu a, Vqyu b) {return a;}
INLINE(Vqbu,VQBU_SVLS) (Vqbu a, Vqbu b) {return QBU_SVLS(a, b);}
INLINE(Vqbi,VQBI_SVLS) (Vqbi a, Vqbu b) {return QBI_SVLS(a, b);}
INLINE(Vqbc,VQBC_SVLS) (Vqbc a, Vqbu b)
{
    return  QBC_ASTV(QBC_SVLS(VQBC_ASTM(a), b));
}

INLINE(Vqhu,VQHU_SVLS) (Vqhu a, Vqhu b) {return QHU_SVLS(a, b);}
INLINE(Vqhi,VQHI_SVLS) (Vqhi a, Vqhu b) {return QHI_SVLS(a, b);}
INLINE(Vqwu,VQWU_SVLS) (Vqwu a, Vqwu b) {return QWU_SVLS(a, b);}
INLINE(Vqwi,VQWI_SVLS) (Vqwi a, Vqwu b) {return QWI_SVLS(a, b);}
INLINE(Vqdu,VQDU_SVLS) (Vqdu a, Vqdu b) {return QDU_SVLS(a, b);}
INLINE(Vqdi,VQDI_SVLS) (Vqdi a, Vqdu b) {return QDI_SVLS(a, b);}
INLINE(Vqqu,VQQU_SVLS) (Vqqu a, Vqqu b)
{
    QUAD_VTYPE  c = {.Q.U=a};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE  r = {.D.U=vget_high_u64(c.D.U)};
    DWRD_VTYPE  n = {.D.U=vget_low_u64(b.V0)};
    if (n.D0.I < 64)
    {
        // c.Lo.U = (l<<b);
        c.D.U = vcopyq_lane_u64(
            c.D.U, 0,
            vshl_u64(l.D.U, n.D.I), 0
        );
        // c.Hi.U = (r<<b)|(l>>(64-b));
        r.D.U = vshl_u64(r.D.U, n.D.I);
        n.D.I = vadd_s64(n.D.I, vdup_n_s64(-64));
        l.D.U = vshl_u64(l.D.U, n.D.I);
        c.D.U = vcopyq_lane_u64(
            c.D.U, 1,
            l.D.U, 0
        );
        
    }
    else
    {
        // c.Lo.U = 0;
        //c.Hi.U = (l<<(b-64));
        n.D.I = vadd_s64(n.D.I, vdup_n_s64(-64));
        l.D.U = vshl_u64(l.D.U, n.D.I);
        c.D.U = vcombine_u64(vdup_n_u64(0), l.D.U);
    }
    return  c.Q.U;
}

#if 0 // _LEAVE_ARM_SVLS
}
#endif

#if 0 // _ENTER_ARM_SVL2
{
#endif

#define     WBU_SVL2(A, B) vshl_u16(WBU_CVHU(A),WBU_CVHI(B))

INLINE(int16x4_t,WBI_SVL2) (float a, float b)
{
    return vget_low_s16(
        vshlq_s16(
            vmovl_s8(vreinterpret_s8_f32(vdup_n_f32(a))),
            vmovl_s8(vreinterpret_s8_f32(vdup_n_f32(b)))
        )
    );
}

#if CHAR_MIN
#   define  WBC_SVL2 WBI_SVL2
#else
#   define  WBC_SVL2 WBU_SVL2
#endif

#define     WHU_SVL2(A, B) vshl_u32(WHU_CVWU(A),WHU_CVWI(B))

INLINE(int32x2_t,WHI_SVL2) (float a, float b)
{
#define     WHI_SVL2(A, B) \
((WHI_SVL2)(_Generic(A,float:A),_Generic(B,float:B)))

    return vget_low_s32(
        vshlq_s32(
            vmovl_s16(vreinterpret_s16_f32(vdup_n_f32(a))),
            vmovl_s16(vreinterpret_s16_f32(vdup_n_f32(b)))
        )
    );
}

#define     WWU_SVL2(A, B) vshl_u64(WWU_CVDU(A),WWU_CVDU(B))

INLINE(int64x1_t,WWI_SVL2) (float a, float b)
{
#define     WWI_SVL2(A, B) \
((WWI_SVL2)(_Generic(A,float:A),_Generic(B,float:B)))

    return vget_low_s64(
        vshlq_s64(
            vmovl_s32(vreinterpret_s32_f32(vdup_n_f32(a))),
            vmovl_s32(vreinterpret_s32_f32(vdup_n_f32(b)))
        )
    );
}

INLINE(Vdhu,VWBU_SVL2) (Vwbu a, Vwbu b)
{
    return  WBU_SVL2(VWBU_ASTM(a), VWBU_ASTM(b));
}

INLINE(Vdhi,VWBI_SVL2) (Vwbi a, Vwbu b)
{
    return  WBI_SVL2(VWBI_ASTM(a), VWBU_ASTM(b));
}

#if CHAR_MIN

INLINE(Vdhi,VWBC_SVL2) (Vwbc a, Vwbu b)
{
    return  WBC_SVL2(VWBC_ASTM(a), VWBU_ASTM(b));
}
#else

INLINE(Vdhu,VWBC_SVL2) (Vwbc a, Vwbu b)
{
    return  WBC_SVL2(VWBC_ASTM(a), VWBU_ASTM(b));
}

#endif


INLINE(Vdwu,VWHU_SVL2) (Vwhu a, Vwhu b)
{
    return  WHU_SVL2(VWHU_ASTM(a), VWHU_ASTM(b));
}

INLINE(Vdwi,VWHI_SVL2) (Vwhi a, Vwhu b)
{
    return  WHI_SVL2(VWHI_ASTM(a), VWHU_ASTM(b));
}


INLINE(Vddu,VWWU_SVL2) (Vwwu a, Vwwu b)
{
    return  WWU_SVL2(VWWU_ASTM(a), VWWU_ASTM(b));
}

INLINE(Vddi,VWWI_SVL2) (Vwwi a, Vwwu b)
{
    return  WWI_SVL2(VWWI_ASTM(a), VWWU_ASTM(b));
}


INLINE(Vqhu,VDBU_SVL2) (Vdbu a, Vdbu b)
{
    return vshlq_u16(VDBU_CVHU(a),VDBU_CVHI(b));
}

INLINE(Vqhi,VDBI_SVL2) (Vdbi a, Vdbu b)
{
    return vshlq_s16(VDBI_CVHI(a),VDBU_CVHI(b));
}
    
#if CHAR_MIN

INLINE(Vqhi,VDBC_SVL2) (Vdbc a, Vdbu b)
{
    return  vshlq_s16(VDBC_CVHI(a), VDBU_CVHU(b));
}
#else

INLINE(Vqhu,VDBC_SVL2) (Vdbc a, Vdbu b)
{
    return  vshlq_u16(VDBC_CVHU(a), VDBU_CVHU(b));
}

#endif


INLINE(Vqwu,VDHU_SVL2) (Vdhu a, Vdhu b)
{
    return vshlq_u32(VDHU_CVWU(a),VDHU_CVWI(b));
}

INLINE(Vqwi,VDHI_SVL2) (Vdhi a, Vdhu b)
{
    return vshlq_s32(VDHI_CVWI(a),VDHU_CVWI(b));
}


INLINE(Vqdu,VDWU_SVL2) (Vdwu a, Vdwu b)
{
    return vshlq_u64(VDWU_CVDU(a),VDWU_CVDI(b));
}

INLINE(Vqdi,VDWI_SVL2) (Vdwi a, Vdwu b)
{
    return vshlq_s32(VDWI_CVDI(a),VDWU_CVDI(b));
}

#if 0 // _LEAVE_ARM_SVL2
}
#endif

#if 0 // _ENTER_ARM_SVRS
{
#endif

INLINE(Vwbu,VWBU_SVRS) (Vwbu a, Vwbu b)
{
    float32x2_t lm = vdup_n_f32(VWBU_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWBU_ASTM(b));
    uint8x8_t   lv = vreinterpret_u8_f32(lm);
    int8x8_t    rv = vreinterpret_s8_f32(rm);
    lv = vshl_u8(lv, vneg_s8(rv));
    lm = vreinterpret_f32_u8(lv);
    return  WBU_ASTV(vget_lane_f32(lm, 0));
}

INLINE(Vwbi,VWBI_SVRS) (Vwbi a, Vwbu b)
{
    float32x2_t lm = vdup_n_f32(VWBI_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWBU_ASTM(b));
    int8x8_t    lv = vreinterpret_s8_f32(lm);
    int8x8_t    rv = vreinterpret_s8_f32(rm);
    lv = vshl_u8(lv, vneg_s8(rv));
    lm = vreinterpret_f32_s8(lv);
    return  WBI_ASTV(vget_lane_f32(lm, 0));
}

INLINE(Vwbc,VWBC_SVRS) (Vwbc a, Vwbu b)
{
#if CHAR_MIN
    return VWBI_ASBC(VWBI_SVRS(VWBC_ASBI(a), b));
#else
    return VWBU_ASBC(VWBU_SVRS(VWBC_ASBU(a), b));
#endif
}

INLINE(Vwhu,VWHU_SVRS) (Vwhu a, Vwhu b)
{
    float32x2_t lm = vdup_n_f32(VWHU_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWHU_ASTM(b));
    uint16x4_t  lv = vreinterpret_u16_f32(lm);
    int16x4_t   rv = vreinterpret_s16_f32(rm);
    lv = vshl_u16(lv, vneg_s16(rv));
    lm = vreinterpret_f32_u16(lv);
    return  WHU_ASTV(vget_lane_f32(lm, 0));
}

INLINE(Vwhi,VWHI_SVRS) (Vwhi a, Vwhu b)
{
    float32x2_t lm = vdup_n_f32(VWHI_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWHU_ASTM(b));
    int16x4_t   lv = vreinterpret_s16_f32(lm);
    int16x4_t   rv = vreinterpret_s16_f32(rm);
    lv = vshl_s16(lv, vneg_s16(rv));
    lm = vreinterpret_f32_s16(lv);
    return  WHI_ASTV(vget_lane_f32(lm, 0));
}


INLINE(Vwwu,VWWU_SVRS) (Vwwu a, Vwwu b)
{
    float32x2_t lm = vdup_n_f32(VWWU_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWWU_ASTM(b));
    uint32x2_t  lv = vreinterpret_u32_f32(lm);
    int32x2_t   rv = vreinterpret_s32_f32(rm);
    lv = vshl_u32(lv, vneg_s32(rv));
    lm = vreinterpret_f32_u32(lv);
    return  WWU_ASTV(vget_lane_f32(lm, 0));
}

INLINE(Vwwi,VWWI_SVRS) (Vwwi a, Vwwu b)
{
    float32x2_t lm = vdup_n_f32(VWWI_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWWU_ASTM(b));
    int32x2_t   lv = vreinterpret_s32_f32(lm);
    int32x2_t   rv = vreinterpret_s32_f32(rm);
    lv = vshl_s32(lv, vneg_s32(rv));
    lm = vreinterpret_f32_s32(lv);
    return  WWI_ASTV(vget_lane_f32(lm, 0));
}


INLINE(Vdbu,VDBU_SVRS) (Vdbu a, Vdbu b)
{
    return  vshl_u8(a, vneg_s8(vreinterpret_s8_u8(b)));
}

INLINE(Vdbi,VDBI_SVRS) (Vdbi a, Vdbu b)
{
    return  vshl_s8(a, vneg_s8(vreinterpret_s8_u8(b)));
}

INLINE(Vdbc,VDBC_SVRS) (Vdbc a, Vdbu b)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_SVRS(VDBC_ASBI(a), b));
#else
    return  VDBU_ASBC(VDBU_SVRS(VDBC_ASBU(a), b));
#endif
}


INLINE(Vdhu,VDHU_SVRS) (Vdhu a, Vdhu b)
{
    return  vshl_u16(a, vneg_s16(vreinterpret_s16_u16(b)));
}

INLINE(Vdhi,VDHI_SVRS) (Vdhi a, Vdhu b)
{
    return  vshl_s16(a, vneg_s16(vreinterpret_s16_u16(b)));
}


INLINE(Vdwu,VDWU_SVRS) (Vdwu a, Vdwu b)
{
    return  vshl_u32(a, vneg_s32(vreinterpret_s32_u32(b)));
}

INLINE(Vdwi,VDWI_SVRS) (Vdwi a, Vdwu b)
{
    return  vshl_s32(a, vneg_s32(vreinterpret_s32_u32(b)));
}


INLINE(Vddu,VDDU_SVRS) (Vddu a, Vddu b)
{
    return  vshl_u64(a, vneg_s64(vreinterpret_s64_u64(b)));
}

INLINE(Vddi,VDDI_SVRS) (Vddi a, Vddu b)
{
    return  vshl_s64(a, vneg_s64(vreinterpret_s64_u64(b)));
}


INLINE(Vqbu,VQBU_SVRS) (Vqbu a, Vqbu b)
{
    return  vshlq_u8(a, vnegq_s8(vreinterpretq_s8_u8(b)));
}

INLINE(Vqbi,VQBI_SVRS) (Vqbi a, Vqbu b)
{
    return  vshlq_s8(a, vnegq_s8(vreinterpretq_s8_u8(b)));
}

INLINE(Vqbc,VQBC_SVRS) (Vqbc a, Vqbu b)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_SVRS(VQBC_ASBI(a), b));
#else
    return  VQBU_ASBC(VQBU_SVRS(VQBC_ASBU(a), b));
#endif
}


INLINE(Vqhu,VQHU_SVRS) (Vqhu a, Vqhu b)
{
    return  vshlq_u16(a, vnegq_s16(vreinterpretq_s16_u16(b)));
}

INLINE(Vqhi,VQHI_SVRS) (Vqhi a, Vqhu b)
{
    return  vshlq_s16(a, vnegq_s16(vreinterpretq_s16_u16(b)));
}


INLINE(Vqwu,VQWU_SVRS) (Vqwu a, Vqwu b)
{
    return  vshlq_u32(a, vnegq_s32(vreinterpretq_s32_u32(b)));
}

INLINE(Vqwi,VQWI_SVRS) (Vqwi a, Vqwu b)
{
    return  vshlq_s32(a, vnegq_s32(vreinterpretq_s32_u32(b)));
}


INLINE(Vqdu,VQDU_SVRS) (Vqdu a, Vqdu b)
{
    return  vshlq_u64(a, vnegq_s64(vreinterpretq_s64_u64(b)));
}

INLINE(Vqdi,VQDI_SVRS) (Vqdi a, Vqdu b)
{
    return  vshlq_s64(a, vnegq_s64(vreinterpretq_s64_u64(b)));
}

#if 0 // _LEAVE_ARM_SVRS
}
#endif

#if 0 // _ENTER_ARM_ROTL
{
#endif

#define     SPC_EMULATED_ROTL(_, A, B)  \
(                                       \
    (_(TYPE))                           \
    (                                   \
        (((unsigned) A)<<B)             \
    |   (((unsigned) A)>>(_(WIDTH)-B))  \
    )                                   \
)


INLINE( uchar, UCHAR_ROTL)  (unsigned a, Rc(1, UCHAR_WIDTH-1) b)
{
#define     UCHAR_ROTL(A, B) \
(\
    (uchar)\
    (\
        (((unsigned) A)<<B)\
    |   (((unsigned) A)>>(UCHAR_WIDTH-B))\
    )\
)

    return  UCHAR_ROTL(a, b);
}

INLINE(  char,  CHAR_ROTL)   (int a, Rc(1,   CHAR_WIDTH-1) b)
{
#define     CHAR_ROTL(A, B) \
(\
    (char)\
    (\
        (((unsigned) A)<<B)\
    |   (((unsigned) A)>>(CHAR_WIDTH-B))\
    )\
)
    return  CHAR_ROTL(a, b);
}

INLINE(ushort, USHRT_ROTL) (unsigned a, Rc(1,  USHRT_WIDTH-1) b)
{
#define     USHRT_ROTL(A, B) \
(\
    (ushort)\
    (\
        (((unsigned) A)<<B)\
    |   (((unsigned) A)>>(USHRT_WIDTH-B))\
    )\
)
    return  USHRT_ROTL(a, b);
}

INLINE(  uint,  UINT_ROTL)   (uint a, Rc(1,   UINT_WIDTH-1) b)
{
#define     UINT_ROTL(A, B) __ror(A,(UINT_WIDTH-B))
    return  UINT_ROTL(a, b);
}

INLINE( ulong, ULONG_ROTL)  (ulong a, Rc(1,  ULONG_WIDTH-1) b)
{
#define     ULONG_ROTL(A, B) __rorl(A,(ULONG_WIDTH-B))
    return  ULONG_ROTL(a, b);
}

INLINE(ullong,ULLONG_ROTL) (ullong a, Rc(1, ULLONG_WIDTH-1) b)
{
#define     ULLONG_ROTL(A, B) __rorll(A,(ULLONG_WIDTH-B))
    return  ULLONG_ROTL(a, b);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE, rotlqu) (QUAD_UTYPE a, Rc(1, 127) b)
{
    return (a<<b)|(a>>(127-b));
}

#endif

INLINE(Vwbu,VWBU_ROTL) (Vwbu a, Rc(1, 7) b)
{
#define     VWBU_ROTL(A, B)                         \
(                                                   \
    ((B > 7) || (B < 1))                            \
    ?   A                                           \
    :   WBU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vsli_n_u8(                      \
                        vshr_n_u8(                  \
                            vreinterpret_u8_f32(    \
                                vdup_n_f32(         \
                                    VWBU_ASTM(A)    \
                                )                   \
                            ),                      \
                            (8-(7&B))               \
                        ),                          \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(             \
                                VWBU_ASTM(A)        \
                            )                       \
                        ),                          \
                        (7&B)                       \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)
    float       m = VWBU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   z = vreinterpret_u8_f32(v);
    int8x8_t    n = vdup_n_s8(b);
    uint8x8_t   r = vshl_u8(z, n);
    n = vadd_s8(n, vdup_n_s8(-8));
    z = vshl_u8(z, n);
    z = vorr_u8(z, r);
    v = vreinterpret_f32_u8(z);
    m = vget_lane_f32(v, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbc,VWBC_ROTL) (Vwbc a, Rc(1, 7) b)
{
#define     VWBC_ROTL(A, B)                         \
(                                                   \
    ((B > 7) || (B < 1))                            \
    ?   A                                           \
    :   WBC_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vsli_n_u8(                      \
                        vshr_n_u8(                  \
                            vreinterpret_u8_f32(    \
                                vdup_n_f32(         \
                                    VWBC_ASTM(A)    \
                                )                   \
                            ),                      \
                            (8-(7&B))               \
                        ),                          \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(             \
                                VWBC_ASTM(A)        \
                            )                       \
                        ),                          \
                        (7&B)                       \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)
    float       m = VWBC_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   z = vreinterpret_u8_f32(v);
    int8x8_t    n = vdup_n_s8(b);
    uint8x8_t   r = vshl_u8(z, n);
    n = vadd_s8(n, vdup_n_s8(-8));
    z = vshl_u8(z, n);
    z = vorr_u8(z, r);
    v = vreinterpret_f32_u8(z);
    m = vget_lane_f32(v, 0);
    return  WBC_ASTV(m);
}

INLINE(Vwhu,VWHU_ROTL) (Vwhu a, Rc(1, 15) b)
{
#define     VWHU_ROTL(A, B)                         \
(                                                   \
    ((B > 15) || (B < 1))                           \
    ?   A                                           \
    :   WHU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u16(               \
                    vsli_n_u16(                     \
                        vshr_n_u16(                 \
                            vreinterpret_u16_f32(   \
                                vdup_n_f32(         \
                                    VWHU_ASTM(A)    \
                                )                   \
                            ),                      \
                            (16-(15&B))             \
                        ),                          \
                        vreinterpret_u16_f32(       \
                            vdup_n_f32(             \
                                VWHU_ASTM(A)        \
                            )                       \
                        ),                          \
                        (15&B)                      \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float       m = VWHU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint16x4_t  z = vreinterpret_u16_f32(v);
    int16x4_t   n = vdup_n_s16(b);
    uint16x4_t  r = vshl_u16(z, n);
    n = vadd_s16(n, vdup_n_s16(-16));
    z = vshl_u16(z, n);
    z = vorr_u16(z, r);
    v = vreinterpret_f32_u16(z);
    m = vget_lane_f32(v, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwwu,VWWU_ROTL) (Vwwu a, Rc(1, 31) b)
{
#define     VWWU_ROTL(A, B)     \
UINT32_ASTV(                    \
    (                           \
        (VWWU_ASTV(A)<<(   B))  \
    |   (VWWU_ASTV(A)>>(32-B))  \
    )                           \
)
    uint32_t m = VWWU_ASTV(a);
    m = (m<<b)|(m>>(32-b));
    return  UINT32_ASTV(m);
}


INLINE(Vdbu,VDBU_ROTL) (Vdbu a, Rc(1, 7) b)
{
#define     VDBU_ROTL(A, B)             \
(                                       \
    ((B > 7) || (B < 1))                \
    ?   A                               \
    :   vsli_n_u8(                      \
            vshr_n_u8(A,(8-(B&7))),     \
            A,                          \
            (B&7)                       \
        )                               \
)
    int8x8_t    n = vdup_n_s8(b);
    uint8x8_t   r = vshl_u8(a, n);
    n = vadd_s8(n, vdup_n_s8(-8));
    a = vshl_u8(a, n);
    return  vorr_u8(a, r);
}

INLINE(Vdbc,VDBC_ROTL) (Vdbc a, Rc(1, 7) b)
{
#define     VDBC_ROTL(A, B) VDBU_ASBC(VDBU_ROTL(VDBC_ASBU(A),B))
    return  VDBU_ASBC( ((VDBU_ROTL)(VDBC_ASBU(a), b)) );
}

INLINE(Vdhu,VDHU_ROTL) (Vdhu a, Rc(1, 15) b)
{
#define    VDHU_ROTL(A, B)              \
(                                       \
    ((B > 15) || (B < 1))               \
    ?   A                               \
    :   vsli_n_u16(                     \
            vshr_n_u16(A,(16-(B&15))),  \
            A,                          \
            (B&15)                      \
        )                               \
)
    int16x4_t   n = vdup_n_s16(b);
    uint16x4_t  r = vshl_u16(a, n);
    n = vadd_s16(n, vdup_n_s16(-16));
    a = vshl_u16(a, n);
    return  vorr_u16(a, r);
}

INLINE(Vdwu,VDWU_ROTL) (Vdwu a, Rc(1, 31) b)
{
#define     VDWU_ROTL(A, B)             \
(                                       \
    ((B > 31) || (B < 1))               \
    ?   A                               \
    :   vsli_n_u32(                     \
            vshr_n_u32(A,(32-(B&31))),  \
            A,                          \
            (B&31)                      \
        )                               \
)
    int32x2_t   n = vdup_n_s32(b);
    uint32x2_t  r = vshl_u32(a, n);
    n = vadd_s32(n, vdup_n_s32(-32));
    a = vshl_u32(a, n);
    return  vorr_u32(a, r);
}

INLINE(Vddu,VDDU_ROTL) (Vddu a, Rc(1, 63) b)
{
#define     VDDU_ROTL(A, B)             \
(                                       \
    ((B > 63) || (B < 1))               \
    ?   A                               \
    :   vsli_n_u64(                     \
            vshr_n_u64(A,(64-(B&63))),  \
            A,                          \
            (B&63)                      \
        )                               \
)
    int64x1_t   n = vdup_n_s64(b);
    uint64x1_t  r = vshl_u64(a, n);
    n = vadd_s64(n, vdup_n_s64(-64));
    a = vshl_u64(a, n);
    return  vorr_u64(a, r);
}



INLINE(Vqbu,VQBU_ROTL) (Vqbu a, Rc(1, 7) b)
{
#define     VQBU_ROTL(A, B)             \
(                                       \
    ((B > 7) || (B < 1))                \
    ?   A                               \
    :   vsliq_n_u8(                     \
            vshrq_n_u8(A,(8-(B&7))),    \
            A,                          \
            (B&7)                       \
        )                               \
)
    int8x16_t   n = vdupq_n_s8(b);
    uint8x16_t  r = vshlq_u8(a, n);
    n = vaddq_s8(n, vdupq_n_s8(-8));
    a = vshlq_u8(a, n);
    return  vorrq_u8(a, r);
}

INLINE(Vqbc,VQBC_ROTL) (Vqbc a, Rc(1, 7) b)
{
#define     VQBC_ROTL(A, B) VQBU_ASBC(VQBU_ROTL(VQBC_ASBU(A),B))
    return  VQBU_ASBC( ((VQBU_ROTL)(VQBC_ASBU(a), b)) );
}

INLINE(Vqhu,VQHU_ROTL) (Vqhu a, Rc(1, 15) b)
{
#define    VQHU_ROTL(A, B)              \
(                                       \
    ((B > 15) || (B < 1))               \
    ?   A                               \
    :   vsliq_n_u16(                    \
            vshrq_n_u16(A,(16-(B&15))), \
            A,                          \
            (B&15)                      \
        )                               \
)
    int16x8_t   n = vdupq_n_s16(b);
    uint16x8_t  r = vshlq_u16(a, n);
    n = vaddq_s16(n, vdupq_n_s16(-16));
    a = vshlq_u16(a, n);
    return  vorrq_u16(a, r);
}

INLINE(Vqwu,VQWU_ROTL) (Vqwu a, Rc(1, 31) b)
{
#define     VQWU_ROTL(A, B)             \
(                                       \
    ((B > 31) || (B < 1))               \
    ?   A                               \
    :   vsliq_n_u32(                    \
            vshrq_n_u32(A,(32-(B&31))), \
            A,                          \
            (B&31)                      \
        )                               \
)
    int32x4_t   n = vdupq_n_s32(b);
    uint32x4_t  r = vshlq_u32(a, n);
    n = vaddq_s32(n, vdupq_n_s32(-32));
    a = vshlq_u32(a, n);
    return  vorrq_u32(a, r);
}

INLINE(Vqdu,VQDU_ROTL) (Vqdu a, Rc(1, 63) b)
{
#define     VQDU_ROTL(A, B)             \
(                                       \
    ((B > 63) || (B < 1))               \
    ?   A                               \
    :   vsliq_n_u64(                    \
            vshrq_n_u64(A,(64-(B&63))), \
            A,                          \
            (B&63)                      \
        )                               \
)
    int64x2_t   n = vdupq_n_s64(b);
    uint64x2_t  r = vshlq_u64(a, n);
    n = vaddq_s64(n, vdupq_n_s64(-64));
    a = vshlq_u64(a, n);
    return  vorrq_u64(a, r);
}

#if 0 // _LEAVE_ARM_ROTL
}
#endif

#if 0 // _ENTER_ARM_ROTR
{
#endif

#define DBU_ROTR(A, B) \
vsri_n_u8(vshl_n_u8(A,(7&(8-B))),A,((B&7)+(0==B)))

INLINE( uchar, UCHAR_ROTR)  (unsigned a, Rc(1, UCHAR_WIDTH-1) b)
{
#define     UCHAR_ROTR(A, B)                \
(                                           \
    (uchar)                                 \
    (                                       \
        ( ((unsigned) A)>>B)                \
    |   ( ((unsigned) A)<<(UCHAR_WIDTH-B))  \
    )                                       \
)

    return  UCHAR_ROTR(a, b);
}

INLINE(  char,  CHAR_ROTR)   (int a, Rc(1, CHAR_WIDTH-1) b)
{
#define     CHAR_ROTR(A, B)                 \
(                                           \
    (char)                                  \
    (                                       \
        ( ((unsigned) A)>>B)                \
    |   ( ((unsigned) A)<<(CHAR_WIDTH-B))   \
    )                                       \
)

    return  CHAR_ROTR(a, b);
}

INLINE(ushort, USHRT_ROTR) (unsigned a, Rc(1, USHRT_WIDTH-1) b)
{
#define     USHRT_ROTR(A, B)                \
(                                           \
    (ushort)                                \
    (                                       \
        ( ((unsigned) A)>>B)                \
    |   ( ((unsigned) A)<<(USHRT_WIDTH-B))  \
    )                                       \
)

    return  USHRT_ROTR(a, b);
}

INLINE(  uint,  UINT_ROTR)   (uint a, Rc(1, UINT_WIDTH-1) b)
{
#define     UINT_ROTR(A, B)   ((uint) __ror(A,B))
    return  __ror(a, b);
}

INLINE( ulong, ULONG_ROTR)  (ulong a, Rc(1,  ULONG_WIDTH-1) b)
{
#define     ULONG_ROTR(A, B)   ((ulong) __rorl(A,B))
    return  ULONG_ROTR(a, b);
}

INLINE(ullong,ULLONG_ROTR) (ullong a, Rc(1, ULLONG_WIDTH-1) b)
{
#define     ULLONG_ROTR(A, B)   ((ullong) __rorll(A,B))
    return  ULLONG_ROTR(a, b);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE, rorsqu) (QUAD_UTYPE a, Rc(1, +127) b)
{
    return (a>>b)|(a<<(128-b));
}

#endif

INLINE(Vwbu,VWBU_ROTR) (Vwbu a, Rc(1, 7) b)
{
#define     VWBU_ROTR(A, B)                     \
(                                               \
    ((B < 1) || (B > 7))                        \
    ?   A                                       \
    :   WBU_ASTV(                               \
            vget_lane_f32(                      \
                vreinterpret_f32_u8(            \
                    vsri_n_u8(                  \
                        vshl_n_u8(              \
                            vreinterpret_u8_f32(\
                                vdup_n_f32(     \
                                    VWBU_ASTM(A)\
                                ),              \
                            ),                  \
                            (8-(B&7))           \
                        ),                      \
                        vreinterpret_u8_f32(    \
                            vdup_n_f32(         \
                                VWBU_ASTM(A)    \
                            ),                  \
                        ),                      \
                        ((B&7)+(B==0))          \
                    )                           \
                ),                              \
                0                               \
            )                                   \
        )                                       \
)
    return (VWBU_ROTL)(a, 8-b);
}

INLINE(Vwbc,VWBC_ROTR) (Vwbc a, Rc(1, 7) b)
{
#define     VWBC_ROTR(A, B)                     \
(                                               \
    ((B < 1) || (B > 7))                        \
    ?   A                                       \
    :   WBC_ASTV(                               \
            vget_lane_f32(                      \
                vreinterpret_f32_u8(            \
                    vsri_n_u8(                  \
                        vshl_n_u8(              \
                            vreinterpret_u8_f32(\
                                vdup_n_f32(     \
                                    VWBC_ASTM(A)\
                                ),              \
                            ),                  \
                            (8-(B&7))           \
                        ),                      \
                        vreinterpret_u8_f32(    \
                            vdup_n_f32(         \
                                VWBC_ASTM(A)    \
                            ),                  \
                        ),                      \
                        ((B&7)+(B==0))          \
                    )                           \
                ),                              \
                0                               \
            )                                   \
        )                                       \
)
    return (VWBC_ROTL)(a, 8-b);
}

INLINE(Vwhu,VWHU_ROTR) (Vwhu a, Rc(1, 15) b)
{
#define     VWHU_ROTR(A, B)                         \
(                                                   \
    ((B < 1) || (B > 7))                            \
    ?   A                                           \
    :   WHU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u16(               \
                    vsri_n_u16(                     \
                        vshl_n_u16(                 \
                            vreinterpret_u16_f32(   \
                                vdup_n_f32(         \
                                    VWHU_ASTM(A)    \
                                ),                  \
                            ),                      \
                            (16-(15&B))             \
                        ),                          \
                        vreinterpret_u16_f32(       \
                            vdup_n_f32(             \
                                VWHU_ASTM(A)        \
                            ),                      \
                        ),                          \
                        ((15&B)+(B==0))             \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)
    return (VWHU_ROTL)(a, 16-b);
}

INLINE(Vwwu,VWWU_ROTR) (Vwwu a, Rc(1, 31) b)
{
#define     VWWU_ROTR(A, B) \
(                           \
    ((B < 1) || (B > 31))   \
    ?   A                   \
    :   UINT32_ASTV(__ror(VWWU_ASTV(A),B))\
)
    return  VWWU_ROTR(a, b);
}


INLINE(Vdbu,VDBU_ROTR) (Vdbu a, Rc(1, 7) b)
{
#define     VDBU_ROTR(A, B)  (((B<1)||(B>7))?A:DBU_ROTR(A,B))
    int8x8_t    n = vdup_n_s8(b);
    n = vneg_s8(n);
    return  vorr_u8(
        vshl_u8(a, n),
        vshl_u8(a, vadd_s8(n, vdup_n_s8(8)))
    );
}

INLINE(Vdbc,VDBC_ROTR) (Vdbc a, Rc(1, 7) b)
{
#define     VDBC_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 7))                \
    ?   A                               \
    :   VDBU_ASBC(                      \
            vsri_n_u8(                  \
                vshl_n_u8(              \
                    VDBC_ASBU(A),       \
                    (8-(B&7))           \
                )                       \
                VDBC_ASBU(A),           \
                ((B&7)+(B==0))          \
            )                           \
        )                               \
)
    uint8x8_t   m = VDBC_ASBU(a);
    int8x8_t    n = vdup_n_s8(b);
    n = vneg_s8(n);
    m = vorr_u8(
        vshl_u8(m, n),
        vshl_u8(m, vadd_s8(n, vdup_n_s8(8)))
    );
    return  VDBU_ASBC(m);
}

INLINE(Vdhu,VDHU_ROTR) (Vdhu a, Rc(1, 15) b)
{
#define     VDHU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 15))               \
    ?   A                               \
    :   vsri_n_u16(                     \
            vshl_n_u16(A,(16-(B&15))),  \
            A,                          \
            ((B&15)+(B==0))             \
        )                               \
)
    int16x4_t    n = vdup_n_s16(b);
    n = vneg_s16(n);
    return  vorr_u16(
        vshl_u16(a, n),
        vshl_u16(a, vadd_s16(n, vdup_n_s16(16)))
    );
}

INLINE(Vdwu,VDWU_ROTR) (Vdwu a, Rc(1, 31) b)
{
#define     VDWU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 31))               \
    ?   A                               \
    :   vsri_n_u32(                     \
            vshl_n_u32(A,(32-(B&31))),  \
            A,                          \
            ((B&31)+(B==0))             \
        )                               \
)
    int32x2_t    n = vdup_n_s32(b);
    n = vneg_s32(n);
    return  vorr_u32(
        vshl_u32(a, n),
        vshl_u32(a, vadd_s32(n, vdup_n_s32(32)))
    );
}

INLINE(Vddu,VDDU_ROTR) (Vddu a, Rc(1, 63) b)
{
#define     VDDU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 63))               \
    ?   A                               \
    :   vsri_n_u64(                     \
            vshl_n_u64(A,(64-(B&63))),  \
            A,                          \
            ((B&63)+(B==0))             \
        )                               \
)
    int64x1_t    n = vdup_n_s64(b);
    n = vneg_s64(n);
    return  vorr_u64(
        vshl_u64(a, n),
        vshl_u64(a, vadd_s64(n, vdup_n_s64(64)))
    );
}



INLINE(Vqbu,VQBU_ROTR) (Vqbu a, Rc(1, 7) b)
{
#define     VQBU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 7))                \
    ?   A                               \
    :   vsriq_n_u8(                     \
            vshlq_n_u8(A,(8-(B&7))),    \
            A,                          \
            ((B&7)+(B==0))              \
        )                               \
)
    int8x16_t   n = vdupq_n_s8(b);
    n = vnegq_s8(n);
    return  vorrq_u8(
        vshlq_u8(a, n),
        vshlq_u8(a, vaddq_s8(n, vdupq_n_s8(8)))
    );
}

INLINE(Vqbc,VQBC_ROTR) (Vqbc a, Rc(1, 7) b)
{
#define     VQBC_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 7))                \
    ?   A                               \
    :   VQBU_ASBC(                      \
            vsriq_n_u8(                 \
                vshlq_n_u8(             \
                    VQBC_ASBU(A),       \
                    (8-(B&7))           \
                )                       \
                VQBC_ASBU(A),           \
                ((B&7)+(B==0))          \
            )                           \
        )                               \
)
    uint8x16_t  m = VQBC_ASBU(a);
    int8x16_t   n = vdupq_n_s8(b);
    n = vnegq_s8(n);
    m = vorrq_u8(
        vshlq_u8(m, n),
        vshlq_u8(m, vaddq_s8(n, vdupq_n_s8(8)))
    );
    return  VQBU_ASBC(m);
}

INLINE(Vqhu,VQHU_ROTR) (Vqhu a, Rc(1, 15) b)
{
#define     VQHU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 15))               \
    ?   A                               \
    :   vsriq_n_u16(                    \
            vshlq_n_u16(A,(16-(B&15))), \
            A,                          \
            ((B&15)+(B==0))             \
        )                               \
)
    int16x8_t    n = vdupq_n_s16(b);
    n = vnegq_s16(n);
    return  vorrq_u16(
        vshlq_u16(a, n),
        vshlq_u16(a, vaddq_s16(n, vdupq_n_s16(16)))
    );
}

INLINE(Vqwu,VQWU_ROTR) (Vqwu a, Rc(1, 31) b)
{
#define     VQWU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 31))               \
    ?   A                               \
    :   vsriq_n_u32(                    \
            vshlq_n_u32(A,(32-(B&31))), \
            A,                          \
            ((B&31)+(B==0))             \
        )                               \
)
    int32x4_t    n = vdupq_n_s32(b);
    n = vnegq_s32(n);
    return  vorrq_u32(
        vshlq_u32(a, n),
        vshlq_u32(a, vaddq_s32(n, vdupq_n_s32(32)))
    );
}

INLINE(Vqdu,VQDU_ROTR) (Vqdu a, Rc(1, 63) b)
{
#define     VQDU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 63))               \
    ?   A                               \
    :   vsriq_n_u64(                    \
            vshlq_n_u64(A,(64-(B&63))), \
            A,                          \
            ((B&63)+(B==0))             \
        )                               \
)
    int64x2_t    n = vdupq_n_s64(b);
    n = vnegq_s64(n);
    return  vorrq_u64(
        vshlq_u64(a, n),
        vshlq_u64(a, vaddq_s64(n, vdupq_n_s64(64)))
    );
}

#if 0 // _LEAVE_ARM_ROTR
}
#endif

#if 0 // _ENTER_ARM_ROVL
{
#endif

INLINE(Vdbu,VDBU_ROVL) (Vdbu a, Vdbu b);
INLINE(Vdhu,VDHU_ROVL) (Vdhu a, Vdhu b);
INLINE(Vdwu,VDWU_ROVL) (Vdwu a, Vdwu b);

INLINE(Vwbu,VWBU_ROVL) (Vwbu a, Vwbu b) 
{
    float       m;
    float32x2_t l = vdup_n_f32(m);
    m = VWBU_ASTM(a);
    float32x2_t r = vdup_n_f32(m);
    return  VDBU_GETL(VDBU_ROVL(VDWF_ASBU(l), VDWF_ASBU(r)));
}

INLINE(Vwbc,VWBC_ROVL) (Vwbc a, Vwbc b) 
{
    return  VWBU_ASBC(VWBU_ROVL(VWBC_ASBU(a), VWBC_ASBU(b)));
}

INLINE(Vwhu,VWHU_ROVL) (Vwhu a, Vwhu b) 
{
    float       m;
    float32x2_t l = vdup_n_f32(m);
    m = VWHU_ASTM(a);
    float32x2_t r = vdup_n_f32(m);
    return  VDHU_GETL(VDHU_ROVL(VDWF_ASHU(l), VDWF_ASHU(r)));
}

INLINE(Vwwu,VWWU_ROVL) (Vwwu a, Vwwu b) 
{
    float       m;
    float32x2_t l = vdup_n_f32(m);
    m = VWWU_ASTM(a);
    float32x2_t r = vdup_n_f32(m);
    return  VDWU_GETL(VDWU_ROVL(VDWF_ASWU(l), VDWF_ASWU(r)));
}

INLINE(Vdbu,VDBU_ROVL) (Vdbu a, Vdbu b) 
{
    int8x8_t   n = vreinterpret_s8_u8(b);
    uint8x8_t  r = vshl_u8(a, vadd_s8(n, vdup_n_s8(-8)));
    return  vorr_u8(r, vshl_u8(a, n));
}

INLINE(Vdbc,VDBC_ROVL) (Vdbc a, Vdbu b)
{
    return  VDBU_ASBC( ((VDBU_ROVL)(VDBC_ASBU(a), b)) );
}

INLINE(Vdhu,VDHU_ROVL) (Vdhu a, Vdhu b) 
{
    int16x4_t   n = vreinterpret_s16_u16(b);
    uint16x4_t  r = vshl_u16(a, vadd_s16(n, vdup_n_s16(-16)));
    return  vorr_u16(r, vshl_u16(a, n));
}

INLINE(Vdwu,VDWU_ROVL) (Vdwu a, Vdwu b) 
{
    int32x2_t   n = vreinterpret_s32_u32(b);
    uint32x2_t  r = vshl_u32(a, vadd_s32(n, vdup_n_s32(-32)));
    return  vorr_u32(r, vshl_u32(a, n));
}

INLINE(Vddu,VDDU_ROVL) (Vddu a, Vddu b) 
{
    int64x1_t   n = vreinterpret_s64_u64(b);
    uint64x1_t  r = vshl_u64(a, vadd_s64(n, vdup_n_s64(-64)));
    return  vorr_u64(r, vshl_u64(a, n));
}

INLINE(Vqbu,VQBU_ROVL) (Vqbu a, Vqbu b) 
{
    int8x16_t   n = vreinterpretq_s8_u8(b);
    uint8x16_t  r = vshlq_u8(a, vaddq_s8(n, vdupq_n_s8(-8)));
    return  vorrq_u8(r, vshlq_u8(a, n));
}

INLINE(Vqbc,VQBC_ROVL) (Vqbc a, Vqbu b)
{
    return  VQBU_ASBC(((VQBU_ROVL)(VQBC_ASBU(a), b)));
}

INLINE(Vqhu,VQHU_ROVL) (Vqhu a, Vqhu b) 
{
    int16x8_t   n = vreinterpretq_s16_u16(b);
    uint16x8_t  r = vshlq_u16(a, vaddq_s16(n, vdupq_n_s16(-16)));
    return  vorrq_u16(r, vshlq_u16(a, n));
}

INLINE(Vqwu,VQWU_ROVL) (Vqwu a, Vqwu b) 
{
    int32x4_t   n = vreinterpretq_s32_u32(b);
    uint32x4_t  r = vshlq_u32(a, vaddq_s32(n, vdupq_n_s32(-32)));
    return  vorrq_u32(r, vshlq_u32(a, n));
}

INLINE(Vqdu,VQDU_ROVL) (Vqdu a, Vqdu b) 
{
    int64x2_t   n = vreinterpretq_s64_u64(b);
    uint64x2_t  r = vshlq_u64(a, vaddq_s64(n, vdupq_n_s64(-64)));
    return  vorrq_u64(r, vshlq_u64(a, n));
}

INLINE(Vqqu,VQQU_ROVL) (Vqqu a, Vqqu b)
{
    (void) b;
    return a;
//    QUAD_VTYPE  l = {.Q.U=(VQQU_SVLL)(a, b)};
//  return l.Q.U;
}

#if 0 // _LEAVE_ARM_ROVL
}
#endif

#if 0 // _ENTER_ARM_ROVR
{
#endif
INLINE(Vdbu,VDBU_ROVR) (Vdbu a, Vdbu b);
INLINE(Vdhu,VDHU_ROVR) (Vdhu a, Vdhu b);
INLINE(Vdwu,VDWU_ROVR) (Vdwu a, Vdwu b);

INLINE(Vwbu,VWBU_ROVR) (Vwbu a, Vwbu b) 
{
    float       m;
    float32x2_t l = vdup_n_f32(m);
    m = VWBU_ASTM(a);
    float32x2_t r = vdup_n_f32(m);
    return  VDBU_GETL(VDBU_ROVR(VDWF_ASBU(l), VDWF_ASBU(r)));
}

INLINE(Vwbc,VWBC_ROVR) (Vwbc a, Vwbc b) 
{
    return  VWBU_ASBC(VWBU_ROVR(VWBC_ASBU(a), VWBC_ASBU(b)));
}

INLINE(Vwhu,VWHU_ROVR) (Vwhu a, Vwhu b) 
{
    float       m;
    float32x2_t l = vdup_n_f32(m);
    m = VWHU_ASTM(a);
    float32x2_t r = vdup_n_f32(m);
    return  VDHU_GETL(VDHU_ROVR(VDWF_ASHU(l), VDWF_ASHU(r)));
}

INLINE(Vwwu,VWWU_ROVR) (Vwwu a, Vwwu b) 
{
    float       m;
    float32x2_t l = vdup_n_f32(m);
    m = VWWU_ASTM(a);
    float32x2_t r = vdup_n_f32(m);
    return  VDWU_GETL(VDWU_ROVR(VDWF_ASWU(l), VDWF_ASWU(r)));
}

INLINE(Vdbu,VDBU_ROVR) (Vdbu a, Vdbu b) 
{
    int8x8_t   n = vreinterpret_s8_u8(b);
    uint8x8_t  r = vshl_u8(a, vneg_s8(n));
    n = vsub_s8(vdup_n_s8(8), n);
    return  vorr_u8(r, vshl_u8(a, n));
}

INLINE(Vdbc,VDBC_ROVR) (Vdbc a, Vdbu b)
{
    return  VDBU_ASBC( ((VDBU_ROVR)(VDBC_ASBU(a), b)) );
}

INLINE(Vdhu,VDHU_ROVR) (Vdhu a, Vdhu b) 
{
    int16x4_t   n = vreinterpret_s16_u16(b);
    uint16x4_t  r = vshl_u16(a, vneg_s16(n));
    n = vsub_s16(vdup_n_s16(16), n);
    return  vorr_u16(r, vshl_u16(a, n));
}

INLINE(Vdwu,VDWU_ROVR) (Vdwu a, Vdwu b) 
{
    int32x2_t   n = vreinterpret_s32_u32(b);
    uint32x2_t  r = vshl_u32(a, vneg_s32(n));
    n = vsub_s32(vdup_n_s32(32), n);
    return  vorr_u32(r, vshl_u32(a, n));
}

INLINE(Vddu,VDDU_ROVR) (Vddu a, Vddu b) 
{
    int64x1_t   n = vreinterpret_s64_u64(b);
    uint64x1_t  r = vshl_u64(a, vneg_s64(n));
    n = vsub_s64(vdup_n_s64(64), n);
    return  vorr_u64(r, vshl_u64(a, n));
}

INLINE(Vqbu,VQBU_ROVR) (Vqbu a, Vqbu b) 
{
    int8x16_t   n = vreinterpretq_s8_u8(b);
    uint8x16_t  r = vshlq_u8(a, vnegq_s8(n));
    n = vsubq_s8(vdupq_n_s8(8), n);
    return  vorrq_u8(r, vshlq_u8(a, n));
}

INLINE(Vqbc,VQBC_ROVR) (Vqbc a, Vqbu b)
{
    return  VQBU_ASBC( ((VQBU_ROVR)(VQBC_ASBU(a), b)) );
}

INLINE(Vqhu,VQHU_ROVR) (Vqhu a, Vqhu b) 
{
    int16x8_t   n = vreinterpretq_s16_u16(b);
    uint16x8_t  r = vshlq_u16(a, vnegq_s16(n));
    n = vsubq_s16(vdupq_n_s16(16), n);
    return  vorrq_u16(r, vshlq_u16(a, n));
}

INLINE(Vqwu,VQWU_ROVR) (Vqwu a, Vqwu b) 
{
    int32x4_t   n = vreinterpretq_s32_u32(b);
    uint32x4_t  r = vshlq_u32(a, vnegq_s32(n));
    n = vsubq_s32(vdupq_n_s32(32), n);
    return  vorrq_u32(r, vshlq_u32(a, n));
}

INLINE(Vqdu,VQDU_ROVR) (Vqdu a, Vqdu b) 
{
    int64x2_t   n = vreinterpretq_s64_u64(b);
    uint64x2_t  r = vshlq_u64(a, vnegq_s64(n));
    n = vsubq_s64(vdupq_n_s64(64), n);
    return  vorrq_u64(r, vshlq_u64(a, n));
}

#if 0 // _LEAVE_ARM_ROVR
}
#endif

#if 0 // _ENTER_ARM_ADDL
{
#endif

INLINE(Vwyu,VWYU_ADDL) (Vwyu a, Vwyu b)
{
#define     VWYU_ADDL(A, B) \
WYU_ASTV(((VWWU_ASTV(VWYU_ASWU(A)))^(VWWU_ASTV(VWYU_ASWU(B)))))
    return  VWYU_ADDL(a, b);
}

INLINE(Vwbu,VWBU_ADDL) (Vwbu a, Vwbu b) 
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    float       bm = VWBU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    az = vadd_u8(az, bz);
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_ADDL) (Vwbi a, Vwbi b) 
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    float       bm = VWBI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    az = vadd_s8(az, bz);
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_ADDL) (Vwbc a, Vwbc b) 
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ADDL(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return  VWBU_ASBC(VWBU_ADDL(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_ADDL) (Vwhu a, Vwhu b) 
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    float       bm = VWHU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    az = vadd_u16(az, bz);
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_ADDL) (Vwhi a, Vwhi b) 
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    float       bm = VWHI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    az = vadd_s16(az, bz);
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}


INLINE(Vwwu,VWWU_ADDL) (Vwwu a, Vwwu b) 
{
#define     VWWU_ADDL(A, B)  UINT32_ASTV((VWWU_ASTV(A)+VWWU_ASTV(B)))
    return  VWWU_ADDL(a, b);
}

INLINE(Vwwi,VWWI_ADDL) (Vwwi a, Vwwi b) 
{
#define     VWWI_ADDL(A, B)  INT32_ASTV((VWWI_ASTV(A)+VWWI_ASTV(B)))
    return  VWWI_ADDL(a, b);
}


INLINE(Vdyu,VDYU_ADDL) (Vdyu a, Vdyu b)
{
#define     VDYU_ADDL(A, B) DYU_ASTV(veor_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_ADDL(a, b);
}

INLINE(Vdbu,VDBU_ADDL) (Vdbu a, Vdbu b) {return vadd_u8(a, b);}
INLINE(Vdbi,VDBI_ADDL) (Vdbi a, Vdbi b) {return vadd_s8(a, b);}
INLINE(Vdbc,VDBC_ADDL) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_ADDL(A, B) VDBI_ASBC(vadd_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_ADDL(A, B) VDBU_ASBC(vadd_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_ADDL(a, b);
}

INLINE(Vdhu,VDHU_ADDL) (Vdhu a, Vdhu b) {return vadd_u16(a, b);}
INLINE(Vdhi,VDHI_ADDL) (Vdhi a, Vdhi b) {return vadd_s16(a, b);}
INLINE(Vdwu,VDWU_ADDL) (Vdwu a, Vdwu b) {return vadd_u32(a, b);}
INLINE(Vdwi,VDWI_ADDL) (Vdwi a, Vdwi b) {return vadd_s32(a, b);}
INLINE(Vddu,VDDU_ADDL) (Vddu a, Vddu b) {return vadd_u64(a, b);}
INLINE(Vddi,VDDI_ADDL) (Vddi a, Vddi b) {return vadd_s64(a, b);}

INLINE(Vqyu,VQYU_ADDL) (Vqyu a, Vqyu b)
{
#define     VQYU_ADDL(A, B) QYU_ASTV(veorq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_ADDL(a, b);
}

INLINE(Vqbu,VQBU_ADDL) (Vqbu a, Vqbu b) {return vaddq_u8(a, b);}
INLINE(Vqbi,VQBI_ADDL) (Vqbi a, Vqbi b) {return vaddq_s8(a, b);}
INLINE(Vqbc,VQBC_ADDL) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_ADDL(A, B) VQBI_ASBC(vaddq_s8(VQBC_ASBI(A), VQBC_ASBI(B)))
#else
#   define  VQBC_ADDL(A, B) VQBU_ASBC(vaddq_u8(VQBC_ASBU(A), VQBC_ASBU(B)))
#endif
    return  VQBC_ADDL(a, b);
}

INLINE(Vqhu,VQHU_ADDL) (Vqhu a, Vqhu b) {return vaddq_u16(a, b);}
INLINE(Vqhi,VQHI_ADDL) (Vqhi a, Vqhi b) {return vaddq_s16(a, b);}
INLINE(Vqwu,VQWU_ADDL) (Vqwu a, Vqwu b) {return vaddq_u32(a, b);}
INLINE(Vqwi,VQWI_ADDL) (Vqwi a, Vqwi b) {return vaddq_s32(a, b);}
INLINE(Vqdu,VQDU_ADDL) (Vqdu a, Vqdu b) {return vaddq_u64(a, b);}
INLINE(Vqdi,VQDI_ADDL) (Vqdi a, Vqdi b) {return vaddq_s64(a, b);}
INLINE(Vqqu,VQQU_ADDL) (Vqqu a, Vqqu b)
{
/*
    Vqqu c = {vaddq_u64(a.V0, b.V0)};
    if (vgetq_lane_u64(c.V0, 0) < vgetq_lane_u64(a.V0, 0))
    {
        uint64x1_t x = vget_low_u64(c.V0);
        x = vadd_u64(x, vdup_n_u64(1));
        c.V0 = vcopyq_lane_u64(c.V0, 1, x, 0);
    }
    return c;
*/
    QUAD_VTYPE x={.Q.U=a}, y={.Q.U=b}, z;
    z.U = x.U+y.U;
    return z.Q.U;
}
INLINE(Vqqi,VQQI_ADDL) (Vqqi a, Vqqi b)
{
    QUAD_VTYPE x={.Q.I=a}, y={.Q.I=b}, z;
    z.U = x.U+y.U;
    return z.Q.I;
#if 0
    uint64x2_t p = vreinterpretq_u64_s64(a.V0);
    uint64x2_t q = vreinterpretq_u64_s64(b.V0);
    uint64x2_t r = vaddq_u64(p, q);
    if (vgetq_lane_u64(r, 0) < vgetq_lane_u64(r, 0))
    {
        uint64x1_t x = vget_low_u64(r);
        x = vadd_u64(x, vdup_n_u64(1));
        r = vcopyq_lane_u64(r, 1, x, 0);
    }
    Vqqi c;
    c.V0 = vreinterpretq_s64_u64(r);
    return  c;
#endif
}

#if 0 // _LEAVE_ARM_ADDL
}
#endif

#if 0 // _ENTER_ARM_ADDS
{
#endif

INLINE( _Bool,  BOOL_ADDS)  (_Bool a,  _Bool b) {return a|b;}

INLINE( uchar, UCHAR_ADDS)  (unsigned a, unsigned b)
{
#define     UCHAR_ADDS  vqaddb_u8
    return  vqaddb_u8(a, b);
}

INLINE( schar, SCHAR_ADDS)  (signed a,  signed b)
{
#define     SCHAR_ADDS  vqaddb_s8
    return  vqaddb_s8(a, b);
}

INLINE(  char,  CHAR_ADDS)   (int a,   int b)
{
#if CHAR_MIN
#   define  CHAR_ADDS(A, B)  ((char) vqaddb_s8(A,B))
    return  vqaddb_s8(a, b);
#else
#   define  CHAR_ADDS(A, B)  ((char) vqaddb_u8(A,B))
    return  vqaddb_u8(a, b);
#endif
}

INLINE(ushort, USHRT_ADDS) (unsigned a, unsigned b)
{
#define     USHRT_ADDS  vqaddh_u16
    return  vqaddh_u16(a, b);
}

INLINE( short,  SHRT_ADDS)  (signed a,  signed b)
{
#define     SHRT_ADDS  vqaddh_s16
    return  vqaddh_s16(a, b);
}

INLINE(  uint,  UINT_ADDS)   (uint a,   uint b)
{
#define     UINT_ADDS  vqadds_u32
    return  vqadds_u32(a, b);
}

INLINE(   int,   INT_ADDS)    (int a,    int b)
{
#define     INT_ADDS  vqadds_s32
    return  vqadds_s32(a, b);
}

INLINE( ulong, ULONG_ADDS)  (ulong a,  ulong b)
{
#if DWRD_NLONG == 2
#   define  ULONG_ADDS(A, B)  ((ulong) vqadds_u32(A,B))
#else
#   define  ULONG_ADDS vqaddd_u64
#endif
    return  ULONG_ADDS(a, b);
}

INLINE(  long,  LONG_ADDS)   (long a,   long b)
{
#if DWRD_NLONG == 2
#   define  LONG_ADDS(A, B)  ((long) vqadds_s32(A,B))
#else
#   define  LONG_ADDS vqaddd_s64
#endif
    return  LONG_ADDS(a, b);
}

INLINE(ullong,ULLONG_ADDS) (ullong a, ullong b)
{
#if QUAD_NLLONG == 2
#   define  ULLONG_ADDS(A, B) ((ullong) vqaddd_u64(A,B))
#else
// ??
#endif
    return  ULLONG_ADDS(a, b);
}

INLINE( llong, LLONG_ADDS)  (llong a,  llong b)
{
#if QUAD_NLLONG == 2
#   define  LLONG_ADDS(A, B) ((llong) vqaddd_s64(A,B))
#else
// ??
#endif
    return  LLONG_ADDS(a, b);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,addsqu) (QUAD_UTYPE a, QUAD_UTYPE b) 
{
    QUAD_TYPE   c;
#if defined(__SIZEOF_INT128__)
    c.U = a+b;
    if (c.U < b)
        c.U = -1;
#else
    c.U = addlqu(a, b);
    if (cltyqu(c.U, b))
        c.Lo.I=-1, c.Hi.I=-1;
#endif
    return  c.U;
}

INLINE(QUAD_ITYPE,addsqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    QUAD_TYPE x={.I=a}, y={.I=b}, z;
#if defined(__SIZEOF_INT128__)
    z.U = x.U+y.U;
    if (0 > a)
    {
        if ((0 >= b) && (z.U > x.U))
        {
            z.Hi.I = INT64_MIN;
            z.Lo.I = 0;
        }
    }
    else 
    {
        if ((0 < b) && (z.U < x.U))
        {
            z.Hi.I = INT64_MAX;
            z.Lo.I = -1;
        }
    }
#else
    z.U = addlqu(x.U, y.U);
    if (zgtyqi(a))
    {
        if (zgeyqi(b) && cgtyqu(z.U, x.U))
        {
            z.Hi.I = INT64_MIN;
            z.Lo.I = 0;
        }
    }
    else 
    {
        if (zltyqi(b) && cltyqu(z.U, x.U))
        {
            z.Hi.I = INT64_MAX;
            z.Lo.I = -1;
        }
    }
#endif
    return  z.I;
}
#endif


INLINE(float,WBU_ADDS) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.B.U = vqadd_u8(x.B.U, y.B.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WBI_ADDS) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.B.I = vqadd_s8(x.B.I, y.B.I);
    return  vget_lane_f32(x.W.F, 0);
}

#if CHAR_MIN
#   define  WBC_ADDS WBI_ADDS
#   define  DBC_ADDS vqadd_s8
#   define  QBC_ADDS vqaddq_s8
#else
#   define  WBC_ADDS WBU_ADDS
#   define  DBC_ADDS vqadd_u8
#   define  QBC_ADDS vqaddq_u8
#endif

INLINE(float,WHU_ADDS) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.H.U = vqadd_u16(x.H.U, y.H.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WHI_ADDS) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.H.I = vqadd_u16(x.H.I, y.H.I);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WWU_ADDS) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.W.U = vqadd_u32(x.W.U, y.W.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WWI_ADDS) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.W.I = vqadd_s32(x.W.I, y.W.I);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(Vwyu,VWYU_ADDS) (Vwyu a, Vwyu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.U = vorr_u8(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwbu,VWBU_ADDS) (Vwbu a, Vwbu b) {a.V0=WBU_ADDS(a.V0,b.V0); return a;}
INLINE(Vwbi,VWBI_ADDS) (Vwbi a, Vwbi b) {a.V0=WBI_ADDS(a.V0,b.V0); return a;}
INLINE(Vwbc,VWBC_ADDS) (Vwbc a, Vwbc b) {a.V0=WBC_ADDS(a.V0,b.V0); return a;}
INLINE(Vwhu,VWHU_ADDS) (Vwhu a, Vwhu b) {a.V0=WHU_ADDS(a.V0,b.V0); return a;}
INLINE(Vwhi,VWHI_ADDS) (Vwhi a, Vwhi b) {a.V0=WHI_ADDS(a.V0,b.V0); return a;}
INLINE(Vwwu,VWWU_ADDS) (Vwwu a, Vwwu b) {a.V0=WWU_ADDS(a.V0,b.V0); return a;}
INLINE(Vwwi,VWWI_ADDS) (Vwwi a, Vwwi b) {a.V0=WWI_ADDS(a.V0,b.V0); return a;}

INLINE(Vdyu,VDYU_ADDS) (Vdyu a, Vdyu b) {a.V0=vorr_u64(a.V0,b.V0); return a;}
INLINE(Vdbu,VDBU_ADDS) (Vdbu a, Vdbu b) {return vqadd_u8(a, b);}
INLINE(Vdbi,VDBI_ADDS) (Vdbi a, Vdbi b) {return vqadd_s8(a, b);}
INLINE(Vdbc,VDBC_ADDS) (Vdbc a, Vdbc b) {a.V0=DBC_ADDS(a.V0,b.V0); return a;}
INLINE(Vdhu,VDHU_ADDS) (Vdhu a, Vdhu b) {return vqadd_u16(a, b);}
INLINE(Vdhi,VDHI_ADDS) (Vdhi a, Vdhi b) {return vqadd_s16(a, b);}
INLINE(Vdwu,VDWU_ADDS) (Vdwu a, Vdwu b) {return vqadd_u32(a, b);}
INLINE(Vdwi,VDWI_ADDS) (Vdwi a, Vdwi b) {return vqadd_s32(a, b);}
INLINE(Vddu,VDDU_ADDS) (Vddu a, Vddu b) {return vqadd_u64(a, b);}
INLINE(Vddi,VDDI_ADDS) (Vddi a, Vddi b) {return vqadd_s64(a, b);}

INLINE(Vqyu,VQYU_ADDS) (Vqyu a, Vqyu b) {a.V0=vorrq_u64(a.V0,b.V0); return a;}
INLINE(Vqbu,VQBU_ADDS) (Vqbu a, Vqbu b) {return vqaddq_u8(a, b);}
INLINE(Vqbi,VQBI_ADDS) (Vqbi a, Vqbi b) {return vqaddq_s8(a, b);}
INLINE(Vqbc,VQBC_ADDS) (Vqbc a, Vqbc b) {a.V0=QBC_ADDS(a.V0,b.V0); return a;}
INLINE(Vqhu,VQHU_ADDS) (Vqhu a, Vqhu b) {return vqaddq_u16(a, b);}
INLINE(Vqhi,VQHI_ADDS) (Vqhi a, Vqhi b) {return vqaddq_s16(a, b);}
INLINE(Vqwu,VQWU_ADDS) (Vqwu a, Vqwu b) {return vqaddq_u32(a, b);}
INLINE(Vqwi,VQWI_ADDS) (Vqwi a, Vqwi b) {return vqaddq_s32(a, b);}
INLINE(Vqdu,VQDU_ADDS) (Vqdu a, Vqdu b) {return vqaddq_u64(a, b);}
INLINE(Vqdi,VQDI_ADDS) (Vqdi a, Vqdi b) {return vqaddq_s64(a, b);}
INLINE(Vqqu,VQQU_ADDS) (Vqqu a, Vqqu b)
{
    QUAD_VTYPE x={.D.U=a.V0}, y={.D.U=b.V0};
    x.U = addsqu(x.U, y.U);
    return  x.Q.U;
}
INLINE(Vqqi,VQQI_ADDS) (Vqqi a, Vqqi b)
{
    QUAD_VTYPE x={.D.I=a.V0}, y={.D.I=b.V0};
    x.U = addsqi(x.I, y.I);
    return  x.Q.I;
}

#if 0 // _LEAVE_ARM_ADDS
}
#endif

#if 0 // _ENTER_ARM_ADD2
{
#endif

INLINE(Vdhu,VWBU_ADD2) (Vwbu a, Vwbu b) 
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    uint8x8_t   l = vreinterpret_u8_f32(p);
    uint8x8_t   r = vreinterpret_u8_f32(q);
    uint16x8_t  c = vaddl_u8(l, r);
    return  vget_low_u16(c);
}

INLINE(Vdhi,VWBI_ADD2) (Vwbi a, Vwbi b) 
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    int8x8_t    l = vreinterpret_s8_f32(p);
    int8x8_t    r = vreinterpret_s8_f32(q);
    int16x8_t   c = vaddl_s8(l, r);
    return  vget_low_s16(c);
}

#if CHAR_MIN
INLINE(Vdhi,VWBC_ADD2) (Vwbc a, Vwbc b) 
{
    return  VWBI_ADD2(VWBC_ASBI(a), VWBC_ASBI(b));
}

#else
INLINE(Vdhu,VWBC_ADD2) (Vwbc a, Vwbc b) 
{
    return  VWBU_ADD2(VWBC_ASBU(a), VWBC_ASBU(b));
}

#endif


INLINE(Vdwu,VWHU_ADD2) (Vwhu a, Vwhu b) 
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    uint16x4_t  l = vreinterpret_u16_f32(p);
    uint16x4_t  r = vreinterpret_u16_f32(q);
    uint32x4_t  c = vaddl_u16(l, r);
    return  vget_low_u32(c);
}

INLINE(Vdwi,VWHI_ADD2) (Vwhi a, Vwhi b) 
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    int16x4_t   l = vreinterpret_u16_f32(p);
    int16x4_t   r = vreinterpret_u16_f32(q);
    int32x4_t   c = vaddl_s16(l, r);
    return  vget_low_s32(c);
}


INLINE(Vddu,VWWU_ADD2) (Vwwu a, Vwwu b) 
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    uint32x2_t  l = vreinterpret_u32_f32(p);
    uint32x2_t  r = vreinterpret_u32_f32(q);
    uint64x2_t  c = vaddl_u32(l, r);
    return  vget_low_u64(c);
}

INLINE(Vddi,VWWI_ADD2) (Vwwi a, Vwwi b) 
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    int32x2_t   l = vreinterpret_u32_f32(p);
    int32x2_t   r = vreinterpret_u32_f32(q);
    int64x2_t   c = vaddl_s32(l, r);
    return  vget_low_s64(c);
}


INLINE(Vqhu,VDBU_ADD2) (Vdbu a, Vdbu b)
{
    return  vaddq_u16(vmovl_u8(a), vmovl_u8(b));
}

INLINE(Vqhi,VDBI_ADD2) (Vdbi a, Vdbi b)
{
    return  vaddq_s16(vmovl_s8(a), vmovl_s8(b));
}

#if CHAR_MIN
INLINE(Vqhi,VDBC_ADD2) (Vdbc a, Vdbc b) 
{
    return  VDBI_ADD2(VDBC_ASBI(a), VDBC_ASBI(b));
}
#else

INLINE(Vqhu,VDBC_ADD2) (Vdbc a, Vdbc b) 
{
    return  VDBU_ADD2(VDBC_ASBU(a), VDBC_ASBU(b));
}
#endif


INLINE(Vqwu,VDHU_ADD2) (Vdhu a, Vdhu b)
{
    return  vaddq_u32(vmovl_u16(a), vmovl_u16(b));
}

INLINE(Vqwi,VDHI_ADD2) (Vdhi a, Vdhi b)
{
    return  vaddq_s32(vmovl_s16(a), vmovl_s16(b));
}


INLINE(Vqdu,VDWU_ADD2) (Vdwu a, Vdwu b)
{
    return  vaddq_u64(vmovl_u32(a), vmovl_u32(b));
}

INLINE(Vqdi,VDWI_ADD2) (Vdwi a, Vdwi b)
{
    return  vaddq_s64(vmovl_s32(a), vmovl_s32(b));
}

#if 0 // _LEAVE_ARM_ADD2
}
#endif

#if 0 // _ENTER_ARM_ADDH
{
#endif

INLINE(flt16_t,  BOOL_ADDH)   (_Bool a, flt16_t b) {return b+a;}
INLINE(flt16_t, UCHAR_ADDH)   (uchar a, flt16_t b) {return a+b;}
INLINE(flt16_t, SCHAR_ADDH)   (schar a, flt16_t b) {return a+b;}
INLINE(flt16_t,  CHAR_ADDH)    (char a, flt16_t b) {return a+b;}
INLINE(flt16_t, USHRT_ADDH)  (ushort a, flt16_t b) {return a+b;}
INLINE(flt16_t,  SHRT_ADDH)   (short a, flt16_t b) {return a+b;}
INLINE(flt16_t,  UINT_ADDH)    (uint a, flt16_t b) {return a+b;}
INLINE(flt16_t,   INT_ADDH)     (int a, flt16_t b) {return a+b;}
INLINE(flt16_t, ULONG_ADDH)   (ulong a, flt16_t b) {return a+b;}
INLINE(flt16_t,  LONG_ADDH)    (long a, flt16_t b) {return a+b;}
INLINE(flt16_t,ULLONG_ADDH)  (ullong a, flt16_t b) {return a+b;}
INLINE(flt16_t, LLONG_ADDH)   (llong a, flt16_t b) {return a+b;}

INLINE(flt16_t, FLT16_ADDH) (flt16_t a, flt16_t b) {return a+b;}
INLINE(float,     FLT_ADDH)   (float a, flt16_t b) {return a+b;}
INLINE(double,    DBL_ADDH)  (double a, flt16_t b) {return a+b;}

#if QUAD_NLLONG == 2
INLINE(flt16_t,addhqu) (QUAD_UTYPE a, flt16_t b) {return a+b;}
INLINE(flt16_t,addhqi) (QUAD_ITYPE a, flt16_t b) {return a+b;}
INLINE(QUAD_FTYPE,addhqf) (QUAD_FTYPE a, flt16_t b) {return a+b;}
#endif


INLINE(Vdhf,VWBU_ADDH) (Vwbu a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vadd_f16(VWBU_CVHF(a), b);
#else
    float32x4_t p = VWBU_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vaddq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBI_ADDH) (Vwbi a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vadd_f16(VWBI_CVHF(a), b);
#else
    float32x4_t p = VWBI_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vaddq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBC_ADDH) (Vwbc a, Vdhf b)
{
#if CHAR_MIN
    return VWBI_ADDH(VWBC_ASBI(a), b);
#else
    return VWBU_ADDH(VWBC_ASBU(a), b);
#endif
}


INLINE(float,WHF_ADDH) (float am, float bm)
{
#if defined(SPC_ARM_FP16_SIMD)
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    ah = vadd_f16(ah, bh);
#else
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x4_t av = vcvt_f32_f16(ah);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    float32x4_t bv = vcvt_f32_f16(bh);
    av = vaddq_f32(av, bv);
    ah = vcvt_f16_f32(av);
#endif
    aw = vreinterpret_f32_f16(ah);
    return  vget_lane_f32(aw, 0);
    
}


INLINE(Vwhf,VWHU_ADDH) (Vwhu a, Vwhf b)
{
    Vwhf    v = VWHU_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_ADDH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHI_ADDH) (Vwhi a, Vwhf b)
{
    Vwhf    v = VWHI_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_ADDH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_ADDH) (Vwhf a, Vwhf b)
{
    float   m = WHF_ADDH(VWHF_ASTM(a), VWHF_ASTM(b));
    return WHF_ASTV(m);
}


INLINE(Vqhf,VDBU_ADDH) (Vdbu a, Vqhf b) 
{
    uint16x8_t  c = vmovl_u8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_u16(c);
    return  vaddq_f16(f, b);
#else

    uint32x4_t  zl = vmovl_u16(vget_low_u16(c));
    float32x4_t fl = vcvtq_f32_u32(zl);
    uint32x4_t  zr = vmovl_u16(vget_high_u16(c));
    float32x4_t fr = vcvtq_f32_u32(zr);
    fl = vaddq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vaddq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBI_ADDH) (Vdbi a, Vqhf b) 
{
    int16x8_t   c = vmovl_s8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_s16(c);
    return  vaddq_f16(f, b);
#else
    int32x4_t   zl = vmovl_s16(vget_low_s16(c));
    float32x4_t fl = vcvtq_f32_s32(zl);
    int32x4_t   zr = vmovl_s16(vget_high_s16(c));
    float32x4_t fr = vcvtq_f32_s32(zr);
    fl = vaddq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vaddq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBC_ADDH) (Vdbc a, Vqhf b)
{
#if CHAR_MIN
    return  VDBI_ADDH(VDBC_ASBI(a), b);
#else
    return  VDBU_ADDH(VDBC_ASBU(a), b);
#endif
}


INLINE(Vdhf,VDHU_ADDH) (Vdhu a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vadd_f16(vcvt_f16_u16(a), b);
#else
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vaddq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHI_ADDH) (Vdhi a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vadd_f16(vcvt_f16_s16(a), b);
#else
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vaddq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHF_ADDH) (Vdhf a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vadd_f16(a, b);
#else
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vaddq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}


INLINE(Vwhf,VDWU_ADDH) (Vdwu a, Vwhf b)
{
    return  VWHF_ADDH(VDWU_CVHF(a), b);
}

INLINE(Vwhf,VDWI_ADDH) (Vdwi a, Vwhf b)
{
    return  VWHF_ADDH(VDWI_CVHF(a), b);
}

INLINE(Vdwf,VDWF_ADDH) (Vdwf a, Vwhf b)
{
    return vadd_f32(a, VWHF_CVWF(b));
}

/*
INLINE(Vhhf,VDDU_ADDH) (Vddu a, Vhhf b);
INLINE(Vhhf,VDDI_ADDH) (Vddi a, Vhhf b);
INLINE(Vddf,VDDF_ADDH) (Vddf a, Vhhf b);


INLINE(Vohf,VQBU_ADDH) (Vqbu a, Vohf b);
INLINE(Vohf,VQBI_ADDH) (Vqbi a, Vohf b);
INLINE(Vohf,VQBC_ADDH) (Vqbc a, Vohf b);

*/

INLINE(Vqhf,VQHU_ADDH) (Vqhu a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vaddq_f16(vcvtq_f16_u16(a), b);
#else
    return vcombine_f16(
        VDHU_ADDH(vget_low_u16(a),  vget_low_f16(b)),
        VDHU_ADDH(vget_high_u16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHI_ADDH) (Vqhi a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vaddq_f16(vcvtq_f16_s16(a), b);
#else
    return vcombine_f16(
        VDHI_ADDH(vget_low_s16(a),  vget_low_f16(b)),
        VDHI_ADDH(vget_high_s16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHF_ADDH) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vaddq_f16(a, b);
#else
    return vcombine_f16(
        VDHF_ADDH(vget_low_f16(a),  vget_low_f16(b)),
        VDHF_ADDH(vget_high_f16(a), vget_high_f16(b))
    );
#endif

}


INLINE(Vdhf,VQWU_ADDH) (Vqwu a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_u32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vaddq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vdhf,VQWI_ADDH) (Vqwi a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_s32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vaddq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vqwf,VQWF_ADDH) (Vqwf a, Vdhf b)
{
    return  vaddq_f32(a, vcvt_f32_f16(b));
}

INLINE(Vwhf,VQDU_ADDH) (Vqdu a, Vwhf b)
{
    return  VWHF_ADDH(VQDU_CVHF(a), b);
}

INLINE(Vwhf,VQDI_ADDH) (Vqdi a, Vwhf b)
{
    return  VWHF_ADDH(VQDI_CVHF(a), b);
}

INLINE(Vqdf,VQDF_ADDH) (Vqdf a, Vwhf b)
{
    return  vaddq_f64(a, VWHF_CVDF(b));
}

#if 0 // _LEAVE_ARM_ADDH
}
#endif

#if 0 // _ENTER_ARM_ADDW
{
#endif

INLINE(float,  BOOL_ADDW)    (_Bool a, float b) {return b+a;}

INLINE(float, UCHAR_ADDW) (unsigned a, float b) 
{
#define     UCHAR_ADDW(A, B) ((uchar) A+(float) B)
    return  (uchar) a+b;
}

INLINE(float, SCHAR_ADDW)   (signed a, float b) 
{
#define     SCHAR_ADDW(A, B) ((schar) A+(float) B)
    return (schar) a+b;
}

INLINE(float,  CHAR_ADDW)       (int a, float b) 
{
#if CHAR_MIN
#   define     CHAR_ADDW(A, B) ((schar) A+(float) B)
#else
#   define     CHAR_ADDW(A, B) ((uchar) A+(float) B)
#endif
    return  CHAR_ADDW(a, b);
}

INLINE(float, USHRT_ADDW)  (unsigned a, float b) 
{
#define     USHRT_ADDW(A, B) ((ushort) A+(float) B)
    return  USHRT_ADDW(a, b);
}

INLINE(float,  SHRT_ADDW)    (signed a, float b) 
{
#define     SHRT_ADDW(A, B) ((short) A+(float) B)
    return  SHRT_ADDW(a, b);
}

INLINE(float,  UINT_ADDW)      (uint a, float b) 
{
#define     UINT_ADDW(A, B) ((uint) A+(float) B)
    return  UINT_ADDW(a, b);
}

INLINE(float,   INT_ADDW)       (int a, float b) 
{
#define     INT_ADDW(A, B) ((int) A+(float) B)
    return  INT_ADDW(a, b);
}

INLINE(float, ULONG_ADDW)     (ulong a, float b) 
{
#define     ULONG_ADDW(A, B) ((ulong) A+(float) B)
    return  ULONG_ADDW(a, b);
}

INLINE(float,  LONG_ADDW)      (long a, float b) 
{
#define     LONG_ADDW(A, B) ((long) A+(float) B)
    return  LONG_ADDW(a, b);
}

INLINE(float, ULLONG_ADDW)   (ullong a, float b) 
{
#define     ULLONG_ADDW(A, B) ((ullong) A+(float) B)
    return  ULLONG_ADDW(a, b);
}

INLINE(float,  LLONG_ADDW)    (llong a, float b) 
{
#define     LLONG_ADDW(A, B) ((llong) A+(float) B)
    return  LLONG_ADDW(a, b);
}

INLINE(float, FLT16_ADDW) (flt16_t a, float b) {return a+b;}
INLINE(float,   FLT_ADDW)   (float a, float b) {return a+b;}
INLINE(double,  DBL_ADDW)  (double a, float b) {return a+b;}

#if QUAD_NLLONG == 2
INLINE(float,addwqu)   (QUAD_UTYPE a, float b) {return a+b;}
INLINE(float,addwqi)   (QUAD_ITYPE a, float b) {return a+b;}
INLINE(QUAD_FTYPE,addwqf) (QUAD_FTYPE a, float b) {return a+b;}
#endif


INLINE(Vqwf,VWBU_ADDW) (Vwbu a, Vqwf b)
{
    return  vaddq_f32(VWBU_CVWF(a), b);
}

INLINE(Vqwf,VWBI_ADDW) (Vwbi a, Vqwf b)
{
    return  vaddq_f32(VWBI_CVWF(a), b);
}

INLINE(Vqwf,VWBC_ADDW) (Vwbc a, Vqwf b)
{
#if CHAR_MIN
    return  VWBI_ADDW(VWBC_ASBI(a), b);
#else
    return  VWBU_ADDW(VWBC_ASBU(a), b);
#endif
}



INLINE(Vdwf,VWHU_ADDW) (Vwhu a, Vdwf b)
{
    return  vadd_f32(VWHU_CVWF(a), b);
}

INLINE(Vdwf,VWHI_ADDW) (Vwhi a, Vdwf b)
{
    return  vadd_f32(VWHI_CVWF(a), b);
}

INLINE(Vdwf,VWHF_ADDW) (Vwhf a, Vdwf b)
{
    return  vadd_f32(VWHF_CVWF(a), b);
}


INLINE(Vwwf,VWWU_ADDW) (Vwwu a, Vwwf b)
{
    return WWF_ASTV((VWWU_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWI_ADDW) (Vwwi a, Vwwf b)
{
    return WWF_ASTV((VWWI_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWF_ADDW) (Vwwf a, Vwwf b)
{
    return WWF_ASTV((VWWF_ASTM(a)+VWWF_ASTM(b)));
}


INLINE(Vqwf,VDHU_ADDW) (Vdhu a, Vqwf b) 
{
    return  vaddq_f32(VDHU_CVWF(a), b);
}

INLINE(Vqwf,VDHI_ADDW) (Vdhi a, Vqwf b) 
{
    return  vaddq_f32(VDHI_CVWF(a), b);
}

INLINE(Vqwf,VDHF_ADDW) (Vdhf a, Vqwf b) 
{
    return  vaddq_f32(VDHF_CVWF(a), b);
}


INLINE(Vdwf,VDWU_ADDW) (Vdwu a, Vdwf b)
{
    return vadd_f32(vcvt_f32_u32(a), b);
}

INLINE(Vdwf,VDWI_ADDW) (Vdwi a, Vdwf b)
{
    return vadd_f32(vcvt_f32_s32(a), b);
}

INLINE(Vdwf,VDWF_ADDW) (Vdwf a, Vdwf b)
{
    return vadd_f32(a, b);
}


INLINE(Vwwf,VDDU_ADDW) (Vddu a, Vwwf b)
{
    return WWF_ASTV((VDDU_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vwwf,VDDI_ADDW) (Vddi a, Vwwf b)
{
    return WWF_ASTV((VDDI_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vddf,VDDF_ADDW) (Vddf a, Vwwf b)
{
    return vadd_f64(a, vdup_n_f64(VWWF_ASTM(b)));
}


INLINE(Vqwf,VQWU_ADDW) (Vqwu a, Vqwf b)
{
    return  vaddq_f32(vcvtq_f32_u32(a), b);
}

INLINE(Vqwf,VQWI_ADDW) (Vqwi a, Vqwf b)
{
    return  vaddq_f32(vcvtq_f32_s32(a), b);
}

INLINE(Vqwf,VQWF_ADDW) (Vqwf a, Vqwf b)
{
    return  vaddq_f32(a, b);
}


INLINE(Vdwf,VQDU_ADDW) (Vqdu a, Vdwf b)
{
    return  vadd_f32(vcvt_f32_f64(vcvtq_f64_u64(a)), b);
}

INLINE(Vdwf,VQDI_ADDW) (Vqdi a, Vdwf b)
{
    return  vadd_f32(vcvt_f32_f64(vcvtq_f64_s64(a)), b);
}

INLINE(Vqdf,VQDF_ADDW) (Vqdf a, Vdwf b)
{
    return  vaddq_f64(a, vcvt_f64_f32(b));
}

#if 0 // _LEAVE_ARM_ADDW
}
#endif

#if 0 // _ENTER_ARM_ADDD
{
#endif

INLINE(double,  BOOL_ADDD)    (_Bool a, double b) {return b+a;}

INLINE(double, UCHAR_ADDD) (unsigned a, double b) 
{
#define     UCHAR_ADDD(A, B) ((uchar) A+(double) B)
    return  (uchar) a+b;
}

INLINE(double, SCHAR_ADDD)   (signed a, double b) 
{
#define     SCHAR_ADDD(A, B) ((schar) A+(double) B)
    return (schar) a+b;
}

INLINE(double,  CHAR_ADDD)       (int a, double b) 
{
#if CHAR_MIN
#   define     CHAR_ADDD(A, B) ((schar) A+(double) B)
#else
#   define     CHAR_ADDD(A, B) ((uchar) A+(double) B)
#endif
    return  CHAR_ADDD(a, b);
}

INLINE(double, USHRT_ADDD)  (unsigned a, double b) 
{
#define     USHRT_ADDD(A, B) ((ushort) A+(double) B)
    return  USHRT_ADDD(a, b);
}

INLINE(double,  SHRT_ADDD)    (signed a, double b) 
{
#define     SHRT_ADDD(A, B) ((short) A+(double) B)
    return  SHRT_ADDD(a, b);
}

INLINE(double,  UINT_ADDD)      (uint a, double b) 
{
#define     UINT_ADDD(A, B) ((uint) A+(double) B)
    return  UINT_ADDD(a, b);
}

INLINE(double,   INT_ADDD)       (int a, double b) 
{
#define     INT_ADDD(A, B) ((int) A+(double) B)
    return  INT_ADDD(a, b);
}

INLINE(double, ULONG_ADDD)     (ulong a, double b) 
{
#define     ULONG_ADDD(A, B) ((ulong) A+(double) B)
    return  ULONG_ADDD(a, b);
}

INLINE(double,  LONG_ADDD)      (long a, double b) 
{
#define     LONG_ADDD(A, B) ((long) A+(double) B)
    return  LONG_ADDD(a, b);
}

INLINE(double, ULLONG_ADDD)   (ullong a, double b) 
{
#define     ULLONG_ADDD(A, B) ((ullong) A+(double) B)
    return  ULLONG_ADDD(a, b);
}

INLINE(double,  LLONG_ADDD)    (llong a, double b) 
{
#define     LLONG_ADDD(A, B) ((llong) A+(double) B)
    return  LLONG_ADDD(a, b);
}

INLINE(double, FLT16_ADDD) (flt16_t a, double b) {return a+b;}
INLINE(double,   FLT_ADDD)   (float a, double b) {return a+b;}
INLINE(double,  DBL_ADDD)  (double a, double b) {return a+b;}

#if QUAD_NLLONG == 2
INLINE(double,adddqu)   (QUAD_UTYPE a, double b) {return a+b;}
INLINE(double,adddqi)   (QUAD_ITYPE a, double b) {return a+b;}
INLINE(QUAD_FTYPE,adddqf) (QUAD_FTYPE a, double b) {return a+b;}
#endif


INLINE(Vqdf,VWHU_ADDD) (Vwhu a, Vqdf b)
{
    return  vaddq_f64(VWHU_CVDF(a), b);
}

INLINE(Vqdf,VWHI_ADDD) (Vwhi a, Vqdf b)
{
    return  vaddq_f64(VWHI_CVDF(a), b);
}

INLINE(Vqdf,VWHF_ADDD) (Vwhf a, Vqdf b)
{
    return  vaddq_f64(VWHF_CVDF(a), b);
}


INLINE(Vddf,VWWU_ADDD) (Vwwu a, Vddf b)
{
    return  vadd_f64(VWWU_CVDF(a), b);
}

INLINE(Vddf,VWWI_ADDD) (Vwwi a, Vddf b)
{
    return  vadd_f64(VWWI_CVDF(a), b);
}

INLINE(Vddf,VWWF_ADDD) (Vwwf a, Vddf b)
{
    return  vadd_f64(VWWF_CVDF(a), b);
}


INLINE(Vqdf,VDWU_ADDD) (Vdwu a, Vqdf b)
{
    return  vaddq_f64(vcvtq_f64_u64(vmovl_u32(a)), b);
}

INLINE(Vqdf,VDWI_ADDD) (Vdwi a, Vqdf b)
{
    return  vaddq_f64(vcvtq_f64_s64(vmovl_s32(a)), b);
}

INLINE(Vqdf,VDWF_ADDD) (Vdwf a, Vqdf b)
{
    return  vaddq_f64(vcvt_f64_f32(a), b);
}


INLINE(Vddf,VDDU_ADDD) (Vddu a, Vddf b)
{
    return  vadd_f64(vcvt_f64_u64(a), b);
}

INLINE(Vddf,VDDI_ADDD) (Vddi a, Vddf b)
{
    return  vadd_f64(vcvt_f64_s64(a), b);
}

INLINE(Vddf,VDDF_ADDD) (Vddf a, Vddf b)
{
    return  vadd_f64(a, b);
}


INLINE(Vqdf,VQDU_ADDD) (Vqdu a, Vqdf b)
{
    return  vaddq_f64(vcvtq_f64_u64(a), b);
}

INLINE(Vqdf,VQDI_ADDD) (Vqdi a, Vqdf b)
{
    return  vaddq_f64(vcvtq_f64_s64(a), b);
}

INLINE(Vqdf,VQDF_ADDD) (Vqdf a, Vqdf b)
{
    return  vaddq_f64(a, b);
}

#if 0 // _LEAVE_ARM_ADDD
}
#endif

#if 0 // _ENTER_ARM_ICRL
{
#endif



INLINE(Vwyu,VWYU_ICRL) (Vwyu a)
{
    int32x2_t   z = vdup_n_s32(-1);
    float32x2_t f = vreinterpret_f32_s32(z);
    float       b = vget_lane_f32(f, 0);
    return  WYU_ASTV(FLT_XORS(VWYU_ASTM(a), b));
}

INLINE(Vwbu,VWBU_ICRL) (Vwbu a)
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    az = vadd_u8(az, vdup_n_u8(1));
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_ICRL) (Vwbi a)
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    az = vadd_s8(az, vdup_n_s8(1));
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_ICRL) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ICRL(VWBC_ASBI(a)));
#else
    return  VWBU_ASBC(VWBU_ICRL(VWBC_ASBU(a)));
#endif
}


INLINE(Vwhu,VWHU_ICRL) (Vwhu a)
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    az = vadd_u16(az, vdup_n_u16(1));
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_ICRL) (Vwhi a)
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    az = vadd_s16(az, vdup_n_s16(1));
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}

INLINE(Vwwu,VWWU_ICRL) (Vwwu a)
{
    return  UINT32_ASTV((1u+VWWU_ASTV(a)));
}

INLINE(Vwwi,VWWI_ICRL) (Vwwi a)
{
    return  INT32_ASTV((1+VWWI_ASTV(a)));
}


INLINE(Vdyu,VDYU_ICRL) (Vdyu a)
{
#define     VDYU_ICRL(A) DYU_ASTV(veor_u64(VDYU_ASTM(A),vdup_n_u64(~0ull)))
    return  VDYU_ICRL(a);
}

INLINE(Vdbu,VDBU_ICRL) (Vdbu a) {return vadd_u8(a,vdup_n_u8(1));}
INLINE(Vdbi,VDBI_ICRL) (Vdbi a) {return vadd_s8(a,vdup_n_u8(1));}
INLINE(Vdbc,VDBC_ICRL) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_ICRL(A) VDBI_ASBC(vadd_s8(VDBC_ASBI(A),vdup_n_s8(1)))
#else
#   define  VDBC_ICRL(A) VDBU_ASBC(vadd_u8(VDBC_ASBU(A),vdup_n_u8(1)))
#endif
    return  VDBC_ICRL(a);
}

INLINE(Vdhu,VDHU_ICRL) (Vdhu a) {return vadd_u16(a,vdup_n_u16(1));}
INLINE(Vdhi,VDHI_ICRL) (Vdhi a) {return vadd_s16(a,vdup_n_s16(1));}
INLINE(Vdwu,VDWU_ICRL) (Vdwu a) {return vadd_u32(a,vdup_n_u32(1));}
INLINE(Vdwi,VDWI_ICRL) (Vdwi a) {return vadd_s32(a,vdup_n_s32(1));}
INLINE(Vddu,VDDU_ICRL) (Vddu a) {return vadd_u64(a,vdup_n_u64(1));}
INLINE(Vddi,VDDI_ICRL) (Vddi a) {return vadd_s64(a,vdup_n_s64(1));}

INLINE(Vqyu,VQYU_ICRL) (Vqyu a)
{
#define     VQYU_ICRL(A) QYU_ASTV(veorq_u64(VQYU_ASTM(A),vdupq_n_u64(~0ull)))
    return  VQYU_ICRL(a);
}

INLINE(Vqbu,VQBU_ICRL) (Vqbu a) {return vaddq_u8(a,vdupq_n_u8(1));}
INLINE(Vqbi,VQBI_ICRL) (Vqbi a) {return vaddq_s8(a,vdupq_n_u8(1));}
INLINE(Vqbc,VQBC_ICRL) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_ICRL(A) VQBI_ASBC(vaddq_s8(VQBC_ASBI(A),vdupq_n_s8(1)))
#else
#   define  VQBC_ICRL(A) VQBU_ASBC(vaddq_u8(VQBC_ASBU(A),vdupq_n_u8(1)))
#endif
    return  VQBC_ICRL(a);
}

INLINE(Vqhu,VQHU_ICRL) (Vqhu a) {return vaddq_u16(a,vdupq_n_u16(1));}
INLINE(Vqhi,VQHI_ICRL) (Vqhi a) {return vaddq_s16(a,vdupq_n_s16(1));}
INLINE(Vqwu,VQWU_ICRL) (Vqwu a) {return vaddq_u32(a,vdupq_n_u32(1));}
INLINE(Vqwi,VQWI_ICRL) (Vqwi a) {return vaddq_s32(a,vdupq_n_s32(1));}
INLINE(Vqdu,VQDU_ICRL) (Vqdu a) {return vaddq_u64(a,vdupq_n_u64(1));}
INLINE(Vqdi,VQDI_ICRL) (Vqdi a) {return vaddq_s64(a,vdupq_n_s64(1));}

#if 0 // _LEAVE_ARM_ICRL
}
#endif

#if 0 // _ENTER_ARM_AVGL
{
#endif
/*
AVeraGe (truncated)

For each corresponding pair of elements, compute the mean.
Fractional results are rounded by truncation.

    -4  = -2.0 = -2
    -3  = -1.5 = -1
    -2  = -1.0 = -1
    -1  = -0.5 = -0
    +0  = +0.0 = +0
    +1  = +0.5 = +0
    +2  = +1.0 = +1
    +3  = +1.5 = +1
    +4  = +2.0 = +2

UHADD is directly equivalent to AVGL. SHADD, however, is
equivalent to to AVGN.

*/

#define     DBU_AVGL    vhadd_u8

INLINE(int8x8_t,DBI_AVGL) (int8x8_t a, int8x8_t b)
{
    DWRD_VTYPE  n, p, v;
    QUAD_VTYPE  x, y, z;
    x.H.I   = vaddl_s8(a, b);
    y.H.I   = vabsq_s16(x.H.I); 
    z.H.U   = vceqq_s16(x.H.I, y.H.I);
    y.H.U   = vshrq_n_u16(y.H.U, 1);
    v.B.I   = vmovn_s16(y.H.I);
    p.B.U   = vmovn_u16(z.H.U);
    n.B.U   = vmvn_u8(p.B.U);
    p.B.U   = vshr_n_u8(p.B.U, 7);
    n.B.U   = vorr_u8(p.B.U, n.B.U);
    return  vmul_s8(v.B.I, n.B.I);
}

#if CHAR_MIN
#   define  DBC_AVGL DBI_AVGL
#else
#   define  DBC_AVGL DBU_AVGL
#endif


#define     DHU_AVGL vhadd_u16

INLINE(int16x4_t,DHI_AVGL) (int16x4_t a, int16x4_t b)
{
    DWRD_VTYPE  n, p, v;
    QUAD_VTYPE  x, y, z;
    x.W.I   = vaddl_s16(a, b);
    y.W.I   = vabsq_s32(x.W.I); 
    z.W.U   = vceqq_s32(x.W.I, y.W.I);
    y.W.U   = vshrq_n_u32(y.W.U, 1);
    v.H.I   = vmovn_s32(y.W.I);
    p.H.U   = vmovn_u32(z.W.U);
    n.H.U   = vmvn_u16(p.H.U);
    p.H.U   = vshr_n_u16(p.H.U, 15);
    n.H.U   = vorr_u8(p.H.U, n.H.U);
    return  vmul_s16(v.H.I, n.H.I);
}

#define     DWU_AVGL vhadd_u32
INLINE(int32x2_t,DWI_AVGL) (int32x2_t a, int32x2_t b)
{
    DWRD_VTYPE  n, p, v;
    QUAD_VTYPE  x, y, z;
    x.D.I   = vaddl_s32(a, b);
    y.D.I   = vabsq_s64(x.D.I); 
    z.D.U   = vceqq_s64(x.D.I, y.D.I);
    y.D.U   = vshrq_n_u64(y.D.U, 1);
    v.W.I   = vmovn_s64(y.D.I);
    p.W.U   = vmovn_u64(z.D.U);
    n.W.U   = vmvn_u32(p.W.U);
    p.W.U   = vshr_n_u32(p.W.U, 31);
    n.W.U   = vorr_u16(p.W.U, n.W.U);
    return  vmul_s32(v.W.I, n.W.I);
}

INLINE(uint64x1_t,DDU_AVGL) (uint64x1_t a, uint64x1_t b)
{
    uint64x1_t c = vand_u64(a, b);
    c = vand_u64(c, vdup_n_u64(1));
    a = vshr_n_u64(a, 1);
    b = vshr_n_u64(b, 1);
    a = vadd_u64(a, b);
    c = vadd_u64(a, c);
    return  c;
}

INLINE(int64x1_t,DDI_AVGL) (int64x1_t a, int64x1_t b)
{
    int64_t l = vget_lane_s64(a, 0);
    int64_t r = vget_lane_s64(b, 0);
    l = INT64_AVGL(l, r);
    return  vdup_n_s64(l);
}

#define QBU_AVGL vhaddq_u8
INLINE(int8x16_t,QBI_AVGL) (int8x16_t a, int8x16_t b)
{
    int8x8_t l = DBI_AVGL(vget_low_s8(a), vget_low_s8(b));
    int8x8_t r = DBI_AVGL(vget_high_s8(a), vget_high_s8(b));
    return  vcombine_s8(l, r);
}

#if CHAR_MIN
#   define  DBC_AVGL DBI_AVGL
#   define  QBC_AVGL QBI_AVGL
#else
#   define  DBC_AVGL DBU_AVGL
#   define  QBC_AVGL QBU_AVGL
#endif

#define QHU_AVGL vhaddq_u16
INLINE(int16x8_t,QHI_AVGL) (int16x8_t a, int16x8_t b)
{
    int16x4_t l = DHI_AVGL(vget_low_s16( a), vget_low_s16( b));
    int16x4_t r = DHI_AVGL(vget_high_s16(a), vget_high_s16(b));
    return  vcombine_s16(l, r);
}

#define QWU_AVGL vhaddq_u32
INLINE(int32x4_t,QWI_AVGL) (int32x4_t a, int32x4_t b)
{
    int32x2_t l = DWI_AVGL(vget_low_s32( a), vget_low_s32( b));
    int32x2_t r = DWI_AVGL(vget_high_s32(a), vget_high_s32(b));
    return  vcombine_s32(l, r);
}

INLINE(uint64x2_t,QDU_AVGL) (uint64x2_t a, uint64x2_t b)
{
    uint64x1_t l = DDU_AVGL(vget_low_u64( a), vget_low_u64( b));
    uint64x1_t r = DDU_AVGL(vget_high_u64(a), vget_high_u64(b));
    return  vcombine_u64(l, r);
}

INLINE(int64x2_t,QDI_AVGL) (int64x2_t a, int64x2_t b)
{
    int64x1_t l = DDI_AVGL(vget_low_s64( a), vget_low_s64( b));
    int64x1_t r = DDI_AVGL(vget_high_s64(a), vget_high_s64(b));
    return  vcombine_s64(l, r);
}

INLINE(Vwbu,VWBU_AVGL) (Vwbu a, Vwbu b) 
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.B.U   = DBU_AVGL(x.B.U, y.B.U);
    a.V0    = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwbi,VWBI_AVGL) (Vwbi a, Vwbi b) 
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.B.I   = DBI_AVGL(x.B.I, y.B.I);
    a.V0    = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwbc,VWBC_AVGL) (Vwbc a, Vwbc b) 
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
#if CHAR_MIN
    x.B.I = DBI_AVGL(x.B.I, y.B.I);
#else
    x.B.U = DBU_AVGL(x.B.U, y.B.U);
#endif
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}


INLINE(Vwhu,VWHU_AVGL) (Vwhu a, Vwhu b) 
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.H.U   = DHU_AVGL(x.H.U, y.H.U);
    a.V0    = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwhi,VWHI_AVGL) (Vwhi a, Vwhi b) 
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.H.I   = DHI_AVGL(x.H.I, y.H.I);
    a.V0    = vget_lane_f32(x.W.F, 0);
    return  a;
}


INLINE(Vwwu,VWWU_AVGL) (Vwwu a, Vwwu b) 
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.W.U   = DHU_AVGL(x.W.U, y.W.U);
    a.V0    = vget_lane_f32(x.W.F, 0);
    return  a;
}

INLINE(Vwwi,VWWI_AVGL) (Vwwi a, Vwwi b) 
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.W.I   = DHI_AVGL(x.W.I, y.W.I);
    a.V0    = vget_lane_f32(x.W.F, 0);
    return  a;
}


INLINE(Vdbu,VDBU_AVGL) (Vdbu a, Vdbu b) {return DBU_AVGL(a, b);}
INLINE(Vdbi,VDBI_AVGL) (Vdbi a, Vdbi b) {return DBI_AVGL(a, b);}
INLINE(Vdbc,VDBC_AVGL) (Vdbc a, Vdbc b)
{
    a.V0 = DBC_AVGL(a.V0, b.V0);
    return a;
}

INLINE(Vdhu,VDHU_AVGL) (Vdhu a, Vdhu b) {return DHU_AVGL(a, b);}
INLINE(Vdhi,VDHI_AVGL) (Vdhi a, Vdhi b) {return DHI_AVGL(a, b);}
INLINE(Vdwu,VDWU_AVGL) (Vdwu a, Vdwu b) {return DWU_AVGL(a, b);}
INLINE(Vdwi,VDWI_AVGL) (Vdwi a, Vdwi b) {return DWI_AVGL(a, b);}
INLINE(Vddu,VDDU_AVGL) (Vddu a, Vddu b) {return DDU_AVGL(a, b);}
INLINE(Vddi,VDDI_AVGL) (Vddi a, Vddi b) {return DDI_AVGL(a, b);}

INLINE(Vqbu,VQBU_AVGL) (Vqbu a, Vqbu b) {return QBU_AVGL(a, b);}
INLINE(Vqbi,VQBI_AVGL) (Vqbi a, Vqbi b) {return QBI_AVGL(a, b);}

INLINE(Vqbc,VQBC_AVGL) (Vqbc a, Vqbc b)
{
    a.V0 = QBC_AVGL(a.V0, b.V0);
    return a;
}

INLINE(Vqhu,VQHU_AVGL) (Vqhu a, Vqhu b) {return QHU_AVGL(a, b);}
INLINE(Vqhi,VQHI_AVGL) (Vqhi a, Vqhi b) {return QHI_AVGL(a, b);}
INLINE(Vqwu,VQWU_AVGL) (Vqwu a, Vqwu b) {return QWU_AVGL(a, b);}
INLINE(Vqwi,VQWI_AVGL) (Vqwi a, Vqwi b) {return QWI_AVGL(a, b);}
INLINE(Vqdu,VQDU_AVGL) (Vqdu a, Vqdu b) {return QDU_AVGL(a, b);}
INLINE(Vqdi,VQDI_AVGL) (Vqdi a, Vqdi b) {return QDI_AVGL(a, b);}
/*  TODO: optimize avglqq */
INLINE(Vqqu,VQQU_AVGL) (Vqqu a, Vqqu b) 
{
    QUAD_VTYPE x={.Q.U=a}, y={.Q.U=b};
    x.U = avglqu(x.U, y.U);
    return  x.Q.U;
}

INLINE(Vqqi,VQQI_AVGL) (Vqqi a, Vqqi b) 
{
    QUAD_VTYPE x={.Q.I=a}, y={.Q.I=b};
    x.I = avglqi(x.I, y.I);
    return  x.Q.I;
}

#if 0 // _LEAVE_ARM_AVGL
}
#endif

#if 0 // _ENTER_ARM_AVGP
{
#endif

/*
    -1.5 = -1
    -0.5 = -0
    +0.5 = +0
    +1.5 = +1
*/
#define     DBU_AVGP vrhadd_u8
#define     DBI_AVGP vrhadd_s8

#if CHAR_MIN
#   define  DBC_AVGP DBI_AVGP
#else
#   define  DBC_AVGP DBU_AVGP
#endif

#define     DHU_AVGP vrhadd_u16
#define     DHI_AVGP vrhadd_s16

#define     DWU_AVGP vrhadd_u32
#define     DWI_AVGP vrhadd_s32

#if 0
INLINE(int8x8_t,DBI_AVGP) (int8x8_t a, int8x8_t b)
{
    int8x8_t c = vorr_s8(a, b);
    c = vand_s8(c, vdup_n_s8(1));
    a = vshr_n_s8(a, 1);
    b = vshr_n_s8(b, 1);
    a = vadd_s8(a, b);
    return  vadd_s8(a, c);
}

INLINE(int8x8_t,DBI_AVGP) (int8x8_t a, int8x8_t b)
{
    int8x8_t c = vorr_s8(a, b);
    c = vand_s8(c, vdup_n_s8(1));
    a = vshr_n_s8(a, 1);
    b = vshr_n_s8(b, 1);
    a = vadd_s8(a, b);
    return  vadd_s8(a, c);
}

INLINE(int16x4_t,DHI_AVGP) (int16x4_t a, int16x4_t b)
{
    int16x4_t c = vorr_s16(a, b);
    c = vand_s16(c, vdup_n_s16(1));
    a = vshr_n_s16(a, 1);
    b = vshr_n_s16(b, 1);
    a = vadd_s16(a, b);
    return  vadd_s16(a, c);
}

INLINE(int32x2_t,DWI_AVGP) (int32x2_t a, int32x2_t b)
{
    int32x2_t c = vorr_s32(a, b);
    c = vand_s32(c, vdup_n_s32(1));
    a = vshr_n_s32(a, 1);
    b = vshr_n_s32(b, 1);
    a = vadd_s32(a, b);
    return  vadd_s32(a, c);
}

#endif

INLINE(uint64x1_t,DDU_AVGP) (uint64x1_t a, uint64x1_t b)
{
    uint64x1_t c = vorr_u64(a, b);
    c = vand_u64(c, vdup_n_u64(1));
    a = vshr_n_u64(a, 1);
    b = vshr_n_u64(b, 1);
    a = vadd_u64(a, b);
    return  vadd_u64(a, c);
}

INLINE( int64x1_t,DDI_AVGP)  (int64x1_t a,  int64x1_t b)
{
    int64x1_t c = vorr_s64(a, b);
    c = vand_s64(c, vdup_n_s64(1));
    a = vshr_n_s64(a, 1);
    b = vshr_n_s64(b, 1);
    a = vadd_s64(a, b);
    return  vadd_s64(a, c);
}

#define     QBU_AVGP vrhaddq_u8
#define     QBI_AVGP vrhaddq_s8

#if CHAR_MIN
#   define  QBC_AVGP QBI_AVGP
#else
#   define  QBC_AVGP QBU_AVGP
#endif

#define     QHU_AVGP vrhaddq_u16
#define     QHI_AVGP vrhaddq_s16
#define     QWU_AVGP vrhaddq_u32
#define     QWI_AVGP vrhaddq_s32

INLINE(uint64x2_t,QDU_AVGP) (uint64x2_t a, uint64x2_t b)
{
    uint64x2_t c = vorrq_u64(a, b);
    c = vandq_u64(c, vdupq_n_u64(1));
    a = vshrq_n_u64(a, 1);
    b = vshrq_n_u64(b, 1);
    a = vaddq_u64(a, b);
    return  vaddq_u64(a, c);
}

INLINE( int64x2_t,QDI_AVGP)  (int64x2_t a,  int64x2_t b)
{
    int64x2_t c = vorrq_s64(a, b);
    c = vandq_s64(c, vdupq_n_s64(1));
    a = vshrq_n_s64(a, 1);
    b = vshrq_n_s64(b, 1);
    a = vaddq_s64(a, b);
    return  vaddq_s64(a, c);
}

INLINE(Vwbu,VWBU_AVGP) (Vwbu a, Vwbu b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.B.U = DBU_AVGP(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbi,VWBI_AVGP) (Vwbi a, Vwbi b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.B.I = DBI_AVGP(x.B.I, y.B.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbc,VWBC_AVGP) (Vwbc a, Vwbc b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
#if CHAR_MIN
    x.B.I = DBI_AVGP(x.B.I, y.B.I);
#else
    x.B.U = DBU_AVGP(x.B.U, y.B.U);
#endif
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhu,VWHU_AVGP) (Vwhu a, Vwhu b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.H.U = DHU_AVGP(x.H.U, y.H.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHI_AVGP) (Vwhi a, Vwhi b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.H.I = DHI_AVGP(x.H.I, y.H.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwwu,VWWU_AVGP) (Vwwu a, Vwwu b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.W.U = DWU_AVGP(x.W.U, y.W.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwwi,VWWI_AVGP) (Vwwi a, Vwwi b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.W.I = DWI_AVGP(x.W.I, y.W.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vdbu,VDBU_AVGP) (Vdbu a, Vdbu b) {return DBU_AVGP(a, b);}
INLINE(Vdbi,VDBI_AVGP) (Vdbi a, Vdbi b) {return DBI_AVGP(a, b);}
INLINE(Vdbc,VDBC_AVGP) (Vdbc a, Vdbc b) 
{
    a.V0 = DBC_AVGP(a.V0, b.V0);
    return  a;
}

INLINE(Vdhu,VDHU_AVGP) (Vdhu a, Vdhu b) {return DHU_AVGP(a, b);}
INLINE(Vdhi,VDHI_AVGP) (Vdhi a, Vdhi b) {return DHI_AVGP(a, b);}
INLINE(Vdwu,VDWU_AVGP) (Vdwu a, Vdwu b) {return DWU_AVGP(a, b);}
INLINE(Vdwi,VDWI_AVGP) (Vdwi a, Vdwi b) {return DWI_AVGP(a, b);}
INLINE(Vddu,VDDU_AVGP) (Vddu a, Vddu b) {return DDU_AVGP(a, b);}
INLINE(Vddi,VDDI_AVGP) (Vddi a, Vddi b) {return DDI_AVGP(a, b);}
INLINE(Vqbu,VQBU_AVGP) (Vqbu a, Vqbu b) {return QBU_AVGP(a, b);}
INLINE(Vqbi,VQBI_AVGP) (Vqbi a, Vqbi b) {return QBI_AVGP(a, b);}
INLINE(Vqbc,VQBC_AVGP) (Vqbc a, Vqbc b) 
{
    a.V0 = QBC_AVGP(a.V0, b.V0);
    return  a;
}

INLINE(Vqhu,VQHU_AVGP) (Vqhu a, Vqhu b) {return QHU_AVGP(a, b);}
INLINE(Vqhi,VQHI_AVGP) (Vqhi a, Vqhi b) {return QHI_AVGP(a, b);}
INLINE(Vqwu,VQWU_AVGP) (Vqwu a, Vqwu b) {return QWU_AVGP(a, b);}
INLINE(Vqwi,VQWI_AVGP) (Vqwi a, Vqwi b) {return QWI_AVGP(a, b);}
INLINE(Vqdu,VQDU_AVGP) (Vqdu a, Vqdu b) {return QDU_AVGP(a, b);}
INLINE(Vqdi,VQDI_AVGP) (Vqdi a, Vqdi b) {return QDI_AVGP(a, b);}


#if 0 // _LEAVE_ARM_AVGP
}
#endif

#if 0 // _ENTER_ARM_AVGN
{
#endif

/*  AVeraGe (rounded to -inf)
    -1.5 = -2
    -0.5 = -1
    +0.5 = +0
    +1.5 = +1
*/

#define     DBU_AVGN DBU_AVGL
#define     DBI_AVGN vhadd_s8

#if CHAR_MIN
#   define  DBC_AVGN DBI_AVGN
#else
#   define  DBC_AVGN DBU_AVGN
#endif

#define     DHU_AVGN DHU_AVGL
#define     DHI_AVGN vhadd_s16

#define     DWU_AVGN DDU_AVGL
#define     DWI_AVGN vhadd_s32

#define     DDU_AVGN DDU_AVGL

INLINE( int64x1_t,DDI_AVGN)  (int64x1_t a,  int64x1_t b)
{
    int64x1_t c = vand_s64(a, b);
    c = vand_s64(c, vdup_n_s64(1));
    a = vshr_n_s64(a, 1);
    b = vshr_n_s64(b, 1);
    b = vadd_s64(b, c);
    return  vadd_s64(a, b);
}

#define     QBU_AVGN vrhaddq_u8
#define     QBI_AVGN vrhaddq_s8

#if CHAR_MIN
#   define  QBC_AVGN QBI_AVGN
#else
#   define  QBC_AVGN QBU_AVGN
#endif

#define     QHU_AVGN QHU_AVGL
#define     QHI_AVGN vhaddq_s16
#define     QWU_AVGN QWU_AVGL
#define     QWI_AVGN vhaddq_s32

#define     QDU_AVGN QDU_AVGL

INLINE( int64x2_t,QDI_AVGN)  (int64x2_t a,  int64x2_t b)
{
    int64x2_t c = vandq_s64(a, b);
    c = vandq_s64(c, vdupq_n_s64(1));
    a = vshrq_n_s64(a, 1);
    b = vshrq_n_s64(b, 1);
    b = vaddq_s64(b, c);
    return  vaddq_s64(a, b);
}

INLINE(Vwbu,VWBU_AVGN) (Vwbu a, Vwbu b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.B.U = DBU_AVGN(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbi,VWBI_AVGN) (Vwbi a, Vwbi b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.B.I = DBI_AVGN(x.B.I, y.B.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbc,VWBC_AVGN) (Vwbc a, Vwbc b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
#if CHAR_MIN
    x.B.I = DBI_AVGN(x.B.I, y.B.I);
#else
    x.B.U = DBU_AVGN(x.B.U, y.B.U);
#endif
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhu,VWHU_AVGN) (Vwhu a, Vwhu b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.H.U = DHU_AVGN(x.H.U, y.H.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHI_AVGN) (Vwhi a, Vwhi b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.H.I = DHI_AVGN(x.H.I, y.H.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwwu,VWWU_AVGN) (Vwwu a, Vwwu b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.W.U = DWU_AVGN(x.W.U, y.W.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwwi,VWWI_AVGN) (Vwwi a, Vwwi b)
{
    DWRD_VTYPE  x={.W.F={a.V0}}, y={.W.F={b.V0}};
    x.W.I = DWI_AVGN(x.W.I, y.W.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vdbu,VDBU_AVGN) (Vdbu a, Vdbu b) {return DBU_AVGN(a, b);}
INLINE(Vdbi,VDBI_AVGN) (Vdbi a, Vdbi b) {return DBI_AVGN(a, b);}
INLINE(Vdbc,VDBC_AVGN) (Vdbc a, Vdbc b) 
{
    a.V0 = DBC_AVGN(a.V0, b.V0);
    return  a;
}

INLINE(Vdhu,VDHU_AVGN) (Vdhu a, Vdhu b) {return DHU_AVGN(a, b);}
INLINE(Vdhi,VDHI_AVGN) (Vdhi a, Vdhi b) {return DHI_AVGN(a, b);}
INLINE(Vdwu,VDWU_AVGN) (Vdwu a, Vdwu b) {return DWU_AVGN(a, b);}
INLINE(Vdwi,VDWI_AVGN) (Vdwi a, Vdwi b) {return DWI_AVGN(a, b);}
INLINE(Vddu,VDDU_AVGN) (Vddu a, Vddu b) {return DDU_AVGN(a, b);}
INLINE(Vddi,VDDI_AVGN) (Vddi a, Vddi b) {return DDI_AVGN(a, b);}
INLINE(Vqbu,VQBU_AVGN) (Vqbu a, Vqbu b) {return QBU_AVGN(a, b);}
INLINE(Vqbi,VQBI_AVGN) (Vqbi a, Vqbi b) {return QBI_AVGN(a, b);}
INLINE(Vqbc,VQBC_AVGN) (Vqbc a, Vqbc b) 
{
    a.V0 = QBC_AVGN(a.V0, b.V0);
    return  a;
}

INLINE(Vqhu,VQHU_AVGN) (Vqhu a, Vqhu b) {return QHU_AVGN(a, b);}
INLINE(Vqhi,VQHI_AVGN) (Vqhi a, Vqhi b) {return QHI_AVGN(a, b);}
INLINE(Vqwu,VQWU_AVGN) (Vqwu a, Vqwu b) {return QWU_AVGN(a, b);}
INLINE(Vqwi,VQWI_AVGN) (Vqwi a, Vqwi b) {return QWI_AVGN(a, b);}
INLINE(Vqdu,VQDU_AVGN) (Vqdu a, Vqdu b) {return QDU_AVGN(a, b);}
INLINE(Vqdi,VQDI_AVGN) (Vqdi a, Vqdi b) {return QDI_AVGN(a, b);}
INLINE(Vqqu,VQQU_AVGN) (Vqqu a, Vqqu b) {return VQQU_AVGL(a, b);}
INLINE(Vqqi,VQQI_AVGN) (Vqqi a, Vqqi b) {return VQQI_AVGL(a, b);}

#if 0 // _LEAVE_ARM_AVGN
}
#endif

#if 0 // _ENTER_ARM_SUBL
{
#endif

INLINE(Vwyu,VWYU_SUBL) (Vwyu a, Vwyu b)
{
#define     VWYU_SUBL(A, B) \
WYU_ASTV(((VWWU_ASTV(VWYU_ASWU(A)))^(VWWU_ASTV(VWYU_ASWU(B)))))
    return  VWYU_SUBL(a, b);
}

INLINE(Vwbu,VWBU_SUBL) (Vwbu a, Vwbu b) 
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    float       bm = VWBU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    az = vsub_u8(az, bz);
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_SUBL) (Vwbi a, Vwbi b) 
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    float       bm = VWBI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    az = vsub_s8(az, bz);
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_SUBL) (Vwbc a, Vwbc b) 
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_SUBL(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return  VWBU_ASBC(VWBU_SUBL(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_SUBL) (Vwhu a, Vwhu b) 
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    float       bm = VWHU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    az = vsub_u16(az, bz);
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_SUBL) (Vwhi a, Vwhi b) 
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    float       bm = VWHI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    az = vsub_s16(az, bz);
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}


INLINE(Vwwu,VWWU_SUBL) (Vwwu a, Vwwu b) 
{
#define     VWWU_SUBL(A, B)  UINT32_ASTV((VWWU_ASTV(A)-VWWU_ASTV(B)))
    return  VWWU_SUBL(a, b);
}

INLINE(Vwwi,VWWI_SUBL) (Vwwi a, Vwwi b) 
{
#define     VWWI_SUBL(A, B)  \
INT32_ASTV(((unsigned) VWWI_ASTV(A)-(unsigned) VWWI_ASTV(B)))
    return  VWWI_SUBL(a, b);
}


INLINE(Vdyu,VDYU_SUBL) (Vdyu a, Vdyu b)
{
#define     VDYU_SUBL(A, B) DYU_ASTV(veor_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_SUBL(a, b);
}

INLINE(Vdbu,VDBU_SUBL) (Vdbu a, Vdbu b) {return vsub_u8(a, b);}
INLINE(Vdbi,VDBI_SUBL) (Vdbi a, Vdbi b) {return vsub_s8(a, b);}
INLINE(Vdbc,VDBC_SUBL) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_SUBL(A, B) VDBI_ASBC(vsub_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_SUBL(A, B) VDBU_ASBC(vsub_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_SUBL(a, b);
}

INLINE(Vdhu,VDHU_SUBL) (Vdhu a, Vdhu b) {return vsub_u16(a, b);}
INLINE(Vdhi,VDHI_SUBL) (Vdhi a, Vdhi b) {return vsub_s16(a, b);}
INLINE(Vdwu,VDWU_SUBL) (Vdwu a, Vdwu b) {return vsub_u32(a, b);}
INLINE(Vdwi,VDWI_SUBL) (Vdwi a, Vdwi b) {return vsub_s32(a, b);}
INLINE(Vddu,VDDU_SUBL) (Vddu a, Vddu b) {return vsub_u64(a, b);}
INLINE(Vddi,VDDI_SUBL) (Vddi a, Vddi b) {return vsub_s64(a, b);}

INLINE(Vqyu,VQYU_SUBL) (Vqyu a, Vqyu b)
{
#define     VQYU_SUBL(A, B) QYU_ASTV(veorq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_SUBL(a, b);
}

INLINE(Vqbu,VQBU_SUBL) (Vqbu a, Vqbu b) {return vsubq_u8(a, b);}
INLINE(Vqbi,VQBI_SUBL) (Vqbi a, Vqbi b) {return vsubq_s8(a, b);}
INLINE(Vqbc,VQBC_SUBL) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_SUBL(A, B) VQBI_ASBC(vsubq_s8(VQBC_ASBI(A), VQBC_ASBI(B)))
#else
#   define  VQBC_SUBL(A, B) VQBU_ASBC(vsubq_u8(VQBC_ASBU(A), VQBC_ASBU(B)))
#endif
    return  VQBC_SUBL(a, b);
}

INLINE(Vqhu,VQHU_SUBL) (Vqhu a, Vqhu b) {return vsubq_u16(a, b);}
INLINE(Vqhi,VQHI_SUBL) (Vqhi a, Vqhi b) {return vsubq_s16(a, b);}
INLINE(Vqwu,VQWU_SUBL) (Vqwu a, Vqwu b) {return vsubq_u32(a, b);}
INLINE(Vqwi,VQWI_SUBL) (Vqwi a, Vqwi b) {return vsubq_s32(a, b);}
INLINE(Vqdu,VQDU_SUBL) (Vqdu a, Vqdu b) {return vsubq_u64(a, b);}
INLINE(Vqdi,VQDI_SUBL) (Vqdi a, Vqdi b) {return vsubq_s64(a, b);}

#if 0 // _LEAVE_ARM_SUBL
}
#endif

#if 0 // _ENTER_ARM_SUB2
{
#endif

INLINE(Vdhu,VWBU_SUB2) (Vwbu a, Vwbu b)
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    uint8x8_t   l = vreinterpret_u8_f32(p);
    uint8x8_t   r = vreinterpret_u8_f32(q);
    uint16x8_t  c = vsubq_u16(vmovl_u8(l), vmovl_u8(r));
    return  vget_low_u16(c);
}

INLINE(Vdhi,VWBI_SUB2) (Vwbi a, Vwbi b)
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    int8x8_t    l = vreinterpret_s8_f32(p);
    int8x8_t    r = vreinterpret_s8_f32(q);
    int16x8_t   c = vsubq_u16(vmovl_s8(l), vmovl_s8(r));
    return  vget_low_s16(c);
}

#if CHAR_MIN
INLINE(Vdhi,VWBC_SUB2) (Vwbc a, Vwbc b)
{
    return  VWBI_SUB2(VWBC_ASBI(a), VWBC_ASBI(b));
}

#else
INLINE(Vdhu,VWBC_SUB2) (Vwbc a, Vwbc b)
{
    return  VWBU_SUB2(VWBC_ASBU(a), VWBC_ASBU(b));
}

#endif


INLINE(Vdwu,VWHU_SUB2) (Vwhu a, Vwhu b)
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    uint16x4_t  l = vreinterpret_u16_f32(p);
    uint16x4_t  r = vreinterpret_u16_f32(q);
    uint32x4_t  c = vsubq_u32(vmovl_u16(l), vmovl_u16(r));
    return  vget_low_u32(c);
}

INLINE(Vdwi,VWHI_SUB2) (Vwhi a, Vwhi b)
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    int16x4_t   l = vreinterpret_u16_f32(p);
    int16x4_t   r = vreinterpret_u16_f32(q);
    int32x4_t   c = vsubq_u32(vmovl_s16(l), vmovl_s16(r));
    return  vget_low_s32(c);
}


INLINE(Vddu,VWWU_SUB2) (Vwwu a, Vwwu b)
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    uint32x2_t  l = vreinterpret_u32_f32(p);
    uint32x2_t  r = vreinterpret_u32_f32(q);
    uint64x2_t  c = vsubq_u64(vmovl_u32(l), vmovl_u32(r));
    return  vget_low_u64(c);
}

INLINE(Vddi,VWWI_SUB2) (Vwwi a, Vwwi b)
{
    float32x2_t p = vdup_n_f32(a.V0);
    float32x2_t q = vdup_n_f32(b.V0);
    int32x2_t   l = vreinterpret_u32_f32(p);
    int32x2_t   r = vreinterpret_u32_f32(q);
    int64x2_t   c = vsubq_s64(vmovl_s32(l), vmovl_s32(r));
    return  vget_low_s64(c);
}

INLINE(Vqhu,VDBU_SUB2) (Vdbu a, Vdbu b)
{
    return  vsubq_u16(vmovl_u8(a), vmovl_u8(b));
}

INLINE(Vqhi,VDBI_SUB2) (Vdbi a, Vdbi b)
{
    return  vsubq_s16(vmovl_s8(a), vmovl_s8(b));
}

#if CHAR_MIN
INLINE(Vqhi,VDBC_SUB2) (Vdbc a, Vdbc b)
{
    return  VDBI_SUB2(VDBC_ASBI(a), VDBC_ASBI(b));
}

#else

INLINE(Vqhu,VDBC_SUB2) (Vdbc a, Vdbc b)
{
    return  VDBU_SUB2(VDBC_ASBU(a), VDBC_ASBU(b));
}
#endif


INLINE(Vqwu,VDHU_SUB2) (Vdhu a, Vdhu b)
{
    return  vsubq_u32(vmovl_u16(a), vmovl_u16(b));
}

INLINE(Vqwi,VDHI_SUB2) (Vdhi a, Vdhi b)
{
    return  vsubq_s32(vmovl_s16(a), vmovl_s16(b));
}


INLINE(Vqdu,VDWU_SUB2) (Vdwu a, Vdwu b)
{
    return  vsubq_u64(vmovl_u32(a), vmovl_u32(b));
}

INLINE(Vqdi,VDWI_SUB2) (Vdwi a, Vdwi b)
{
    return  vsubq_s64(vmovl_s32(a), vmovl_s32(b));
}

#if 0 // _LEAVE_ARM_SUB2
}
#endif

#if 0 // _ENTER_ARM_SUBS
{
#endif

INLINE( _Bool,  BOOL_SUBS)  (_Bool a,  _Bool b) {return a^b;}

INLINE( uchar, UCHAR_SUBS)  (uchar a,  uchar b)
{
    return  vqsubb_u8(a, b);
}

INLINE( schar, SCHAR_SUBS)  (schar a,  schar b)
{
    return  vqsubb_s8(a, b);
}

INLINE(  char,  CHAR_SUBS)   (char a,   char b)
{
#if CHAR_MIN
    return  vqsubb_s8(a, b);
#else
    return  vqsubb_u8(a, b);
#endif
}

INLINE(ushort, USHRT_SUBS) (ushort a, ushort b)
{
    return  vqsubh_u16(a, b);
}

INLINE( short,  SHRT_SUBS)  (short a,  short b)
{
    return  vqsubh_s16(a, b);
}

INLINE(  uint,  UINT_SUBS)   (uint a,   uint b)
{
    return  vqsubs_u32(a, b);
}

INLINE(   int,   INT_SUBS)    (int a,    int b)
{
    return  vqsubs_s32(a, b);
}

INLINE( ulong, ULONG_SUBS)  (ulong a,  ulong b)
{
#if DWRD_NLONG == 2
#   define  ULONG_SUBS(A, B) ((ulong) vqsubs_u32(A, B))
#else
#   define  ULONG_SUBS(A, B) vqsubd_u64(A, B)
#endif
    return  ULONG_SUBS(a, b);
}

INLINE(  long,  LONG_SUBS)   (long a,   long b)
{
#if DWRD_NLONG == 2
#   define  LONG_SUBS(A, B) ((long) vqsubs_s32(A, B))
#else
#   define  LONG_SUBS(A, B) vqsubd_s64(A, B)
#endif
    return  LONG_SUBS(a, b);
}

INLINE(ullong,ULLONG_SUBS) (ullong a, ullong b)
{
#if QUAD_NLLONG == 2
#   define  ULLONG_SUBS(A, B) ((ullong) vqsubd_u64(A, B))
#else
// ??
#   define  ULLONG_SUBS(A, B) vqsubq_u128(A, B)
#endif
    return  ULLONG_SUBS(a, b);
}

INLINE( llong, LLONG_SUBS)  (llong a,  llong b)
{
#if QUAD_NLLONG == 2
#   define  LLONG_SUBS(A, B) ((llong) vqsubd_s64(A, B))
#else
// ??
#   define  LLONG_SUBS(A, B) vqsubq_s128(A, B)
#endif
    return  LLONG_SUBS(a, b);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,subsqu) (QUAD_UTYPE a, QUAD_UTYPE b) 
{
    b -= a;
    return (b <= a) ? b : 0;
}

INLINE(QUAD_ITYPE,subsqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    QUAD_ITYPE c = a-b;
    if (a <= 0)
    {
        if ((b > 0) && (c > a))
        {
            return (QUAD_TYPE){.Hi.U=UINT64_C(1)<<63}.I;
        }
    }
    else 
    {
        if ((b < 0) && (c < a))
        {
            QUAD_UTYPE r = 0;
            r = (~r)>>1;
            return (QUAD_ITYPE) r;
        }
    }
    return c;
}

#endif

INLINE(Vdyu,VDYU_SUBS) (Vdyu a, Vdyu b)
{
#define     VDYU_SUBS(A, B) DYU_ASTV(veor_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_SUBS(a, b);
}

INLINE(Vdbu,VDBU_SUBS) (Vdbu a, Vdbu b) {return vqsub_u8(a, b);}
INLINE(Vdbi,VDBI_SUBS) (Vdbi a, Vdbi b) {return vqsub_s8(a, b);}
INLINE(Vdbc,VDBC_SUBS) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_SUBS(A, B) VDBI_ASBC(vqsub_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_SUBS(A, B) VDBU_ASBC(vqsub_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_SUBS(a, b);
}
INLINE(Vdhu,VDHU_SUBS) (Vdhu a, Vdhu b) {return vqsub_u16(a, b);}
INLINE(Vdhi,VDHI_SUBS) (Vdhi a, Vdhi b) {return vqsub_s16(a, b);}
INLINE(Vdwu,VDWU_SUBS) (Vdwu a, Vdwu b) {return vqsub_u32(a, b);}
INLINE(Vdwi,VDWI_SUBS) (Vdwi a, Vdwi b) {return vqsub_s32(a, b);}
INLINE(Vddu,VDDU_SUBS) (Vddu a, Vddu b) {return vqsub_u64(a, b);}
INLINE(Vddi,VDDI_SUBS) (Vddi a, Vddi b) {return vqsub_s64(a, b);}


INLINE(Vqyu,VQYU_SUBS) (Vqyu a, Vqyu b)
{
#define     VQYU_SUBS(A, B) QYU_ASTV(vorrq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_SUBS(a, b);
}
INLINE(Vqbu,VQBU_SUBS) (Vqbu a, Vqbu b) {return vqsubq_u8(a, b);}
INLINE(Vqbi,VQBI_SUBS) (Vqbi a, Vqbi b) {return vqsubq_s8(a, b);}
INLINE(Vqbc,VQBC_SUBS) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_SUBS(A, B) VQBI_ASBC(vqsubq_s8(VQBC_ASBI(A), VQBC_ASBI(B)))
#else
#   define  VQBC_SUBS(A, B) VQBU_ASBC(vqsubq_u8(VQBC_ASBU(A), VQBC_ASBU(B)))
#endif
    return  VQBC_SUBS(a, b);
}
INLINE(Vqhu,VQHU_SUBS) (Vqhu a, Vqhu b) {return vqsubq_u16(a, b);}
INLINE(Vqhi,VQHI_SUBS) (Vqhi a, Vqhi b) {return vqsubq_s16(a, b);}
INLINE(Vqwu,VQWU_SUBS) (Vqwu a, Vqwu b) {return vqsubq_u32(a, b);}
INLINE(Vqwi,VQWI_SUBS) (Vqwi a, Vqwi b) {return vqsubq_s32(a, b);}
INLINE(Vqdu,VQDU_SUBS) (Vqdu a, Vqdu b) {return vqsubq_u64(a, b);}
INLINE(Vqdi,VQDI_SUBS) (Vqdi a, Vqdi b) {return vqsubq_s64(a, b);}

#if 0 // _LEAVE_ARM_SUBS
}
#endif

#if 0 // _ENTER_ARM_SUBH
{
#endif

INLINE(flt16_t,  BOOL_SUBH)   (_Bool a, flt16_t b) {return a-b;}
INLINE(flt16_t, UCHAR_SUBH)   (uchar a, flt16_t b) {return a-b;}
INLINE(flt16_t, SCHAR_SUBH)   (schar a, flt16_t b) {return a-b;}
INLINE(flt16_t,  CHAR_SUBH)    (char a, flt16_t b) {return a-b;}
INLINE(flt16_t, USHRT_SUBH)  (ushort a, flt16_t b) {return a-b;}
INLINE(flt16_t,  SHRT_SUBH)   (short a, flt16_t b) {return a-b;}
INLINE(flt16_t,  UINT_SUBH)    (uint a, flt16_t b) {return a-b;}
INLINE(flt16_t,   INT_SUBH)     (int a, flt16_t b) {return a-b;}
INLINE(flt16_t, ULONG_SUBH)   (ulong a, flt16_t b) {return a-b;}
INLINE(flt16_t,  LONG_SUBH)    (long a, flt16_t b) {return a-b;}
INLINE(flt16_t,ULLONG_SUBH)  (ullong a, flt16_t b) {return a-b;}
INLINE(flt16_t, LLONG_SUBH)   (llong a, flt16_t b) {return a-b;}

INLINE(flt16_t, FLT16_SUBH) (flt16_t a, flt16_t b) {return a-b;}
INLINE(float,     FLT_SUBH)   (float a, flt16_t b) {return a-b;}
INLINE(double,    DBL_SUBH)  (double a, flt16_t b) {return a-b;}

#if QUAD_NLLONG == 2
INLINE(flt16_t,subhqu) (QUAD_UTYPE a, flt16_t b) {return a-b;}
INLINE(flt16_t,subhqi) (QUAD_ITYPE a, flt16_t b) {return a-b;}
INLINE(QUAD_FTYPE,subhqf) (QUAD_FTYPE a, flt16_t b) {return a-b;}
#endif


INLINE(Vdhf,VWBU_SUBH) (Vwbu a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsub_f16(VWBU_CVHF(a), b);
#else
    float32x4_t p = VWBU_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vsubq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBI_SUBH) (Vwbi a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsub_f16(VWBI_CVHF(a), b);
#else
    float32x4_t p = VWBI_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vsubq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBC_SUBH) (Vwbc a, Vdhf b)
{
#if CHAR_MIN
    return VWBI_SUBH(VWBC_ASBI(a), b);
#else
    return VWBU_SUBH(VWBC_ASBU(a), b);
#endif
}


INLINE(float,WHF_SUBH) (float am, float bm)
{
#if defined(SPC_ARM_FP16_SIMD)
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    ah = vsub_f16(ah, bh);
#else
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x4_t av = vcvt_f32_f16(ah);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    float32x4_t bv = vcvt_f32_f16(bh);
    av = vsubq_f32(av, bv);
    ah = vcvt_f16_f32(av);
#endif
    aw = vreinterpret_f32_f16(ah);
    return  vget_lane_f32(aw, 0);
    
}


INLINE(Vwhf,VWHU_SUBH) (Vwhu a, Vwhf b)
{
    Vwhf    v = VWHU_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_SUBH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHI_SUBH) (Vwhi a, Vwhf b)
{
    Vwhf    v = VWHI_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_SUBH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_SUBH) (Vwhf a, Vwhf b)
{
    float   m = WHF_SUBH(VWHF_ASTM(a), VWHF_ASTM(b));
    return WHF_ASTV(m);
}


INLINE(Vqhf,VDBU_SUBH) (Vdbu a, Vqhf b) 
{
    uint16x8_t  c = vmovl_u8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_u16(c);
    return  vsubq_f16(f, b);
#else

    uint32x4_t  zl = vmovl_u16(vget_low_u16(c));
    float32x4_t fl = vcvtq_f32_u32(zl);
    uint32x4_t  zr = vmovl_u16(vget_high_u16(c));
    float32x4_t fr = vcvtq_f32_u32(zr);
    fl = vsubq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vsubq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBI_SUBH) (Vdbi a, Vqhf b) 
{
    int16x8_t   c = vmovl_s8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_s16(c);
    return  vsubq_f16(f, b);
#else
    int32x4_t   zl = vmovl_s16(vget_low_s16(c));
    float32x4_t fl = vcvtq_f32_s32(zl);
    int32x4_t   zr = vmovl_s16(vget_high_s16(c));
    float32x4_t fr = vcvtq_f32_s32(zr);
    fl = vsubq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vsubq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBC_SUBH) (Vdbc a, Vqhf b)
{
#if CHAR_MIN
    return  VDBI_SUBH(VDBC_ASBI(a), b);
#else
    return  VDBU_SUBH(VDBC_ASBU(a), b);
#endif
}


INLINE(Vdhf,VDHU_SUBH) (Vdhu a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsub_f16(vcvt_f16_u16(a), b);
#else
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vsubq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHI_SUBH) (Vdhi a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsub_f16(vcvt_f16_s16(a), b);
#else
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vsubq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHF_SUBH) (Vdhf a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsub_f16(a, b);
#else
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vsubq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}


INLINE(Vwhf,VDWU_SUBH) (Vdwu a, Vwhf b)
{
    return  VWHF_SUBH(VDWU_CVHF(a), b);
}

INLINE(Vwhf,VDWI_SUBH) (Vdwi a, Vwhf b)
{
    return  VWHF_SUBH(VDWI_CVHF(a), b);
}

INLINE(Vdwf,VDWF_SUBH) (Vdwf a, Vwhf b)
{
    return vsub_f32(a, VWHF_CVWF(b));
}

/*
INLINE(Vhhf,VDDU_SUBH) (Vddu a, Vhhf b);
INLINE(Vhhf,VDDI_SUBH) (Vddi a, Vhhf b);
INLINE(Vddf,VDDF_SUBH) (Vddf a, Vhhf b);


INLINE(Vohf,VQBU_SUBH) (Vqbu a, Vohf b);
INLINE(Vohf,VQBI_SUBH) (Vqbi a, Vohf b);
INLINE(Vohf,VQBC_SUBH) (Vqbc a, Vohf b);

*/

INLINE(Vqhf,VQHU_SUBH) (Vqhu a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsubq_f16(vcvtq_f16_u16(a), b);
#else
    return vcombine_f16(
        VDHU_SUBH(vget_low_u16(a),  vget_low_f16(b)),
        VDHU_SUBH(vget_high_u16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHI_SUBH) (Vqhi a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsubq_f16(vcvtq_f16_s16(a), b);
#else
    return vcombine_f16(
        VDHI_SUBH(vget_low_s16(a),  vget_low_f16(b)),
        VDHI_SUBH(vget_high_s16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHF_SUBH) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsubq_f16(a, b);
#else
    return vcombine_f16(
        VDHF_SUBH(vget_low_f16(a),  vget_low_f16(b)),
        VDHF_SUBH(vget_high_f16(a), vget_high_f16(b))
    );
#endif

}


INLINE(Vdhf,VQWU_SUBH) (Vqwu a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_u32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vsubq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vdhf,VQWI_SUBH) (Vqwi a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_s32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vsubq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vqwf,VQWF_SUBH) (Vqwf a, Vdhf b)
{
    return  vsubq_f32(a, vcvt_f32_f16(b));
}

INLINE(Vwhf,VQDU_SUBH) (Vqdu a, Vwhf b)
{
    return  VWHF_SUBH(VQDU_CVHF(a), b);
}

INLINE(Vwhf,VQDI_SUBH) (Vqdi a, Vwhf b)
{
    return  VWHF_SUBH(VQDI_CVHF(a), b);
}

INLINE(Vqdf,VQDF_SUBH) (Vqdf a, Vwhf b)
{
    return  vsubq_f64(a, VWHF_CVDF(b));
}

#if 0 // _LEAVE_ARM_SUBH
}
#endif

#if 0 // _ENTER_ARM_SUBW
{
#endif

INLINE(float,  BOOL_SUBW)   (_Bool a, float b) {return a-b;}
INLINE(float, UCHAR_SUBW)   (uchar a, float b) {return a-b;}
INLINE(float, SCHAR_SUBW)   (schar a, float b) {return a-b;}
INLINE(float,  CHAR_SUBW)    (char a, float b) {return a-b;}
INLINE(float, USHRT_SUBW)  (ushort a, float b) {return a-b;}
INLINE(float,  SHRT_SUBW)   (short a, float b) {return a-b;}
INLINE(float,  UINT_SUBW)    (uint a, float b) {return a-b;}
INLINE(float,   INT_SUBW)     (int a, float b) {return a-b;}
INLINE(float, ULONG_SUBW)   (ulong a, float b) {return a-b;}
INLINE(float,  LONG_SUBW)    (long a, float b) {return a-b;}
INLINE(float,ULLONG_SUBW)  (ullong a, float b) {return a-b;}
INLINE(float, LLONG_SUBW)   (llong a, float b) {return a-b;}

INLINE(float, FLT16_SUBW) (flt16_t a, float b) {return a-b;}
INLINE(float,   FLT_SUBW)   (float a, float b) {return a-b;}
INLINE(double,  DBL_SUBW)  (double a, float b) {return a-b;}

#if QUAD_NLLONG == 2
INLINE(float,subwqu)   (QUAD_UTYPE a, float b) {return a-b;}
INLINE(float,subwqi)   (QUAD_ITYPE a, float b) {return a-b;}
INLINE(QUAD_FTYPE,subwqf) (QUAD_FTYPE a, float b) {return a-b;}
#endif


INLINE(Vqwf,VWBU_SUBW) (Vwbu a, Vqwf b)
{
    return  vsubq_f32(VWBU_CVWF(a), b);
}

INLINE(Vqwf,VWBI_SUBW) (Vwbi a, Vqwf b)
{
    return  vsubq_f32(VWBI_CVWF(a), b);
}

INLINE(Vqwf,VWBC_SUBW) (Vwbc a, Vqwf b)
{
#if CHAR_MIN
    return  VWBI_SUBW(VWBC_ASBI(a), b);
#else
    return  VWBU_SUBW(VWBC_ASBU(a), b);
#endif
}



INLINE(Vdwf,VWHU_SUBW) (Vwhu a, Vdwf b)
{
    return  vsub_f32(VWHU_CVWF(a), b);
}

INLINE(Vdwf,VWHI_SUBW) (Vwhi a, Vdwf b)
{
    return  vsub_f32(VWHI_CVWF(a), b);
}

INLINE(Vdwf,VWHF_SUBW) (Vwhf a, Vdwf b)
{
    return  vsub_f32(VWHF_CVWF(a), b);
}


INLINE(Vwwf,VWWU_SUBW) (Vwwu a, Vwwf b)
{
    return WWF_ASTV((VWWU_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWI_SUBW) (Vwwi a, Vwwf b)
{
    return WWF_ASTV((VWWI_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWF_SUBW) (Vwwf a, Vwwf b)
{
    return WWF_ASTV((VWWF_ASTM(a)+VWWF_ASTM(b)));
}


INLINE(Vqwf,VDHU_SUBW) (Vdhu a, Vqwf b) 
{
    return  vsubq_f32(VDHU_CVWF(a), b);
}

INLINE(Vqwf,VDHI_SUBW) (Vdhi a, Vqwf b) 
{
    return  vsubq_f32(VDHI_CVWF(a), b);
}

INLINE(Vqwf,VDHF_SUBW) (Vdhf a, Vqwf b) 
{
    return  vsubq_f32(VDHF_CVWF(a), b);
}


INLINE(Vdwf,VDWU_SUBW) (Vdwu a, Vdwf b)
{
    return vsub_f32(vcvt_f32_u32(a), b);
}

INLINE(Vdwf,VDWI_SUBW) (Vdwi a, Vdwf b)
{
    return vsub_f32(vcvt_f32_s32(a), b);
}

INLINE(Vdwf,VDWF_SUBW) (Vdwf a, Vdwf b)
{
    return vsub_f32(a, b);
}


INLINE(Vwwf,VDDU_SUBW) (Vddu a, Vwwf b)
{
    return WWF_ASTV((VDDU_ASTV(a)-VWWF_ASTM(b)));
}

INLINE(Vwwf,VDDI_SUBW) (Vddi a, Vwwf b)
{
    return WWF_ASTV((VDDI_ASTV(a)-VWWF_ASTM(b)));
}

INLINE(Vddf,VDDF_SUBW) (Vddf a, Vwwf b)
{
    return vsub_f64(a, vdup_n_f64(VWWF_ASTM(b)));
}


INLINE(Vqwf,VQWU_SUBW) (Vqwu a, Vqwf b)
{
    return  vsubq_f32(vcvtq_f32_u32(a), b);
}

INLINE(Vqwf,VQWI_SUBW) (Vqwi a, Vqwf b)
{
    return  vsubq_f32(vcvtq_f32_s32(a), b);
}

INLINE(Vqwf,VQWF_SUBW) (Vqwf a, Vqwf b)
{
    return  vsubq_f32(a, b);
}


INLINE(Vdwf,VQDU_SUBW) (Vqdu a, Vdwf b)
{
    return  vsub_f32(vcvt_f32_f64(vcvtq_f64_u64(a)), b);
}

INLINE(Vdwf,VQDI_SUBW) (Vqdi a, Vdwf b)
{
    return  vsub_f32(vcvt_f32_f64(vcvtq_f64_s64(a)), b);
}

INLINE(Vqdf,VQDF_SUBW) (Vqdf a, Vdwf b)
{
    return  vsubq_f64(a, vcvt_f64_f32(b));
}

#if 0 // _LEAVE_ARM_SUBW
}
#endif

#if 0 // _ENTER_ARM_SUBD
{
#endif

INLINE(double,  BOOL_SUBD)   (_Bool a, double b) {return a-b;}
INLINE(double, UCHAR_SUBD)   (uchar a, double b) {return a-b;}
INLINE(double, SCHAR_SUBD)   (schar a, double b) {return a-b;}
INLINE(double,  CHAR_SUBD)    (char a, double b) {return a-b;}
INLINE(double, USHRT_SUBD)  (ushort a, double b) {return a-b;}
INLINE(double,  SHRT_SUBD)   (short a, double b) {return a-b;}
INLINE(double,  UINT_SUBD)    (uint a, double b) {return a-b;}
INLINE(double,   INT_SUBD)     (int a, double b) {return a-b;}
INLINE(double, ULONG_SUBD)   (ulong a, double b) {return a-b;}
INLINE(double,  LONG_SUBD)    (long a, double b) {return a-b;}
INLINE(double,ULLONG_SUBD)  (ullong a, double b) {return a-b;}
INLINE(double, LLONG_SUBD)   (llong a, double b) {return a-b;}

INLINE(double, FLT16_SUBD) (flt16_t a, double b) {return a-b;}
INLINE(double,   FLT_SUBD)   (float a, double b) {return a-b;}
INLINE(double,  DBL_SUBD)  (double a, double b) {return a-b;}

#if QUAD_NLLONG == 2
INLINE(double,subdqu)   (QUAD_UTYPE a, double b) {return a-b;}
INLINE(double,subdqi)   (QUAD_ITYPE a, double b) {return a-b;}
INLINE(QUAD_FTYPE,subdqf) (QUAD_FTYPE a, double b) {return a-b;}
#endif


INLINE(Vqdf,VWHU_SUBD) (Vwhu a, Vqdf b)
{
    return  vsubq_f64(VWHU_CVDF(a), b);
}

INLINE(Vqdf,VWHI_SUBD) (Vwhi a, Vqdf b)
{
    return  vsubq_f64(VWHI_CVDF(a), b);
}

INLINE(Vqdf,VWHF_SUBD) (Vwhf a, Vqdf b)
{
    return  vsubq_f64(VWHF_CVDF(a), b);
}


INLINE(Vddf,VWWU_SUBD) (Vwwu a, Vddf b)
{
    return  vsub_f64(VWWU_CVDF(a), b);
}

INLINE(Vddf,VWWI_SUBD) (Vwwi a, Vddf b)
{
    return  vsub_f64(VWWI_CVDF(a), b);
}

INLINE(Vddf,VWWF_SUBD) (Vwwf a, Vddf b)
{
    return  vsub_f64(VWWF_CVDF(a), b);
}


INLINE(Vqdf,VDWU_SUBD) (Vdwu a, Vqdf b)
{
    return  vsubq_f64(vcvtq_f64_u64(vmovl_u32(a)), b);
}

INLINE(Vqdf,VDWI_SUBD) (Vdwi a, Vqdf b)
{
    return  vsubq_f64(vcvtq_f64_s64(vmovl_s32(a)), b);
}

INLINE(Vqdf,VDWF_SUBD) (Vdwf a, Vqdf b)
{
    return  vsubq_f64(vcvt_f64_f32(a), b);
}


INLINE(Vddf,VDDU_SUBD) (Vddu a, Vddf b)
{
    return  vsub_f64(vcvt_f64_u64(a), b);
}

INLINE(Vddf,VDDI_SUBD) (Vddi a, Vddf b)
{
    return  vsub_f64(vcvt_f64_s64(a), b);
}

INLINE(Vddf,VDDF_SUBD) (Vddf a, Vddf b)
{
    return  vsub_f64(a, b);
}


INLINE(Vqdf,VQDU_SUBD) (Vqdu a, Vqdf b)
{
    return  vsubq_f64(vcvtq_f64_u64(a), b);
}

INLINE(Vqdf,VQDI_SUBD) (Vqdi a, Vqdf b)
{
    return  vsubq_f64(vcvtq_f64_s64(a), b);
}

INLINE(Vqdf,VQDF_SUBD) (Vqdf a, Vqdf b)
{
    return  vsubq_f64(a, b);
}

#if 0 // _LEAVE_ARM_SUBD
}
#endif

#if 0 // _ENTER_ARM_DCRL
{
#endif

INLINE(Vwyu,VWYU_DCRL) (Vwyu a)
{
    int32x2_t   z = vdup_n_s32(-1);
    float32x2_t f = vreinterpret_f32_s32(z);
    float       b = vget_lane_f32(f, 0);
    return  WYU_ASTV(FLT_XORS(VWYU_ASTM(a), b));
}

INLINE(Vwbu,VWBU_DCRL) (Vwbu a)
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    az = vsub_u8(az, vdup_n_u8(1));
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_DCRL) (Vwbi a)
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    az = vsub_s8(az, vdup_n_s8(1));
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_DCRL) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_DCRL(VWBC_ASBI(a)));
#else
    return  VWBU_ASBC(VWBU_DCRL(VWBC_ASBU(a)));
#endif
}


INLINE(Vwhu,VWHU_DCRL) (Vwhu a)
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    az = vsub_u16(az, vdup_n_u16(1));
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_DCRL) (Vwhi a)
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    az = vsub_s16(az, vdup_n_s16(1));
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}

INLINE(Vwwu,VWWU_DCRL) (Vwwu a)
{
    return  UINT32_ASTV((VWWU_ASTV(a)-1));
}

INLINE(Vwwi,VWWI_DCRL) (Vwwi a)
{
    return  INT32_ASTV((VWWI_ASTV(a)-1));
}


INLINE(Vdyu,VDYU_DCRL) (Vdyu a)
{
#define     VDYU_DCRL(A) DYU_ASTV(veor_u64(VDYU_ASTM(A),vdup_n_u64(~0ull)))
    return  VDYU_DCRL(a);
}

INLINE(Vdbu,VDBU_DCRL) (Vdbu a) {return vsub_u8(a,vdup_n_u8(1));}
INLINE(Vdbi,VDBI_DCRL) (Vdbi a) {return vsub_s8(a,vdup_n_u8(1));}
INLINE(Vdbc,VDBC_DCRL) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_DCRL(A) VDBI_ASBC(vsub_s8(VDBC_ASBI(A),vdup_n_s8(1)))
#else
#   define  VDBC_DCRL(A) VDBU_ASBC(vsub_u8(VDBC_ASBU(A),vdup_n_u8(1)))
#endif
    return  VDBC_DCRL(a);
}

INLINE(Vdhu,VDHU_DCRL) (Vdhu a) {return vsub_u16(a,vdup_n_u16(1));}
INLINE(Vdhi,VDHI_DCRL) (Vdhi a) {return vsub_s16(a,vdup_n_s16(1));}
INLINE(Vdwu,VDWU_DCRL) (Vdwu a) {return vsub_u32(a,vdup_n_u32(1));}
INLINE(Vdwi,VDWI_DCRL) (Vdwi a) {return vsub_s32(a,vdup_n_s32(1));}
INLINE(Vddu,VDDU_DCRL) (Vddu a) {return vsub_u64(a,vdup_n_u64(1));}
INLINE(Vddi,VDDI_DCRL) (Vddi a) {return vsub_s64(a,vdup_n_s64(1));}

INLINE(Vqyu,VQYU_DCRL) (Vqyu a)
{
#define     VQYU_DCRL(A) QYU_ASTV(veorq_u64(VQYU_ASTM(A),vdupq_n_u64(~0ull)))
    return  VQYU_DCRL(a);
}

INLINE(Vqbu,VQBU_DCRL) (Vqbu a) {return vsubq_u8(a,vdupq_n_u8(1));}
INLINE(Vqbi,VQBI_DCRL) (Vqbi a) {return vsubq_s8(a,vdupq_n_u8(1));}
INLINE(Vqbc,VQBC_DCRL) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_DCRL(A) VQBI_ASBC(vsubq_s8(VQBC_ASBI(A),vdupq_n_s8(1)))
#else
#   define  VQBC_DCRL(A) VQBU_ASBC(vsubq_u8(VQBC_ASBU(A),vdupq_n_u8(1)))
#endif
    return  VQBC_DCRL(a);
}

INLINE(Vqhu,VQHU_DCRL) (Vqhu a) {return vsubq_u16(a,vdupq_n_u16(1));}
INLINE(Vqhi,VQHI_DCRL) (Vqhi a) {return vsubq_s16(a,vdupq_n_s16(1));}
INLINE(Vqwu,VQWU_DCRL) (Vqwu a) {return vsubq_u32(a,vdupq_n_u32(1));}
INLINE(Vqwi,VQWI_DCRL) (Vqwi a) {return vsubq_s32(a,vdupq_n_s32(1));}
INLINE(Vqdu,VQDU_DCRL) (Vqdu a) {return vsubq_u64(a,vdupq_n_u64(1));}
INLINE(Vqdi,VQDI_DCRL) (Vqdi a) {return vsubq_s64(a,vdupq_n_s64(1));}

#if 0 // _LEAVE_ARM_DCRL
}
#endif

#if 0 // _ENTER_ARM_MULL
{
#endif

INLINE(Vwyu,VWYU_MULL) (Vwyu a, Vwyu b)
{
#define     VWYU_MULL(A, B) \
WYU_ASTV(((VWWU_ASTV(VWYU_ASWU(A)))&(VWWU_ASTV(VWYU_ASWU(B)))))
    return  VWYU_MULL(a, b);
}

INLINE(Vwbu,VWBU_MULL) (Vwbu a, Vwbu b) 
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    float       bm = VWBU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    az = vmul_u8(az, bz);
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_MULL) (Vwbi a, Vwbi b) 
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    float       bm = VWBI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    az = vmul_s8(az, bz);
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_MULL) (Vwbc a, Vwbc b) 
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_MULL(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return  VWBU_ASBC(VWBU_MULL(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_MULL) (Vwhu a, Vwhu b) 
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    float       bm = VWHU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    az = vmul_u16(az, bz);
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_MULL) (Vwhi a, Vwhi b) 
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    float       bm = VWHI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    az = vmul_s16(az, bz);
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}


INLINE(Vwwu,VWWU_MULL) (Vwwu a, Vwwu b) 
{
#define     VWWU_MULL(A, B)  UINT32_ASTV((VWWU_ASTV(A)*VWWU_ASTV(B)))
    return  VWWU_MULL(a, b);
}

INLINE(Vwwi,VWWI_MULL) (Vwwi a, Vwwi b) 
{
#define     VWWI_MULL(A, B)  \
INT32_ASTV(((unsigned) VWWI_ASTV(A)*(unsigned) VWWI_ASTV(B)))
    return  VWWI_MULL(a, b);
}


INLINE(Vdyu,VDYU_MULL) (Vdyu a, Vdyu b)
{
#define     VDYU_MULL(A, B) DYU_ASTV(vand_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_MULL(a, b);
}

INLINE(Vdbu,VDBU_MULL) (Vdbu a, Vdbu b) {return vmul_u8(a, b);}
INLINE(Vdbi,VDBI_MULL) (Vdbi a, Vdbi b) {return vmul_s8(a, b);}
INLINE(Vdbc,VDBC_MULL) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_MULL(A, B) VDBI_ASBC(vmul_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_MULL(A, B) VDBU_ASBC(vmul_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_MULL(a, b);
}

INLINE(Vdhu,VDHU_MULL) (Vdhu a, Vdhu b) {return vmul_u16(a, b);}
INLINE(Vdhi,VDHI_MULL) (Vdhi a, Vdhi b) {return vmul_s16(a, b);}
INLINE(Vdwu,VDWU_MULL) (Vdwu a, Vdwu b) {return vmul_u32(a, b);}
INLINE(Vdwi,VDWI_MULL) (Vdwi a, Vdwi b) {return vmul_s32(a, b);}
INLINE(Vddu,VDDU_MULL) (Vddu a, Vddu b)
{
    return  vdup_n_u64(vget_lane_u64(a, 0)*vget_lane_u64(b, 0));
}

INLINE(Vddi,VDDI_MULL) (Vddi a, Vddi b)
{
    return  VDDU_ASDI(
        VDDU_MULL(VDDI_ASDU(a), VDDI_ASDU(b))
    );
}

INLINE(Vqyu,VQYU_MULL) (Vqyu a, Vqyu b)
{
#define     VQYU_MULL(A, B) QYU_ASTV(vandq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_MULL(a, b);
}

INLINE(Vqbu,VQBU_MULL) (Vqbu a, Vqbu b) {return vmulq_u8(a, b);}
INLINE(Vqbi,VQBI_MULL) (Vqbi a, Vqbi b) {return vmulq_s8(a, b);}
INLINE(Vqbc,VQBC_MULL) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_MULL(A, B) VQBI_ASBC(vmulq_s8(VQBC_ASBI(A), VQBC_ASBI(B)))
#else
#   define  VQBC_MULL(A, B) VQBU_ASBC(vmulq_u8(VQBC_ASBU(A), VQBC_ASBU(B)))
#endif
    return  VQBC_MULL(a, b);
}

INLINE(Vqhu,VQHU_MULL) (Vqhu a, Vqhu b) {return vmulq_u16(a, b);}
INLINE(Vqhi,VQHI_MULL) (Vqhi a, Vqhi b) {return vmulq_s16(a, b);}
INLINE(Vqwu,VQWU_MULL) (Vqwu a, Vqwu b) {return vmulq_u32(a, b);}
INLINE(Vqwi,VQWI_MULL) (Vqwi a, Vqwi b) {return vmulq_s32(a, b);}
INLINE(Vqdu,VQDU_MULL) (Vqdu a, Vqdu b)
{
    a = vsetq_lane_u64(
        (vgetq_lane_u64(a, 0)*vgetq_lane_u64(b, 0)),
        a,
        0
    );
    a = vsetq_lane_u64(
        (vgetq_lane_u64(a, 1)*vgetq_lane_u64(b, 1)),
        a,
        1
    );
    return a;
}

INLINE(Vqdi,VQDI_MULL) (Vqdi a, Vqdi b)
{
    return  VQDU_ASDI(
        VQDU_MULL(VQDI_ASDU(a), VQDI_ASDU(b))
    );
}

#if 0 // _LEAVE_ARM_MULL
}
#endif

#if 0 // _ENTER_ARM_MUL2
{
#endif

INLINE(Vdhu,VWBU_MUL2) (Vwbu a, Vwbu b) 
{
    float       am = VWBU_ASTM(a);
    float       bm = VWBU_ASTM(b);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    uint16x8_t  cz = vmull_u8(az, bz);
    return vget_low_u16(cz);
}

INLINE(Vdhi,VWBI_MUL2) (Vwbi a, Vwbi b) 
{
    float       am = VWBI_ASTM(a);
    float       bm = VWBI_ASTM(b);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    int8x8_t    az = vreinterpret_s8_f32(af);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    int16x8_t   cz = vmull_s8(az, bz);
    return vget_low_s16(cz);
}

#if CHAR_MIN

INLINE(Vdhi,VWBC_MUL2) (Vwbc a, Vwbc b)
{
    return  VWBI_MUL2(VWBC_ASBI(a), VWBC_ASBI(b));
}

#else

INLINE(Vdhu,VWBC_MUL2) (Vwbc a, Vwbc b)
{
    return  VWBU_MUL2(VWBC_ASBU(a), VWBC_ASBU(b));
}

#endif


INLINE(Vdwu,VWHU_MUL2) (Vwhu a, Vwhu b) 
{
    float       am = VWHU_ASTM(a);
    float       bm = VWHU_ASTM(b);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    uint32x4_t  cz = vmull_u16(az, bz);
    return  vget_low_u32(cz);
}

INLINE(Vdwi,VWHI_MUL2) (Vwhi a, Vwhi b) 
{
    float       am = VWHI_ASTM(a);
    float       bm = VWHI_ASTM(b);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    int16x4_t   az = vreinterpret_s16_f32(af);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    int32x4_t   cz = vmull_s16(az, bz);
    return  vget_low_s32(cz);
}


INLINE(Vddu,VWWU_MUL2) (Vwwu a, Vwwu b) 
{
    float       am = VWWU_ASTM(a);
    float       bm = VWWU_ASTM(b);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    uint32x2_t  az = vreinterpret_u32_f32(af);
    uint32x2_t  bz = vreinterpret_u32_f32(bf);
    uint64x2_t  cz = vmull_u32(az, bz);
    return  vget_low_u64(cz);
}

INLINE(Vddi,VWWI_MUL2) (Vwwi a, Vwwi b) 
{
    float       am = VWWI_ASTM(a);
    float       bm = VWWI_ASTM(b);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    int32x2_t   az = vreinterpret_s32_f32(af);
    int32x2_t   bz = vreinterpret_s32_f32(bf);
    int64x2_t   cz = vmull_s32(az, bz);
    return  vget_low_s64(cz);
}

INLINE(Vqhu,VDBU_MUL2) (Vdbu a, Vdbu b) {return vmull_u8(a, b);}

INLINE(Vqhi,VDBI_MUL2) (Vdbi a, Vdbi b) {return vmull_s8(a, b);}

#if CHAR_MIN

INLINE(Vqhi,VDBC_MUL2) (Vdbc a, Vdbc b)
{
    return  vmull_s8(VDBC_ASBI(a), VDBC_ASBI(b));
}

#else

INLINE(Vqhu,VDBC_MUL2) (Vdbc a, Vdbc b)
{
    return  vmull_u8(VDBC_ASBU(a), VDBC_ASBU(b));
}

#endif

INLINE(Vqwu,VDHU_MUL2) (Vdhu a, Vdhu b) {return vmull_u16(a, b);}

INLINE(Vqwi,VDHI_MUL2) (Vdhi a, Vdhi b) {return vmull_s16(a, b);}


INLINE(Vqdu,VDWU_MUL2) (Vdwu a, Vdwu b) {return vmull_u32(a, b);}

INLINE(Vqdi,VDWI_MUL2) (Vdwi a, Vdwi b) {return vmull_s32(a, b);}

#if 0

INLINE(Vqqu,VDDU_MUL2) (Vddu a, Vddu b)
{
    return  (Vqqu)
    {
        ((unsigned __int128) vget_lane_u64(a, 0))
    *   vget_lane_u64(b, 0)
    };
}

INLINE(Vqqi,VDDI_MUL2) (Vddi a, Vddi b)
{
    return  (Vqqi)
    {
        ((signed __int128) vget_lane_s64(a, 0))
    *   vget_lane_s64(b, 0)
    };
}

#endif

#if 0 // _LEAVE_ARM_MUL2
}
#endif

#if 0 // _ENTER_ARM_MULS
{
#endif

INLINE( _Bool,  BOOL_MULS)  (_Bool a,  _Bool b) {return a&b;}


INLINE( uchar, UCHAR_MULS)  (uchar a,  uchar b)
{
    return  vqmovnh_u16((a*b));
}

INLINE( schar, SCHAR_MULS)  (schar a,  schar b)
{
    return  vqmovnh_s16((a*b));
}

INLINE(  char,  CHAR_MULS)   (char a,   char b)
{
#if CHAR_MIN
    return  vqmovnh_s16((a*b));
#else
    return  vqmovnh_u16((a*b));
#endif
}


INLINE(ushort, USHRT_MULS) (ushort a, ushort b)
{
    return  vqmovns_u32((a*b));
}

INLINE( short,  SHRT_MULS)  (short a,  short b)
{
    return  vqmovns_s32((a*b));
}


INLINE(  uint,  UINT_MULS)   (uint a,   uint b)
{
    return  vqmovnd_u64(((uint64_t) a*b));
}

INLINE(   int,   INT_MULS)    (int a,    int b)
{
    return  vqmovnd_s64(((int64_t) a*b));
}


INLINE( ulong, ULONG_MULS)  (ulong a,  ulong b)
{
#if DWRD_NLONG == 2
    return  vqmovnd_u64(((uint64_t) a*b));
#else
    return (b*=a) < a ? UINT64_MAX : b;
#endif
}

INLINE(  long,  LONG_MULS)   (long a,   long b)
{
#if DWRD_NLONG == 2
    return  vqmovnd_s64(((int64_t) a*b));
#else
/* TODO: fix this
*/
    QUAD_ITYPE c = (QUAD_ITYPE) a*(QUAD_ITYPE) b;
    if (c > INT64_MAX)
        return  INT64_MAX;
    if (c < INT64_MIN)
        return  INT64_MIN;
    return c;
#endif
}


INLINE(ullong,ULLONG_MULS) (ullong a, ullong b)
{
    return (b*=a) < a ? ULLONG_MAX : b;
}

INLINE( llong, LLONG_MULS)  (llong a,  llong b)
{
    QUAD_ITYPE c = (QUAD_ITYPE) a*(QUAD_ITYPE) b;
    if (c > LLONG_MAX)
        return LLONG_MAX;
    if (c < LLONG_MIN)
        return LLONG_MIN;
    return c;
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,mulsqu) (QUAD_UTYPE a, QUAD_UTYPE b) 
{
    return (b*=a) >= a ? b : (((QUAD_UTYPE) 0)-1);
}

INLINE(QUAD_ITYPE,mulsqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    QUAD_ITYPE c = a*b;
    if (a <= 0)
    {
        if ((b < 0) && (c > a))
        {
            QUAD_UTYPE r = 1;
            r <<= 127;
            return  (QUAD_ITYPE) r;
        }
    }
    else 
    {
        if ((b > 0) && (c < a))
        {
            QUAD_UTYPE r = 0;
            r -= 1;
            r >>= 1;
            return  (QUAD_ITYPE) r;
        }
    }
    return c;
}

#endif

INLINE(Vwyu,VWYU_MULS) (Vwyu a, Vwyu b)
{
    float ml = VWYU_ASTM(a);
    float mr = VWYU_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    uint8x8_t   zl = vreinterpret_u8_f32(vl);
    uint8x8_t   zr = vreinterpret_u8_f32(vr);
    zl = vand_u8(zl, zr);
    vl = vreinterpret_f32_u8(zl);
    ml = vget_lane_f32(vl, 0);
    return  WYU_ASTV(ml);
}


INLINE(Vwbu,VWBU_MULS) (Vwbu a, Vwbu b)
{
    float       ml = VWBU_ASTM(a);
    float       mr = VWBU_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    uint8x8_t   zl = vreinterpret_u8_f32(vl);
    uint8x8_t   zr = vreinterpret_u8_f32(vr);
    zl = vqmovn_u16(vmull_u8(zl, zr));
    vl = vreinterpret_f32_u8(zl);
    ml = vget_lane_f32(vl, 0);
    return  WBU_ASTV(ml);
}

INLINE(Vwbi,VWBI_MULS) (Vwbi a, Vwbi b)
{
    float       ml = VWBI_ASTM(a);
    float       mr = VWBI_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    int8x8_t    zl = vreinterpret_s8_f32(vl);
    int8x8_t    zr = vreinterpret_s8_f32(vr);
    zl = vqmovn_s16(vmull_s8(zl, zr));
    vl = vreinterpret_f32_s8(zl);
    ml = vget_lane_f32(vl, 0);
    return  WBI_ASTV(ml);
}

INLINE(Vwbc,VWBC_MULS) (Vwbc a, Vwbc b)
{
#if CHAR_MIN
    return VWBI_ASBC(VWBI_MULS(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return VWBU_ASBC(VWBU_MULS(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_MULS) (Vwhu a, Vwhu b)
{
    float       ml = VWHU_ASTM(a);
    float       mr = VWHU_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    uint16x4_t  zl = vreinterpret_u16_f32(vl);
    uint16x4_t  zr = vreinterpret_u16_f32(vr);
    zl = vqmovn_u32(vmull_u16(zl, zr));
    vl = vreinterpret_f32_u16(zl);
    ml = vget_lane_f32(vl, 0);
    return  WHU_ASTV(ml);
}

INLINE(Vwhi,VWHI_MULS) (Vwhi a, Vwhi b)
{
    float       ml = VWHI_ASTM(a);
    float       mr = VWHI_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    int16x4_t   zl = vreinterpret_s16_f32(vl);
    int16x4_t   zr = vreinterpret_s16_f32(vr);
    zl = vqmovn_u32(vmull_s16(zl, zr));
    vl = vreinterpret_f32_s16(zl);
    ml = vget_lane_f32(vl, 0);
    return  WHI_ASTV(ml);
}


INLINE(Vwwu,VWWU_MULS) (Vwwu a, Vwwu b)
{
    float       ml = VWWU_ASTM(a);
    float       mr = VWWU_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    uint32x2_t  zl = vreinterpret_u32_f32(vl);
    uint32x2_t  zr = vreinterpret_u32_f32(vr);
    zl = vqmovn_u64(vmull_u32(zl, zr));
    vl = vreinterpret_f32_u32(zl);
    ml = vget_lane_f32(vl, 0);
    return  WWU_ASTV(ml);
}

INLINE(Vwwi,VWWI_MULS) (Vwwi a, Vwwi b)
{
    float       ml = VWWI_ASTM(a);
    float       mr = VWWI_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    uint32x2_t  zl = vreinterpret_s32_f32(vl);
    uint32x2_t  zr = vreinterpret_s32_f32(vr);
    zl = vqmovn_s64(vmull_s32(zl, zr));
    vl = vreinterpret_f32_s32(zl);
    ml = vget_lane_f32(vl, 0);
    return  WWI_ASTV(ml);
}


INLINE(Vdyu,VDYU_MULS) (Vdyu a, Vdyu b)
{
    return VDDU_ASYU(vand_u64(VDYU_ASDU(a), VDYU_ASDU(b)));
}


INLINE(Vdbu,VDBU_MULS) (Vdbu a, Vdbu b) 
{
    return  vqmovn_u16(vmull_u8(a, b));
}

INLINE(Vdbi,VDBI_MULS) (Vdbi a, Vdbi b) 
{
    return  vqmovn_s16(vmull_s8(a, b));
}

INLINE(Vdbc,VDBC_MULS) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_MULS(VDBC_ASBI(a), VDBC_ASBI(b)));
#else
    return  VDBU_ASBC(VDBU_MULS(VDBC_ASBU(a), VDBC_ASBU(b)));
#endif

}


INLINE(Vdhu,VDHU_MULS) (Vdhu a, Vdhu b) 
{
    return  vqmovn_u32(vmull_u16(a, b));
}

INLINE(Vdhi,VDHI_MULS) (Vdhi a, Vdhi b) 
{
    return  vqmovn_s32(vmull_s16(a, b));
}

INLINE(Vdwu,VDWU_MULS) (Vdwu a, Vdwu b) 
{
    return  vqmovn_u64(vmull_u32(a, b));
}

INLINE(Vdwi,VDWI_MULS) (Vdwi a, Vdwi b) 
{
    return  vqmovn_s64(vmull_s32(a, b));
}

INLINE(Vddu,VDDU_MULS) (Vddu a, Vddu b) 
{
    uint64x1_t c = vdup_n_u64(
        (vget_lane_u64(a, 0)*vget_lane_u64(b, 0))
    );
    uint64x1_t m = vcgt_u64(b, c);
    m = vand_u64(m, vdup_n_u64(UINT64_MAX));
    return  vorr_u64(c, m);
}
    

INLINE(Vddi,VDDI_MULS) (Vddi a, Vddi b) 
{
    int64_t     l = vget_lane_s64(a, 0);
    int64_t     r = vget_lane_s64(b, 0);
    uint64_t    c = (uint64_t) l*(uint64_t) r;
    if (l < 0)
    {
        if ((r < 0)  && (c < INT64_MAX))
            c = INT64_MIN;
    }
    else
    {
        if ((r > 0) && (c > INT64_MAX))
            c = INT64_MAX;
    }
    return  vdup_n_s64(c);
}



INLINE(Vqyu,VQYU_MULS) (Vqyu a, Vqyu b)
{
    return VQDU_ASYU(vandq_u64(VQYU_ASDU(a), VQYU_ASDU(b)));
}


INLINE(Vqbu,VQBU_MULS) (Vqbu a, Vqbu b) 
{
    return  vcombine_u8(
        VDBU_MULS(vget_low_u8(a), vget_low_u8(b)),
        VDBU_MULS(vget_high_u8(a),vget_high_u8(b))
    );
}

INLINE(Vqbi,VQBI_MULS) (Vqbi a, Vqbi b) 
{
    return  vcombine_s8(
        VDBI_MULS(vget_low_s8(a), vget_low_s8(b)),
        VDBI_MULS(vget_high_s8(a),vget_high_s8(b))
    );
}

INLINE(Vqbc,VQBC_MULS) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_MULS(VQBC_ASBI(a), VQBC_ASBI(b)));
#else
    return  VQBU_ASBC(VQBU_MULS(VQBC_ASBU(a), VQBC_ASBU(b)));
#endif

}


INLINE(Vqhu,VQHU_MULS) (Vqhu a, Vqhu b) 
{
    return  vcombine_u16(
        VDHU_MULS(vget_low_u16(a), vget_low_u16(b)),
        VDHU_MULS(vget_high_u16(a),vget_high_u16(b))
    );
}

INLINE(Vqhi,VQHI_MULS) (Vqhi a, Vqhi b) 
{
    return  vcombine_s16(
        VDHI_MULS(vget_low_s16(a), vget_low_s16(b)),
        VDHI_MULS(vget_high_s16(a),vget_high_s16(b))
    );
}


INLINE(Vqwu,VQWU_MULS) (Vqwu a, Vqwu b) 
{
    return  vcombine_u32(
        VDWU_MULS(vget_low_u32(a), vget_low_u32(b)),
        VDWU_MULS(vget_high_u32(a),vget_high_u32(b))
    );
}

INLINE(Vqwi,VQWI_MULS) (Vqwi a, Vqwi b) 
{
    return  vcombine_s32(
        VDWI_MULS(vget_low_s32(a), vget_low_s32(b)),
        VDWI_MULS(vget_high_s32(a),vget_high_s32(b))
    );
}


INLINE(Vqdu,VQDU_MULS) (Vqdu a, Vqdu b) 
{
    return  vcombine_u64(
        VDDU_MULS(vget_low_u64(a), vget_low_u64(b)),
        VDDU_MULS(vget_high_u64(a),vget_high_u64(b))
    );
}

INLINE(Vqdi,VQDI_MULS) (Vqdi a, Vqdi b) 
{
    return  vcombine_s64(
        VDDI_MULS(vget_low_s64(a), vget_low_s64(b)),
        VDDI_MULS(vget_high_s64(a),vget_high_s64(b))
    );
}

#if 0 // _LEAVE_ARM_MULS
}
#endif


#if 0 // _ENTER_ARM_MULR
{
#endif

INLINE( uint8x8_t,DBU_MULR)  (uint8x8_t a,  uint8x8_t b)
{
    QUAD_VTYPE c = {.H.U=vmull_u8(a, b)};
    return  vshrn_n_u16(c.H.U, 8);
}

INLINE(  int8x8_t,DBI_MULR)   (int8x8_t a,   int8x8_t b)
{
    QUAD_VTYPE c = {.H.U=vmull_s8(a, b)};
    return  vshrn_n_s16(c.H.I, 8);
}

#if CHAR_MIN
#   define DBC_MULR DBI_MULR
#   define QBC_MULR QBI_MULR
#else
#   define DBC_MULR DBU_MULR
#   define QBC_MULR QBU_MULR
#endif

INLINE(uint16x4_t,DHU_MULR) (uint16x4_t a, uint16x4_t b)
{
    QUAD_VTYPE c = {.W.U=vmull_u16(a, b)};
    return  vshrn_n_u32(c.W.U, 16);
}

INLINE( int16x4_t,DHI_MULR)  (int16x4_t a,  int16x4_t b)
{
    QUAD_VTYPE c = {.W.I=vmull_s16(a, b)};
    return  vshrn_n_s32(c.W.I, 16);
}

INLINE(uint32x2_t,DWU_MULR) (uint32x2_t a, uint32x2_t b)
{
    QUAD_VTYPE c = {.D.U=vmull_u32(a, b)};
    return  vshrn_n_u64(c.D.U, 32);
}

INLINE( int32x2_t,DWI_MULR)  (int32x2_t a,  int32x2_t b)
{
    QUAD_VTYPE c = {.D.U=vmull_s32(a, b)};
    return  vshrn_n_s64(c.D.I, 32);
}

INLINE(uint64x1_t,DDU_MULR) (uint64x1_t a, uint64x1_t b)
{
    QUAD_UTYPE x = vget_lane_u64(a, 0);
    QUAD_UTYPE y = vget_lane_u64(b, 0);
    x = (x*y)>>64;
    return  vdup_n_u64(x);
}

INLINE( int64x1_t,DDI_MULR)  (int64x1_t a,  int64x1_t b)
{
    QUAD_ITYPE x = vget_lane_s64(a, 0);
    QUAD_ITYPE y = vget_lane_s64(b, 0);
    x = (x*y)>>64;
    return  vdup_n_s64(x);
}

INLINE(uint8x16_t,QBU_MULR)  (uint8x16_t a,  uint8x16_t b)
{
    uint8x8_t   x=vget_low_u8(a), y=vget_low_u8(b), z;
    uint16x8_t  c = vmull_u8(x, y);
    z = vshrn_n_u16(c, 8);
    x=vget_high_u8(a), y=vget_high_u8(b);
    c = vmull_u8(x, y);
    x = vshrn_n_u16(c, 8);
    return vcombine_u8(x, z);
}

INLINE( int8x16_t,QBI_MULR)  (int8x16_t a,  int8x16_t b)
{
    int8x8_t   x=vget_low_s8(a), y=vget_low_s8(b), z;
    int16x8_t  c = vmull_s8(x, y);
    z = vshrn_n_s16(c, 8);
    x=vget_high_s8(a), y=vget_high_s8(b);
    c = vmull_s8(x, y);
    x = vshrn_n_s16(c, 8);
    return  vcombine_s8(x, z);
}

INLINE(uint16x8_t,QHU_MULR)  (uint16x8_t a,  uint16x8_t b)
{
    uint16x4_t   x=vget_low_u16(a), y=vget_low_u16(b), z;
    uint32x4_t  c = vmull_u16(x, y);
    z = vshrn_n_u32(c, 16);
    x=vget_high_u16(a), y=vget_high_u16(b);
    c = vmull_u16(x, y);
    x = vshrn_n_u32(c, 16);
    return  vcombine_u16(x, z);
}

INLINE(int16x8_t,QHI_MULR)  (int16x8_t a,  int16x8_t b)
{
    int16x4_t  x=vget_low_s16(a), y=vget_low_s16(b), z;
    int32x4_t  c = vmull_s16(x, y);
    z = vshrn_n_s32(c, 16);
    x=vget_high_s16(a), y=vget_high_s16(b);
    c = vmull_s16(x, y);
    x = vshrn_n_s32(c, 16);
    return  vcombine_s16(x, z);
}

INLINE(uint32x4_t,QWU_MULR)  (uint32x4_t a,  uint32x4_t b)
{
    uint32x2_t   x=vget_low_u32(a), y=vget_low_u32(b), z;
    uint64x2_t  c = vmull_u32(x, y);
    x=vget_high_u32(a), y=vget_high_u32(b);
    z = vshrn_n_u64(c, 32);
    c = vmull_u32(x, y);
    x = vshrn_n_u64(c, 32);
    return  vcombine_u32(x, z);
}


INLINE( int32x4_t,QWI_MULR)   (int32x4_t a,   int32x4_t b)
{
    int32x2_t   x=vget_low_s32(a), y=vget_low_s32(b), z;
    int64x2_t   c = vmull_s32(x, y);
    x=vget_high_s32(a), y=vget_high_s32(b);
    z = vshrn_n_s64(c, 32);
    c = vmull_s32(x, y);
    x = vshrn_n_s64(c, 32);
    return  vcombine_s32(x, z);
}

INLINE(uint64x2_t,QDU_MULR)  (uint64x2_t a,  uint64x2_t b)
{
    uint64x1_t x, y, z;
    x = vget_low_u64(a);
    y = vget_low_u64(b);
    z = DDU_MULR(x, y);
    x = vget_high_u64(a);
    y = vget_high_u64(b);
    return vcombine_u64(z, DDU_MULR(x, y));
    
}

INLINE( int64x2_t,QDI_MULR)   (int64x2_t a,   int64x2_t b)
{
    int64x1_t x, y, z;
    x = vget_low_s64(a);
    y = vget_low_s64(b);
    z = DDI_MULR(x, y);
    x = vget_high_s64(a);
    y = vget_high_s64(b);
    return  vcombine_s64(z, DDI_MULR(x, y));
    
}

INLINE(Vwbu,VWBU_MULR) (Vwbu a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.U = DBU_MULR(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbi,VWBI_MULR) (Vwbi a, Vwbi b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.U = DBI_MULR(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhu,VWHU_MULR) (Vwhu a, Vwhu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.U = DHU_MULR(x.H.U, y.H.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHI_MULR) (Vwhi a, Vwhi b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHI_MULR(x.H.I, y.H.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwwu,VWWU_MULR) (Vwwu a, Vwwu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.W.U = DWU_MULR(x.W.U, y.W.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwwi,VWWI_MULR) (Vwwi a, Vwwi b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.W.I = DWI_MULR(x.W.I, y.W.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vdbu,VDBU_MULR) (Vdbu a, Vdbu b) {return DBU_MULR(a, b);}
INLINE(Vdbi,VDBI_MULR) (Vdbi a, Vdbi b) {return DBI_MULR(a, b);}
INLINE(Vdbc,VDBC_MULR) (Vdbc a, Vdbc b) {a.V0=DBC_MULR(a.V0,b.V0); return a;}
INLINE(Vdhu,VDHU_MULR) (Vdhu a, Vdhu b) {return DHU_MULR(a, b);}
INLINE(Vdhi,VDHI_MULR) (Vdhi a, Vdhi b) {return DHI_MULR(a, b);}
INLINE(Vdwu,VDWU_MULR) (Vdwu a, Vdwu b) {return DWU_MULR(a, b);}
INLINE(Vdwi,VDWI_MULR) (Vdwi a, Vdwi b) {return DWI_MULR(a, b);}
INLINE(Vddu,VDDU_MULR) (Vddu a, Vddu b) {return DDU_MULR(a, b);}
INLINE(Vddi,VDDI_MULR) (Vddi a, Vddi b) {return DDI_MULR(a, b);}

INLINE(Vqbu,VQBU_MULR) (Vqbu a, Vqbu b) {return QBU_MULR(a, b);}
INLINE(Vqbi,VQBI_MULR) (Vqbi a, Vqbi b) {return QBI_MULR(a, b);}
INLINE(Vqbc,VQBC_MULR) (Vqbc a, Vqbc b) {a.V0=QBC_MULR(a.V0,b.V0); return a;}
INLINE(Vqhu,VQHU_MULR) (Vqhu a, Vqhu b) {return QHU_MULR(a, b);}
INLINE(Vqhi,VQHI_MULR) (Vqhi a, Vqhi b) {return QHI_MULR(a, b);}
INLINE(Vqwu,VQWU_MULR) (Vqwu a, Vqwu b) {return QWU_MULR(a, b);}
INLINE(Vqwi,VQWI_MULR) (Vqwi a, Vqwi b) {return QWI_MULR(a, b);}
INLINE(Vqdu,VQDU_MULR) (Vqdu a, Vqdu b) {return QDU_MULR(a, b);}
INLINE(Vqdi,VQDI_MULR) (Vqdi a, Vqdi b) {return QDI_MULR(a, b);}


#if 0 // _LEAVE_ARM_MULR
}
#endif

#if 0 // _ENTER_ARM_MULH
{
#endif

INLINE(flt16_t,  BOOL_MULH)   (_Bool a, flt16_t b) {return a*b;}
INLINE(flt16_t, UCHAR_MULH)   (uchar a, flt16_t b) {return a*b;}
INLINE(flt16_t, SCHAR_MULH)   (schar a, flt16_t b) {return a*b;}
INLINE(flt16_t,  CHAR_MULH)    (char a, flt16_t b) {return a*b;}
INLINE(flt16_t, USHRT_MULH)  (ushort a, flt16_t b) {return a*b;}
INLINE(flt16_t,  SHRT_MULH)   (short a, flt16_t b) {return a*b;}
INLINE(flt16_t,  UINT_MULH)    (uint a, flt16_t b) {return a*b;}
INLINE(flt16_t,   INT_MULH)     (int a, flt16_t b) {return a*b;}
INLINE(flt16_t, ULONG_MULH)   (ulong a, flt16_t b) {return a*b;}
INLINE(flt16_t,  LONG_MULH)    (long a, flt16_t b) {return a*b;}
INLINE(flt16_t,ULLONG_MULH)  (ullong a, flt16_t b) {return a*b;}
INLINE(flt16_t, LLONG_MULH)   (llong a, flt16_t b) {return a*b;}

INLINE(flt16_t, FLT16_MULH) (flt16_t a, flt16_t b) {return a*b;}
INLINE(float,     FLT_MULH)   (float a, flt16_t b) {return a*b;}
INLINE(double,    DBL_MULH)  (double a, flt16_t b) {return a*b;}

#if QUAD_NLLONG == 2
INLINE(flt16_t,mulhqu) (QUAD_UTYPE a, flt16_t b) {return a*b;}
INLINE(flt16_t,mulhqi) (QUAD_ITYPE a, flt16_t b) {return a*b;}
INLINE(QUAD_FTYPE,mulhqf) (QUAD_FTYPE a, flt16_t b) {return a*b;}
#endif


INLINE(Vdhf,VWBU_MULH) (Vwbu a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmul_f16(VWBU_CVHF(a), b);
#else
    float32x4_t p = VWBU_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vmulq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBI_MULH) (Vwbi a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmul_f16(VWBI_CVHF(a), b);
#else
    float32x4_t p = VWBI_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vmulq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBC_MULH) (Vwbc a, Vdhf b)
{
#if CHAR_MIN
    return VWBI_MULH(VWBC_ASBI(a), b);
#else
    return VWBU_MULH(VWBC_ASBU(a), b);
#endif
}


INLINE(float,WHF_MULH) (float am, float bm)
{
#if defined(SPC_ARM_FP16_SIMD)
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    ah = vmul_f16(ah, bh);
#else
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x4_t av = vcvt_f32_f16(ah);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    float32x4_t bv = vcvt_f32_f16(bh);
    av = vmulq_f32(av, bv);
    ah = vcvt_f16_f32(av);
#endif
    aw = vreinterpret_f32_f16(ah);
    return  vget_lane_f32(aw, 0);
    
}


INLINE(Vwhf,VWHU_MULH) (Vwhu a, Vwhf b)
{
    Vwhf    v = VWHU_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_MULH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHI_MULH) (Vwhi a, Vwhf b)
{
    Vwhf    v = VWHI_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_MULH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_MULH) (Vwhf a, Vwhf b)
{
    float   m = WHF_MULH(VWHF_ASTM(a), VWHF_ASTM(b));
    return WHF_ASTV(m);
}


INLINE(Vqhf,VDBU_MULH) (Vdbu a, Vqhf b) 
{
    uint16x8_t  c = vmovl_u8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_u16(c);
    return  vmulq_f16(f, b);
#else

    uint32x4_t  zl = vmovl_u16(vget_low_u16(c));
    float32x4_t fl = vcvtq_f32_u32(zl);
    uint32x4_t  zr = vmovl_u16(vget_high_u16(c));
    float32x4_t fr = vcvtq_f32_u32(zr);
    fl = vmulq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vmulq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBI_MULH) (Vdbi a, Vqhf b) 
{
    int16x8_t   c = vmovl_s8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_s16(c);
    return  vmulq_f16(f, b);
#else
    int32x4_t   zl = vmovl_s16(vget_low_s16(c));
    float32x4_t fl = vcvtq_f32_s32(zl);
    int32x4_t   zr = vmovl_s16(vget_high_s16(c));
    float32x4_t fr = vcvtq_f32_s32(zr);
    fl = vmulq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vmulq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBC_MULH) (Vdbc a, Vqhf b)
{
#if CHAR_MIN
    return  VDBI_MULH(VDBC_ASBI(a), b);
#else
    return  VDBU_MULH(VDBC_ASBU(a), b);
#endif
}


INLINE(Vdhf,VDHU_MULH) (Vdhu a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmul_f16(vcvt_f16_u16(a), b);
#else
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vmulq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHI_MULH) (Vdhi a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmul_f16(vcvt_f16_s16(a), b);
#else
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vmulq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHF_MULH) (Vdhf a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmul_f16(a, b);
#else
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vmulq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}


INLINE(Vwhf,VDWU_MULH) (Vdwu a, Vwhf b)
{
    return  VWHF_MULH(VDWU_CVHF(a), b);
}

INLINE(Vwhf,VDWI_MULH) (Vdwi a, Vwhf b)
{
    return  VWHF_MULH(VDWI_CVHF(a), b);
}

INLINE(Vdwf,VDWF_MULH) (Vdwf a, Vwhf b)
{
    return vmul_f32(a, VWHF_CVWF(b));
}

/*
INLINE(Vhhf,VDDU_MULH) (Vddu a, Vhhf b);
INLINE(Vhhf,VDDI_MULH) (Vddi a, Vhhf b);
INLINE(Vddf,VDDF_MULH) (Vddf a, Vhhf b);


INLINE(Vohf,VQBU_MULH) (Vqbu a, Vohf b);
INLINE(Vohf,VQBI_MULH) (Vqbi a, Vohf b);
INLINE(Vohf,VQBC_MULH) (Vqbc a, Vohf b);

*/

INLINE(Vqhf,VQHU_MULH) (Vqhu a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmulq_f16(vcvtq_f16_u16(a), b);
#else
    return vcombine_f16(
        VDHU_MULH(vget_low_u16(a),  vget_low_f16(b)),
        VDHU_MULH(vget_high_u16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHI_MULH) (Vqhi a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmulq_f16(vcvtq_f16_s16(a), b);
#else
    return vcombine_f16(
        VDHI_MULH(vget_low_s16(a),  vget_low_f16(b)),
        VDHI_MULH(vget_high_s16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHF_MULH) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmulq_f16(a, b);
#else
    return vcombine_f16(
        VDHF_MULH(vget_low_f16(a),  vget_low_f16(b)),
        VDHF_MULH(vget_high_f16(a), vget_high_f16(b))
    );
#endif

}


INLINE(Vdhf,VQWU_MULH) (Vqwu a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_u32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vmulq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vdhf,VQWI_MULH) (Vqwi a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_s32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vmulq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vqwf,VQWF_MULH) (Vqwf a, Vdhf b)
{
    return  vmulq_f32(a, vcvt_f32_f16(b));
}

INLINE(Vwhf,VQDU_MULH) (Vqdu a, Vwhf b)
{
    return  VWHF_MULH(VQDU_CVHF(a), b);
}

INLINE(Vwhf,VQDI_MULH) (Vqdi a, Vwhf b)
{
    return  VWHF_MULH(VQDI_CVHF(a), b);
}

INLINE(Vqdf,VQDF_MULH) (Vqdf a, Vwhf b)
{
    return  vmulq_f64(a, VWHF_CVDF(b));
}

#if 0 // _LEAVE_ARM_MULH
}
#endif

#if 0 // _ENTER_ARM_MULW
{
#endif

INLINE(float,  BOOL_MULW)   (_Bool a, float b) 
{
    uint32x2_t m = vdup_n_u32(a);
    m = vtst_u32(m, vdup_n_u32(UINT32_MAX));
    m = vorr_u32(m, vdup_n_u64(UINT32_C(0x80000000)));
    float32x2_t f = vdup_n_f32(b);
    uint32x2_t  r = vreinterpret_u32_f32(f);
    r = vand_u32(r, m);
    f = vreinterpret_f32_u32(r);
    return  vget_lane_f32(f, 0);
}

INLINE(float, UCHAR_MULW)   (uchar a, float b) {return a*b;}
INLINE(float, SCHAR_MULW)   (schar a, float b) {return a*b;}
INLINE(float,  CHAR_MULW)    (char a, float b) {return a*b;}
INLINE(float, USHRT_MULW)  (ushort a, float b) {return a*b;}
INLINE(float,  SHRT_MULW)   (short a, float b) {return a*b;}
INLINE(float,  UINT_MULW)    (uint a, float b) {return a*b;}
INLINE(float,   INT_MULW)     (int a, float b) {return a*b;}
INLINE(float, ULONG_MULW)   (ulong a, float b) {return a*b;}
INLINE(float,  LONG_MULW)    (long a, float b) {return a*b;}
INLINE(float,ULLONG_MULW)  (ullong a, float b) {return a*b;}
INLINE(float, LLONG_MULW)   (llong a, float b) {return a*b;}

INLINE(float, FLT16_MULW) (flt16_t a, float b) {return a*b;}
INLINE(float,   FLT_MULW)   (float a, float b) {return a*b;}
INLINE(double,  DBL_MULW)  (double a, float b) {return a*b;}

#if QUAD_NLLONG == 2
INLINE(float,mulwqu)   (QUAD_UTYPE a, float b) {return a*b;}
INLINE(float,mulwqi)   (QUAD_ITYPE a, float b) {return a*b;}
INLINE(QUAD_FTYPE,mulwqf) (QUAD_FTYPE a, float b) {return a*b;}
#endif


INLINE(Vqwf,VWBU_MULW) (Vwbu a, Vqwf b)
{
    return  vmulq_f32(VWBU_CVWF(a), b);
}

INLINE(Vqwf,VWBI_MULW) (Vwbi a, Vqwf b)
{
    return  vmulq_f32(VWBI_CVWF(a), b);
}

INLINE(Vqwf,VWBC_MULW) (Vwbc a, Vqwf b)
{
#if CHAR_MIN
    return  VWBI_MULW(VWBC_ASBI(a), b);
#else
    return  VWBU_MULW(VWBC_ASBU(a), b);
#endif
}



INLINE(Vdwf,VWHU_MULW) (Vwhu a, Vdwf b)
{
    return  vmul_f32(VWHU_CVWF(a), b);
}

INLINE(Vdwf,VWHI_MULW) (Vwhi a, Vdwf b)
{
    return  vmul_f32(VWHI_CVWF(a), b);
}

INLINE(Vdwf,VWHF_MULW) (Vwhf a, Vdwf b)
{
    return  vmul_f32(VWHF_CVWF(a), b);
}


INLINE(Vwwf,VWWU_MULW) (Vwwu a, Vwwf b)
{
    return WWF_ASTV((VWWU_ASTV(a)*VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWI_MULW) (Vwwi a, Vwwf b)
{
    return WWF_ASTV((VWWI_ASTV(a)*VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWF_MULW) (Vwwf a, Vwwf b)
{
    return WWF_ASTV((VWWF_ASTM(a)*VWWF_ASTM(b)));
}


INLINE(Vqwf,VDHU_MULW) (Vdhu a, Vqwf b) 
{
    return  vmulq_f32(VDHU_CVWF(a), b);
}

INLINE(Vqwf,VDHI_MULW) (Vdhi a, Vqwf b) 
{
    return  vmulq_f32(VDHI_CVWF(a), b);
}

INLINE(Vqwf,VDHF_MULW) (Vdhf a, Vqwf b) 
{
    return  vmulq_f32(VDHF_CVWF(a), b);
}


INLINE(Vdwf,VDWU_MULW) (Vdwu a, Vdwf b)
{
    return vmul_f32(vcvt_f32_u32(a), b);
}

INLINE(Vdwf,VDWI_MULW) (Vdwi a, Vdwf b)
{
    return vmul_f32(vcvt_f32_s32(a), b);
}

INLINE(Vdwf,VDWF_MULW) (Vdwf a, Vdwf b)
{
    return vmul_f32(a, b);
}


INLINE(Vwwf,VDDU_MULW) (Vddu a, Vwwf b)
{
    return WWF_ASTV((VDDU_ASTV(a)*VWWF_ASTM(b)));
}

INLINE(Vwwf,VDDI_MULW) (Vddi a, Vwwf b)
{
    return WWF_ASTV((VDDI_ASTV(a)*VWWF_ASTM(b)));
}

INLINE(Vddf,VDDF_MULW) (Vddf a, Vwwf b)
{
    return vmul_f64(a, vdup_n_f64(VWWF_ASTM(b)));
}


INLINE(Vqwf,VQWU_MULW) (Vqwu a, Vqwf b)
{
    return  vmulq_f32(vcvtq_f32_u32(a), b);
}

INLINE(Vqwf,VQWI_MULW) (Vqwi a, Vqwf b)
{
    return  vmulq_f32(vcvtq_f32_s32(a), b);
}

INLINE(Vqwf,VQWF_MULW) (Vqwf a, Vqwf b)
{
    return  vmulq_f32(a, b);
}


INLINE(Vdwf,VQDU_MULW) (Vqdu a, Vdwf b)
{
    return  vmul_f32(vcvt_f32_f64(vcvtq_f64_u64(a)), b);
}

INLINE(Vdwf,VQDI_MULW) (Vqdi a, Vdwf b)
{
    return  vmul_f32(vcvt_f32_f64(vcvtq_f64_s64(a)), b);
}

INLINE(Vqdf,VQDF_MULW) (Vqdf a, Vdwf b)
{
    return  vmulq_f64(a, vcvt_f64_f32(b));
}

#if 0 // _LEAVE_ARM_MULW
}
#endif

#if 0 // _ENTER_ARM_MULD
{
#endif

INLINE(double,  BOOL_MULD)   (_Bool a, double b) 
{
    uint64x1_t m = vdup_n_u64(a);
    m = vtst_u64(m, vdup_n_u64(UINT64_MAX));
    m = vorr_u64(m, vdup_n_u64(UINT64_C(0x8000000000000000)));
    float64x1_t f = vdup_n_f64(b);
    uint64x1_t  r = vreinterpret_u64_f64(f);
    r = vand_u64(r, m);
    f = vreinterpret_f64_u64(r);
    return  vget_lane_f64(f, 0);
}

INLINE(double, UCHAR_MULD)   (uchar a, double b) {return a*b;}
INLINE(double, SCHAR_MULD)   (schar a, double b) {return a*b;}
INLINE(double,  CHAR_MULD)    (char a, double b) {return a*b;}
INLINE(double, USHRT_MULD)  (ushort a, double b) {return a*b;}
INLINE(double,  SHRT_MULD)   (short a, double b) {return a*b;}
INLINE(double,  UINT_MULD)    (uint a, double b) {return a*b;}
INLINE(double,   INT_MULD)     (int a, double b) {return a*b;}
INLINE(double, ULONG_MULD)   (ulong a, double b) {return a*b;}
INLINE(double,  LONG_MULD)    (long a, double b) {return a*b;}
INLINE(double,ULLONG_MULD)  (ullong a, double b) {return a*b;}
INLINE(double, LLONG_MULD)   (llong a, double b) {return a*b;}

INLINE(double, FLT16_MULD) (flt16_t a, double b) {return a*b;}
INLINE(double,   FLT_MULD)   (float a, double b) {return a*b;}
INLINE(double,   DBL_MULD)  (double a, double b) {return a*b;}

#if QUAD_NLLONG == 2
INLINE(double,muldqu)   (QUAD_UTYPE a, double b) {return a*b;}
INLINE(double,muldqi)   (QUAD_ITYPE a, double b) {return a*b;}
INLINE(QUAD_FTYPE,muldqf) (QUAD_FTYPE a, double b) {return a*b;}
#endif


INLINE(Vqdf,VWHU_MULD) (Vwhu a, Vqdf b)
{
    return  vmulq_f64(VWHU_CVDF(a), b);
}

INLINE(Vqdf,VWHI_MULD) (Vwhi a, Vqdf b)
{
    return  vmulq_f64(VWHI_CVDF(a), b);
}

INLINE(Vqdf,VWHF_MULD) (Vwhf a, Vqdf b)
{
    return  vmulq_f64(VWHF_CVDF(a), b);
}


INLINE(Vddf,VWWU_MULD) (Vwwu a, Vddf b)
{
    return  vmul_f64(VWWU_CVDF(a), b);
}

INLINE(Vddf,VWWI_MULD) (Vwwi a, Vddf b)
{
    return  vmul_f64(VWWI_CVDF(a), b);
}

INLINE(Vddf,VWWF_MULD) (Vwwf a, Vddf b)
{
    return  vmul_f64(VWWF_CVDF(a), b);
}


INLINE(Vqdf,VDWU_MULD) (Vdwu a, Vqdf b)
{
    return  vmulq_f64(vcvtq_f64_u64(vmovl_u32(a)), b);
}

INLINE(Vqdf,VDWI_MULD) (Vdwi a, Vqdf b)
{
    return  vmulq_f64(vcvtq_f64_s64(vmovl_s32(a)), b);
}

INLINE(Vqdf,VDWF_MULD) (Vdwf a, Vqdf b)
{
    return  vmulq_f64(vcvt_f64_f32(a), b);
}


INLINE(Vddf,VDDU_MULD) (Vddu a, Vddf b)
{
    return  vmul_f64(vcvt_f64_u64(a), b);
}

INLINE(Vddf,VDDI_MULD) (Vddi a, Vddf b)
{
    return  vmul_f64(vcvt_f64_s64(a), b);
}

INLINE(Vddf,VDDF_MULD) (Vddf a, Vddf b)
{
    return  vmul_f64(a, b);
}


INLINE(Vqdf,VQDU_MULD) (Vqdu a, Vqdf b)
{
    return  vmulq_f64(vcvtq_f64_u64(a), b);
}

INLINE(Vqdf,VQDI_MULD) (Vqdi a, Vqdf b)
{
    return  vmulq_f64(vcvtq_f64_s64(a), b);
}

INLINE(Vqdf,VQDF_MULD) (Vqdf a, Vqdf b)
{
    return  vmulq_f64(a, b);
}

#if 0 // _LEAVE_ARM_MULD
}
#endif

#if 0 // _ENTER_ARM_FAML
{
#endif

INLINE(flt16_t, FLT16_FAML)(flt16_t a, flt16_t b, flt16_t c) {return a+b*c;}
INLINE(  float,   FLT_FAML)  (float a,   float b,   float c) {return a+b*c;}
INLINE(  float,   DBL_FAML) (double a,  double b,  double c) {return a+b*c;}

INLINE(Vwyu,VWYU_FAML) (Vwyu a, Vwyu b, Vwyu c)
{
    return  VWYU_ADDL(a, VWYU_MULL(b, c));
}

INLINE(Vwbu,VWBU_FAML) (Vwbu a, Vwbu b, Vwbu c) 
{
    float       am = VWBU_ASTM(a);
    float       bm = VWBU_ASTM(b);
    float       cm = VWBU_ASTM(c);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    float32x2_t cf = vdup_n_f32(cm);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    uint8x8_t   cz = vreinterpret_u8_f32(cf);
    az = vmla_u8(az, bz, cz);
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_FAML) (Vwbi a, Vwbi b, Vwbi c) 
{
    float       am = VWBI_ASTM(a);
    float       bm = VWBI_ASTM(b);
    float       cm = VWBI_ASTM(c);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    float32x2_t cf = vdup_n_f32(cm);
    int8x8_t    az = vreinterpret_s8_f32(af);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    int8x8_t    cz = vreinterpret_s8_f32(cf);
    az = vmla_s8(az, bz, cz);
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_FAML) (Vwbc a, Vwbc b, Vwbc c) 
{
#if CHAR_MIN
    Vwbi r = VWBI_FAML(VWBC_ASBI(a), VWBC_ASBI(b), VWBC_ASBI(c));
    return  VWBI_ASBC(r);
#else
    Vwbu r = VWBU_FAML(VWBC_ASBU(a), VWBC_ASBU(b), VWBC_ASBU(c));
    return  VWBU_ASBC(r);
#endif
}


INLINE(Vwhu,VWHU_FAML) (Vwhu a, Vwhu b, Vwhu c) 
{
    float       am = VWHU_ASTM(a);
    float       bm = VWHU_ASTM(b);
    float       cm = VWHU_ASTM(c);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    float32x2_t cf = vdup_n_f32(cm);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    uint16x4_t  cz = vreinterpret_u16_f32(cf);
    az = vmla_u16(az, bz, cz);
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_FAML) (Vwhi a, Vwhi b, Vwhi c) 
{
    float       am = VWHI_ASTM(a);
    float       bm = VWHI_ASTM(b);
    float       cm = VWHI_ASTM(c);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    float32x2_t cf = vdup_n_f32(cm);
    int16x4_t   az = vreinterpret_s16_f32(af);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    uint16x4_t  cz = vreinterpret_s16_f32(cf);
    az = vmla_s16(az, bz, cz);
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}


INLINE(Vwwu,VWWU_FAML) (Vwwu a, Vwwu b, Vwwu c) 
{
    float       am = VWWU_ASTM(a);
    float       bm = VWWU_ASTM(b);
    float       cm = VWWU_ASTM(c);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    float32x2_t cf = vdup_n_f32(cm);
    int16x4_t   az = vreinterpret_u32_f32(af);
    int16x4_t   bz = vreinterpret_u32_f32(bf);
    uint16x4_t  cz = vreinterpret_u32_f32(cf);
    az = vmla_u32(az, bz, cz);
    af = vreinterpret_f32_u32(az);
    am = vget_lane_f32(af, 0);
    return  WWU_ASTV(am);
}

INLINE(Vwwi,VWWI_FAML) (Vwwi a, Vwwi b, Vwwi c) 
{
    float       am = VWWI_ASTM(a);
    float       bm = VWWI_ASTM(b);
    float       cm = VWWI_ASTM(c);
    float32x2_t af = vdup_n_f32(am);
    float32x2_t bf = vdup_n_f32(bm);
    float32x2_t cf = vdup_n_f32(cm);
    int16x4_t   az = vreinterpret_s32_f32(af);
    int16x4_t   bz = vreinterpret_s32_f32(bf);
    uint16x4_t  cz = vreinterpret_s32_f32(cf);
    az = vmla_s32(az, bz, cz);
    af = vreinterpret_f32_u32(az);
    am = vget_lane_f32(af, 0);
    return  WWI_ASTV(am);
}


INLINE(Vdyu,VDYU_FAML) (Vdyu a, Vdyu b, Vdyu c)
{
    return  VDYU_ADDL(a, VDYU_MULL(b, c));
}

INLINE(Vdbu,VDBU_FAML) (Vdbu a, Vdbu b, Vdbu c) {return vmla_u8(a, b, c);}
INLINE(Vdbi,VDBI_FAML) (Vdbi a, Vdbi b, Vdbi c) {return vmla_s8(a, b, c);}
INLINE(Vdbc,VDBC_FAML) (Vdbc a, Vdbc b, Vdbc c)
{
#if CHAR_MIN
#   define  VDBC_FAML(A, B) VDBI_ASBC(vmul_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_FAML(A, B) VDBU_ASBC(vmul_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_FAML(a, b);
}

INLINE(Vdhu,VDHU_FAML) (Vdhu a, Vdhu b, Vdhu c) {return vmla_u16(a, b, c);}
INLINE(Vdhi,VDHI_FAML) (Vdhi a, Vdhi b, Vdhi c) {return vmla_s16(a, b, c);}
INLINE(Vdhf,VDHF_FAML) (Vdhf a, Vdhf b, Vdhf c)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmla_f16(a, b, c);
#else
    return vcvt_f16_f32(
        vmlaq_f32(
            vcvt_f32_f16(a),
            vcvt_f32_f16(b),
            vcvt_f32_f16(c)
        )
    );
#endif
}

INLINE(Vdwu,VDWU_FAML) (Vdwu a, Vdwu b, Vdwu c) {return vmla_u32(a, b, c);}
INLINE(Vdwi,VDWI_FAML) (Vdwi a, Vdwi b, Vdwi c) {return vmla_s32(a, b, c);}
INLINE(Vddu,VDDU_FAML) (Vddu a, Vddu b, Vddu c)
{
    return  vadd_u64(
        a,
        vdup_n_u64(vget_lane_u64(b, 0)*vget_lane_u64(c, 0))
    );
}

INLINE(Vddi,VDDI_FAML) (Vddi a, Vddi b, Vddi c)
{
    return  vadd_s64(
        a,
        vdup_n_s64(vget_lane_s64(b, 0)*vget_lane_s64(c, 0))
    );
}


INLINE(Vqyu,VQYU_FAML) (Vqyu a, Vqyu b, Vqyu c)
{
    return  VQYU_ADDL(a, VQYU_MULL(b, c));
}

INLINE(Vqbu,VQBU_FAML) (Vqbu a, Vqbu b, Vqbu c) {return vmlaq_u8(a, b, c);}
INLINE(Vqbi,VQBI_FAML) (Vqbi a, Vqbi b, Vqbi c) {return vmlaq_s8(a, b, c);}
INLINE(Vqbc,VQBC_FAML) (Vqbc a, Vqbc b, Vqbc c)
{
#if CHAR_MIN
    return  VQBI_ASBC(
        VQBI_FAML(VQBC_ASBI(a), VQBC_ASBI(a), VQBC_ASBI(c))
    );
#else
    return  VQBU_ASBC(
        VQBU_FAML(VQBC_ASBU(a), VQBC_ASBU(a), VQBC_ASBU(c))
    );
#endif
}

INLINE(Vqhu,VQHU_FAML) (Vqhu a, Vqhu b, Vqhu c) {return vmlaq_u16(a, b, c);}
INLINE(Vqhi,VQHI_FAML) (Vqhi a, Vqhi b, Vqhi c) {return vmlaq_s16(a, b, c);}
INLINE(Vqhf,VQHF_FAML) (Vqhf a, Vqhf b, Vqhf c)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmlaq_f16(a, b, c);
#else
    return  vcombine_f16(
        VDHF_FAML(vget_low_f16( a), vget_low_f16( b), vget_low_f16( c)),
        VDHF_FAML(vget_high_f16(a), vget_high_f16(b), vget_high_f16(c))
    );
#endif
}

INLINE(Vqwu,VQWU_FAML) (Vqwu a, Vqwu b, Vqwu c) {return vmlaq_u32(a, b, c);}
INLINE(Vqwi,VQWI_FAML) (Vqwi a, Vqwi b, Vqwi c) {return vmlaq_s32(a, b, c);}
INLINE(Vqwf,VQWF_FAML) (Vqwf a, Vqwi b, Vqwi c) {return vmlaq_s32(a, b, c);}
INLINE(Vqdu,VQDU_FAML) (Vqdu a, Vqdu b, Vqdu c)
{
    return  vaddq_u64(
        a,
        vcombine_u64(
            vdup_n_u64(vgetq_lane_u64(b,0)*vgetq_lane_u64(c,0)),
            vdup_n_u64(vgetq_lane_u64(b,1)*vgetq_lane_u64(c,1))
        )
    );
}

INLINE(Vqdi,VQDI_FAML) (Vqdi a, Vqdi b, Vqdi c)
{
    return  vaddq_s64(
        a,
        vcombine_s64(
            vdup_n_s64(vgetq_lane_s64(b,0)*vgetq_lane_s64(c,0)),
            vdup_n_s64(vgetq_lane_s64(b,1)*vgetq_lane_s64(c,1))
        )
    );
}

#if 0 // _LEAVE_ARM_FAML
}
#endif

#if 0 // _ENTER_ARM_FAM2
{
#endif

INLINE(Vdhu,VDHU_FAM2) (Vdhu a, Vwbu b, Vwbu c)
{
    return  vget_low_u16(
        vmlal_u8(
            vcombine_u16(a, a),
            vreinterpret_u8_f32(vdup_n_f32(VWBU_ASTM(b))),
            vreinterpret_u8_f32(vdup_n_f32(VWBU_ASTM(c)))
        )
    );
}

INLINE(Vdhi,VDHI_FAM2) (Vdhi a, Vwbi b, Vwbi c)
{
    return  vget_low_s16(
        vmlal_s8(
            vcombine_s16(a, a),
            vreinterpret_s8_f32(vdup_n_f32(VWBI_ASTM(b))),
            vreinterpret_s8_f32(vdup_n_f32(VWBI_ASTM(c)))
        )
    );
}

INLINE(Vdwu,VDWU_FAM2) (Vdwu a, Vwhu b, Vwhu c)
{
    return  vget_low_u32(
        vmlal_u16(
            vcombine_u32(a, a),
            vreinterpret_u16_f32(vdup_n_f32(VWHU_ASTM(b))),
            vreinterpret_u16_f32(vdup_n_f32(VWHU_ASTM(c)))
        )
    );
}

INLINE(Vdwi,VDWI_FAM2) (Vdwi a, Vwhi b, Vwhi c)
{
    return  vget_low_s32(
        vmlal_s16(
            vcombine_s32(a, a),
            vreinterpret_s16_f32(vdup_n_f32(VWHI_ASTM(b))),
            vreinterpret_s16_f32(vdup_n_f32(VWHI_ASTM(c)))
        )
    );
}

INLINE(Vddu,VDDU_FAM2) (Vddu a, Vwwu b, Vwwu c)
{
    return  vget_low_u64(
        vmlal_u32(
            vcombine_u64(a, a),
            vreinterpret_u32_f32(vdup_n_f32(VWWU_ASTM(b))),
            vreinterpret_u32_f32(vdup_n_f32(VWWU_ASTM(c)))
        )
    );
}

INLINE(Vddi,VDDI_FAM2) (Vddi a, Vwwi b, Vwwi c)
{
    return  vget_low_s64(
        vmlal_s32(
            vcombine_s64(a, a),
            vreinterpret_s32_f32(vdup_n_f32(VWWI_ASTM(b))),
            vreinterpret_s32_f32(vdup_n_f32(VWWI_ASTM(c)))
        )
    );
}


INLINE(Vqhu,VQHU_FAM2) (Vqhu a, Vdbu b, Vdbu c) {return  vmlal_u8(a, b, c);}
INLINE(Vqhi,VQHI_FAM2) (Vqhi a, Vdbi b, Vdbi c) {return  vmlal_s8(a, b, c);}
INLINE(Vqwu,VQWU_FAM2) (Vqwu a, Vdhu b, Vdhu c) {return  vmlal_u16(a, b, c);}
INLINE(Vqwi,VQWI_FAM2) (Vqwi a, Vdhi b, Vdhi c) {return  vmlal_s16(a, b, c);}
INLINE(Vqdu,VQDU_FAM2) (Vqdu a, Vdwu b, Vdwu c) {return  vmlal_u32(a, b, c);}
INLINE(Vqdi,VQDI_FAM2) (Vqdi a, Vdwi b, Vdwi c) {return  vmlal_s32(a, b, c);}

#if 0 // _LEAVE_ARM_FAM2
}
#endif

#if 0 // _ENTER_ARM_FAMF
{
#endif

INLINE(flt16_t, FLT16_FAMF) (flt16_t a, flt16_t b, flt16_t c)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vfmah_f16(a, b, c);
#else
    return a+b*c;
#endif

}

INLINE(float, FLT_FAMF) (float a, float b, float c)
{
    return a+b*c;
}

INLINE(float, DBL_FAMF) (double a, double b, double c)
{
    return a+b*c;
}


INLINE(Vwhf,VWHF_FAMF) (Vwhf a, Vwhf b, Vwhf c)
{
    float ma = VWHF_ASTM(a);
    float mb = VWHF_ASTM(b);
    float mc = VWHF_ASTM(c);
    float32x2_t va = vdup_n_f32(ma);
    float32x2_t vb = vdup_n_f32(mb);
    float32x2_t vc = vdup_n_f32(mc);
    float16x4_t fa = vreinterpret_f16_f32(va);
    float16x4_t fb = vreinterpret_f16_f32(vb);
    float16x4_t fc = vreinterpret_f16_f32(vc);
#if defined(SPC_ARM_FP16_SIMD)
    fa = vfma_f16(fa, fb, fc);
#else
    float32x4_t qa = vcvt_f32_f16(fa);
    float32x4_t qb = vcvt_f32_f16(fb);
    float32x4_t qc = vcvt_f32_f16(fc);
    qa = vfmaq_f32(qa, qb, qc);
    fa = vcvt_f16_f32(qa);
#endif
    va = vreinterpret_f32_f16(fa);
    ma = vget_lane_f32(va, 0);
    return  WHF_ASTV(ma);
}

INLINE(Vwwf,VWWF_FAMF) (Vwwf a, Vwwf b, Vwwf c)
{
    float x = VWWF_ASTM(a);
    float y = VWWF_ASTM(b);
    float z = VWWF_ASTM(c);
    return WWF_ASTV((x+y*z));
}


INLINE(Vdhf,VDHF_FAMF) (Vdhf a, Vdhf b, Vdhf c)
{
#if defined(SPC_ARM_FP16_SIMD)
#   define  VDHF_FAMF(A, B, C) vfma_f16(A,B,C)
#else
#   define  VDHF_FAMF(A, B, C) \
 vcvt_f16_f32(              \
    vfmaq_f32(              \
        vcvt_f32_f16(A),    \
        vcvt_f32_f16(B),    \
        vcvt_f32_f16(C)     \
    )                       \
)
#endif

    return  VDHF_FAMF(a, b, c);
}

INLINE(Vdwf,VDWF_FAMF) (Vdwf a, Vdwf b, Vdwf c)
{
#define     VDWF_FAMF(A, B, C) vfma_f32(A,B,C)
    return  VDWF_FAMF(a, b, c);
}

INLINE(Vddf,VDDF_FAMF) (Vddf a, Vddf b, Vddf c)
{
#define     VDDF_FAMF(A, B, C) vfma_f64(A,B,C)
    return  VDDF_FAMF(a, b, c);
}



INLINE(Vqhf,VQHF_FAMF) (Vqhf a, Vqhf b, Vqhf c)
{
#if defined(SPC_ARM_FP16_SIMD)
#   define  VQHF_FAMF(A, B, C) vfmaq_f16(A,B,C)
    return  VDHF_FAMF(a, b, c);
#else
    return  vcombine_f16(
        VDHF_FAMF(
            vget_low_f16(a),
            vget_low_f16(b),
            vget_low_f16(c)
        ),
        VDHF_FAMF(
            vget_high_f16(a),
            vget_high_f16(b),
            vget_high_f16(c)
        )
    );
#endif

}

INLINE(Vqwf,VQWF_FAMF) (Vqwf a, Vqwf b, Vqwf c)
{
#define     VQWF_FAMF(A, B, C) vfmaq_f32(A,B,C)
    return  VQWF_FAMF(a, b, c);
}

INLINE(Vqdf,VQDF_FAMF) (Vqdf a, Vqdf b, Vqdf c)
{
#define     VQDF_FAMF(A, B, C) vfmaq_f64(A,B,C)
    return  VQDF_FAMF(a, b, c);
}

#if 0 // _LEAVE_ARM_FAMF
}
#endif

#if 0 // _ENTER_ARM_FSML
{
#endif

#define     DBU_FSML vmls_u8
#define     DBI_FSML vmls_s8
#if CHAR_MIN
#   define  DBC_FSML vmls_s8
#else
#   define  DBC_FSML vmls_u8
#endif

#define     DHU_FSML vmls_u16
#define     DHI_FSML vmls_s16

#define     DWU_FSML vmls_u32
#define     DWI_FSML vmls_s32

INLINE(uint64x1_t,DDU_FSML) (uint64x1_t a, uint64x1_t b, uint64x1_t c)
{
    uint64_t p = vget_lane_u64(b, 0)*vget_lane_u64(c, 0);
    b = vdup_n_u64(p);
    return  vsub_u64(a, b);
}

INLINE( int64x1_t,DDI_FSML)  (int64x1_t a,  int64x1_t b,  int64x1_t c)
{
    DWRD_VTYPE x={.D.I=a}, y={.D.I=b}, z={.D.I=c};
    x.D.U = DDU_FSML(x.D.U, y.D.U, z.D.U);
    return  x.D.I;
}

#define     QBU_FSML vmlsq_u8
#define     QBI_FSML vmlsq_s8
#if CHAR_MIN
#   define  QBC_FSML vmlsq_s8
#else
#   define  QBC_FSML vmlsq_u8
#endif

#define     QHU_FSML vmlsq_u16
#define     QHI_FSML vmlsq_s16

#define     QWU_FSML vmlsq_u32
#define     QWI_FSML vmlsq_s32

INLINE(uint64x2_t,QDU_FSML) (uint64x2_t a, uint64x2_t b, uint64x2_t c)
{
    uint64_t p;
    p = vgetq_lane_u64(b, 0)*vgetq_lane_u64(c, 0);
    b = vsetq_lane_u64(p, b, 0);
    p = vgetq_lane_u64(b, 1)*vgetq_lane_u64(c, 1);
    b = vsetq_lane_u64(p, b, 1);
    return  vsubq_u64(a, b);
}

INLINE( int64x2_t,QDI_FSML)  (int64x2_t a,  int64x2_t b,  int64x2_t c)
{
    QUAD_VTYPE x={.D.I=a}, y={.D.I=b}, z={.D.I=c};
    x.D.U = QDU_FSML(x.D.U, y.D.U, z.D.U);
    return  x.D.I;
}

INLINE(float, WBZ_FSML) (float a, float b, float c)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}}, z={.W.F={c}};
    x.B.U = DBU_FSML(x.B.U, y.B.U, z.B.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float, WHZ_FSML) (float a, float b, float c)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}}, z={.W.F={c}};
    x.H.U = DHU_FSML(x.H.U, y.H.U, z.H.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float, WWZ_FSML) (float a, float b, float c)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}}, z={.W.F={c}};
    x.W.U = DHU_FSML(x.W.U, y.W.U, z.W.U);
    return  vget_lane_f32(x.W.F, 0);
}


INLINE(Vwbu,VWBU_FSML) (Vwbu a, Vwbu b, Vwbu c) 
{
    a.V0 = WBZ_FSML(a.V0, b.V0, c.V0);
    return a;
}

INLINE(Vwbi,VWBI_FSML) (Vwbi a, Vwbi b, Vwbi c) 
{
    a.V0 = WBZ_FSML(a.V0, b.V0, c.V0);
    return a;
}

INLINE(Vwbc,VWBC_FSML) (Vwbc a, Vwbc b, Vwbc c) 
{
    a.V0 = WBZ_FSML(a.V0, b.V0, c.V0);
    return a;
}


INLINE(Vwhu,VWHU_FSML) (Vwhu a, Vwhu b, Vwhu c) 
{
    a.V0 = WHZ_FSML(a.V0, b.V0, c.V0);
    return a;
}

INLINE(Vwhi,VWHI_FSML) (Vwhi a, Vwhi b, Vwhi c) 
{
    a.V0 = WHZ_FSML(a.V0, b.V0, c.V0);
    return a;
}


INLINE(Vwwu,VWWU_FSML) (Vwwu a, Vwwu b, Vwwu c) 
{
    a.V0 = WHZ_FSML(a.V0, b.V0, c.V0);
    return a;
}

INLINE(Vwwi,VWWI_FSML) (Vwwi a, Vwwi b, Vwwi c) 
{
    a.V0 = WHZ_FSML(a.V0, b.V0, c.V0);
    return a;
}

INLINE(Vdbu,VDBU_FSML) (Vdbu a, Vdbu b, Vdbu c) {return DBU_FSML(a, b, c);}
INLINE(Vdbi,VDBI_FSML) (Vdbi a, Vdbi b, Vdbi c) {return DBI_FSML(a, b, c);}
INLINE(Vdbc,VDBC_FSML) (Vdbc a, Vdbc b, Vdbc c)
{
    a.V0 = DBC_FSML(a.V0, b.V0, c.V0);
    return a;
}

INLINE(Vdhu,VDHU_FSML) (Vdhu a, Vdhu b, Vdhu c) {return DHU_FSML(a, b, c);}
INLINE(Vdhi,VDHI_FSML) (Vdhi a, Vdhi b, Vdhi c) {return DHI_FSML(a, b, c);}

INLINE(Vdwu,VDWU_FSML) (Vdwu a, Vdwu b, Vdwu c) {return DWU_FSML(a, b, c);}
INLINE(Vdwi,VDWI_FSML) (Vdwi a, Vdwi b, Vdwi c) {return DWI_FSML(a, b, c);}

INLINE(Vddu,VDDU_FSML) (Vddu a, Vddu b, Vddu c) {return DDU_FSML(a, b, c);}
INLINE(Vddi,VDDI_FSML) (Vddi a, Vddi b, Vddi c) {return DDI_FSML(a, b, c);}


INLINE(Vqbu,VQBU_FSML) (Vqbu a, Vqbu b, Vqbu c) {return QBU_FSML(a, b, c);}
INLINE(Vqbi,VQBI_FSML) (Vqbi a, Vqbi b, Vqbi c) {return QBI_FSML(a, b, c);}
INLINE(Vqbc,VQBC_FSML) (Vqbc a, Vqbc b, Vqbc c)
{
    a.V0 = QBC_FSML(a.V0, b.V0, c.V0);
    return a;
}

INLINE(Vqhu,VQHU_FSML) (Vqhu a, Vqhu b, Vqhu c) {return QHU_FSML(a, b, c);}
INLINE(Vqhi,VQHI_FSML) (Vqhi a, Vqhi b, Vqhi c) {return QHI_FSML(a, b, c);}

INLINE(Vqwu,VQWU_FSML) (Vqwu a, Vqwu b, Vqwu c) {return QWU_FSML(a, b, c);}
INLINE(Vqwi,VQWI_FSML) (Vqwi a, Vqwi b, Vqwi c) {return QWI_FSML(a, b, c);}

INLINE(Vqdu,VQDU_FSML) (Vqdu a, Vqdu b, Vqdu c) {return QDU_FSML(a, b, c);}
INLINE(Vqdi,VQDI_FSML) (Vqdi a, Vqdi b, Vqdi c) {return QDI_FSML(a, b, c);}

INLINE(Vqqu,VQQU_FSML) (Vqqu a, Vqqu b, Vqqu c)
{
    QUAD_VTYPE x={.Q.U=a}, y={.Q.U=b}, z={.Q.U=c};
#if 1
    x.U -= y.U*z.U;
#else
#endif
    return x.Q.U;
}

INLINE(Vqqi,VQQI_FSML) (Vqqi a, Vqqi b, Vqqi c)
{
    QUAD_VTYPE x={.Q.I=a}, y={.Q.I=b}, z={.Q.I=c};
#if 1
    x.U -= y.U*z.U;
#else
#endif
    return  x.Q.I;
}

#if 0 // _LEAVE_ARM_FSML
}
#endif

#if 0 // _ENTER_ARM_SUML
{
#endif

INLINE(   _Bool,VWYU_SUML) (Vwyu a)
{
    uint32_t v = VWWU_ASTV(VWYU_ASWU(a));
    v = v^(v>>16);
    v = v^(v>>8);
    v = v^(v>>4);
    v = v^(v>>2);
    v = v^(v>>1);
    return v;
}

INLINE( uint8_t,VWBU_SUML) (Vwbu a)
{
    uint8x8_t   v = vdup_n_u8(0);
    float32x2_t m = vreinterpret_f32_u8(v);
    m = vset_lane_f32(VWBU_ASTM(a), m, 0);
    return  vaddv_u8(v);
}

INLINE(  int8_t,VWBI_SUML) (Vwbi a)
{
    int8x8_t    v = vdup_n_s8(0);
    float32x2_t m = vreinterpret_f32_s8(v);
    m = vset_lane_f32(VWBI_ASTM(a), m, 0);
    return  vaddv_s8(v);
}


INLINE(    char,VWBC_SUML) (Vwbc a)
{
#if CHAR_MIN
    return VWBI_SUML(VWBC_ASBI(a));
#else
    return VWBU_SUML(VWBC_ASBU(a));
#endif
}

INLINE(uint16_t,VWHU_SUML) (Vwhu a)
{
    uint16x4_t  v = vdup_n_u16(0);
    float32x2_t m = vreinterpret_f32_u16(v);
    m = vset_lane_f32(VWHU_ASTM(a), m, 0);
    return  vaddv_u16(v);
}

INLINE( int16_t,VWHI_SUML) (Vwhi a)
{
    int16x4_t   v = vdup_n_s16(0);
    float32x2_t m = vreinterpret_f32_s16(v);
    m = vset_lane_f32(VWHI_ASTM(a), m, 0);
    return  vaddv_s16(v);
}


INLINE(   _Bool,VDYU_SUML) (Vdyu a)
{
    uint64x1_t  m = VDYU_ASTM(a);
    uint8x8_t   b = vreinterpret_u8_u64(m);
    return  vaddv_u8(vcnt_u8(b));
}

INLINE( uint8_t,VDBU_SUML) (Vdbu a) {return vaddv_u8(a);}
INLINE(  int8_t,VDBI_SUML) (Vdbi a) {return vaddv_s8(a);}
INLINE(    char,VDBC_SUML) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_SUML(A) vaddv_s8(VDBC_ASBI(A))
#else
#   define  VDBC_SUML(A) vaddv_u8(VDBC_ASBU(A))
#endif
    return  VDBC_SUML(a);
}

INLINE( uint16_t,VDHU_SUML) (Vdhu a) {return vaddv_u16(a);}
INLINE(  int16_t,VDHI_SUML) (Vdhi a) {return vaddv_s16(a);}
INLINE(  flt16_t,VDHF_SUML) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
// no such thing as vaddv_f16
#endif
    return  vaddvq_f32(vcvt_f32_f16(a));
}

INLINE(uint32_t,VDWU_SUML) (Vdwu a) {return vaddv_u32(a);}
INLINE( int32_t,VDWI_SUML) (Vdwi a) {return vaddv_s32(a);}
INLINE(   float,VDWF_SUML) (Vdwf a) {return vaddv_f32(a);}

INLINE(   _Bool,VQYU_SUML) (Vqyu a)
{
    uint64x2_t  m = VQYU_ASTM(a);
    uint8x16_t  b = vreinterpretq_u8_u64(m);
    return  vaddvq_u8(vcntq_u8(b));
}

INLINE( uint8_t,VQBU_SUML) (Vqbu a) {return vaddvq_u8(a);}
INLINE(  int8_t,VQBI_SUML) (Vqbi a) {return vaddvq_s8(a);}
INLINE(    char,VQBC_SUML) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_SUML(A, B) vaddvq_s8(VQBC_ASBI(A))
#else
#   define  VQBC_SUML(A, B) vaddvq_u8(VQBC_ASBU(A))
#endif
    return  VQBC_SUML(a, b);
}

INLINE(uint16_t,VQHU_SUML) (Vqhu a) {return vaddvq_u16(a);}
INLINE( int16_t,VQHI_SUML) (Vqhi a) {return vaddvq_s16(a);}
INLINE( flt16_t,VQHF_SUML) (Vqhf a)
{
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    return  vaddvq_f32(wl)+vaddvq_f32(wr);
}

INLINE(uint32_t,VQWU_SUML) (Vqwu a) {return vaddvq_u32(a);}
INLINE( int32_t,VQWI_SUML) (Vqwi a) {return vaddvq_s32(a);}
INLINE(   float,VQWF_SUML) (Vqwf a) {return vaddvq_f32(a);}
INLINE(uint64_t,VQDU_SUML) (Vqdu a) {return vaddvq_u64(a);}
INLINE( int64_t,VQDI_SUML) (Vqdi a) {return vaddvq_s64(a);}
INLINE(  double,VQDF_SUML) (Vqdf a) {return vaddvq_f64(a);}

#if 0 // _LEAVE_ARM_SUML
}
#endif

#if 0 // _ENTER_ARM_SUM2
{
#endif

INLINE( uint16_t,VWBU_SUM2) (Vwbu a)
{
    uint8x8_t   v = vdup_n_u8(0);
    float32x2_t m = vreinterpret_f32_u8(v);
    m = vset_lane_f32(VWBU_ASTM(a), m, 0);
    return  vaddlv_u8(v);
}

INLINE(  int16_t,VWBI_SUM2) (Vwbi a)
{
    int8x8_t    v = vdup_n_s8(0);
    float32x2_t m = vreinterpret_f32_s8(v);
    m = vset_lane_f32(VWBI_ASTM(a), m, 0);
    return  vaddlv_s8(v);
}

#if CHAR_MIN

INLINE( int16_t,VWBC_SUM2) (Vwbc a)
{
    return VWBI_SUM2(VWBC_ASBI(a));
}

#else

INLINE(uint16_t,VWBC_SUM2) (Vwbc a)
{
    return VWBU_SUM2(VWBC_ASBU(a));
}

#endif

INLINE(uint32_t,VWHU_SUM2) (Vwhu a)
{
    uint16x4_t  v = vdup_n_u16(0);
    float32x2_t m = vreinterpret_f32_u16(v);
    m = vset_lane_f32(VWHU_ASTM(a), m, 0);
    return  vaddlv_u16(v);
}

INLINE( int32_t,VWHI_SUM2) (Vwhi a)
{
    int16x4_t   v = vdup_n_s16(0);
    float32x2_t m = vreinterpret_f32_s16(v);
    m = vset_lane_f32(VWHI_ASTM(a), m, 0);
    return  vaddlv_s16(v);
}

INLINE(uint16_t,VDBU_SUM2) (Vdbu a) {return vaddlv_u8(a);}
INLINE( int16_t,VDBI_SUM2) (Vdbi a) {return vaddlv_s8(a);}
#if CHAR_MIN
INLINE( int16_t,VDBC_SUM2) (Vdbc a)
{
#   define  VDBC_SUM2(A) vaddlv_s8(VDBC_ASBI(A))
    return  VDBC_SUM2(a);
}
#else
INLINE(uint16_t,VDBC_SUM2) (Vdbc a)
{
#   define  VDBC_SUM2(A) vaddlv_u8(VDBC_ASBU(A))
    return  VDBC_SUM2(a);
}
#endif

INLINE(uint32_t,VDHU_SUM2) (Vdhu a) {return vaddlv_u16(a);}
INLINE( int32_t,VDHI_SUM2) (Vdhi a) {return vaddlv_s16(a);}


INLINE(uint64_t,VDWU_SUM2) (Vdwu a) {return vaddlv_u32(a);}
INLINE( int64_t,VDWI_SUM2) (Vdwi a) {return vaddlv_s32(a);}
INLINE(  double,VDWF_SUM2) (Vdwf a) {return vaddvq_f64(vcvt_f64_f32(a));}

INLINE(uint16_t,VQBU_SUM2) (Vqbu a) {return vaddlvq_u8(a);}
INLINE( int16_t,VQBI_SUM2) (Vqbi a) {return vaddlvq_s8(a);}
#if CHAR_MIN
INLINE( int16_t,VQBC_SUM2) (Vqbc a)
{
#   define  VQBC_SUM2(A) vaddlvq_s8(VQBC_ASBI(A))
    return  VQBC_SUM2(a);
}

#else

INLINE(uint16_t,VQBC_SUM2) (Vqbc a)
{
#   define  VQBC_SUM2(A) vaddlvq_u8(VQBC_ASBU(A))
    return  VQBC_SUM2(a);
}

#endif

INLINE(uint32_t,VQHU_SUM2) (Vqhu a) {return vaddlvq_u16(a);}
INLINE( int32_t,VQHI_SUM2) (Vqhi a) {return vaddlvq_s16(a);}
INLINE(   float,VQHF_SUM2) (Vqhf a)
{
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    return  vaddvq_f32(wl)+vaddvq_f32(wr);
}

INLINE(uint64_t,VQWU_SUM2) (Vqwu a) {return vaddlvq_u32(a);}
INLINE( int64_t,VQWI_SUM2) (Vqwi a) {return vaddlvq_s32(a);}
INLINE(  double,VQWF_SUM2) (Vqwf a)
{
    float32x2_t hl = vget_low_f32(a);
    float32x2_t hr = vget_high_f32(a);
    float64x2_t wl = vcvt_f64_f32(hl);
    float64x2_t wr = vcvt_f64_f32(hr);
    return  vaddvq_f64(wl)+vaddvq_f64(wr);
}

#if 0 // _LEAVE_ARM_SUM2
}
#endif

#if 0 // _ENTER_ARM_SUMS
{
#endif

INLINE(   _Bool,VWYU_SUMS) (Vwyu a)
{
    float32x2_t m = vdup_n_f32(VWYU_ASTM(a));
    uint32x2_t  r = vdup_n_u32(~0u);
    uint32x2_t  l = vreinterpret_u32_f32(m);
    l = vtst_u32(l, r);
    return  vget_lane_u32(l, 0);
}

INLINE( uint8_t,VWBU_SUMS) (Vwbu a)
{
    float32x2_t m = vdup_n_f32(0);
    m = vset_lane_f32(VWBU_ASTM(a), m, 0);
    return  vqmovnh_u16(vaddlv_u8(vreinterpret_u8_f32(m)));
}

INLINE(  int8_t,VWBI_SUMS) (Vwbi a)
{
    float32x2_t m = vdup_n_f32(0);
    m = vset_lane_f32(VWBI_ASTM(a), m, 0);
    return  vqmovnh_s16(vaddlv_s8(vreinterpret_s8_f32(m)));
}


INLINE(    char,VWBC_SUMS) (Vwbc a)
{
#if CHAR_MIN
#   define  VWBC_SUMS(A) ((char) VWBI_SUMS(VWBC_ASBI(A)))
#else
#   define  VWBC_SUMS(A) ((char) VWBU_SUMS(VWBC_ASBU(A)))
#endif
    return  VWBC_SUMS(a);
}

INLINE(uint16_t,VWHU_SUMS) (Vwhu a)
{
    float32x2_t m = vdup_n_f32(0);
    m = vset_lane_f32(VWHU_ASTM(a), m, 0);
    return  vqmovns_u32(vaddlv_u16(vreinterpret_u16_f32(m)));
}

INLINE( int16_t,VWHI_SUMS) (Vwhi a)
{
    float32x2_t m = vdup_n_f32(0);
    m = vset_lane_f32(VWHI_ASTM(a), m, 0);
    return  vqmovns_s32(vaddlv_s16(vreinterpret_s16_f32(m)));
}


INLINE(   _Bool,VDYU_SUMS) (Vdyu a)
{
    uint64x1_t  m = VDYU_ASTM(a);
    m = vtst_u64(m, vdup_n_u64(~0ull));
    return vget_lane_u64(m, 0);
}


INLINE( uint8_t,VDBU_SUMS) (Vdbu a)
{
    return  vqmovnh_u16(vaddlv_u8(a));
}

INLINE(  int8_t,VDBI_SUMS) (Vdbi a)
{
    return vqmovnh_s16(vaddlv_s8(a));
}

INLINE(    char,VDBC_SUMS) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_SUMS(A) ((char) VDBI_SUMS(VDBC_ASBI(A)))
#else
#   define  VDBC_SUMS(A) ((char) VDBU_SUMS(VDBC_ASBI(A)))
#endif
    return  VDBC_SUMS(a);
}


INLINE(uint16_t,VDHU_SUMS) (Vdhu a)
{
    return vqmovns_u32(vaddlv_u16(a));
}

INLINE( int16_t,VDHI_SUMS) (Vdhi a)
{
    return vqmovns_s32(vaddlv_s16(a));
}


INLINE(uint32_t,VDWU_SUMS) (Vdwu a)
{
    return  vqmovnd_u64(vaddlv_u32(a));
}

INLINE( int32_t,VDWI_SUMS) (Vdwi a)
{
    return  vqmovnd_s64(vaddlv_s32(a));
}


INLINE(_Bool,VQYU_SUMS) (Vqyu a)
{
    uint64x2_t n = VQYU_ASTM(a);
    n = vtstq_u64(n, vdupq_n_u64(~0ull));
    uint64x1_t r = vorr_u64(
        vget_low_u64(n),
        vget_high_u64(n)
    );
    return  vget_lane_u64(r, 0);
}


INLINE( uint8_t,VQBU_SUMS) (Vqbu a)
{
    return  vqmovnh_u16(vaddlvq_u8(a));
}

INLINE(  int8_t,VQBI_SUMS) (Vqbi a)
{
    return  vqmovnh_s16(vaddlvq_s8(a));
}

INLINE(    char,VQBC_SUMS) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_SUMS(A) ((char) vqmovnh_s16(vaddlvq_s8(VQBC_ASBI(A))))
#else
#   define  VQBC_SUMS(A) ((char) vqmovnh_u16(vaddlvq_u8(VQBC_ASBU(A))))
#endif
    return  VQBC_SUMS(a);
}


INLINE(uint16_t,VQHU_SUMS) (Vqhu a)
{
    return  vqmovns_u32(vaddlvq_u16(a));
}

INLINE( int16_t,VQHI_SUMS) (Vqhi a)
{
    return  vqmovns_u32(vaddlvq_s16(a));
}


INLINE(uint32_t,VQWU_SUMS) (Vqwu a)
{
    return  vqmovnd_u64(vaddlvq_u32(a));
}

INLINE( int32_t,VQWI_SUMS) (Vqwi a)
{
    return  vqmovnd_s64(vaddlvq_s32(a));
}


INLINE(uint64_t,VQDU_SUMS) (Vqdu a)
{
    return  vqaddd_u64(vget_low_u64(a), vget_high_u64(a));
}

INLINE( int64_t,VQDI_SUMS) (Vqdi a)
{
    return  vqaddd_s64(vget_low_s64(a), vget_high_s64(a));
}

#if 0 // _LEAVE_ARM_SUMS
}
#endif

#if 0 // _ENTER_ARM_SUMF
{
#endif

INLINE(flt16_t,VWHF_SUMF) (Vwhf a)
{
    float16x4_t h = vdup_n_f16(0.0f16);
    float32x2_t w = vreinterpret_f32_f16(h);
    w = vset_lane_f32(VWHF_ASTM(a), w, 0);
    float32x4_t q = vcvt_f32_f16(w);
    return vaddvq_f32(q);
}


INLINE(flt16_t,VDHF_SUMF) (Vdhf a)
{
    return vaddvq_f32(vcvt_f32_f16(a));
}

INLINE(  float,VDWF_SUMF) (Vdwf a)
{
    return  vaddv_f32(a);
}


INLINE(flt16_t,VQHF_SUMF) (Vqhf a)
{
    float32x4_t l = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t r = vcvt_f32_f16(vget_high_f16(a));
    return vaddvq_f32(l)+vaddvq_f32(r);
}

INLINE(  float,VQWF_SUMF) (Vqwf a)
{
    return  vaddvq_f32(a);
}

INLINE( double,VQDF_SUMF) (Vqdf a)
{
    return  vaddvq_f64(a);
}


#if 0 // _LEAVE_ARM_SUMF
}
#endif

#if 0 // _ENTER_ARM_REML
{
#endif

INLINE(Vwbu,VWBU_REML) (Vwbu a, Vwbu b)
{
    return  UINT8_NEWW(
        (VWBU_GET1(a, 0)%VWBU_GET1(b, 0)),
        (VWBU_GET1(a, 1)%VWBU_GET1(b, 1)),
        (VWBU_GET1(a, 2)%VWBU_GET1(b, 2)),
        (VWBU_GET1(a, 3)%VWBU_GET1(b, 3))
    );
}

INLINE(Vwbi,VWBI_REML) (Vwbi a, Vwbi b)
{
    return  VWBI_NEWL(
        (VWBI_GET1(a, 0)%VWBI_GET1(b, 0)),
        (VWBI_GET1(a, 1)%VWBI_GET1(b, 1)),
        (VWBI_GET1(a, 2)%VWBI_GET1(b, 2)),
        (VWBI_GET1(a, 3)%VWBI_GET1(b, 3))
    );
}

INLINE(Vwbc,VWBC_REML) (Vwbc a, Vwbc b)
{
    return  VWBC_NEWL(
        (VWBC_GET1(a, 0)%VWBC_GET1(b, 0)),
        (VWBC_GET1(a, 1)%VWBC_GET1(b, 1)),
        (VWBC_GET1(a, 2)%VWBC_GET1(b, 2)),
        (VWBC_GET1(a, 3)%VWBC_GET1(b, 3))
    );
}

INLINE(Vwhu,VWHU_REML) (Vwhu a, Vwhu b)
{
    return  VWHU_NEWL(
        (VWHU_GET1(a, 0)%VWHU_GET1(b, 0)),
        (VWHU_GET1(a, 1)%VWHU_GET1(b, 1))
    );
}

INLINE(Vwhi,VWHI_REML) (Vwhi a, Vwhi b)
{
    return  VWHI_NEWL(
        (VWHI_GET1(a, 0)%VWHI_GET1(b, 0)),
        (VWHI_GET1(a, 1)%VWHI_GET1(b, 1))
    );
}

INLINE(Vwwu,VWWU_REML) (Vwwu a, Vwwu b)
{
    return  UINT_ASTV( (VWWU_ASTV(a)%VWWU_ASTV(b)) );
}

INLINE(Vwwi,VWWI_REML) (Vwwi a, Vwwi b)
{
    return  INT_ASTV( (VWWI_ASTV(a)%VWWI_ASTV(b)) );
}


INLINE(Vdbu,VDBU_REML) (Vdbu a, Vdbu b)
{
    return  VDBU_NEWL(
        (VDBU_GET1(a,0)%VDBU_GET1(b,0)),
        (VDBU_GET1(a,1)%VDBU_GET1(b,1)),
        (VDBU_GET1(a,2)%VDBU_GET1(b,2)),
        (VDBU_GET1(a,3)%VDBU_GET1(b,3)),
        (VDBU_GET1(a,4)%VDBU_GET1(b,4)),
        (VDBU_GET1(a,5)%VDBU_GET1(b,5)),
        (VDBU_GET1(a,6)%VDBU_GET1(b,6)),
        (VDBU_GET1(a,7)%VDBU_GET1(b,7))
    );
}

INLINE(Vdbi,VDBI_REML) (Vdbi a, Vdbi b)
{
    return  VDBI_NEWL(
        (VDBI_GET1(a,0)%VDBI_GET1(b,0)),
        (VDBI_GET1(a,1)%VDBI_GET1(b,1)),
        (VDBI_GET1(a,2)%VDBI_GET1(b,2)),
        (VDBI_GET1(a,3)%VDBI_GET1(b,3)),
        (VDBI_GET1(a,4)%VDBI_GET1(b,4)),
        (VDBI_GET1(a,5)%VDBI_GET1(b,5)),
        (VDBI_GET1(a,6)%VDBI_GET1(b,6)),
        (VDBI_GET1(a,7)%VDBI_GET1(b,7))
    );
}

INLINE(Vdbc,VDBC_REML) (Vdbc a, Vdbc b)
{
    return  VDBC_NEWL(
        (VDBC_GET1(a,0)%VDBC_GET1(b,0)),
        (VDBC_GET1(a,1)%VDBC_GET1(b,1)),
        (VDBC_GET1(a,2)%VDBC_GET1(b,2)),
        (VDBC_GET1(a,3)%VDBC_GET1(b,3)),
        (VDBC_GET1(a,4)%VDBC_GET1(b,4)),
        (VDBC_GET1(a,5)%VDBC_GET1(b,5)),
        (VDBC_GET1(a,6)%VDBC_GET1(b,6)),
        (VDBC_GET1(a,7)%VDBC_GET1(b,7))
    );
}

INLINE(Vdhu,VDHU_REML) (Vdhu a, Vdhu b)
{
    return VDHU_NEWL(
        (VDHU_GET1(a, 0)%VDHU_GET1(b, 0)),
        (VDHU_GET1(a, 1)%VDHU_GET1(b, 1)),
        (VDHU_GET1(a, 2)%VDHU_GET1(b, 2)),
        (VDHU_GET1(a, 3)%VDHU_GET1(b, 3))
    );
}

INLINE(Vdhi,VDHI_REML) (Vdhi a, Vdhi b)
{
    return VDHI_NEWL(
        (VDHI_GET1(a, 0)%VDHI_GET1(b, 0)),
        (VDHI_GET1(a, 1)%VDHI_GET1(b, 1)),
        (VDHI_GET1(a, 2)%VDHI_GET1(b, 2)),
        (VDHI_GET1(a, 3)%VDHI_GET1(b, 3))
    );
}

INLINE(Vdwu,VDWU_REML) (Vdwu a, Vdwu b)
{
    return VDWU_NEWL(
        (VDWU_GET1(a, 0)%VDWU_GET1(b, 0)),
        (VDWU_GET1(a, 1)%VDWU_GET1(b, 1))
    );
}

INLINE(Vdwi,VDWI_REML) (Vdwi a, Vdwi b)
{
    return VDWI_NEWL(
        (VDWI_GET1(a, 0)%VDWI_GET1(b, 0)),
        (VDWI_GET1(a, 1)%VDWI_GET1(b, 1))
    );
}

INLINE(Vddu,VDDU_REML) (Vddu a, Vddu b)
{
    return  UINT64_ASTV( (VDDU_ASTV(a)%VDDU_ASTV(b)) );
}

INLINE(Vddi,VDDI_REML) (Vddi a, Vddi b)
{
    return  INT64_ASTV( (VDDI_ASTV(a)%VDDI_ASTV(b)) );
}


INLINE(Vqbu,VQBU_REML) (Vqbu a, Vqbu b)
{
    return  VQBU_NEWL(
        (VQBU_GET1(a, 0)%VQBU_GET1(b, 0)),
        (VQBU_GET1(a, 1)%VQBU_GET1(b, 1)),
        (VQBU_GET1(a, 2)%VQBU_GET1(b, 2)),
        (VQBU_GET1(a, 3)%VQBU_GET1(b, 3)),
        (VQBU_GET1(a, 4)%VQBU_GET1(b, 4)),
        (VQBU_GET1(a, 5)%VQBU_GET1(b, 5)),
        (VQBU_GET1(a, 6)%VQBU_GET1(b, 6)),
        (VQBU_GET1(a, 7)%VQBU_GET1(b, 7)),
        (VQBU_GET1(a, 8)%VQBU_GET1(b, 8)),
        (VQBU_GET1(a, 9)%VQBU_GET1(b, 9)),
        (VQBU_GET1(a,10)%VQBU_GET1(b,10)),
        (VQBU_GET1(a,11)%VQBU_GET1(b,11)),
        (VQBU_GET1(a,12)%VQBU_GET1(b,12)),
        (VQBU_GET1(a,13)%VQBU_GET1(b,13)),
        (VQBU_GET1(a,14)%VQBU_GET1(b,14)),
        (VQBU_GET1(a,15)%VQBU_GET1(b,15))
    );
}

INLINE(Vqbi,VQBI_REML) (Vqbi a, Vqbi b)
{
    return  VQBI_NEWL(
        (VQBI_GET1(a, 0)%VQBI_GET1(b, 0)),
        (VQBI_GET1(a, 1)%VQBI_GET1(b, 1)),
        (VQBI_GET1(a, 2)%VQBI_GET1(b, 2)),
        (VQBI_GET1(a, 3)%VQBI_GET1(b, 3)),
        (VQBI_GET1(a, 4)%VQBI_GET1(b, 4)),
        (VQBI_GET1(a, 5)%VQBI_GET1(b, 5)),
        (VQBI_GET1(a, 6)%VQBI_GET1(b, 6)),
        (VQBI_GET1(a, 7)%VQBI_GET1(b, 7)),
        (VQBI_GET1(a, 8)%VQBI_GET1(b, 8)),
        (VQBI_GET1(a, 9)%VQBI_GET1(b, 9)),
        (VQBI_GET1(a,10)%VQBI_GET1(b,10)),
        (VQBI_GET1(a,11)%VQBI_GET1(b,11)),
        (VQBI_GET1(a,12)%VQBI_GET1(b,12)),
        (VQBI_GET1(a,13)%VQBI_GET1(b,13)),
        (VQBI_GET1(a,14)%VQBI_GET1(b,14)),
        (VQBI_GET1(a,15)%VQBI_GET1(b,15))
    );
}


INLINE(Vqbc,VQBC_REML) (Vqbc a, Vqbc b)
{
    return  VQBC_NEWL(
        (VQBC_GET1(a, 0)%VQBC_GET1(b, 0)),
        (VQBC_GET1(a, 1)%VQBC_GET1(b, 1)),
        (VQBC_GET1(a, 2)%VQBC_GET1(b, 2)),
        (VQBC_GET1(a, 3)%VQBC_GET1(b, 3)),
        (VQBC_GET1(a, 4)%VQBC_GET1(b, 4)),
        (VQBC_GET1(a, 5)%VQBC_GET1(b, 5)),
        (VQBC_GET1(a, 6)%VQBC_GET1(b, 6)),
        (VQBC_GET1(a, 7)%VQBC_GET1(b, 7)),
        (VQBC_GET1(a, 8)%VQBC_GET1(b, 8)),
        (VQBC_GET1(a, 9)%VQBC_GET1(b, 9)),
        (VQBC_GET1(a,10)%VQBC_GET1(b,10)),
        (VQBC_GET1(a,11)%VQBC_GET1(b,11)),
        (VQBC_GET1(a,12)%VQBC_GET1(b,12)),
        (VQBC_GET1(a,13)%VQBC_GET1(b,13)),
        (VQBC_GET1(a,14)%VQBC_GET1(b,14)),
        (VQBC_GET1(a,15)%VQBC_GET1(b,15))
    );
}

INLINE(Vqhu,VQHU_REML) (Vqhu a, Vqhu b)
{
    return  VQHU_NEWL(
        (VQHU_GET1(a, 0)%VQHU_GET1(b, 0)),
        (VQHU_GET1(a, 1)%VQHU_GET1(b, 1)),
        (VQHU_GET1(a, 2)%VQHU_GET1(b, 2)),
        (VQHU_GET1(a, 3)%VQHU_GET1(b, 3)),
        (VQHU_GET1(a, 4)%VQHU_GET1(b, 4)),
        (VQHU_GET1(a, 5)%VQHU_GET1(b, 5)),
        (VQHU_GET1(a, 6)%VQHU_GET1(b, 6)),
        (VQHU_GET1(a, 7)%VQHU_GET1(b, 7))
    );
}

INLINE(Vqhi,VQHI_REML) (Vqhi a, Vqhi b)
{
    return  VQHI_NEWL(
        (VQHI_GET1(a, 0)%VQHI_GET1(b, 0)),
        (VQHI_GET1(a, 1)%VQHI_GET1(b, 1)),
        (VQHI_GET1(a, 2)%VQHI_GET1(b, 2)),
        (VQHI_GET1(a, 3)%VQHI_GET1(b, 3)),
        (VQHI_GET1(a, 4)%VQHI_GET1(b, 4)),
        (VQHI_GET1(a, 5)%VQHI_GET1(b, 5)),
        (VQHI_GET1(a, 6)%VQHI_GET1(b, 6)),
        (VQHI_GET1(a, 7)%VQHI_GET1(b, 7))
    );
}

INLINE(Vqwu,VQWU_REML) (Vqwu a, Vqwu b)
{
    return  VQWU_NEWL(
        (VQWU_GET1(a, 0)%VQWU_GET1(b, 0)),
        (VQWU_GET1(a, 1)%VQWU_GET1(b, 1)),
        (VQWU_GET1(a, 2)%VQWU_GET1(b, 2)),
        (VQWU_GET1(a, 3)%VQWU_GET1(b, 3))
    );
}

INLINE(Vqwi,VQWI_REML) (Vqwi a, Vqwi b)
{
    return  VQWI_NEWL(
        (VQWI_GET1(a, 0)%VQWI_GET1(b, 0)),
        (VQWI_GET1(a, 1)%VQWI_GET1(b, 1)),
        (VQWI_GET1(a, 2)%VQWI_GET1(b, 2)),
        (VQWI_GET1(a, 3)%VQWI_GET1(b, 3))
    );
}

INLINE(Vqdu,VQDU_REML) (Vqdu a, Vqdu b)
{
    return  VQDU_NEWL(
        (VQDU_GET1(a, 0)%VQDU_GET1(b, 0)),
        (VQDU_GET1(a, 1)%VQDU_GET1(b, 1))
    );
}

INLINE(Vqdi,VQDI_REML) (Vqdi a, Vqdi b)
{
    return  VQDI_NEWL(
        (VQDI_GET1(a, 0)%VQDI_GET1(b, 0)),
        (VQDI_GET1(a, 1)%VQDI_GET1(b, 1))
    );
}

#if 0 // _LEAVE_ARM_REML
}
#endif

#if 0 // _ENTER_ARM_REM2
{
#endif

INLINE(Vwbu,VDHU_REM2) (Vdhu a, Vwbu b)
{
    return  UINT8_NEWW(
        (vget_lane_u16(a, 0)%VWBU_GET1(b, 0)),
        (vget_lane_u16(a, 1)%VWBU_GET1(b, 1)),
        (vget_lane_u16(a, 2)%VWBU_GET1(b, 2)),
        (vget_lane_u16(a, 3)%VWBU_GET1(b, 3))
    );
}

INLINE(Vwbi,VDHI_REM2) (Vdhi a, Vwbi b)
{
    return  VWBI_NEWL(
        (vget_lane_s16(a, 0)%VWBI_GET1(b, 0)),
        (vget_lane_s16(a, 1)%VWBI_GET1(b, 1)),
        (vget_lane_s16(a, 2)%VWBI_GET1(b, 2)),
        (vget_lane_s16(a, 3)%VWBI_GET1(b, 3))
    );
}


INLINE(Vwhu,VDWU_REM2) (Vdwu a, Vwhu b)
{
    return  VWHU_NEWL(
        (vget_lane_u32(a, 0)%VWHU_GET1(b, 0)),
        (vget_lane_u32(a, 1)%VWHU_GET1(b, 1))
    );
}

INLINE(Vwhi,VDWI_REM2) (Vdwi a, Vwhi b)
{
    return  VWHI_NEWL(
        (vget_lane_s32(a, 0)%VWHI_GET1(b, 0)),
        (vget_lane_s32(a, 1)%VWHI_GET1(b, 1))
    );
}


INLINE(Vwwu,VDDU_REM2) (Vddu a, Vwwu b)
{
    return  UINT32_ASTV((vget_lane_u64(a, 0)%VWWU_ASTV(b)));
}

INLINE(Vwwi,VDDI_REM2) (Vddi a, Vwwi b)
{
    return  INT32_ASTV((vget_lane_s64(a, 0)%VWWI_ASTV(b)));
}


INLINE(Vdbu,VQHU_REM2) (Vqhu a, Vdbu b)
{
    return  VDBU_NEWL(
        (vgetq_lane_u16(a, 0)%vget_lane_u8(b, 0)),
        (vgetq_lane_u16(a, 1)%vget_lane_u8(b, 1)),
        (vgetq_lane_u16(a, 2)%vget_lane_u8(b, 2)),
        (vgetq_lane_u16(a, 3)%vget_lane_u8(b, 3)),
        (vgetq_lane_u16(a, 4)%vget_lane_u8(b, 4)),
        (vgetq_lane_u16(a, 5)%vget_lane_u8(b, 5)),
        (vgetq_lane_u16(a, 6)%vget_lane_u8(b, 6)),
        (vgetq_lane_u16(a, 7)%vget_lane_u8(b, 7))
    );
}

INLINE(Vdbi,VQHI_REM2) (Vqhi a, Vdbi b)
{
    return  VDBU_NEWL(
        (vgetq_lane_s16(a, 0)%vget_lane_s8(b, 0)),
        (vgetq_lane_s16(a, 1)%vget_lane_s8(b, 1)),
        (vgetq_lane_s16(a, 2)%vget_lane_s8(b, 2)),
        (vgetq_lane_s16(a, 3)%vget_lane_s8(b, 3)),
        (vgetq_lane_s16(a, 4)%vget_lane_s8(b, 4)),
        (vgetq_lane_s16(a, 5)%vget_lane_s8(b, 5)),
        (vgetq_lane_s16(a, 6)%vget_lane_s8(b, 6)),
        (vgetq_lane_s16(a, 7)%vget_lane_s8(b, 7))
    );
}


INLINE(Vdhu,VQWU_REM2) (Vqwu a, Vdhu b)
{
    return  VDHU_NEWL(
        (vgetq_lane_u32(a, 0)%vget_lane_u16(b, 0)),
        (vgetq_lane_u32(a, 1)%vget_lane_u16(b, 1)),
        (vgetq_lane_u32(a, 2)%vget_lane_u16(b, 2)),
        (vgetq_lane_u32(a, 3)%vget_lane_u16(b, 3))
    );
}

INLINE(Vdhi,VQWI_REM2) (Vqwi a, Vdhi b)
{
    return  VDHI_NEWL(
        (vgetq_lane_s32(a, 0)%vget_lane_s16(b, 0)),
        (vgetq_lane_s32(a, 1)%vget_lane_s16(b, 1)),
        (vgetq_lane_s32(a, 2)%vget_lane_s16(b, 2)),
        (vgetq_lane_s32(a, 3)%vget_lane_s16(b, 3))
    );
}


INLINE(Vdwu,VQDU_REM2) (Vqdu a, Vdwu b)
{
    return  VDWU_NEWL(
        (vgetq_lane_u64(a, 0)%vget_lane_u32(b, 0)),
        (vgetq_lane_u64(a, 1)%vget_lane_u32(b, 1))
    );
}

INLINE(Vdwi,VQDI_REM2) (Vqdi a, Vdwi b)
{
    return  VDWI_NEWL(
        (vgetq_lane_s64(a, 0)%vget_lane_s32(b, 0)),
        (vgetq_lane_s64(a, 1)%vget_lane_s32(b, 1))
    );
}

#if 0 // _LEAVE_ARM_REM2
}
#endif

#if 0 // _ENTER_ARM_TSTS
{
#endif

INLINE(int16_t,FLT16_TSTS) (flt16_t a, unsigned b)
{
    HALF_TYPE z = {.F=a};
    return  (z.U&b) ? -1 : 0;
}

INLINE(int32_t,FLT_TSTS) (float a, unsigned b)
{
    WORD_TYPE z = {.F=a};
    return  (z.U&b) ? -1 : 0;
}

INLINE(int64_t,DBL_TSTS) (double a, uint64_t b)
{
    DWRD_TYPE z = {.F=a};
    return  (z.U&b) ? -1 : 0;
}

INLINE(float,WBZ_TSTS) (float a, float b) 
{
    uint8x8_t l = vreinterpret_u8_f32(vdup_n_f32(a));
    uint8x8_t r = vreinterpret_u8_f32(vdup_n_f32(b));
    l = vtst_u8(l, r);
    float32x2_t v = vreinterpret_f32_u8(l);
    return  vget_lane_f32(v, 0);
}

INLINE(float,WHZ_TSTS) (float a, float b) 
{
    uint16x4_t l = vreinterpret_u16_f32(vdup_n_f32(a));
    uint16x4_t r = vreinterpret_u16_f32(vdup_n_f32(b));
    l = vtst_u16(l, r);
    float32x2_t v = vreinterpret_f32_u16(l);
    return  vget_lane_f32(v, 0);
}

INLINE(float,WWZ_TSTS) (float a, float b) 
{
    uint32x2_t l = vreinterpret_u32_f32(vdup_n_f32(a));
    uint32x2_t r = vreinterpret_u32_f32(vdup_n_f32(b));
    l = vtst_u32(l, r);
    float32x2_t v = vreinterpret_f32_u32(l);
    return  vget_lane_f32(v, 0);
}


INLINE(Vwbu,VWBU_TSTS) (Vwbu a, Vwbu b) 
{
    return  ((Vwbu){WBZ_TSTS(a.V0, b.V0)});
}

INLINE(Vwbi,VWBI_TSTS) (Vwbi a, Vwbu b) 
{
    return  ((Vwbi){WBZ_TSTS(a.V0, b.V0)});
}

INLINE(Vwbc,VWBC_TSTS) (Vwbc a, Vwbu b) 
{
    return  ((Vwbc){WBZ_TSTS(a.V0, b.V0)});
}


INLINE(Vwhu,VWHU_TSTS) (Vwhu a, Vwhu b)
{
    return  ((Vwhu){WHZ_TSTS(a.V0, b.V0)});
}

INLINE(Vwhi,VWHI_TSTS) (Vwhi a, Vwhu b)
{
    return  ((Vwhi){WHZ_TSTS(a.V0, b.V0)});
}

INLINE(Vwhi,VWHF_TSTS) (Vwhi a, Vwhu b)
{
    return  ((Vwhi){WHZ_TSTS(a.V0, b.V0)});
}


INLINE(Vwwu,VWWU_TSTS) (Vwwu a, Vwwu b)
{
    return  ((Vwwu){WWZ_TSTS(a.V0, b.V0)});
}

INLINE(Vwwi,VWWI_TSTS) (Vwwi a, Vwwu b)
{
    return  ((Vwwi){WWZ_TSTS(a.V0, b.V0)});
}

INLINE(Vwwi,VWWF_TSTS) (Vwwf a, Vwwu b)
{
    return  ((Vwwi){WWZ_TSTS(a.V0, b.V0)});
}


INLINE(Vdbu,VDBU_TSTS) (Vdbu a, Vdbu b) {return vtst_u8(a, b);}
INLINE(Vdbi,VDBI_TSTS) (Vdbi a, Vdbu b) 
{
    return  VDBU_ASTI(VDBU_TSTS(VDBI_ASTU(a), b));
}

INLINE(Vdbc,VDBC_TSTS) (Vdbc a, Vdbu b) 
{
    return  VDBU_ASBC(VDBU_TSTS(VDBC_ASTU(a), b));
}


INLINE(Vdhu,VDHU_TSTS) (Vdhu a, Vdhu b) {return vtst_u16(a, b);}
INLINE(Vdhi,VDHI_TSTS) (Vdhi a, Vdhu b) 
{
    return  VDHU_ASTI(VDHU_TSTS(VDHI_ASTU(a), b));
}

INLINE(Vdhi,VDHF_TSTS) (Vdhf a, Vdhu b)
{
    uint16x4_t l = vreinterpret_u16_f16(a);
    l = vtst_u16(l, b);
    return  vreinterpret_s16_u16(l);
}

INLINE(Vdwu,VDWU_TSTS) (Vdwu a, Vdwu b) {return vtst_u32(a, b);}
INLINE(Vdwi,VDWI_TSTS) (Vdwi a, Vdwu b) 
{
    return  VDWU_ASTI(VDWU_TSTS(VDWI_ASTU(a), b));
}

INLINE(Vdwi,VDWF_TSTS) (Vdwf a, Vdwu b)
{
    uint32x2_t l = vreinterpret_u32_f32(a);
    l = vtst_u32(l, b);
    return  vreinterpret_s32_u32(l);
}

INLINE(Vddu,VDDU_TSTS) (Vddu a, Vddu b) {return vtst_u64(a, b);}
INLINE(Vddi,VDDI_TSTS) (Vddi a, Vddu b) 
{
    return  VDDU_ASTI(VDDU_TSTS(VDDI_ASTU(a), b));
}

INLINE(Vddf,VDDF_TSTS) (Vddf a, Vddu b)
{
    uint64x1_t l = vreinterpret_u64_f64(a);
    l = vtst_u64(l, b);
    return  vreinterpret_s64_u64(l);
}


INLINE(Vqbu,VQBU_TSTS) (Vqbu a, Vqbu b) {return vtstq_u8(a, b);}
INLINE(Vqbi,VQBI_TSTS) (Vqbi a, Vqbu b) 
{
    return  VQBU_ASTI(VQBU_TSTS(VQBI_ASTU(a), b));
}

INLINE(Vqbc,VQBC_TSTS) (Vqbc a, Vqbu b) 
{
    return  VQBU_ASBC(VQBU_TSTS(VQBC_ASTU(a), b));
}


INLINE(Vqhu,VQHU_TSTS) (Vqhu a, Vqhu b) {return vtstq_u16(a, b);}
INLINE(Vqhi,VQHI_TSTS) (Vqhi a, Vqhu b) 
{
    return  VQHU_ASTI(VQHU_TSTS(VQHI_ASTU(a), b));
}

INLINE(Vqhi,VQHF_TSTS) (Vqhf a, Vqhu b)
{
    uint16x8_t l = vreinterpretq_u16_f16(a);
    l = vtstq_u16(l, b);
    return  vreinterpretq_s16_u16(l);
}

INLINE(Vqwu,VQWU_TSTS) (Vqwu a, Vqwu b) {return vtstq_u32(a, b);}
INLINE(Vqwi,VQWI_TSTS) (Vqwi a, Vqwu b) 
{
    return  VQWU_ASTI(VQWU_TSTS(VQWI_ASTU(a), b));
}

INLINE(Vqwi,VQWF_TSTS) (Vqwf a, Vqwu b)
{
    uint32x4_t l = vreinterpretq_u32_f32(a);
    l = vtstq_u32(l, b);
    return  vreinterpretq_s32_u32(l);
}

INLINE(Vqdu,VQDU_TSTS) (Vqdu a, Vqdu b) {return vtstq_u64(a, b);}
INLINE(Vqdi,VQDI_TSTS) (Vqdi a, Vqdu b) 
{
    return  VQDU_ASTI(VQDU_TSTS(VQDI_ASTU(a), b));
}

INLINE(Vqdi,VQDF_TSTS) (Vqdf a, Vqdu b)
{
    uint64x2_t l = vreinterpretq_u64_f64(a);
    l = vtstq_u64(l, b);
    return  vreinterpretq_s64_u64(l);
}

#if 0 // _LEAVE_ARM_TSTS
}
#endif

#if 0 // _ENTER_ARM_TSTY
{
#endif

INLINE(_Bool,FLT16_TSTY) (flt16_t a, unsigned b)
{
#define FLT16_TSTY(A, B) ((_Bool) (B&(HALF_TYPE){.F=A}.U))
    HALF_TYPE z = {.F=a};
    return  b&z.U;
}

INLINE(_Bool,FLT_TSTY) (float a, uint32_t b)
{
#define FLT_TSTY(A, B) ((_Bool) (B&(WORD_TYPE){.F=A}.U))
    HALF_TYPE z = {.F=a};
    return  b&z.U;
}

INLINE(_Bool,DBL_TSTY) (double a, uint64_t b)
{
#define DBL_TSTY(A, B) ((_Bool) (B&(DWRD_TYPE){.F=A}.U))
    DWRD_TYPE z = {.F=a};
    return  b&z.U;
}

INLINE(float,WBZ_TSTY) (float a, float b) 
{
    float32x2_t p = {a};
    float32x2_t q = {b};
    uint8x8_t   l = vreinterpret_u8_f32(p);
    uint8x8_t   r = vreinterpret_u8_f32(q);
    l = vtst_u8(l, r);
    l = vshr_n_u8(l, 7);
    p = vreinterpret_f32_u8(l);
    return  vget_lane_f32(p, 0);
}

INLINE(float,WHZ_TSTY) (float a, float b) 
{
    float32x2_t p = {a};
    float32x2_t q = {b};
    uint16x4_t  l = vreinterpret_u16_f32(p);
    uint16x4_t  r = vreinterpret_u16_f32(q);
    l = vtst_u16(l, r);
    l = vshr_n_u16(l, 15);
    p = vreinterpret_f32_u16(l);
    return  vget_lane_f32(p, 0);
}

INLINE(float,WWZ_TSTY) (float a, float b) 
{
    float32x2_t p = {a};
    float32x2_t q = {b};
    uint32x2_t  l = vreinterpret_u32_f32(p);
    uint32x2_t  r = vreinterpret_u32_f32(q);
    l = vtst_u32(l, r);
    l = vshr_n_u32(l, 31);
    p = vreinterpret_f32_u32(l);
    return  vget_lane_f32(p, 0);
}


INLINE(Vwbu,VWBU_TSTY) (Vwbu a, Vwbu b) 
{
    return  ((Vwbu){WBZ_TSTY(a.V0, b.V0)});
}

INLINE(Vwbi,VWBI_TSTY) (Vwbi a, Vwbu b) 
{
    return  ((Vwbi){WBZ_TSTY(a.V0, b.V0)});
}

INLINE(Vwbc,VWBC_TSTY) (Vwbc a, Vwbu b) 
{
    return  ((Vwbc){WBZ_TSTY(a.V0, b.V0)});
}


INLINE(Vwhu,VWHU_TSTY) (Vwhu a, Vwhu b)
{
    return  ((Vwhu){WHZ_TSTY(a.V0, b.V0)});
}

INLINE(Vwhi,VWHI_TSTY) (Vwhi a, Vwhu b)
{
    return  ((Vwhi){WHZ_TSTY(a.V0, b.V0)});
}

INLINE(Vwhi,VWHF_TSTY) (Vwhi a, Vwhu b)
{
    return  ((Vwhi){WHZ_TSTY(a.V0, b.V0)});
}


INLINE(Vwwu,VWWU_TSTY) (Vwwu a, Vwwu b)
{
    return  ((Vwwu){WWZ_TSTY(a.V0, b.V0)});
}

INLINE(Vwwi,VWWI_TSTY) (Vwwi a, Vwwu b)
{
    return  ((Vwwi){WWZ_TSTY(a.V0, b.V0)});
}

INLINE(Vwwi,VWWF_TSTY) (Vwwf a, Vwwu b)
{
    return  ((Vwwi){WWZ_TSTY(a.V0, b.V0)});
}


INLINE(Vdbu,VDBU_TSTY) (Vdbu a, Vdbu b) 
{
    return  vshr_n_u8(vtst_u8(a, b), 7);
}

INLINE(Vdbi,VDBI_TSTY) (Vdbi a, Vdbu b) 
{
    return  VDBU_ASTI(VDBU_TSTY(VDBI_ASTU(a), b));
}

INLINE(Vdbc,VDBC_TSTY) (Vdbc a, Vdbu b) 
{
    return  VDBU_ASBC(VDBU_TSTY(VDBC_ASTU(a), b));
}


INLINE(Vdhu,VDHU_TSTY) (Vdhu a, Vdhu b) 
{
    return  vshr_n_u16(vtst_u16(a, b), 15);
}

INLINE(Vdhi,VDHI_TSTY) (Vdhi a, Vdhu b) 
{
    return  VDHU_ASTI(VDHU_TSTY(VDHI_ASTU(a), b));
}

INLINE(Vdhi,VDHF_TSTY) (Vdhf a, Vdhu b) 
{
    return  VDHU_ASTI(VDHU_TSTY(VDHF_ASTU(a), b));
}


INLINE(Vdwu,VDWU_TSTY) (Vdwu a, Vdwu b) 
{
    return  vshr_n_u32(vtst_u32(a, b), 31);
}

INLINE(Vdwi,VDWI_TSTY) (Vdwi a, Vdwu b) 
{
    return  VDWU_ASTI(VDWU_TSTY(VDWI_ASTU(a), b));
}

INLINE(Vdwi,VDWF_TSTY) (Vdwf a, Vdwu b) 
{
    return  VDWU_ASTI(VDWU_TSTY(VDWF_ASTU(a), b));
}


INLINE(Vddu,VDDU_TSTY) (Vddu a, Vddu b) 
{
    return  vshr_n_u64(vtst_u64(a, b), 63);
}

INLINE(Vddi,VDDI_TSTY) (Vddi a, Vddu b) 
{
    return  VDDU_ASTI(VDDU_TSTY(VDDI_ASTU(a), b));
}

INLINE(Vddi,VDDF_TSTY) (Vddf a, Vddu b) 
{
    return  VDDU_ASTI(VDDU_TSTY(VDDF_ASTU(a), b));
}


INLINE(Vqbu,VQBU_TSTY) (Vqbu a, Vqbu b) 
{
    return  vshrq_n_u8(vtstq_u8(a, b), 7);
}

INLINE(Vqbi,VQBI_TSTY) (Vqbi a, Vqbu b) 
{
    return  VQBU_ASTI(VQBU_TSTY(VQBI_ASTU(a), b));
}

INLINE(Vqbc,VQBC_TSTY) (Vqbc a, Vqbu b) 
{
    return  VQBU_ASBC(VQBU_TSTY(VQBC_ASTU(a), b));
}


INLINE(Vqhu,VQHU_TSTY) (Vqhu a, Vqhu b) 
{
    return  vshrq_n_u16(vtstq_u16(a, b), 15);
}

INLINE(Vqhi,VQHI_TSTY) (Vqhi a, Vqhu b) 
{
    return  VQHU_ASTI(VQHU_TSTY(VQHI_ASTU(a), b));
}

INLINE(Vqhi,VQHF_TSTY) (Vqhf a, Vqhu b) 
{
    return  VQHU_ASTI(VQHU_TSTY(VQHF_ASTU(a), b));
}


INLINE(Vqwu,VQWU_TSTY) (Vqwu a, Vqwu b) 
{
    return  vshrq_n_u32(vtstq_u32(a, b), 31);
}

INLINE(Vqwi,VQWI_TSTY) (Vqwi a, Vqwu b) 
{
    return  VQWU_ASTI(VQWU_TSTY(VQWI_ASTU(a), b));
}

INLINE(Vqwi,VQWF_TSTY) (Vqwf a, Vqwu b) 
{
    return  VQWU_ASTI(VQWU_TSTY(VQWF_ASTU(a), b));
}


INLINE(Vqdu,VQDU_TSTY) (Vqdu a, Vqdu b) 
{
    return  vshrq_n_u64(vtstq_u64(a, b), 63);
}

INLINE(Vqdi,VQDI_TSTY) (Vqdi a, Vqdu b) 
{
    return  VQDU_ASTI(VQDU_TSTY(VQDI_ASTU(a), b));
}

INLINE(Vqdi,VQDF_TSTY) (Vqdf a, Vqdu b) 
{
    return  VQDU_ASTI(VQDU_TSTY(VQDF_ASTU(a), b));
}

INLINE(Vqqu,VQQU_TSTY) (Vqqu a, Vqqu b)
{
    a.V0 = vtstq_u64(a.V0, b.V0);
    uint64x1_t l = vget_low_u64(a.V0);
    uint64x1_t r = vget_high_u64(a.V0);
    l = vorr_u64(l, r);
    l = vshl_n_u64(l, 63);
    a.V0 = vcombine_u64(l, vdup_n_u64(0));
    return  a;
}
    
INLINE(Vqqi,VQQI_TSTY) (Vqqi a, Vqqu b)
{
    b.V0 = vtstq_u64(vreinterpretq_u64_s64(a.V0), b.V0);
    uint64x1_t l = vget_low_u64(b.V0);
    uint64x1_t r = vget_high_u64(b.V0);
    l = vorr_u64(l, r);
    l = vshl_n_u64(l, 63);
    b.V0 = vcombine_u64(l, vdup_n_u64(0));
    a.V0 = vreinterpretq_u64_s64(b.V0);
    return  a;
}
    
INLINE(Vqqi,VQQF_TSTY) (Vqqf a, Vqqu b)
{
    return VQQI_TSTY(VQQF_ASTI(a), b);
}
 

#if 0 // _LEAVE_ARM_TSTY
}
#endif

#if 0 // _ENTER_ARM_CEQS
{
#endif

INLINE(int16_t,  FLT16_CEQS) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vceqh_f16(a, b);
#else
    return  0-(a==b);
#endif
}

INLINE(int32_t,    FLT_CEQS)   (float a,   float b) {return vceqs_f32(a, b);}
INLINE(int64_t,    DBL_CEQS)  (double a,  double b) {return vceqd_f64(a, b);}

INLINE(int64x2_t,QQF_ZEQS) (QUAD_FTYPE x)
{
    QUAD_VTYPE p, n, c={.F=x};
    DWRD_VTYPE s={.D.U=vdup_n_u64((1ULL<<63))};
    p.D.U = vdupq_n_s64(0);
    n.D.U = vcopyq_lane_u64(p.D.U, 1, s.D.U, 0);
    p.D.U = vceqq_u64(p.D.U, c.D.U); // x==+0.0L
    n.D.U = vceqq_u64(n.D.U, c.D.U); // x==-0.0L
    c.D.U = vorrq_u64(p.D.U, n.D.U); // p|n
    return  c.D.I;
}


INLINE(int64x2_t,QQF_CEQS) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE x, z, l={.F=a}, r={.F=b};

    // exact match
    x.D.U = vceqq_u64(l.D.U, r.D.U);

    // !a
    l.D.I = QQF_ZEQS(a);
    
    // !b
    r.D.I = QQF_ZEQS(b);
    
    z.D.U = vandq_u64(l.D.U, r.D.U);
    return  vorrq_u64(x.D.U, z.D.U);
}

INLINE(QUAD_ITYPE,ceqsqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE c={.D.I=QQF_CEQS(a, b)};
    return  c.I;
}

INLINE(float,WBZ_CEQS) (float a, float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.B.U = vceq_u8(x.B.U, y.B.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WHZ_CEQS) (float a, float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.U = vceq_u16(x.H.U, y.H.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WWZ_CEQS) (float a, float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = vceq_u32(x.W.U, y.W.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WHF_CEQS) (float a, float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
#if defined(SPC_ARM_FP16_SIMD)
    x.H.U = vceq_f16(x.H.F, y.H.F);
#else
    QUAD_VTYPE  z;
    float32x4_t l = vcvt_f32_f16(x.H.F);
    float32x4_t r = vcvt_f32_f16(y.H.F);
    z.W.U = vceqq_f32(l, r);
    x.H.U = vmovn_u32(z.W.U);
#endif
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WWF_CEQS) (float a, float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = vceq_f32(x.W.F, y.W.F);
    return  vget_lane_f32(x.W.F, 0);
}

#define     DYU_CEQS 
#define     DBU_CEQS            vceq_u8
#define     DBI_CEQS(A, B) vreinterpret_s8_u8(vceq_s8(A,B))
#if CHAR_MIN
#   define  DBC_CEQS            DBI_CEQS
#else
#   define  DBC_CEQS            DBU_CEQS
#endif

#define     DHU_CEQS            vceq_u16
#define     DHI_CEQS(A, B) vreinterpret_s16_u16(vceq_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CEQS(A, B) vreinterpret_s16_u16(vceq_f16(A,B))
#else
INLINE(int16x4_t,DHF_CEQS) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE x={.H.F=a}, y={.H.F=b};
    QUAD_VTYPE l, r;
    l.W.F = vcvt_f32_f16(x.H.F);
    r.W.F = vcvt_f32_f16(y.H.F);
    l.W.U = vceqq_f32(l.W.F, r.W.F);
    x.H.U = vmovn_u32(l.W.U);
    return x.H.I;
}
#endif

#define     DWU_CEQS        vceq_u32
#define     DWI_CEQS(A, B)  vreinterpret_s32_u32(vceq_s32(A,B))
#define     DWF_CEQS(A, B)  vreinterpret_s32_u32(vceq_f32(A,B))
#define     DDU_CEQS        vceq_u64
#define     DDI_CEQS(A, B)  vreinterpret_s64_u64(vceq_s64(A,B))
#define     DDF_CEQS(A, B)  vreinterpret_s64_u64(vceq_f64(A,B))

#define     QBU_CEQS        vceqq_u8
#define     QBI_CEQS(A, B)  vreinterpretq_s8_u8(vceqq_s8(A,B))
#if CHAR_MIN
#   define  QBC_CEQS        QBI_CEQS
#else
#   define  QBC_CEQS        QBU_CEQS
#endif

#define     QHU_CEQS        vceqq_u16
#define     QHI_CEQS(A, B)  vreinterpretq_s16_u16(vceqq_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CEQS(A, B)  vreinterpretq_s16_u16(vceqq_f16(A,B))
#else
INLINE(int16x8_t,QHF_CEQS) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    QUAD_VTYPE l, r;
    z.W.F = vcvt_f32_f16(vget_low_f16(x.H.F));
    r.W.F = vcvt_f32_f16(vget_low_f16(y.H.F));
    l.W.U = vceqq_f32(z.W.F, r.W.F);
    z.W.F = vcvt_f32_f16(vget_high_f16(z.H.F));
    r.W.F = vcvt_f32_f16(vget_high_f16(y.H.F));
    r.W.U = vceqq_f32(z.W.F, r.W.F);
    z.H.U = vcombine_u16(vmovn_u32(l.W.U),vmovn_u32(r.W.U));
    return  z.H.I;
#endif
}

#define     QWU_CEQS        vceqq_u32
#define     QWI_CEQS(A, B)  vreinterpretq_s32_u32(vceqq_s32(A,B))
#define     QWF_CEQS(A, B)  vreinterpretq_s32_u32(vceqq_f32(A,B))

#define     QDU_CEQS        vceqq_u64
#define     QDI_CEQS(A, B)  vreinterpretq_s64_u64(vceqq_s64(A,B))
#define     QDF_CEQS(A, B)  vreinterpretq_s64_u64(vceqq_f64(A,B))

INLINE(uint64x2_t,QQU_CEQS) (uint64x2_t a, uint64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_u64(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.D.U = vand_u64(l.D.U, r.D.U);
    return  vdupq_lane_u64(l.D.U, 0);
}

INLINE(int64x2_t,QQI_CEQS) (int64x2_t a, int64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_s64(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.D.U = vand_u64(l.D.U, r.D.U);
    return  vdupq_lane_s64(l.D.I, 0);
}

#define     VWYU_CEQS VWYU_XORN
INLINE(Vwbu,VWBU_CEQS) (Vwbu a, Vwbu b) {a.V0=WBZ_CEQS(a.V0, b.V0); return a;}
INLINE(Vwbi,VWBI_CEQS) (Vwbi a, Vwbi b) {a.V0=WBZ_CEQS(a.V0, b.V0); return a;}
INLINE(Vwbc,VWBC_CEQS) (Vwbc a, Vwbc b) {a.V0=WBZ_CEQS(a.V0, b.V0); return a;}
INLINE(Vwhu,VWHU_CEQS) (Vwhu a, Vwhu b) {a.V0=WHZ_CEQS(a.V0, b.V0); return a;}
INLINE(Vwhi,VWHI_CEQS) (Vwhi a, Vwhi b) {a.V0=WHZ_CEQS(a.V0, b.V0); return a;}
INLINE(Vwhi,VWHF_CEQS) (Vwhf a, Vwhf b) {return ((Vwhi){WHF_CEQS(a.V0,b.V0)});}
INLINE(Vwwu,VWWU_CEQS) (Vwwu a, Vwwu b) {a.V0=WWZ_CEQS(a.V0, b.V0); return a;}
INLINE(Vwwi,VWWI_CEQS) (Vwwi a, Vwwi b) {a.V0=WWZ_CEQS(a.V0, b.V0); return a;}
INLINE(Vwwi,VWWF_CEQS) (Vwwf a, Vwwf b) {return ((Vwwi){WWF_CEQS(a.V0,b.V0)});}

#define     VDYU_CEQS VDYU_XORN
INLINE(Vdbu,VDBU_CEQS) (Vdbu a, Vdbu b) {return DBU_CEQS(a, b);}
INLINE(Vdbi,VDBI_CEQS) (Vdbi a, Vdbi b) {return DBI_CEQS(a, b);}
INLINE(Vdbc,VDBC_CEQS) (Vdbc a, Vdbc b) {a.V0=DBC_CEQS(a.V0, b.V0); return a;}
INLINE(Vdhu,VDHU_CEQS) (Vdhu a, Vdhu b) {return DHU_CEQS(a, b);}
INLINE(Vdhi,VDHI_CEQS) (Vdhi a, Vdhi b) {return DHI_CEQS(a, b);}
INLINE(Vdhi,VDHF_CEQS) (Vdhf a, Vdhf b) {return DHF_CEQS(a, b);}
INLINE(Vdwu,VDWU_CEQS) (Vdwu a, Vdwu b) {return DWU_CEQS(a, b);}
INLINE(Vdwi,VDWI_CEQS) (Vdwi a, Vdwi b) {return DWI_CEQS(a, b);}
INLINE(Vdwi,VDWF_CEQS) (Vdwf a, Vdwf b) {return DWF_CEQS(a, b);}
INLINE(Vddu,VDDU_CEQS) (Vddu a, Vddu b) {return DDU_CEQS(a, b);}
INLINE(Vddi,VDDI_CEQS) (Vddi a, Vddi b) {return DDI_CEQS(a, b);}
INLINE(Vddi,VDDF_CEQS) (Vddf a, Vddf b) {return DDF_CEQS(a, b);}

#define     VQYU_CEQS VQYU_XORN
INLINE(Vqbu,VQBU_CEQS) (Vqbu a, Vqbu b) {return QBU_CEQS(a,b);}
INLINE(Vqbi,VQBI_CEQS) (Vqbi a, Vqbi b) {return QBI_CEQS(a,b);}
INLINE(Vqbc,VQBC_CEQS) (Vqbc a, Vqbc b) {a.V0=QBC_CEQS(a.V0, b.V0); return a;}
INLINE(Vqhu,VQHU_CEQS) (Vqhu a, Vqhu b) {return QHU_CEQS(a,b);}
INLINE(Vqhi,VQHI_CEQS) (Vqhi a, Vqhi b) {return QHI_CEQS(a,b);}
INLINE(Vqhi,VQHF_CEQS) (Vqhf a, Vqhf b) {return QHF_CEQS(a,b);}
INLINE(Vqwu,VQWU_CEQS) (Vqwu a, Vqwu b) {return QWU_CEQS(a,b);}
INLINE(Vqwi,VQWI_CEQS) (Vqwi a, Vqwi b) {return QWI_CEQS(a,b);}
INLINE(Vqwi,VQWF_CEQS) (Vqwf a, Vqwf b) {return QWF_CEQS(a,b);}
INLINE(Vqdu,VQDU_CEQS) (Vqdu a, Vqdu b) {return QDU_CEQS(a,b);}
INLINE(Vqdi,VQDI_CEQS) (Vqdi a, Vqdi b) {return QDI_CEQS(a,b);}
INLINE(Vqdi,VQDF_CEQS) (Vqdf a, Vqdf b) {return QDF_CEQS(a,b);}
INLINE(Vqqu,VQQU_CEQS) (Vqqu a, Vqqu b) {a.V0=QQU_CEQS(a.V0, b.V0); return a;}
INLINE(Vqqi,VQQI_CEQS) (Vqqi a, Vqqi b) {a.V0=QQI_CEQS(a.V0, b.V0); return a;}
INLINE(Vqqi,VQQF_CEQS) (Vqqf a, Vqqf b) {return ((Vqqi){QQF_CEQS(a.V0,b.V0)});}

#if 0 // _LEAVE_ARM_CEQS
}
#endif

#if 0 // _ENTER_ARM_CEQL
{
#endif

INLINE(int16_t,FLT16_CEQL) (flt16_t a, flt16_t b) {return a==b;}
INLINE(int32_t,  FLT_CEQL)   (float a,   float b) {return a==b;}
INLINE(int64_t,  DBL_CEQL)  (double a,  double b) {return a==b;}

INLINE(QUAD_ITYPE,ceqlqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE c = {.D.I=QQF_CEQS(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vdup_n_u64(0)};
    l.D.U = vshr_n_u64(l.D.U, 63);
    c.D.U = vcombine_u64(l.D.U, r.D.U);
    return  c.I;
}

INLINE(float,WBZ_CEQL) (float a, float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.B.U = vceq_u8(x.B.U, y.B.U);
    x.B.U = vshr_n_u8(x.B.U, 7);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WHZ_CEQL) (float a, float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.U = vceq_u16(x.H.U, y.H.U);
    x.H.U = vshr_n_u16(x.H.U, 15);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WWZ_CEQL) (float a, float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = vceq_u32(x.W.U, y.W.U);
    x.W.U = vshr_n_u32(x.W.U, 31);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WHF_CEQL) (float a, float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
#if defined(SPC_ARM_FP16_SIMD)
    x.H.U = vceq_f16(x.H.F, y.H.F);
#else
    QUAD_VTYPE  z;
    float32x4_t l = vcvt_f32_f16(x.H.F);
    float32x4_t r = vcvt_f32_f16(y.H.F);
    z.W.U = vceqq_f32(l, r);
    x.H.U = vmovn_u32(z.W.U);
#endif
    x.H.U = vshr_n_u16(x.H.U, 15);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WWF_CEQL) (float a, float b)
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = vceq_f32(x.W.F, y.W.F);
    x.W.U = vshr_n_u32(x.W.U, 31);
    return  vget_lane_f32(x.W.F, 0);
}


#define     DBU_CEQL(A, B)            vshr_n_u8(vceq_u8(A,B),7)
#define     DBI_CEQL(A, B)  VDBU_ASTI(vshr_n_u8(vceq_s8(A,B),7))
#if CHAR_MIN
#   define  DBC_CEQL        DBI_CEQL
#else
#   define  DBC_CEQL        DBU_CEQL
#endif

#define     DHU_CEQL(A, B)            vshr_n_u16(vceq_u16(A,B),15)
#define     DHI_CEQL(A, B)  VDHU_ASTI(vshr_n_u16(vceq_s16(A,B),15))

#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CEQL(A, B)  VDHU_ASTI(vshr_n_u16(vceq_f16(A,B),15))
#else
INLINE(int16x4_t,DHF_CEQL) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE x={.H.F=a}, y={.H.F=b};
    QUAD_VTYPE l, r;
    l.W.F = vcvt_f32_f16(x.H.F);
    r.W.F = vcvt_f32_f16(y.H.F);
    l.W.U = vceqq_f32(l.W.F, r.W.F);
    x.H.U = vmovn_u32(l.W.U);
    x.H.U = vshr_n_u16(x.H.U, 15);
    return x.H.I;
}

#endif

#define     DWU_CEQL(A, B)            vshr_n_u32(vceq_u32(A,B),31)
#define     DWI_CEQL(A, B)  VDWU_ASTI(vshr_n_u32(vceq_s32(A,B),31))
#define     DWF_CEQL(A, B)  VDWU_ASTI(vshr_n_u32(vceq_f32(A,B),31))

#define     DDU_CEQL(A, B)            vshr_n_u64(vceq_u64(A,B),63)
#define     DDI_CEQL(A, B)  VDDU_ASTI(vshr_n_u64(vceq_s64(A,B),63))
#define     DDF_CEQL(A, B)  VDDU_ASTI(vshr_n_u64(vceq_f64(A,B),63))

#define     QBU_CEQL(A, B)            vshrq_n_u8(vceqq_u8(A,B),7)
#define     QBI_CEQL(A, B)  VQBU_ASTI(vshrq_n_u8(vceqq_s8(A,B),7))
#if CHAR_MIN
#   define  QBC_CEQL        QBI_CEQL
#else
#   define  QBC_CEQL        QBU_CEQL
#endif

#define     QHU_CEQL(A, B)            vshrq_n_u16(vceqq_u16(A,B),15)
#define     QHI_CEQL(A, B)  VQHU_ASTI(vshrq_n_u16(vceqq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CEQL(A, B)  VQHU_ASTI(vshrq_n_u16(vceqq_f16(A,B),15))
#else
INLINE(int16x8_t,QHF_CEQL) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE x={.H.F=a}, y={.H.F=b}, z;
    QUAD_VTYPE l, r;
    z.W.F = vcvt_f32_f16(vget_low_f16(x.H.F));
    r.W.F = vcvt_f32_f16(vget_low_f16(y.H.F));
    l.W.U = vceqq_f32(z.W.F, r.W.F);
    z.W.F = vcvt_f32_f16(vget_high_f16(z.H.F));
    r.W.F = vcvt_f32_f16(vget_high_f16(y.H.F));
    r.W.U = vceqq_f32(z.W.F, r.W.F);
    z.H.U = vcombine_u16(vmovn_u32(l.W.U),vmovn_u32(r.W.U));
    z.H.U = vshlq_n_u16(z.H.U, 15);
    return  z.H.I;
}

#endif

#define     QWU_CEQL(A, B)            vshrq_n_u32(vceqq_u32(A,B),31)
#define     QWI_CEQL(A, B)  VQWU_ASTI(vshrq_n_u32(vceqq_s32(A,B),31))
#define     QWF_CEQL(A, B)  VQWU_ASTI(vshrq_n_u32(vceqq_f32(A,B),31))

#define     QDU_CEQL(A, B)            vshrq_n_u64(vceqq_u64(A,B),63)
#define     QDI_CEQL(A, B)  VQDU_ASTI(vshrq_n_u64(vceqq_s64(A,B),63))
#define     QDF_CEQL(A, B)  VQDU_ASTI(vshrq_n_u64(vceqq_f64(A,B),63))


INLINE(uint64x2_t,QQU_CEQL) (uint64x2_t a, uint64x2_t b) 
{
    DWRD_VTYPE l, r;
    QUAD_VTYPE c = {.D.U=QQU_CEQS(a, b)};
    l.D.U = vget_low_u64(c.D.U);
    r.D.U = vget_high_u64(c.D.U);
    l.B.U = vand_u8(l.D.U, r.D.U);
    l.B.U = vshl_n_u64(l.B.U, 63);
    return  vcombine_u64(l.D.U, vdup_n_u64(0));
}

INLINE(int64x2_t,QQI_CEQL) (int64x2_t a, int64x2_t b) 
{
    QUAD_VTYPE l={.D.I=a}, r={.D.I=b};
    l.D.U = QQU_CEQL(l.D.U, r.D.U);
    return l.D.I;
}

INLINE(int64x2_t,QQF_CEQL) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE c = {.D.I=QQF_CEQS(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.I=vdup_n_s64(0)};
    l.D.U = vshr_n_u64(l.D.U, 63);
    return  vcombine_s64(l.D.I, r.D.I);
}

INLINE(Vwbu,VWBU_CEQL) (Vwbu a, Vwbu b) {a.V0=WBZ_CEQL(a.V0, b.V0); return a;}
INLINE(Vwbi,VWBI_CEQL) (Vwbi a, Vwbi b) {a.V0=WBZ_CEQL(a.V0, b.V0); return a;}
INLINE(Vwbc,VWBC_CEQL) (Vwbc a, Vwbc b) {a.V0=WBZ_CEQL(a.V0, b.V0); return a;}
INLINE(Vwhu,VWHU_CEQL) (Vwhu a, Vwhu b) {a.V0=WHZ_CEQL(a.V0, b.V0); return a;}
INLINE(Vwhi,VWHI_CEQL) (Vwhi a, Vwhi b) {a.V0=WHZ_CEQL(a.V0, b.V0); return a;}
INLINE(Vwhi,VWHF_CEQL) (Vwhf a, Vwhf b) {return ((Vwhi){WHF_CEQL(a.V0,b.V0)});}
INLINE(Vwwu,VWWU_CEQL) (Vwwu a, Vwwu b) {a.V0=WWZ_CEQL(a.V0, b.V0); return a;}
INLINE(Vwwi,VWWI_CEQL) (Vwwi a, Vwwi b) {a.V0=WWZ_CEQL(a.V0, b.V0); return a;}
INLINE(Vwwi,VWWF_CEQL) (Vwwf a, Vwwf b) {return ((Vwwi){WWF_CEQL(a.V0,b.V0)});}

INLINE(Vdbu,VDBU_CEQL) (Vdbu a, Vdbu b) {return DBU_CEQL(a, b);}
INLINE(Vdbi,VDBI_CEQL) (Vdbi a, Vdbi b) {return DBI_CEQL(a, b);}
INLINE(Vdbc,VDBC_CEQL) (Vdbc a, Vdbc b) {a.V0=DBC_CEQL(a.V0, b.V0); return a;}
INLINE(Vdhu,VDHU_CEQL) (Vdhu a, Vdhu b) {return DHU_CEQL(a, b);}
INLINE(Vdhi,VDHI_CEQL) (Vdhi a, Vdhi b) {return DHI_CEQL(a, b);}
INLINE(Vdhi,VDHF_CEQL) (Vdhf a, Vdhf b) {return DHF_CEQL(a, b);}
INLINE(Vdwu,VDWU_CEQL) (Vdwu a, Vdwu b) {return DWU_CEQL(a, b);}
INLINE(Vdwi,VDWI_CEQL) (Vdwi a, Vdwi b) {return DWI_CEQL(a, b);}
INLINE(Vdwi,VDWF_CEQL) (Vdwf a, Vdwf b) {return DWF_CEQL(a, b);}
INLINE(Vddu,VDDU_CEQL) (Vddu a, Vddu b) {return DDU_CEQL(a, b);}
INLINE(Vddi,VDDI_CEQL) (Vddi a, Vddi b) {return DDI_CEQL(a, b);}
INLINE(Vddi,VDDF_CEQL) (Vddf a, Vddf b) {return DDF_CEQL(a, b);}

INLINE(Vqbu,VQBU_CEQL) (Vqbu a, Vqbu b) {return QBU_CEQL(a,b);}
INLINE(Vqbi,VQBI_CEQL) (Vqbi a, Vqbi b) {return QBI_CEQL(a,b);}
INLINE(Vqbc,VQBC_CEQL) (Vqbc a, Vqbc b) {a.V0=QBC_CEQL(a.V0, b.V0); return a;}
INLINE(Vqhu,VQHU_CEQL) (Vqhu a, Vqhu b) {return QHU_CEQL(a,b);}
INLINE(Vqhi,VQHI_CEQL) (Vqhi a, Vqhi b) {return QHI_CEQL(a,b);}
INLINE(Vqhi,VQHF_CEQL) (Vqhf a, Vqhf b) {return QHF_CEQL(a,b);}
INLINE(Vqwu,VQWU_CEQL) (Vqwu a, Vqwu b) {return QWU_CEQL(a,b);}
INLINE(Vqwi,VQWI_CEQL) (Vqwi a, Vqwi b) {return QWI_CEQL(a,b);}
INLINE(Vqwi,VQWF_CEQL) (Vqwf a, Vqwf b) {return QWF_CEQL(a,b);}
INLINE(Vqdu,VQDU_CEQL) (Vqdu a, Vqdu b) {return QDU_CEQL(a,b);}
INLINE(Vqdi,VQDI_CEQL) (Vqdi a, Vqdi b) {return QDI_CEQL(a,b);}
INLINE(Vqdi,VQDF_CEQL) (Vqdf a, Vqdf b) {return QDF_CEQL(a,b);}
INLINE(Vqqu,VQQU_CEQL) (Vqqu a, Vqqu b) {a.V0=QQU_CEQL(a.V0, b.V0); return a;}
INLINE(Vqqi,VQQI_CEQL) (Vqqi a, Vqqi b) {a.V0=QQI_CEQL(a.V0, b.V0); return a;}
INLINE(Vqqi,VQQF_CEQL) (Vqqf a, Vqqf b) {return ((Vqqi){QQF_CEQL(a.V0,b.V0)});}

#if 0 // _LEAVE_ARM_CEQL
}
#endif

#if 0 // _ENTER_ARM_CEQY
{
#endif

INLINE(_Bool,FLT16_CEQY) (flt16_t a, flt16_t b) {return a==b;}
INLINE(_Bool,  FLT_CEQY)   (float a,   float b) {return a==b;}
INLINE(_Bool,  DBL_CEQY)  (double a,  double b) {return a==b;}

INLINE(_Bool,WWZ_CEQY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = vceq_u32(x.W.U, y.W.U);
    return  vget_lane_u32(x.W.U, 0);
}

INLINE(_Bool,VWYU_CEQY) (Vwyu a, Vwyu b) 
{
    WORD_TYPE p={.F=a.V0}, q={.F=b.V0};
    return  p.U==q.U;
}

INLINE(_Bool,VWBU_CEQY) (Vwbu a, Vwbu b) {return WWZ_CEQY(a.V0, b.V0);}         
INLINE(_Bool,VWBI_CEQY) (Vwbi a, Vwbi b) {return WWZ_CEQY(a.V0, b.V0);}
INLINE(_Bool,VWBC_CEQY) (Vwbc a, Vwbc b) {return WWZ_CEQY(a.V0, b.V0);}
INLINE(_Bool,VWHU_CEQY) (Vwhu a, Vwhu b) {return WWZ_CEQY(a.V0, b.V0);}         
INLINE(_Bool,VWHI_CEQY) (Vwhi a, Vwhi b) {return WWZ_CEQY(a.V0, b.V0);}         
INLINE(_Bool,VWHF_CEQY) (Vwhf a, Vwhf b) 
{
    Vwhi c = VWHF_CEQS(a, b);
    return  UINT32_MAX==((WORD_TYPE){.F=c.V0}).U;
}

INLINE(_Bool,VWWU_CEQY) (Vwwu a, Vwwu b) {return WWZ_CEQY(a.V0, b.V0);}         
INLINE(_Bool,VWWI_CEQY) (Vwwi a, Vwwi b) {return WWZ_CEQY(a.V0, b.V0);}         
INLINE(_Bool,VWWF_CEQY) (Vwwf a, Vwwf b) {return a.V0==b.V0;}

INLINE(_Bool,VDYU_CEQY) (Vdyu a, Vdyu b)
{
    a.V0 = vceq_u64(a.V0, b.V0);
    return  vget_lane_u64(a.V0, 0);
}

INLINE(_Bool,VDBU_CEQY) (Vdbu a, Vdbu b) 
{
    DWRD_VTYPE c = {.B.U=vceq_u8(a, b)};
    return  c.U==UINT64_MAX;
}

INLINE(_Bool,VDBI_CEQY) (Vdbi a, Vdbi b) 
{
    DWRD_VTYPE c = {.B.U=vceq_s8(a, b)};
    return  c.U==UINT64_MAX;
}

INLINE(_Bool,VDBC_CEQY) (Vdbc a, Vdbc b) 
{
    DWRD_VTYPE c;
#if CHAR_MIN
    c.B.U = vceq_s8(a.V0, b.V0);
#else
    c.B.U = vceq_u8(a.V0, b.V0);
#endif
    return c.U==UINT64_MAX;
}

INLINE(_Bool,VDHU_CEQY) (Vdhu a, Vdhu b) 
{
    DWRD_VTYPE c = {.H.U=vceq_u16(a, b)};
    return  c.U==UINT64_MAX;
}

INLINE(_Bool,VDHI_CEQY) (Vdhi a, Vdhi b) 
{
    DWRD_VTYPE c = {.H.U=vceq_s16(a, b)};
    return  c.U==UINT64_MAX;
}

INLINE(_Bool,VDHF_CEQY) (Vdhf a, Vdhf b) 
{
    DWRD_VTYPE c;
#if defined(SPC_ARM_FP16_SIMD)
    c.H.U = vceq_f16(a, b);
#else
    QUAD_VTYPE l, r;
    l.W.F = vcvt_f32_f16(a);
    r.W.F = vcvt_f32_f16(b);
    l.W.U = vceqq_f32(l.W.F, r.W.F);
    c.H.U = vmovn_u32(l.W.U);
#endif
    return  c.U==UINT64_MAX;
}

INLINE(_Bool,VDWU_CEQY) (Vdwu a, Vdwu b) 
{
    DWRD_VTYPE c = {.W.U=vceq_u32(a, b)};
    return  c.U==UINT64_MAX;
}

INLINE(_Bool,VDWI_CEQY) (Vdwi a, Vdwi b) 
{
    DWRD_VTYPE c = {.W.U=vceq_s32(a, b)};
    return  c.U==UINT64_MAX;
}

INLINE(_Bool,VDWF_CEQY) (Vdwf a, Vdwf b) 
{
    DWRD_VTYPE c = {.W.U=vceq_f32(a, b)};
    return  c.U==UINT64_MAX;
}

INLINE(_Bool,VDDU_CEQY) (Vddu a, Vddu b) 
{
    return  vget_lane_u64(a,0)==vget_lane_u64(b,0);
}

INLINE(_Bool,VDDI_CEQY) (Vddi a, Vddi b) 
{
    return  vget_lane_s64(a,0)==vget_lane_s64(b,0);
}

INLINE(_Bool,VDDF_CEQY) (Vddf a, Vddf b) 
{
    return  vget_lane_f64(a,0)==vget_lane_f64(b,0);
}


INLINE(_Bool,QQU_CEQY) (uint64x2_t a, uint64x2_t b)
{
//  !((a.Lo^b.Lo)|(a.Hi^b.Hi))
    a = veorq_u64(a, b);
    return  !(vgetq_lane_u64(a, 0)|vgetq_lane_u64(a, 1));
}

INLINE(_Bool,QQI_CEQY) (int64x2_t a, int64x2_t b)
{
    a = veorq_s64(a, b);
    return  !(vgetq_lane_s64(a, 0)|vgetq_lane_s64(a, 1));
}

INLINE(_Bool,QQF_CEQY) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return a==b;
}

INLINE(_Bool,VQYU_CEQY) (Vqyu a, Vqyu b) {return QQU_CEQY(a.V0, b.V0);}

INLINE(_Bool,VQBU_CEQY) (Vqbu a, Vqbu b) 
{
    QUAD_VTYPE c = {.B.U=vceqq_u8(a, b)};
    return  UINT64_MAX==(vgetq_lane_u64(c.D.U,0)&vgetq_lane_u64(c.D.U,1));
}

INLINE(_Bool,VQBI_CEQY) (Vqbi a, Vqbi b) 
{
    QUAD_VTYPE c = {.B.U=vceqq_s8(a, b)};
    return  UINT64_MAX==(vgetq_lane_u64(c.D.U,0)&vgetq_lane_u64(c.D.U,1));
}

INLINE(_Bool,VQBC_CEQY) (Vqbc a, Vqbc b) 
{
    QUAD_VTYPE c;
#if CHAR_MIN
    c.B.U = vceqq_s8(a.V0, b.V0);
#else
    c.B.U = vceqq_u8(a.V0, b.V0);
#endif
    return  UINT64_MAX==(vgetq_lane_u64(c.D.U,0)&vgetq_lane_u64(c.D.U,1));
}


INLINE(_Bool,VQHU_CEQY) (Vqhu a, Vqhu b) 
{
    QUAD_VTYPE c = {.H.U=vceqq_u16(a, b)};
    return  UINT64_MAX==(vgetq_lane_u64(c.D.U,0)&vgetq_lane_u64(c.D.U,1));
}

INLINE(_Bool,VQHI_CEQY) (Vqhi a, Vqhi b) 
{
    QUAD_VTYPE c = {.H.U=vceqq_s16(a, b)};
    return  UINT64_MAX==(vgetq_lane_u64(c.D.U,0)&vgetq_lane_u64(c.D.U,1));
}

INLINE(_Bool,VQHF_CEQY) (Vqhf a, Vqhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    QUAD_VTYPE c = {.H.U=vceqq_f16(a, b)};
    return  UINT64_MAX==(vgetq_lane_u64(c.D.U,0)&vgetq_lane_u64(c.D.U,1));
#else
    return (
        VDHF_CEQY( vget_low_f16(a),  vget_low_f16(b))
    &&  VDHF_CEQY(vget_high_f16(a), vget_high_f16(b))
    );
#endif
}


INLINE(_Bool,VQWU_CEQY) (Vqwu a, Vqwu b) 
{
    QUAD_VTYPE c = {.W.U=vceqq_u32(a, b)};
    return  UINT64_MAX==(vgetq_lane_u64(c.D.U,0)&vgetq_lane_u64(c.D.U,1));
}

INLINE(_Bool,VQWI_CEQY) (Vqwi a, Vqwi b) 
{
    QUAD_VTYPE c = {.W.U=vceqq_s32(a, b)};
    return  UINT64_MAX==(vgetq_lane_u64(c.D.U,0)&vgetq_lane_u64(c.D.U,1));
}

INLINE(_Bool,VQWF_CEQY) (Vqwf a, Vqwf b) 
{
    QUAD_VTYPE c = {.W.U=vceqq_f32(a, b)};
    return  UINT64_MAX==(vgetq_lane_u64(c.D.U,0)&vgetq_lane_u64(c.D.U,1));
}


INLINE(_Bool,VQDU_CEQY) (Vqdu a, Vqdu b) 
{
    return  (vgetq_lane_u64(a,0) == vgetq_lane_u64(b, 0))
    ?   vgetq_lane_u64(a,1)==vgetq_lane_u64(b,1)
    :   0;
}

INLINE(_Bool,VQDI_CEQY) (Vqdi a, Vqdi b) 
{
    return  (vgetq_lane_s64(a,0) == vgetq_lane_s64(b, 0))
    ?   vgetq_lane_s64(a,1)==vgetq_lane_s64(b,1)
    :   0;
}

INLINE(_Bool,VQDF_CEQY) (Vqdf a, Vqdf b) 
{
    return  (vgetq_lane_f64(a,0) == vgetq_lane_f64(b, 0))
    ?   vgetq_lane_f64(a,1)==vgetq_lane_f64(b,1)
    :   0;
}

INLINE(_Bool,VQQU_CEQY) (Vqqu a, Vqqu b) {return  QQU_CEQY(a.V0, b.V0);}
INLINE(_Bool,VQQI_CEQY) (Vqqi a, Vqqi b) {return  QQU_CEQY(a.V0, b.V0);}
INLINE(_Bool,VQQF_CEQY) (Vqqf a, Vqqf b) {return  a.V0==b.V0;}

#if 0 // _LEAVE_ARM_CEQY
}
#endif

#if 0 // _ENTER_ARM_CNES
{
#endif

INLINE(uint8x8_t,DBU_CNES) (uint8x8_t a, uint8x8_t b)
{
    a = vceq_u8(a, b);
    return  vmvn_u8(a);
}

INLINE( int8x8_t,DBI_CNES)  (int8x8_t a,  int8x8_t b)
{
    DWRD_VTYPE c = {.B.U=vceq_s8(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    return  c.B.I;
}

#if CHAR_MIN
#   define  DBC_CNES    DBI_CNES
#else
#   define  DBC_CNES    DBU_CNES
#endif

INLINE(uint16x4_t,DHU_CNES)  (uint16x4_t a,  uint16x4_t b)
{
    a = vceq_u16(a, b);
    return  vmvn_u16(a);
}

INLINE( int16x4_t,DHI_CNES)   (int16x4_t a,  int16x4_t b)
{
    DWRD_VTYPE c = {.H.U=vceq_s16(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    return  c.H.I;
}

INLINE( int16x4_t,DHF_CNES) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE c = {.H.I=DHF_CEQS(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    return  c.H.I;
}


INLINE(uint32x2_t,DWU_CNES)  (uint32x2_t a,  uint32x2_t b)
{
    a = vceq_u32(a, b);
    return  vmvn_u32(a);
}

INLINE( int32x2_t,DWI_CNES)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_s32(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    return  c.W.I;
}

INLINE( int32x2_t,DWF_CNES) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_f32(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    return  c.W.I;
}


INLINE(uint64x1_t,DDU_CNES)  (uint64x1_t a,  uint64x1_t b)
{
    DWRD_VTYPE c = {.D.U=vceq_u64(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    return  c.D.U;
}

INLINE( int64x1_t,DDI_CNES)   (int64x1_t a,   int64x1_t b)
{
    DWRD_VTYPE c = {.D.U=vceq_s64(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    return  c.D.I;
}

INLINE( int64x1_t,DDF_CNES) (float64x1_t a, float64x1_t b)
{
    DWRD_VTYPE c = {.D.U=vceq_f64(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    return  c.D.I;
}


INLINE(uint8x16_t,QBU_CNES)  (uint8x16_t a,  uint8x16_t b)
{
    a = vceqq_u8(a, b);
    return  vmvnq_u8(a);
}

INLINE( int8x16_t,QBI_CNES)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE c = {.B.U=vceqq_s8(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    return  c.B.I;
}

#if CHAR_MIN
#   define  QBC_CNES    QBI_CNES
#else
#   define  QBC_CNES    QBU_CNES
#endif

INLINE(uint16x8_t,QHU_CNES)  (uint16x8_t a,  uint16x8_t b)
{
    a = vceqq_u16(a, b);
    return  vmvnq_u16(a);
}

INLINE( int16x8_t,QHI_CNES)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE c = {.H.U=vceqq_s16(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    return  c.H.I;
}

INLINE( int16x8_t,QHF_CNES) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE c = {.H.I = QHF_CEQS(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    return  c.H.I;
}


INLINE(uint32x4_t,QWU_CNES)  (uint32x4_t a,  uint32x4_t b)
{
    a = vceqq_u32(a, b);
    return  vmvnq_u32(a);
}

INLINE( int32x4_t,QWI_CNES)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE c = {.W.U=vceqq_s32(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    return  c.W.I;
}

INLINE( int32x4_t,QWF_CNES) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE c = {.W.U=vceqq_f32(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    return  c.W.I;
}


INLINE(uint64x2_t,QDU_CNES)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_u64(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    return  c.D.U;
}

INLINE( int64x2_t,QDI_CNES)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_s64(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    return  c.D.I;
}

INLINE( int64x2_t,QDF_CNES) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_f64(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    return  c.D.I;
}


INLINE(uint64x2_t,QQU_CNES)  (uint64x2_t a,  uint64x2_t b)
{
/*
is dupqdu(invsdu(x)) rly faster than invsqdu(dupqdu(x))?
*/
    QUAD_VTYPE c = {.D.U=vceqq_u64(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vand_u8(l.B.U, r.B.U);
    l.B.U = vmvn_u8(l.B.U);
    return  vdupq_lane_u64(l.D.U, 0);
}

INLINE( int64x2_t,QQI_CNES)   (int64x2_t a,  int64x2_t b)
{
    QUAD_VTYPE l={.D.I=a}, r={.D.I=b};
    l.D.U = QQU_CNES(l.D.U, r.D.U);
    return  l.D.I;
}

INLINE(QUAD_FTYPE,QQF_CNES) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE c = {.Q.I=QQF_CEQS(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    return  c.F;
}


INLINE(int16_t,  FLT16_CNES) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FLT16_SIMD)
    return  ~vceqh_f16(a, b);
#else
    DWRD_VTYPE x={.H.F={a}}, y={.H.F={b}}, z;
    z.H.I = DHF_CNES(x.H.F, y.H.F);
    return  vget_lane_s16(z.H.I, 0);
#endif
}

INLINE(int32_t, FLT_CNES)   (float a,   float b) 
{
    return  ~vceqs_f32(a, b);
}

INLINE(int64_t, DBL_CNES)  (double a,  double b) 
{
    return  ~vceqd_f64(a, b);
}

INLINE(QUAD_ITYPE,cnesqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  invsqi(ceqsqf(a, b));
}

INLINE(float,WBZ_CNES) (float a, float b)
{
    DWRD_VTYPE l = {.W.F={a}};
    DWRD_VTYPE r = {.W.F={b}};
    l.B.U = DBU_CNES(l.B.U, r.B.U);
    return  vget_lane_f32(l.W.F, 0);
}

INLINE(float,WHZ_CNES) (float a, float b)
{
    DWRD_VTYPE l = {.W.F={a}};
    DWRD_VTYPE r = {.W.F={b}};
    l.H.U = DHU_CNES(l.H.U, r.H.U);
    return  vget_lane_f32(l.W.F, 0);
}

INLINE(float,WWZ_CNES) (float a, float b)
{
    DWRD_VTYPE l = {.W.F={a}};
    DWRD_VTYPE r = {.W.F={b}};
    l.W.U = DWU_CNES(l.W.U, r.W.U);
    return  vget_lane_f32(l.W.F, 0);
}

INLINE(float,WHF_CNES) (float a, float b)
{
    DWRD_VTYPE p={.W.F={a}}, q={.W.F={b}};
    p.H.I = DHF_CNES(p.H.F, q.H.F);
    return  vget_lane_f32(p.W.F, 0);
}

INLINE(float,WWF_CNES) (float a, float b)
{
    DWRD_VTYPE l={.W.F={a}}, r={.W.F={b}};
    l.W.I = DWF_CNES(l.W.F, r.W.F);
    return  vget_lane_f32(l.W.F, 0);
}

#define     VWYU_CNES VWYU_XORS
INLINE(Vwbu,VWBU_CNES) (Vwbu a, Vwbu b) {a.V0=WBZ_CNES(a.V0, b.V0); return a;}
INLINE(Vwbi,VWBI_CNES) (Vwbi a, Vwbi b) {a.V0=WBZ_CNES(a.V0, b.V0); return a;}
INLINE(Vwbc,VWBC_CNES) (Vwbc a, Vwbc b) {a.V0=WBZ_CNES(a.V0, b.V0); return a;}
INLINE(Vwhu,VWHU_CNES) (Vwhu a, Vwhu b) {a.V0=WHZ_CNES(a.V0, b.V0); return a;}
INLINE(Vwhi,VWHI_CNES) (Vwhi a, Vwhi b) {a.V0=WHZ_CNES(a.V0, b.V0); return a;}
INLINE(Vwhi,VWHF_CNES) (Vwhf a, Vwhf b) {return ((Vwhi){WHF_CNES(a.V0,b.V0)});}
INLINE(Vwwu,VWWU_CNES) (Vwwu a, Vwwu b) {a.V0=WWZ_CNES(a.V0, b.V0); return a;}
INLINE(Vwwi,VWWI_CNES) (Vwwi a, Vwwi b) {a.V0=WWZ_CNES(a.V0, b.V0); return a;}
INLINE(Vwwi,VWWF_CNES) (Vwwf a, Vwwf b) {return ((Vwwi){WWF_CNES(a.V0,b.V0)});}

#define     VDYU_CNES VDYU_XORS
INLINE(Vdbu,VDBU_CNES) (Vdbu a, Vdbu b) {return DBU_CNES(a, b);}
INLINE(Vdbi,VDBI_CNES) (Vdbi a, Vdbi b) {return DBI_CNES(a, b);}
INLINE(Vdbc,VDBC_CNES) (Vdbc a, Vdbc b) {a.V0=DBC_CNES(a.V0, b.V0); return a;}
INLINE(Vdhu,VDHU_CNES) (Vdhu a, Vdhu b) {return DHU_CNES(a, b);}
INLINE(Vdhi,VDHI_CNES) (Vdhi a, Vdhi b) {return DHI_CNES(a, b);}
INLINE(Vdhi,VDHF_CNES) (Vdhf a, Vdhf b) {return DHF_CNES(a, b);}
INLINE(Vdwu,VDWU_CNES) (Vdwu a, Vdwu b) {return DWU_CNES(a, b);}
INLINE(Vdwi,VDWI_CNES) (Vdwi a, Vdwi b) {return DWI_CNES(a, b);}
INLINE(Vdwi,VDWF_CNES) (Vdwf a, Vdwf b) {return DWF_CNES(a, b);}
INLINE(Vddu,VDDU_CNES) (Vddu a, Vddu b) {return DDU_CNES(a, b);}
INLINE(Vddi,VDDI_CNES) (Vddi a, Vddi b) {return DDI_CNES(a, b);}
INLINE(Vddi,VDDF_CNES) (Vddf a, Vddf b) {return DDF_CNES(a, b);}

#define     VQYU_CNES VQYU_XORS
INLINE(Vqbu,VQBU_CNES) (Vqbu a, Vqbu b) {return QBU_CNES(a,b);}
INLINE(Vqbi,VQBI_CNES) (Vqbi a, Vqbi b) {return QBI_CNES(a,b);}
INLINE(Vqbc,VQBC_CNES) (Vqbc a, Vqbc b) {a.V0=QBC_CNES(a.V0, b.V0); return a;}
INLINE(Vqhu,VQHU_CNES) (Vqhu a, Vqhu b) {return QHU_CNES(a,b);}
INLINE(Vqhi,VQHI_CNES) (Vqhi a, Vqhi b) {return QHI_CNES(a,b);}
INLINE(Vqhi,VQHF_CNES) (Vqhf a, Vqhf b) {return QHF_CNES(a,b);}
INLINE(Vqwu,VQWU_CNES) (Vqwu a, Vqwu b) {return QWU_CNES(a,b);}
INLINE(Vqwi,VQWI_CNES) (Vqwi a, Vqwi b) {return QWI_CNES(a,b);}
INLINE(Vqwi,VQWF_CNES) (Vqwf a, Vqwf b) {return QWF_CNES(a,b);}
INLINE(Vqdu,VQDU_CNES) (Vqdu a, Vqdu b) {return QDU_CNES(a,b);}
INLINE(Vqdi,VQDI_CNES) (Vqdi a, Vqdi b) {return QDI_CNES(a,b);}
INLINE(Vqdi,VQDF_CNES) (Vqdf a, Vqdf b) {return QDF_CNES(a,b);}
INLINE(Vqqu,VQQU_CNES) (Vqqu a, Vqqu b) {a.V0=QQU_CNES(a.V0, b.V0); return a;}
INLINE(Vqqi,VQQI_CNES) (Vqqi a, Vqqi b) {a.V0=QQI_CNES(a.V0, b.V0); return a;}
INLINE(Vqqi,VQQF_CNES) (Vqqf a, Vqqf b) {return ((Vqqi){QQF_CNES(a.V0,b.V0)});}

#if 0 // _LEAVE_ARM_CNES
}
#endif

#if 0 // _ENTER_ARM_CNEL
{
#endif

INLINE(uint8x8_t,DBU_CNEL) (uint8x8_t a, uint8x8_t b)
{
    a = vceq_u8(a, b);
    a = vmvn_u8(a);
    return  vshr_n_u8(a, 7);
}

INLINE( int8x8_t,DBI_CNEL)  (int8x8_t a,  int8x8_t b)
{
    uint8x8_t c = vceq_s8(a, b);
    c = vmvn_u8(c);
    c = vshr_n_u8(c, 7);
    return  vreinterpret_s8_u8(c);
}

#if CHAR_MIN
#   define  DBC_CNEL    DBI_CNEL
#else
#   define  DBC_CNEL    DBU_CNEL
#endif

INLINE(uint16x4_t,DHU_CNEL)  (uint16x4_t a,  uint16x4_t b)
{
    a = vceq_u16(a, b);
    b = vmvn_u16(a);
    return  vshr_n_u16(b, 15);
}

INLINE( int16x4_t,DHI_CNEL)   (int16x4_t a,  int16x4_t b)
{
    uint16x4_t c = vceq_s16(a, b);
    c = vmvn_u16(c);
    c = vshr_n_u16(c, 15);
    return  vreinterpret_s16_u16(c);
}

INLINE( int16x4_t,DHF_CNEL) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE c = {.H.I=DHF_CEQS(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.H.U = vshr_n_u16(c.H.U, 15);
    return  c.H.I;
}


INLINE(uint32x2_t,DWU_CNEL)  (uint32x2_t a,  uint32x2_t b)
{
    a = vceq_u32(a, b);
    a = vmvn_u32(a);
    return  vshr_n_u32(a, 31);
}

INLINE( int32x2_t,DWI_CNEL)   (int32x2_t a,   int32x2_t b)
{
    uint32x2_t c = vceq_s32(a, b);
    c = vmvn_u32(c);
    c = vshr_n_u32(c, 31);
    return  vreinterpret_s32_u32(c);
}

INLINE( int32x2_t,DWF_CNEL) (float32x2_t a, float32x2_t b)
{
    uint32x2_t c = vceq_f32(a, b);
    c = vmvn_u32(c);
    c = vshr_n_u32(c, 31);
    return  vreinterpret_s32_u32(c);
}


INLINE(uint64x1_t,DDU_CNEL)  (uint64x1_t a,  uint64x1_t b)
{
    DWRD_VTYPE c = {.D.U=vceq_u64(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    return  vshr_n_u64(c.D.U, 63);
}

INLINE( int64x1_t,DDI_CNEL)   (int64x1_t a,   int64x1_t b)
{
    DWRD_VTYPE c = {.D.U=vceq_s64(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.D.U = vshr_n_u64(c.D.U, 63);
    return  c.D.I;
}

INLINE( int64x1_t,DDF_CNEL) (float64x1_t a, float64x1_t b)
{
    DWRD_VTYPE c = {.D.U=vceq_f64(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.D.U = vshr_n_u64(c.D.U, 63);
    return  c.D.I;
}

INLINE(uint8x16_t,QBU_CNEL) (uint8x16_t a, uint8x16_t b)
{
    a = vceqq_u8(a, b);
    a = vmvnq_u8(a);
    return  vshrq_n_u8(a, 7);
}

INLINE( int8x16_t,QBI_CNEL)  (int8x16_t a,  int8x16_t b)
{
    uint8x16_t c = vceqq_s8(a, b);
    c = vmvnq_u8(c);
    c = vshrq_n_u8(c, 7);
    return  vreinterpretq_s8_u8(c);
}

#if CHAR_MIN
#   define  QBC_CNEL    QBI_CNEL
#else
#   define  QBC_CNEL    QBU_CNEL
#endif

INLINE(uint16x8_t,QHU_CNEL)  (uint16x8_t a,  uint16x8_t b)
{
    a = vceqq_u16(a, b);
    b = vmvnq_u16(a);
    return  vshrq_n_u16(b, 15);
}

INLINE( int16x8_t,QHI_CNEL)   (int16x8_t a,  int16x8_t b)
{
    uint16x8_t c = vceqq_s16(a, b);
    c = vmvnq_u16(c);
    c = vshrq_n_u16(c, 15);
    return  vreinterpretq_s16_u16(c);
}

INLINE( int16x8_t,QHF_CNEL) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE c = {.H.I=QHF_CEQS(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    c.H.U = vshrq_n_u16(c.H.U, 15);
    return  c.H.I;
}


INLINE(uint32x4_t,QWU_CNEL)  (uint32x4_t a,  uint32x4_t b)
{
    a = vceqq_u32(a, b);
    a = vmvnq_u32(a);
    return  vshrq_n_u32(a, 31);
}

INLINE( int32x4_t,QWI_CNEL)   (int32x4_t a,   int32x4_t b)
{
    uint32x4_t c = vceqq_s32(a, b);
    c = vmvnq_u32(c);
    c = vshrq_n_u32(c, 31);
    return  vreinterpretq_s32_u32(c);
}

INLINE( int32x4_t,QWF_CNEL) (float32x4_t a, float32x4_t b)
{
    uint32x4_t c = vceqq_f32(a, b);
    c = vmvnq_u32(c);
    c = vshrq_n_u32(c, 31);
    return  vreinterpretq_s32_u32(c);
}


INLINE(uint64x2_t,QDU_CNEL)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_u64(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    return  vshrq_n_u64(c.D.U, 63);
}

INLINE( int64x2_t,QDI_CNEL)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_s64(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    c.D.U = vshrq_n_u64(c.D.U, 63);
    return  c.D.I;
}

INLINE( int64x2_t,QDF_CNEL) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_f64(a, b)};
    c.B.U = vmvnq_u8(c.B.U);
    c.D.U = vshrq_n_u64(c.D.U, 63);
    return  c.D.I;
}

INLINE(uint64x2_t,QQU_CNEL)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_u64(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vand_u8(l.B.U, r.B.U);
    l.B.U = vmvn_u8(l.B.U);
    l.D.U = vshr_n_u64(l.D.U, 63);
    r.D.U = vdup_n_u64(0);
    return  vcombine_u64(l.D.U, r.D.U);
}

INLINE( int64x2_t,QQI_CNEL)   (int64x2_t a,  int64x2_t b)
{
    QUAD_VTYPE l={.D.I=a}, r={.D.I=b};
    l.D.U = QQU_CNEL(l.D.U, r.D.U);
    return  l.D.I;
}

INLINE(QUAD_FTYPE,QQF_CNEL) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE c = {.Q.I=QQF_CEQS(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    l.B.U = vmvn_u8(l.B.U);
    l.D.U = vshr_n_u64(l.D.U, 63);
    c.D.U = vcombine_u64(l.D.U, vdup_n_u64(0));
    return  c.F;
}


INLINE(int16_t,  FLT16_CNEL) (flt16_t a, flt16_t b)
{
    DWRD_VTYPE x={.H.F={a}}, y={.H.F={b}};
    x.H.I = DHF_CNEL(x.H.F, y.H.F);
    return  vget_lane_s16(x.H.I, 0);
}

INLINE(int32_t, FLT_CNEL)   (float a,   float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.I = DWF_CNEL(x.W.F, y.W.F);
    return  vget_lane_s32(x.W.I, 0);
}

INLINE(int64_t, DBL_CNEL)  (double a,  double b) 
{
    DWRD_VTYPE x={.D.F={a}}, y={.D.F={b}};
    x.D.I = DDF_CNEL(x.D.F, y.D.F);
    return  vget_lane_s64(x.D.I, 0);
}

INLINE(QUAD_ITYPE,cnelqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    QUAD_VTYPE c = {.F=QQF_CNEL(a, b)};
    return  c.I;
}

INLINE(float,WBZ_CNEL) (float a, float b)
{
    DWRD_VTYPE l={.W.F={a}}, r={.W.F={b}};
    l.B.U = DBU_CNEL(l.B.U, r.B.U);
    return  vget_lane_f32(l.W.F, 0);
}

INLINE(float,WHZ_CNEL) (float a, float b)
{
    DWRD_VTYPE l={.W.F={a}}, r={.W.F={b}};
    l.H.U = DHU_CNEL(l.H.U, r.H.U);
    return  vget_lane_f32(l.W.F, 0);
}

INLINE(float,WWZ_CNEL) (float a, float b)
{
    DWRD_VTYPE l={.W.F={a}}, r={.W.F={b}};
    l.W.U = DWU_CNEL(l.W.U, r.W.U);
    return  vget_lane_f32(l.W.F, 0);
}

INLINE(float,WHF_CNEL) (float a, float b)
{
    DWRD_VTYPE l={.W.F={a}}, r={.W.F={b}};
    l.W.U = DHF_CNEL(l.W.U, r.W.U);
    return  vget_lane_f32(l.W.F, 0);
}

INLINE(float,WWF_CNEL) (float a, float b)
{
    DWRD_VTYPE l={.W.F={a}}, r={.W.F={b}};
    l.W.I = DWF_CNEL(l.W.F, r.W.F);
    return  vget_lane_f32(l.W.F, 0);
}

#define     VWYU_CNEL VWYU_XORS
INLINE(Vwbu,VWBU_CNEL) (Vwbu a, Vwbu b) {a.V0=WBZ_CNEL(a.V0, b.V0); return a;}
INLINE(Vwbi,VWBI_CNEL) (Vwbi a, Vwbi b) {a.V0=WBZ_CNEL(a.V0, b.V0); return a;}
INLINE(Vwbc,VWBC_CNEL) (Vwbc a, Vwbc b) {a.V0=WBZ_CNEL(a.V0, b.V0); return a;}
INLINE(Vwhu,VWHU_CNEL) (Vwhu a, Vwhu b) {a.V0=WHZ_CNEL(a.V0, b.V0); return a;}
INLINE(Vwhi,VWHI_CNEL) (Vwhi a, Vwhi b) {a.V0=WHZ_CNEL(a.V0, b.V0); return a;}
INLINE(Vwhi,VWHF_CNEL) (Vwhf a, Vwhf b) {return ((Vwhi){WHF_CNEL(a.V0,b.V0)});}
INLINE(Vwwu,VWWU_CNEL) (Vwwu a, Vwwu b) {a.V0=WWZ_CNEL(a.V0, b.V0); return a;}
INLINE(Vwwi,VWWI_CNEL) (Vwwi a, Vwwi b) {a.V0=WWZ_CNEL(a.V0, b.V0); return a;}
INLINE(Vwwi,VWWF_CNEL) (Vwwf a, Vwwf b) {return ((Vwwi){WWF_CNEL(a.V0,b.V0)});}

#define     VDYU_CNEL VDYU_XORS
INLINE(Vdbu,VDBU_CNEL) (Vdbu a, Vdbu b) {return DBU_CNEL(a, b);}
INLINE(Vdbi,VDBI_CNEL) (Vdbi a, Vdbi b) {return DBI_CNEL(a, b);}
INLINE(Vdbc,VDBC_CNEL) (Vdbc a, Vdbc b) {a.V0=DBC_CNEL(a.V0, b.V0); return a;}
INLINE(Vdhu,VDHU_CNEL) (Vdhu a, Vdhu b) {return DHU_CNEL(a, b);}
INLINE(Vdhi,VDHI_CNEL) (Vdhi a, Vdhi b) {return DHI_CNEL(a, b);}
INLINE(Vdhi,VDHF_CNEL) (Vdhf a, Vdhf b) {return DHF_CNEL(a, b);}
INLINE(Vdwu,VDWU_CNEL) (Vdwu a, Vdwu b) {return DWU_CNEL(a, b);}
INLINE(Vdwi,VDWI_CNEL) (Vdwi a, Vdwi b) {return DWI_CNEL(a, b);}
INLINE(Vdwi,VDWF_CNEL) (Vdwf a, Vdwf b) {return DWF_CNEL(a, b);}
INLINE(Vddu,VDDU_CNEL) (Vddu a, Vddu b) {return DDU_CNEL(a, b);}
INLINE(Vddi,VDDI_CNEL) (Vddi a, Vddi b) {return DDI_CNEL(a, b);}
INLINE(Vddi,VDDF_CNEL) (Vddf a, Vddf b) {return DDF_CNEL(a, b);}

#define     VQYU_CNEL VQYU_XORS
INLINE(Vqbu,VQBU_CNEL) (Vqbu a, Vqbu b) {return QBU_CNEL(a,b);}
INLINE(Vqbi,VQBI_CNEL) (Vqbi a, Vqbi b) {return QBI_CNEL(a,b);}
INLINE(Vqbc,VQBC_CNEL) (Vqbc a, Vqbc b) {a.V0=QBC_CNEL(a.V0, b.V0); return a;}
INLINE(Vqhu,VQHU_CNEL) (Vqhu a, Vqhu b) {return QHU_CNEL(a,b);}
INLINE(Vqhi,VQHI_CNEL) (Vqhi a, Vqhi b) {return QHI_CNEL(a,b);}
INLINE(Vqhi,VQHF_CNEL) (Vqhf a, Vqhf b) {return QHF_CNEL(a,b);}
INLINE(Vqwu,VQWU_CNEL) (Vqwu a, Vqwu b) {return QWU_CNEL(a,b);}
INLINE(Vqwi,VQWI_CNEL) (Vqwi a, Vqwi b) {return QWI_CNEL(a,b);}
INLINE(Vqwi,VQWF_CNEL) (Vqwf a, Vqwf b) {return QWF_CNEL(a,b);}
INLINE(Vqdu,VQDU_CNEL) (Vqdu a, Vqdu b) {return QDU_CNEL(a,b);}
INLINE(Vqdi,VQDI_CNEL) (Vqdi a, Vqdi b) {return QDI_CNEL(a,b);}
INLINE(Vqdi,VQDF_CNEL) (Vqdf a, Vqdf b) {return QDF_CNEL(a,b);}
INLINE(Vqqu,VQQU_CNEL) (Vqqu a, Vqqu b) {a.V0=QQU_CNEL(a.V0, b.V0); return a;}
INLINE(Vqqi,VQQI_CNEL) (Vqqi a, Vqqi b) {a.V0=QQI_CNEL(a.V0, b.V0); return a;}
INLINE(Vqqi,VQQF_CNEL) (Vqqf a, Vqqf b) {return ((Vqqi){QQF_CNEL(a.V0,b.V0)});}

#if 0 // _LEAVE_ARM_CNEL
}
#endif

#if 0 // _ENTER_ARM_CNEY
{
#endif

INLINE(_Bool,FLT16_CNEY) (flt16_t a, flt16_t b) {return a!=b;}

INLINE(_Bool,  FLT_CNEY)   (float a,   float b) {return a!=b;}

INLINE(_Bool,  DBL_CNEY)  (double a,  double b) {return a!=b;}

#if CHAR_MIN
#   define  DBC_CNEY DBI_CNEY
#   define  QBC_CNEY QBI_CNEY
#else
#   define  DBC_CNEY QBU_CNEY
#   define  QBC_CNEY QBU_CNEY
#endif

INLINE(_Bool,WYU_CNEY)       (float a,       float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    y.B.U = vmvn_u8(y.B.U);
    x.W.U = vceq_u32(x.W.U, y.W.U);
    return  vget_lane_u32(x.W.U, 0);
}

INLINE(_Bool,WBZ_CNEY)       (float a,       float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.B.U = vceq_u8(x.B.U, y.B.U);
    return  !vget_lane_u32(x.W.U, 0);
}

INLINE(_Bool,WHZ_CNEY)       (float a,       float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.U = vceq_u16(x.B.U, y.W.U);
    return  !vget_lane_u32(x.W.U, 0);
}

INLINE(_Bool,WHF_CNEY)       (float a,       float b) 
{
    DWRD_VTYPE c = {.W.F={WHF_CEQS(a, b)}};
    return  !vget_lane_u32(c.W.U, 0);
}

INLINE(_Bool,WWZ_CNEY)       (float a,       float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.W.U = vceq_u32(x.W.U, y.W.U);
    return  !vget_lane_u32(x.W.U, 0);
}

INLINE(_Bool,DYU_CNEY)  (uint64x1_t a,  uint64x1_t b) 
{
    DWRD_VTYPE c = {.D.U=b};
    c.B.U = vmvn_u8(c.B.U);
    a = vceq_u64(a, c.D.U);
    return  vget_lane_u64(a, 0);
}

INLINE(_Bool,DBU_CNEY)   (uint8x8_t a,   uint8x8_t b)
{
    DWRD_VTYPE c = {.B.U=vceq_u8(a, b)};
    return  !vget_lane_u64(c.D.U, 0);
}

INLINE(_Bool,DBI_CNEY)    (int8x8_t a,    int8x8_t b)
{
    DWRD_VTYPE c = {.B.U=vceq_s8(a, b)};
    return  !vget_lane_u64(c.D.U, 0);
}

INLINE(_Bool,DHU_CNEY)  (uint16x4_t a,  uint16x4_t b)
{
    DWRD_VTYPE c = {.H.U=vceq_u16(a, b)};
    return  !vget_lane_u64(c.D.U, 0);
}

INLINE(_Bool,DHI_CNEY)   (int16x4_t a,   int16x4_t b)
{
    DWRD_VTYPE c = {.H.U=vceq_s16(a, b)};
    return  !vget_lane_u64(c.D.U, 0);
}

INLINE(_Bool,DHF_CNEY) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE c = {.H.I=DHF_CEQS(a, b)};
    return  !vget_lane_u64(c.D.U, 0);
}

INLINE(_Bool,DWU_CNEY)  (uint32x2_t a,  uint32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_u32(a, b)};
    return  !vget_lane_u64(c.D.U, 0);
}

INLINE(_Bool,DWI_CNEY)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_s32(a, b)};
    return  !vget_lane_u64(c.D.U, 0);
}

INLINE(_Bool,DWF_CNEY) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_f32(a, b)};
    return  !vget_lane_u64(c.D.U, 0);
}

INLINE(_Bool,DDU_CNEY)  (uint64x1_t a,  uint64x1_t b)
{
    a = vceq_u64(a, b);
    return  !vget_lane_u64(a, 0);
}

INLINE(_Bool,DDI_CNEY)   (int64x1_t a,   int64x1_t b)
{
    uint64x1_t c = vceq_s64(a, b);
    return  !vget_lane_u64(c, 0);
}

INLINE(_Bool,DDF_CNEY) (float64x1_t a, float64x1_t b)
{
    uint64x1_t c = vceq_f64(a, b);
    return  !vget_lane_f64(c, 0);
}


INLINE(_Bool,QYU_CNEY)  (uint64x2_t a,  uint64x2_t b) 
{
    QUAD_VTYPE c={.D.U=b};
    c.B.U = vmvnq_u8(c.B.U);
    a = vceqq_u64(a, c.D.U);
    return  vgetq_lane_u64(a, 0)&vgetq_lane_u64(a, 1);
}

INLINE(_Bool,QBU_CNEY)  (uint8x16_t a,  uint8x16_t b)
{
    QUAD_VTYPE  c = {.B.U=vceqq_u8(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QBI_CNEY)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE  c = {.B.U=vceqq_s8(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QHU_CNEY)  (uint16x8_t a,  uint16x8_t b)
{
    QUAD_VTYPE  c = {.H.U=vceqq_u16(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QHI_CNEY)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE  c = {.H.U=vceqq_s16(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QHF_CNEY) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE  c = {.H.I=VQHF_CEQS(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QWU_CNEY)  (uint32x4_t a,  uint32x4_t b)
{
    QUAD_VTYPE  c = {.W.U=vceqq_u32(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QWI_CNEY)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE  c = {.W.U=vceqq_s32(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QWF_CNEY) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE  c = {.W.U=vceqq_f32(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QDU_CNEY)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE  c = {.D.U=vceqq_u64(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QDI_CNEY)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE  c = {.D.U=vceqq_s64(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QDF_CNEY) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE  c = {.D.U=vceqq_f64(a, b)};
    DWRD_VTYPE  l = {.D.U=vget_low_u64(c.D.U)};
    uint64x1_t  r = vget_high_u64(c.D.U);
    r = vorr_u64(l.D.U, r);
    return  !vget_lane_u64(r, 0);
}

INLINE(_Bool,QQU_CNEY)  (uint64x2_t a,  uint64x2_t b)
{
    a = vceqq_u64(a, b);
    return !(vgetq_lane_u64(a, 0)&vgetq_lane_u64(a, 1));
}

INLINE(_Bool,QQI_CNEY)   (int64x2_t a,   int64x2_t b)
{
    uint64x2_t c = vceqq_s64(a, b);
    return !(vgetq_lane_u64(a, 0)&vgetq_lane_u64(a, 1));
}

INLINE(_Bool,VWYU_CNEY) (Vwyu a, Vwyu b) {return WYU_CNEY(a.V0, b.V0);}
INLINE(_Bool,VWBU_CNEY) (Vwbu a, Vwbu b) {return WBZ_CNEY(a.V0, b.V0);}         
INLINE(_Bool,VWBI_CNEY) (Vwbi a, Vwbi b) {return WBZ_CNEY(a.V0, b.V0);}
INLINE(_Bool,VWBC_CNEY) (Vwbc a, Vwbc b) {return WBZ_CNEY(a.V0, b.V0);}
INLINE(_Bool,VWHU_CNEY) (Vwhu a, Vwhu b) {return WHZ_CNEY(a.V0, b.V0);}         
INLINE(_Bool,VWHI_CNEY) (Vwhi a, Vwhi b) {return WHZ_CNEY(a.V0, b.V0);}         
INLINE(_Bool,VWHF_CNEY) (Vwhf a, Vwhf b) {return WHF_CNEY(a.V0, b.V0);}
INLINE(_Bool,VWWU_CNEY) (Vwwu a, Vwwu b) {return WWZ_CNEY(a.V0, b.V0);}         
INLINE(_Bool,VWWI_CNEY) (Vwwi a, Vwwi b) {return WWZ_CNEY(a.V0, b.V0);}         
INLINE(_Bool,VWWF_CNEY) (Vwwf a, Vwwf b) {return a.V0!=b.V0;}

INLINE(_Bool,VDYU_CNEY) (Vdyu a, Vdyu b) {return DYU_CNEY(a.V0, b.V0);}
INLINE(_Bool,VDBU_CNEY) (Vdbu a, Vdbu b) {return DBU_CNEY(a, b);}
INLINE(_Bool,VDBI_CNEY) (Vdbi a, Vdbi b) {return DBI_CNEY(a, b);}
INLINE(_Bool,VDBC_CNEY) (Vdbc a, Vdbc b) {return DBC_CNEY(a.V0, b.V0);}
INLINE(_Bool,VDHU_CNEY) (Vdhu a, Vdhu b) {return DHU_CNEY(a, b);}
INLINE(_Bool,VDHI_CNEY) (Vdhi a, Vdhi b) {return DHI_CNEY(a, b);}
INLINE(_Bool,VDHF_CNEY) (Vdhf a, Vdhf b) {return DHF_CNEY(a, b);}
INLINE(_Bool,VDWU_CNEY) (Vdwu a, Vdwu b) {return DWU_CNEY(a, b);}
INLINE(_Bool,VDWI_CNEY) (Vdwi a, Vdwi b) {return DWI_CNEY(a, b);}
INLINE(_Bool,VDWF_CNEY) (Vdwf a, Vdwf b) {return DWF_CNEY(a, b);}
INLINE(_Bool,VDDU_CNEY) (Vddu a, Vddu b) {return DDU_CNEY(a, b);} 
INLINE(_Bool,VDDI_CNEY) (Vddi a, Vddi b) {return DDI_CNEY(a, b);} 
INLINE(_Bool,VDDF_CNEY) (Vddf a, Vddf b) {return DDF_CNEY(a, b);} 

INLINE(_Bool,VQYU_CNEY) (Vqyu a, Vqyu b) {return QYU_CNEY(a.V0, b.V0);}
INLINE(_Bool,VQBU_CNEY) (Vqbu a, Vqbu b) {return QBU_CNEY(a, b);}
INLINE(_Bool,VQBI_CNEY) (Vqbi a, Vqbi b) {return QBI_CNEY(a, b);}
INLINE(_Bool,VQBC_CNEY) (Vqbc a, Vqbc b) {return QBC_CNEY(a.V0, b.V0);}
INLINE(_Bool,VQHU_CNEY) (Vqhu a, Vqhu b) {return QHU_CNEY(a, b);}
INLINE(_Bool,VQHI_CNEY) (Vqhi a, Vqhi b) {return QHI_CNEY(a, b);}
INLINE(_Bool,VQHF_CNEY) (Vqhf a, Vqhf b) {return QHF_CNEY(a, b);}
INLINE(_Bool,VQWU_CNEY) (Vqwu a, Vqwu b) {return QWU_CNEY(a, b);}
INLINE(_Bool,VQWI_CNEY) (Vqwi a, Vqwi b) {return QWI_CNEY(a, b);}
INLINE(_Bool,VQWF_CNEY) (Vqwf a, Vqwf b) {return QWF_CNEY(a, b);}
INLINE(_Bool,VQDU_CNEY) (Vqdu a, Vqdu b) {return QDU_CNEY(a, b);} 
INLINE(_Bool,VQDI_CNEY) (Vqdi a, Vqdi b) {return QDI_CNEY(a, b);} 
INLINE(_Bool,VQDF_CNEY) (Vqdf a, Vqdf b) {return QDF_CNEY(a, b);} 
INLINE(_Bool,VQQU_CNEY) (Vqqu a, Vqqu b) {return QQU_CNEY(a.V0, b.V0);} 
INLINE(_Bool,VQQI_CNEY) (Vqqi a, Vqqi b) {return QQI_CNEY(a.V0, b.V0);} 
INLINE(_Bool,VQQF_CNEY) (Vqqf a, Vqqf b) {return a.V0!=b.V0;}

#if 0 // _LEAVE_ARM_CNEY
}
#endif

#if 0 // _ENTER_ARM_CLTS
{
#endif

INLINE(int16_t,  FLT16_CLTS) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vclth_f16(a, b);
#else
    return  0-(a<b);
#endif
}

INLINE(int32_t,    FLT_CLTS)   (float a,   float b) {return  vclts_f32(a, b);}

INLINE(int64_t,    DBL_CLTS)  (double a,  double b) {return  vcltd_f64(a, b);}

INLINE(QUAD_ITYPE,cltsqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (QUAD_ITYPE) 0-(a<b);
}

#define     DBU_CLTS            vclt_u8
#define     DBI_CLTS(A, B)      vreinterpret_s16_u16(vclt_s8(A,B))
#define     DBC_CLTS            VDBC_BASE(CLTS)
#define     DHU_CLTS            vclt_u16
#define     DHI_CLTS(A, B)      vreinterpret_s16_u16(vclt_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CLTS(A, B)      vreinterpret_s16_u16(vclt_f16(A,B))
#else
INLINE(int16x4_t,DHF_CLTS) (float16x4_t a, float16x4_t b)
{
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(a);
    uint32x4_t  v = vcltq_f32(l, r);
    return  vreinterpret_s16_u16(vmovn_u32(v));
    //MY_NOT_IMPLEMENTED(0, __func__);
    //return  DHF_VOID;
}
#endif

#define     DWU_CLTS                             vclt_u32
#define     DWI_CLTS(A, B)  vreinterpret_s32_u32(vclt_s32(A,B))
#define     DWF_CLTS(A, B)  vreinterpret_s32_u32(vclt_f32(A,B))
#define     DDU_CLTS                             vclt_u64
#define     DDI_CLTS(A, B)  vreinterpret_s64_u64(vclt_s32(A,B))
#define     DDF_CLTS(A, B)  vreinterpret_s64_u64(vclt_f32(A,B))


#define     QBU_CLTS                              vcltq_u8
#define     QBI_CLTS(A, B)  vreinterpretq_s16_u16(vcltq_s8(A,B))
#define     QBC_CLTS        VQBC_BASE(CLTS)
#define     QHU_CLTS                              vcltq_u16
#define     QHI_CLTS(A, B)  vreinterpretq_s16_u16(vcltq_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CLTS(A, B)  vreinterpretq_s16_u16(vcltq_f16(A,B))
#else
INLINE(int16x8_t,QHF_CLTS) (float16x8_t a, float16x8_t b)
{
    return vcombine_s16(
        DHF_CLTS(vget_low_f16(a), vget_low_f16(b)),
        DHF_CLTS(vget_high_f16(a), vget_high_f16(b))
    );
}
#endif

#define     QWU_CLTS                              vcltq_u32
#define     QWI_CLTS(A, B)  vreinterpretq_s32_u32(vcltq_s32(A,B))
#define     QWF_CLTS(A, B)  vreinterpretq_s32_u32(vcltq_f32(A,B))
#define     QDU_CLTS                              vcltq_u64
#define     QDI_CLTS(A, B)  vreinterpretq_s64_u64(vcltq_s32(A,B))
#define     QDF_CLTS(A, B)  vreinterpretq_s64_u64(vcltq_f32(A,B))


INLINE(Vwbu,VWBU_CLTS) (Vwbu a, Vwbu b)
{
    float32x2_t l = vset_lane_f32(VWBU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWBU_ASTM(a), r, 0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CLTS(p, q)),
            0
        )
    );
}

INLINE(Vwbi,VWBI_CLTS) (Vwbi a, Vwbi b)
{
    float32x2_t l = vset_lane_f32(VWBI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWBI_ASTM(a), r, 0);
    int8x8_t    p = vreinterpret_s8_f32(l);
    int8x8_t    q = vreinterpret_s8_f32(r);
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(vclt_s8(p, q)),
            0
        )
    );
}

INLINE(Vwbc,VWBC_CLTS) (Vwbc a, Vwbc b)
{
    float32x2_t x = vset_lane_f32(VWBC_ASTM(a), x, 0);
    float32x2_t y = vset_lane_f32(VWBC_ASTM(a), y, 0);
    float32x2_t r;
#if CHAR_MIN
    int8x8_t    p = vreinterpret_s8_f32(x);
    int8x8_t    q = vreinterpret_s8_f32(y);
    r = vreinterpret_f32_u8(vclt_s8(p, q));
#else
    uint8x8_t   p = vreinterpret_u8_f32(x);
    uint8x8_t   q = vreinterpret_u8_f32(y);
    r = vreinterpret_f32_u8(vclt_u8(p, q));
#endif
    return  WBC_ASTV(vget_lane_f32(r, 0));
}


INLINE(Vwhu,VWHU_CLTS) (Vwhu a, Vwhu b)
{
    float32x2_t l = vset_lane_f32(VWHU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHU_ASTM(a), r, 0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vclt_u16(p, q)),
            0
        )
    );
}

INLINE(Vwhi,VWHI_CLTS) (Vwhi a, Vwhi b)
{
    float32x2_t l = vset_lane_f32(VWHI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHI_ASTM(a), r, 0);
    int16x4_t   p = vreinterpret_s16_f32(l);
    int16x4_t   q = vreinterpret_s16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vclt_s16(p, q)),
            0
        )
    );
}

#if defined(SPC_ARM_FP16_SIMD)

INLINE(Vwhi,VWHF_CLTS) (Vwhf a, Vwhf b)
{
    float32x2_t l = vset_lane_f32(VWHF_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHF_ASTM(a), r, 0);
    float16x4_t p = vreinterpret_f16_f32(l);
    float16x4_t q = vreinterpret_f16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vclt_f16(p, q)),
            0
        )
    );
}

#else

INLINE(Vwhf,VWHF_CLTS) (Vwhf a, Vwhf b)
{
    MY_NOT_IMPLEMENTED(0, __func__);
    return  VWHF_VOID;
}

#endif


INLINE(Vwwu,VWWU_CLTS) (Vwwu a, Vwwu b)
{
    float32x2_t l = vset_lane_f32(VWWU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWWU_ASTM(a), r, 0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vclt_u32(p, q)),
            0
        )
    );
}

INLINE(Vwwi,VWWI_CLTS) (Vwwi a, Vwwi b)
{
    float32x2_t l = vset_lane_f32(VWWI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWWI_ASTM(a), r, 0);
    int32x2_t   p = vreinterpret_s32_f32(l);
    int32x2_t   q = vreinterpret_s32_f32(r);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vclt_s32(p, q)),
            0
        )
    );
}

INLINE(Vwwi,VWWF_CLTS) (Vwwf a, Vwwf b)
{
    float32x2_t p = vset_lane_f32(VWWF_ASTM(a), p, 0);
    float32x2_t q = vset_lane_f32(VWWF_ASTM(a), q, 0);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vclt_f32(p, q)),
            0
        )
    );
}


INLINE(Vdbu,VDBU_CLTS) (Vdbu a, Vdbu b) {return DBU_CLTS(a, b);}
INLINE(Vdbi,VDBI_CLTS) (Vdbi a, Vdbi b) {return DBI_CLTS(a, b);}
INLINE(Vdbc,VDBC_CLTS) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(DBC_CLTS(VDBC_ASTM(b), VDBC_ASTM(b)));
}

INLINE(Vdhu,VDHU_CLTS) (Vdhu a, Vdhu b) {return DHU_CLTS(a, b);}
INLINE(Vdhi,VDHI_CLTS) (Vdhi a, Vdhi b) {return DHI_CLTS(a, b);}
INLINE(Vdhi,VDHF_CLTS) (Vdhf a, Vdhf b) {return DHF_CLTS(a, b);}
INLINE(Vdwu,VDWU_CLTS) (Vdwu a, Vdwu b) {return DWU_CLTS(a, b);}
INLINE(Vdwi,VDWI_CLTS) (Vdwi a, Vdwi b) {return DWI_CLTS(a, b);}
INLINE(Vdwi,VDWF_CLTS) (Vdwf a, Vdwf b) {return DWF_CLTS(a, b);}
INLINE(Vddu,VDDU_CLTS) (Vddu a, Vddu b) {return DDU_CLTS(a, b);}
INLINE(Vddi,VDDI_CLTS) (Vddi a, Vddi b) {return DDI_CLTS(a, b);}
INLINE(Vddi,VDDF_CLTS) (Vddf a, Vddf b) {return DDF_CLTS(a, b);}


INLINE(Vqbu,VQBU_CLTS) (Vqbu a, Vqbu b) {return QBU_CLTS(a, b);}
INLINE(Vqbi,VQBI_CLTS) (Vqbi a, Vqbi b) {return QBI_CLTS(a, b);}
INLINE(Vqbc,VQBC_CLTS) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CLTS(VQBC_ASTM(b), VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CLTS) (Vqhu a, Vqhu b) {return QHU_CLTS(a, b);}
INLINE(Vqhi,VQHI_CLTS) (Vqhi a, Vqhi b) {return QHI_CLTS(a, b);}
INLINE(Vqhi,VQHF_CLTS) (Vqhf a, Vqhf b) {return QHF_CLTS(a, b);}
INLINE(Vqwu,VQWU_CLTS) (Vqwu a, Vqwu b) {return QWU_CLTS(a, b);}
INLINE(Vqwi,VQWI_CLTS) (Vqwi a, Vqwi b) {return QWI_CLTS(a, b);}
INLINE(Vqwi,VQWF_CLTS) (Vqwf a, Vqwf b) {return QWF_CLTS(a, b);}
INLINE(Vqdu,VQDU_CLTS) (Vqdu a, Vqdu b) {return QDU_CLTS(a, b);}
INLINE(Vqdi,VQDI_CLTS) (Vqdi a, Vqdi b) {return QDI_CLTS(a, b);}
INLINE(Vqdi,VQDF_CLTS) (Vqdf a, Vqdf b) {return QDF_CLTS(a, b);}

#if 0 // _LEAVE_ARM_CLTS
}
#endif

#if 0 // _ENTER_ARM_CLTL
{
#endif

INLINE(_Bool, FLT16_CLTL) (flt16_t a, flt16_t b) {return a<b;}
INLINE(_Bool,   FLT_CLTL)   (float a,   float b) {return a<b;}
INLINE(_Bool,   DBL_CLTL)  (double a,  double b) {return a<b;}

INLINE(_Bool,cltlqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (a<b);
}

INLINE( Wbu,WBU_CLTL) (Wbu a, Wbu b)
{
#define     WBU_CLTL    WBU_CLTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    uint8x8_t   r = vclt_u8(
        vreinterpret_u8_f32(p),
        vreinterpret_u8_f32(q)
    );
    return  vget_lane_f32(
        vreinterpret_f32_u8(vshr_n_u8(r, 7)),
        0
    );
}

INLINE( Wbu,WBI_CLTL) (Wbi a, Wbi b)
{
#define     WBI_CLTL    WBI_CLTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vshr_n_u8(
                vclt_s8(
                    vreinterpret_s8_f32(p),
                    vreinterpret_s8_f32(q)
                ),
                7
            )
        ),
        0
    );
}

#if CHAR_MIN
#   define  WBC_CLTL    WBI_CLTL
#else
#   define  WBC_CLTL    WBU_CLTL
#endif

INLINE( Whu,WHU_CLTL) (Whu a, Whu b)
{
#define     WHU_CLTL    WHU_CLTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vclt_u16(
                    vreinterpret_u16_f32(p),
                    vreinterpret_u16_f32(q)
                ),
                15
            )
        ),
        0
    );
}

INLINE( Whu,WHI_CLTL) (Whi a, Whi b)
{
#define     WHI_CLTL    WHI_CLTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vclt_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        0
    );
}

INLINE( Whu,WHF_CLTL) (Whf a, Whf b)
{
#define     WHF_CLTL    WHF_CLTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vclt_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        0
    );
#else
    float32x4_t l = vcvt_f32_f16(vreinterpret_f16_f32(p));
    float32x4_t r = vcvt_f32_f16(vreinterpret_f16_f32(q));
    uint16x4_t  v = vshr_n_u16(vmovn_u32(vcltq_f32(l, r)), 15);
    return  vget_lane_f32(vreinterpret_f32_u16(v), 0);
#endif
}

INLINE( Wwu,WWU_CLTL) (Wwu a, Wwu b)
{
#define     WWU_CLTL    WWU_CLTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vclt_u32(
                    vreinterpret_u32_f32(p),
                    vreinterpret_u32_f32(q)
                ),
                31
            )
        ),
        0
    );
}

INLINE( Wwu,WWI_CLTL) (Wwi a, Wwi b)
{
#define     WWI_CLTL    WWI_CLTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vclt_s32(
                    vreinterpret_s32_f32(p),
                    vreinterpret_s32_f32(q)
                ),
                31
            )
        ),
        0
    );
}

INLINE( Wwu,WWF_CLTL) (Wwf a, Wwf b)
{
#define     WWF_CLTL    WWF_CLTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(vshr_n_u32(vclt_f32(p, q), 31)),
        0
    );
}

#define     DBU_CLTL(A, B)            vshr_n_u8(vclt_u8(A,B),7)
#define     DBI_CLTL(A, B)  VDBU_ASTI(vshr_n_u8(vclt_s8(A,B),7))
#if CHAR_MIN
#   define  DBC_CLTL        DBI_CLTL
#else
#   define  DBC_CLTL        DBU_CLTL
#endif

#define     DHU_CLTL(A, B)            vshr_n_u16(vclt_u16(A,B),15)
#define     DHI_CLTL(A, B)  VDHU_ASTI(vshr_n_u16(vclt_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CLTL(A, B)  VDHU_ASTI(vshr_n_u16(vclt_f16(A,B),15))
#else
#   define  DHF_CLTL(A, B)      \
VDHU_ASHI(                      \
    vshr_n_u16(                 \
        vmovn_u32(              \
            vcltq_f32(          \
                vcvt_f32_f16(a),\
                vcvt_f32_f16(b) \
            )                   \
        ),                      \
        15                      \
    )                           \
)
#endif

#define     DWU_CLTL(A, B)            vshr_n_u32(vclt_u32(A,B),31)
#define     DWI_CLTL(A, B)  VDWU_ASTI(vshr_n_u32(vclt_s32(A,B),31))
#define     DWF_CLTL(A, B)  VDWU_ASTI(vshr_n_u32(vclt_f32(A,B),31))

#define     DDU_CLTL(A, B)            vshr_n_u64(vclt_u64(A,B),63)
#define     DDI_CLTL(A, B)  VDDU_ASTI(vshr_n_u64(vclt_s64(A,B),63))
#define     DDF_CLTL(A, B)  VDDU_ASTI(vshr_n_u64(vclt_f64(A,B),63))


#define     QBU_CLTL(A, B)            vshrq_n_u8(vcltq_u8(A,B),7)
#define     QBI_CLTL(A, B)  VQBU_ASTI(vshrq_n_u8(vcltq_s8(A,B),7))
#if CHAR_MIN
#   define  QBC_CLTL        QBI_CLTL
#else
#   define  QBC_CLTL        QBU_CLTL
#endif

#define     QHU_CLTL(A, B)            vshrq_n_u16(vcltq_u16(A,B),15)
#define     QHI_CLTL(A, B)  VQHU_ASTI(vshrq_n_u16(vcltq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CLTL(A, B)  VQHU_ASTI(vshrq_n_u16(vcltq_f16(A,B),15))
#else
#   define  QHF_CLTL(A, B)                          \
vreinterpretq_s16_u16(                              \
    vshrq_n_u16(                                    \
        vcombine_u16(                               \
            vmovn_u32(                              \
                vcltq_f32(                          \
                    vcvt_f32_f16(vget_low_f16(A)),  \
                    vcvt_f32_f16(vget_low_f16(B))   \
                )                                   \
            ),                                      \
            vmovn_u32(                              \
                vcltq_f32(                          \
                    vcvt_f32_f16(vget_high_f16(A)), \
                    vcvt_f32_f16(vget_high_f16(B))  \
                )                                   \
            )                                       \
        ),                                          \
        15                                          \
    )                                               \
)
#endif

#define     QWU_CLTL(A, B)            vshrq_n_u32(vcltq_u32(A,B),31)
#define     QWI_CLTL(A, B)  VQWU_ASTI(vshrq_n_u32(vcltq_s32(A,B),31))
#define     QWF_CLTL(A, B)  VQWU_ASTI(vshrq_n_u32(vcltq_f32(A,B),31))

#define     QDU_CLTL(A, B)            vshrq_n_u64(vcltq_u64(A,B),63)
#define     QDI_CLTL(A, B)  VQDU_ASTI(vshrq_n_u64(vcltq_s64(A,B),63))
#define     QDF_CLTL(A, B)  VQDU_ASTI(vshrq_n_u64(vcltq_f64(A,B),63))


INLINE(Vwyu,VWYU_CLTL) (Vwyu a, Vwyu b)
{
    uint32_t p = VWWU_ASTV(VWYU_ASWU(a));
    uint32_t q = VWWU_ASTV(VWYU_ASWU(b));
    return  VWWU_ASYU(UINT_ASTV((~p&q)));
}

INLINE(Vwbu,VWBU_CLTL) (Vwbu a, Vwbu b)
{
#define     VWBU_CLTL(A, B) WBU_ASTV(WBU_CLTL(VWBU_ASTM(A),VWBU_ASTM(B)))
    return  VWBU_CLTL(a, b);
}

INLINE(Vwbi,VWBI_CLTL) (Vwbi a, Vwbi b)
{
#define     VWBI_CLTL(A, B) WBI_ASTV(WBI_CLTL(VWBI_ASTM(A),VWBI_ASTM(B)))
    return  VWBI_CLTL(a, b);
}

INLINE(Vwbc,VWBC_CLTL) (Vwbc a, Vwbc b)
{
#define     VWBC_CLTL(A, B) WBC_ASTV(WBC_CLTL(VWBC_ASTM(A),VWBC_ASTM(B)))
    return  VWBC_CLTL(a, b);
}


INLINE(Vwhu,VWHU_CLTL) (Vwhu a, Vwhu b)
{
#define     VWHU_CLTL(A, B) WHU_ASTV(WHU_CLTL(VWHU_ASTM(A),VWHU_ASTM(B)))
    return  VWHU_CLTL(a, b);
}

INLINE(Vwhi,VWHI_CLTL) (Vwhi a, Vwhi b)
{
#define     VWHI_CLTL(A, B) WHI_ASTV(WHI_CLTL(VWHI_ASTM(A),VWHI_ASTM(B)))
    return  VWHI_CLTL(a, b);
}

INLINE(Vwhi,VWHF_CLTL) (Vwhf a, Vwhf b)
{
#define     VWHF_CLTL(A, B) WHI_ASTV(WHF_CLTL(VWHF_ASTM(A),VWHF_ASTM(B)))
    return  VWHF_CLTL(a, b);
}


INLINE(Vwwu,VWWU_CLTL) (Vwwu a, Vwwu b)
{
#define     VWWU_CLTL(A, B) WWU_ASTV(WWU_CLTL(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_CLTL(a, b);
}

INLINE(Vwwi,VWWI_CLTL) (Vwwi a, Vwwi b)
{
#define     VWWI_CLTL(A, B) WWI_ASTV(WWI_CLTL(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_CLTL(a, b);
}

INLINE(Vwwi,VWWF_CLTL) (Vwwf a, Vwwf b)
{
#define     VWWF_CLTL(A, B) WWI_ASTV(WWF_CLTL(VWWF_ASTM(A),VWWF_ASTM(B)))
    return  VWWF_CLTL(a, b);
}


INLINE(Vdyu,VDYU_CLTL) (Vdyu a, Vdyu b)
{
    uint64x1_t p = VDYU_ASDU(a);
    uint64x1_t q = VDYU_ASDU(b);
    p = vbic_u64(q, p);
    return  VDDU_ASYU(p);
}


INLINE(Vdbu,VDBU_CLTL) (Vdbu a, Vdbu b) {return DBU_CLTL(a, b);}
INLINE(Vdbi,VDBI_CLTL) (Vdbi a, Vdbi b) {return DBI_CLTL(a, b);}
INLINE(Vdbc,VDBC_CLTL) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(
        DBC_CLTL(
            VDBC_ASTM(a),
            VDBC_ASTM(b)
        )
    );
}

INLINE(Vdhu,VDHU_CLTL) (Vdhu a, Vdhu b) {return DHU_CLTL(a, b);}
INLINE(Vdhi,VDHI_CLTL) (Vdhi a, Vdhi b) {return DHI_CLTL(a, b);}
INLINE(Vdhi,VDHF_CLTL) (Vdhf a, Vdhf b) {return DHF_CLTL(a, b);}

INLINE(Vdwu,VDWU_CLTL) (Vdwu a, Vdwu b) {return DWU_CLTL(a, b);}
INLINE(Vdwi,VDWI_CLTL) (Vdwi a, Vdwi b) {return DWI_CLTL(a, b);}
INLINE(Vdwi,VDWF_CLTL) (Vdwf a, Vdwf b) {return DWF_CLTL(a, b);}

INLINE(Vddu,VDDU_CLTL) (Vddu a, Vddu b) {return DDU_CLTL(a, b);}
INLINE(Vddi,VDDI_CLTL) (Vddi a, Vddi b) {return DDI_CLTL(a, b);}
INLINE(Vddi,VDDF_CLTL) (Vddf a, Vddf b) {return DDF_CLTL(a, b);}


INLINE(Vqyu,VQYU_CLTL) (Vqyu a, Vqyu b)
{
    uint64x2_t p = VQYU_ASDU(a);
    uint64x2_t q = VQYU_ASDU(b);
    p = vbicq_u64(q, p);
    return  VQDU_ASYU(p);
}

INLINE(Vqbu,VQBU_CLTL) (Vqbu a, Vqbu b) {return QBU_CLTL(a,b);}
INLINE(Vqbi,VQBI_CLTL) (Vqbi a, Vqbi b) {return QBI_CLTL(a,b);}
INLINE(Vqbc,VQBC_CLTL) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CLTL(VQBC_ASTM(a),VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CLTL) (Vqhu a, Vqhu b) {return QHU_CLTL(a,b);}
INLINE(Vqhi,VQHI_CLTL) (Vqhi a, Vqhi b) {return QHI_CLTL(a,b);}
INLINE(Vqhi,VQHF_CLTL) (Vqhf a, Vqhf b) {return QHF_CLTL(a,b);}
INLINE(Vqwu,VQWU_CLTL) (Vqwu a, Vqwu b) {return QWU_CLTL(a,b);}
INLINE(Vqwi,VQWI_CLTL) (Vqwi a, Vqwi b) {return QWI_CLTL(a,b);}
INLINE(Vqwi,VQWF_CLTL) (Vqwf a, Vqwf b) {return QWF_CLTL(a,b);}
INLINE(Vqdu,VQDU_CLTL) (Vqdu a, Vqdu b) {return QDU_CLTL(a,b);}
INLINE(Vqdi,VQDI_CLTL) (Vqdi a, Vqdi b) {return QDI_CLTL(a,b);}
INLINE(Vqdi,VQDF_CLTL) (Vqdf a, Vqdf b) {return QDF_CLTL(a,b);}

#if 0 // _LEAVE_ARM_CLTL
}
#endif


#if 0 // _ENTER_ARM_CLTY
{
#endif

INLINE(_Bool,FLT16_CLTY) (flt16_t a, flt16_t b) {return a<b;}
INLINE(_Bool,  FLT_CLTY)   (float a,   float b) {return a<b;}
INLINE(_Bool,  DBL_CLTY)  (double a,  double b) {return a<b;}

INLINE(_Bool,WBU_CLTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.B.U = vclt_u8(x.B.U, y.B.U);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WBI_CLTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.B.U = vclt_s8(x.B.I, y.B.I);
    return  -1==vget_lane_s32(x.W.I, 0);
}
#if CHAR_MIN
#   define WBC_CLTY WBI_CLTY
#else
#   define WBC_CLTY WBU_CLTY
#endif

INLINE(_Bool,WHU_CLTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.U = vclt_u16(x.H.U, y.H.U);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WHI_CLTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.U = vclt_s16(x.H.I, y.H.I);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WHF_CLTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.U = VDHF_CLTS(x.H.F, y.H.F);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WWU_CLTY) (float a, float b) 
{
    WORD_TYPE p={.F=a}, q={.F=b};
    return  p.U < q.U;
}

INLINE(_Bool,WWI_CLTY) (float a, float b) 
{
    WORD_TYPE p={.F=a}, q={.F=b};
    return  p.I < q.I;
}


INLINE(_Bool,VWYU_CLTY) (Vwyu a, Vwyu b)
{
    WORD_TYPE p={.F=a.V0}, q={b.V0};
    return  (!p.U) && (-1==q.I);
}

INLINE(_Bool,VWBU_CLTY) (Vwbu a, Vwbu b) {return WBU_CLTY(a.V0, b.V0);}         
INLINE(_Bool,VWBI_CLTY) (Vwbi a, Vwbi b) {return WBI_CLTY(a.V0, b.V0);}
INLINE(_Bool,VWBC_CLTY) (Vwbc a, Vwbc b) {return WBC_CLTY(a.V0, b.V0);}
INLINE(_Bool,VWHU_CLTY) (Vwhu a, Vwhu b) {return WHU_CLTY(a.V0, b.V0);}
INLINE(_Bool,VWHI_CLTY) (Vwhi a, Vwhi b) {return WHI_CLTY(a.V0, b.V0);}         
INLINE(_Bool,VWHF_CLTY) (Vwhf a, Vwhf b) {return WHF_CLTY(a.V0, b.V0);}
INLINE(_Bool,VWWU_CLTY) (Vwwu a, Vwwu b) {return WWU_CLTY(a.V0, b.V0);}         
INLINE(_Bool,VWWI_CLTY) (Vwwi a, Vwwi b) {return WWI_CLTY(a.V0, b.V0);}         
INLINE(_Bool,VWWF_CLTY) (Vwwf a, Vwwf b) {return a.V0<b.V0;}

INLINE(_Bool,VDYU_CLTY) (Vdyu a, Vdyu b)
{
    DWRD_VTYPE x={.D.U=a.V0}, y={.D.U=b.V0};
    return (!x.U) && (-1==y.I);
}

INLINE(_Bool,VDBU_CLTY) (Vdbu a, Vdbu b)
{
    DWRD_VTYPE x={.B.U=a}, y={.B.U=b};
    x.B.U = vclt_u8(x.B.U, y.B.U);
    return  -1==x.I;
}

INLINE(_Bool,VDBI_CLTY) (Vdbi a, Vdbi b)
{
    DWRD_VTYPE x={.B.I=a}, y={.B.I=b};
    x.B.U = vclt_s8(x.B.I, y.B.I);
    return  -1==x.I;
}

INLINE(_Bool,VDBC_CLTY) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
    return  VDBI_CLTY(a.V0, b.V0);
#else
    return  VDBU_CLTY(a.V0, b.V0);
#endif
}

INLINE(_Bool,VDHU_CLTY) (Vdhu a, Vdhu b)
{
    DWRD_VTYPE x={.H.U=a}, y={.H.U=b};
    x.H.U = vclt_u16(x.H.U, y.H.U);
    return  -1==x.I;
}

INLINE(_Bool,VDHI_CLTY) (Vdhi a, Vdhi b)
{
    DWRD_VTYPE x={.H.I=a}, y={.H.I=b};
    x.H.U = vclt_s16(x.H.I, y.H.I);
    return  -1==x.I;
}

INLINE(_Bool,VDHF_CLTY) (Vdhf a, Vdhf b)
{
    DWRD_VTYPE x={.H.F=a}, y={.H.F=b};
    x.H.I = VDHF_CLTS(a, b);
    return  -1==x.I;
}

INLINE(_Bool,VDWU_CLTY) (Vdwu a, Vdwu b)
{
    DWRD_VTYPE x={.W.U=a}, y={.W.U=b};
    x.W.U = vclt_u32(x.W.U, y.W.U);
    return  -1==x.I;
}

INLINE(_Bool,VDWI_CLTY) (Vdwi a, Vdwi b)
{
    DWRD_VTYPE x={.W.I=a}, y={.W.I=b};
    x.W.U = vclt_u32(x.W.I, y.W.I);
    return  -1==x.I;
}

INLINE(_Bool,VDWF_CLTY) (Vdwf a, Vdwf b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.W.U = vclt_f32(x.W.F, y.W.F);
    return  -1==x.I;
}

INLINE(_Bool,VDDU_CLTY) (Vddu a, Vddu b)
{
    uint64x1_t  c = vclt_u64(a, b);
    return  vget_lane_u64(c, 0);
}

INLINE(_Bool,VDDI_CLTY) (Vddi a, Vddi b)
{
    uint64x1_t  c = vclt_s64(a, b);
    return  vget_lane_u64(c, 0);
}

INLINE(_Bool,VDDF_CLTY) (Vddf a, Vddf b)
{
    uint64x1_t  c = vclt_f64(a, b);
    return  vget_lane_u64(c, 0);
}


INLINE(_Bool,VQYU_CLTY) (Vqyu a, Vqyu b)
{
    uint64_t x0 = vgetq_lane_u64(a.V0, 0);
    uint64_t x1 = vgetq_lane_u64(a.V0, 1);

    uint64_t x2 = vgetq_lane_u64(b.V0, 0);
    uint64_t x3 = vgetq_lane_u64(b.V0, 1);
    return  (!(x0|x1)) && (UINT64_MAX==(x2&x3));
}

#define     MY_CLTYZ(C) \
(UINT64_MAX==vget_lane_u64(vand_u64(vget_low_u64(C),vget_high_u64(C)),0))

INLINE(_Bool,VQBU_CLTY) (Vqbu a, Vqbu b)
{
    uint8x16_t  u = vcltq_u8(a, b);
    uint64x2_t  c = vreinterpretq_u64_u8(u);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQBI_CLTY) (Vqbi a, Vqbi b)
{
    uint8x16_t  u = vcltq_s8(a, b);
    uint64x2_t  c = vreinterpretq_u64_u8(u);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQBC_CLTY) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
    return  VQBI_CLTY(a.V0, b.V0);
#else
    return  VQBU_CLTY(a.V0, b.V0);
#endif
}

INLINE(_Bool,VQHU_CLTY) (Vqhu a, Vqhu b)
{
    uint16x8_t  u = vcltq_u16(a, b);
    uint64x2_t  c = vreinterpretq_u64_u16(u);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQHI_CLTY) (Vqhi a, Vqhi b)
{
    uint16x8_t  u = vcltq_s16(a, b);
    uint64x2_t  c = vreinterpretq_u64_u16(u);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQHF_CLTY) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x8_t  u = vcltq_f16(a, b);
#else
    float32x4_t p, q;
    uint32x4_t  m;
    uint16x4_t  l, r;
    p = vcvt_f32_f16(vget_low_f16(a));
    q = vcvt_f32_f16(vget_low_f16(b));
    m = vcltq_f32(p, q);
    l = vmovn_u32(m);
    p = vcvt_f32_f16(vget_high_f16(a));
    q = vcvt_f32_f16(vget_high_f16(b));
    m = vcltq_f32(p, q);
    r = vmovn_u32(m);
    uint16x8_t u = vcombine_u16(l, r);
#endif
    uint64x2_t  c = vreinterpretq_u64_u16(u);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQWU_CLTY) (Vqwu a, Vqwu b)
{
    uint32x4_t  u = vcltq_u32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQWI_CLTY) (Vqwi a, Vqwi b)
{
    uint32x4_t  u = vcltq_s32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQWF_CLTY) (Vqwf a, Vqwf b)
{
    uint32x4_t  u = vcltq_f32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQDU_CLTY) (Vqdu a, Vqdu b)
{
    uint64x2_t  c = vcltq_u64(a, b);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQDI_CLTY) (Vqdi a, Vqdi b)
{
    uint64x2_t  c = vcltq_s64(a, b);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQDF_CLTY) (Vqdf a, Vqdf b)
{
    uint64x2_t  c = vcltq_f64(a, b);
    return  MY_CLTYZ(c);
}

INLINE(_Bool,VQQU_CLTY) (Vqqu a, Vqqu b)
{
    return  
    (vgetq_lane_u64(a.V0,1)==vgetq_lane_u64(b.V0,1))
    ?   (vgetq_lane_u64(a.V0,0)<vgetq_lane_u64(b.V0,0))
    :   (vgetq_lane_u64(a.V0,1)<vgetq_lane_u64(b.V0,1));
}

INLINE(_Bool,VQQI_CLTY) (Vqqi a, Vqqi b)
{
    return  
    (vgetq_lane_s64(a.V0,1)==vgetq_lane_s64(b.V0,1))
    ?   (vgetq_lane_s64(a.V0,0)<vgetq_lane_s64(b.V0,0))
    :   (vgetq_lane_s64(a.V0,1)<vgetq_lane_s64(b.V0,1));
}

INLINE(_Bool,VQQF_CLTY) (Vqqf a, Vqqf b) {return a.V0 < b.V0;}

#if 0 // _LEAVE_ARM_CLTY
}
#endif

#if 0 // _ENTER_ARM_CLES
{
#endif

INLINE(int16_t,  FLT16_CLES) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcleh_f16(a, b);
#else
    return  0-(a<=b);
#endif
}

INLINE(int32_t,    FLT_CLES)   (float a,   float b) {return  vcles_f32(a, b);}
INLINE(int64_t,    DBL_CLES)  (double a,  double b) {return  vcled_f64(a, b);}

INLINE(QUAD_ITYPE,clesqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (QUAD_ITYPE) 0-(a<=b);
}


#define     DBU_CLES                             vcle_u8
#define     DBI_CLES(A, B)  vreinterpret_s16_u16(vcle_s8(A,B))
#define     DBC_CLES        VDBC_BASE(CLES)
#define     DHU_CLES                             vcle_u16
#define     DHI_CLES(A, B)  vreinterpret_s16_u16(vcle_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CLES(A, B)  vreinterpret_s16_u16(vcle_f16(A,B))
#else
INLINE(float16x4_t,DHF_CLES) (float16x4_t a, float16x4_t b)
{
#   define  DHF_CLES    DHF_CLES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  DHF_VOID;
}
#endif
#define     DWU_CLES                             vcle_u32
#define     DWI_CLES(A, B)  vreinterpret_s32_u32(vcle_s32(A,B))
#define     DWF_CLES(A, B)  vreinterpret_s32_u32(vcle_f32(A,B))
#define     DDU_CLES                             vcle_u64
#define     DDI_CLES(A, B)  vreinterpret_s64_u64(vcle_s32(A,B))
#define     DDF_CLES(A, B)  vreinterpret_s64_u64(vcle_f32(A,B))


#define     QBU_CLES                              vcleq_u8
#define     QBI_CLES(A, B)  vreinterpretq_s16_u16(vcleq_s8(A,B))
#define     QBC_CLES        VQBC_BASE(CLES)
#define     QHU_CLES                              vcleq_u16
#define     QHI_CLES(A, B)  vreinterpretq_s16_u16(vcleq_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CLES(A, B)  vreinterpretq_s16_u16(vcleq_f16(A,B))
#else
INLINE(float16x8_t,QHF_CLES) (float16x8_t a, float16x8_t b)
{
#   define  QHF_CLES    QHF_CLES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  QHF_VOID;
}
#endif

#define     QWU_CLES                              vcleq_u32
#define     QWI_CLES(A, B)  vreinterpretq_s32_u32(vcleq_s32(A,B))
#define     QWF_CLES(A, B)  vreinterpretq_s32_u32(vcleq_f32(A,B))
#define     QDU_CLES                              vcleq_u64
#define     QDI_CLES(A, B)  vreinterpretq_s64_u64(vcleq_s32(A,B))
#define     QDF_CLES(A, B)  vreinterpretq_s64_u64(vcleq_f32(A,B))


INLINE(Vwbu,VWBU_CLES) (Vwbu a, Vwbu b)
{
    float32x2_t l = vset_lane_f32(VWBU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWBU_ASTM(a), r, 0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CLES(p, q)),
            0
        )
    );
}

INLINE(Vwbi,VWBI_CLES) (Vwbi a, Vwbi b)
{
    float32x2_t l = vset_lane_f32(VWBI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWBI_ASTM(a), r, 0);
    int8x8_t    p = vreinterpret_s8_f32(l);
    int8x8_t    q = vreinterpret_s8_f32(r);
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(vcle_s8(p, q)),
            0
        )
    );
}

INLINE(Vwbc,VWBC_CLES) (Vwbc a, Vwbc b)
{
    float32x2_t x = vset_lane_f32(VWBC_ASTM(a), x, 0);
    float32x2_t y = vset_lane_f32(VWBC_ASTM(a), y, 0);
    float32x2_t r;
#if CHAR_MIN
    int8x8_t    p = vreinterpret_s8_f32(x);
    int8x8_t    q = vreinterpret_s8_f32(y);
    r = vreinterpret_f32_u8(vcle_s8(p, q));
#else
    uint8x8_t   p = vreinterpret_u8_f32(x);
    uint8x8_t   q = vreinterpret_u8_f32(y);
    r = vreinterpret_f32_u8(vcle_u8(p, q));
#endif
    return  WBC_ASTV(vget_lane_f32(r, 0));
}


INLINE(Vwhu,VWHU_CLES) (Vwhu a, Vwhu b)
{
    float32x2_t l = vset_lane_f32(VWHU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHU_ASTM(a), r, 0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcle_u16(p, q)),
            0
        )
    );
}

INLINE(Vwhi,VWHI_CLES) (Vwhi a, Vwhi b)
{
    float32x2_t l = vset_lane_f32(VWHI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHI_ASTM(a), r, 0);
    int16x4_t   p = vreinterpret_s16_f32(l);
    int16x4_t   q = vreinterpret_s16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcle_s16(p, q)),
            0
        )
    );
}

#if defined(SPC_ARM_FP16_SIMD)
INLINE(Vwhi,VWHF_CLES) (Vwhf a, Vwhf b)
{
#   define  VWHF_CLES VWHF_CLES
    float32x2_t l = vset_lane_f32(VWHF_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHF_ASTM(a), r, 0);
    float16x4_t p = vreinterpret_f16_f32(l);
    float16x4_t q = vreinterpret_f16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcle_f16(p, q)),
            0
        )
    );
}
#else
INLINE(Vwhf,VWHF_CLES) (Vwhf a, Vwhf b)
{
#   define  VWHF_CLES    VWHF_CLES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  VWHF_VOID;
}
#endif


INLINE(Vwwu,VWWU_CLES) (Vwwu a, Vwwu b)
{
    float32x2_t l = vset_lane_f32(VWWU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWWU_ASTM(a), r, 0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcle_u32(p, q)),
            0
        )
    );
}

INLINE(Vwwi,VWWI_CLES) (Vwwi a, Vwwi b)
{
    float32x2_t l = vset_lane_f32(VWWI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWWI_ASTM(a), r, 0);
    int32x2_t   p = vreinterpret_s32_f32(l);
    int32x2_t   q = vreinterpret_s32_f32(r);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcle_s32(p, q)),
            0
        )
    );
}

INLINE(Vwwi,VWWF_CLES) (Vwwf a, Vwwf b)
{
    float32x2_t p = vset_lane_f32(VWWF_ASTM(a), p, 0);
    float32x2_t q = vset_lane_f32(VWWF_ASTM(a), q, 0);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcle_f32(p, q)),
            0
        )
    );
}



INLINE(Vdbu,VDBU_CLES) (Vdbu a, Vdbu b) {return DBU_CLES(a, b);}
INLINE(Vdbi,VDBI_CLES) (Vdbi a, Vdbi b) {return DBI_CLES(a, b);}
INLINE(Vdbc,VDBC_CLES) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(DBC_CLES(VDBC_ASTM(b), VDBC_ASTM(b)));
}

INLINE(Vdhu,VDHU_CLES) (Vdhu a, Vdhu b) {return DHU_CLES(a, b);}
INLINE(Vdhi,VDHI_CLES) (Vdhi a, Vdhi b) {return DHI_CLES(a, b);}
INLINE(Vdhi,VDHF_CLES) (Vdhf a, Vdhf b) {return DHF_CLES(a, b);}
INLINE(Vdwu,VDWU_CLES) (Vdwu a, Vdwu b) {return DWU_CLES(a, b);}
INLINE(Vdwi,VDWI_CLES) (Vdwi a, Vdwi b) {return DWI_CLES(a, b);}
INLINE(Vdwi,VDWF_CLES) (Vdwf a, Vdwf b) {return DWF_CLES(a, b);}
INLINE(Vddu,VDDU_CLES) (Vddu a, Vddu b) {return DDU_CLES(a, b);}
INLINE(Vddi,VDDI_CLES) (Vddi a, Vddi b) {return DDI_CLES(a, b);}
INLINE(Vddi,VDDF_CLES) (Vddf a, Vddf b) {return DDF_CLES(a, b);}


INLINE(Vqbu,VQBU_CLES) (Vqbu a, Vqbu b) {return QBU_CLES(a, b);}
INLINE(Vqbi,VQBI_CLES) (Vqbi a, Vqbi b) {return QBI_CLES(a, b);}
INLINE(Vqbc,VQBC_CLES) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CLES(VQBC_ASTM(b), VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CLES) (Vqhu a, Vqhu b) {return QHU_CLES(a, b);}
INLINE(Vqhi,VQHI_CLES) (Vqhi a, Vqhi b) {return QHI_CLES(a, b);}
INLINE(Vqhi,VQHF_CLES) (Vqhf a, Vqhf b) {return QHF_CLES(a, b);}
INLINE(Vqwu,VQWU_CLES) (Vqwu a, Vqwu b) {return QWU_CLES(a, b);}
INLINE(Vqwi,VQWI_CLES) (Vqwi a, Vqwi b) {return QWI_CLES(a, b);}
INLINE(Vqwi,VQWF_CLES) (Vqwf a, Vqwf b) {return QWF_CLES(a, b);}
INLINE(Vqdu,VQDU_CLES) (Vqdu a, Vqdu b) {return QDU_CLES(a, b);}
INLINE(Vqdi,VQDI_CLES) (Vqdi a, Vqdi b) {return QDI_CLES(a, b);}
INLINE(Vqdi,VQDF_CLES) (Vqdf a, Vqdf b) {return QDF_CLES(a, b);}

#if 0 // _LEAVE_ARM_CLES
}
#endif

#if 0 // _ENTER_ARM_CLEL
{
#endif

INLINE(_Bool, FLT16_CLEL) (flt16_t a, flt16_t b) {return a<=b;}
INLINE(_Bool,   FLT_CLEL)   (float a,   float b) {return a<=b;}
INLINE(_Bool,   DBL_CLEL)  (double a,  double b) {return a<=b;}

INLINE(QUAD_ITYPE,clelqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (a<=b);
}

INLINE( Wbu,WBU_CLEL) (Wbu a, Wbu b)
{
#define     WBU_CLEL    WBU_CLEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    uint8x8_t   r = vcle_u8(
        vreinterpret_u8_f32(p),
        vreinterpret_u8_f32(q)
    );
    return  vget_lane_f32(
        vreinterpret_f32_u8(vshr_n_u8(r, 7)),
        0
    );
}

INLINE( Wbu,WBI_CLEL) (Wbi a, Wbi b)
{
#define     WBI_CLEL    WBI_CLEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vshr_n_u8(
                vcle_s8(
                    vreinterpret_s8_f32(p),
                    vreinterpret_s8_f32(q)
                ),
                7
            )
        ),
        0
    );
}

#if CHAR_MIN
#   define  WBC_CLEL    WBI_CLEL
#else
#   define  WBC_CLEL    WBU_CLEL
#endif

INLINE( Whu,WHU_CLEL) (Whu a, Whu b)
{
#define     WHU_CLEL    WHU_CLEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcle_u16(
                    vreinterpret_u16_f32(p),
                    vreinterpret_u16_f32(q)
                ),
                15
            )
        ),
        0
    );
}

INLINE( Whu,WHI_CLEL) (Whi a, Whi b)
{
#define     WHI_CLEL    WHI_CLEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcle_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        0
    );
}

INLINE( Whu,WHF_CLEL) (Whf a, Whf b)
{
#define     WHF_CLEL    WHF_CLEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcle_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        0
    );
#else
    float32x4_t l = vcvt_f32_f16(vreinterpret_f16_f32(p));
    float32x4_t r = vcvt_f32_f16(vreinterpret_f16_f32(q));
    uint16x4_t  v = vshr_n_u16(vmovn_u32(vcleq_f32(l, r)), 15);
    return  vget_lane_f32(vreinterpret_f32_u16(v), 0);
#endif
}

INLINE( Wwu,WWU_CLEL) (Wwu a, Wwu b)
{
#define     WWU_CLEL    WWU_CLEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcle_u32(
                    vreinterpret_u32_f32(p),
                    vreinterpret_u32_f32(q)
                ),
                31
            )
        ),
        0
    );
}

INLINE( Wwu,WWI_CLEL) (Wwi a, Wwi b)
{
#define     WWI_CLEL    WWI_CLEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcle_s32(
                    vreinterpret_s32_f32(p),
                    vreinterpret_s32_f32(q)
                ),
                31
            )
        ),
        0
    );
}

INLINE( Wwu,WWF_CLEL) (Wwf a, Wwf b)
{
#define     WWF_CLEL    WWF_CLEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(vshr_n_u32(vcle_f32(p, q), 31)),
        0
    );
}

#define     DBU_CLEL(A, B)            vshr_n_u8(vcle_u8(A,B),7)
#define     DBI_CLEL(A, B)  VDBU_ASTI(vshr_n_u8(vcle_s8(A,B),7))
#if CHAR_MIN
#   define  DBC_CLEL        DBI_CLEL
#else
#   define  DBC_CLEL        DBU_CLEL
#endif

#define     DHU_CLEL(A, B)            vshr_n_u16(vcle_u16(A,B),15)
#define     DHI_CLEL(A, B)  VDHU_ASTI(vshr_n_u16(vcle_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CLEL(A, B)  VDHU_ASTI(vshr_n_u16(vcle_f16(A,B),15))
#else
#   define  DHF_CLEL(A, B)      \
VDHU_ASHI(                      \
    vshr_n_u16(                 \
        vmovn_u32(              \
            vcleq_f32(          \
                vcvt_f32_f16(a),\
                vcvt_f32_f16(b) \
            )                   \
        ),                      \
        15                      \
    )                           \
)
#endif

#define     DWU_CLEL(A, B)            vshr_n_u32(vcle_u32(A,B),31)
#define     DWI_CLEL(A, B)  VDWU_ASTI(vshr_n_u32(vcle_s32(A,B),31))
#define     DWF_CLEL(A, B)  VDWU_ASTI(vshr_n_u32(vcle_f32(A,B),31))

#define     DDU_CLEL(A, B)            vshr_n_u64(vcle_u64(A,B),63)
#define     DDI_CLEL(A, B)  VDDU_ASTI(vshr_n_u64(vcle_s64(A,B),63))
#define     DDF_CLEL(A, B)  VDDU_ASTI(vshr_n_u64(vcle_f64(A,B),63))


#define     QBU_CLEL(A, B)            vshrq_n_u8(vcleq_u8(A,B),7)
#define     QBI_CLEL(A, B)  VQBU_ASTI(vshrq_n_u8(vcleq_s8(A,B),7))
#if CHAR_MIN
#   define  QBC_CLEL        QBI_CLEL
#else
#   define  QBC_CLEL        QBU_CLEL
#endif

#define     QHU_CLEL(A, B)            vshrq_n_u16(vcleq_u16(A,B),15)
#define     QHI_CLEL(A, B)  VQHU_ASTI(vshrq_n_u16(vcleq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CLEL(A, B)  VQHU_ASTI(vshrq_n_u16(vcleq_f16(A,B),15))
#else
#   define  QHF_CLEL(A, B)                          \
vreinterpretq_s16_u16(                              \
    vshrq_n_u16(                                    \
        vcombine_u16(                               \
            vmovn_u32(                              \
                vcleq_f32(                          \
                    vcvt_f32_f16(vget_low_f16(A)),  \
                    vcvt_f32_f16(vget_low_f16(B))   \
                )                                   \
            ),                                      \
            vmovn_u32(                              \
                vcleq_f32(                          \
                    vcvt_f32_f16(vget_high_f16(A)), \
                    vcvt_f32_f16(vget_high_f16(B))  \
                )                                   \
            )                                       \
        ),                                          \
        15                                          \
    )                                               \
)
#endif

#define     QWU_CLEL(A, B)            vshrq_n_u32(vcleq_u32(A,B),31)
#define     QWI_CLEL(A, B)  VQWU_ASTI(vshrq_n_u32(vcleq_s32(A,B),31))
#define     QWF_CLEL(A, B)  VQWU_ASTI(vshrq_n_u32(vcleq_f32(A,B),31))

#define     QDU_CLEL(A, B)            vshrq_n_u64(vcleq_u64(A,B),63)
#define     QDI_CLEL(A, B)  VQDU_ASTI(vshrq_n_u64(vcleq_s64(A,B),63))
#define     QDF_CLEL(A, B)  VQDU_ASTI(vshrq_n_u64(vcleq_f64(A,B),63))


INLINE(Vwbu,VWBU_CLEL) (Vwbu a, Vwbu b)
{
#define     VWBU_CLEL(A, B) WBU_ASTV(WBU_CLEL(VWBU_ASTM(A),VWBU_ASTM(B)))
    return  VWBU_CLEL(a, b);
}

INLINE(Vwbi,VWBI_CLEL) (Vwbi a, Vwbi b)
{
#define     VWBI_CLEL(A, B) WBI_ASTV(WBI_CLEL(VWBI_ASTM(A),VWBI_ASTM(B)))
    return  VWBI_CLEL(a, b);
}

INLINE(Vwbc,VWBC_CLEL) (Vwbc a, Vwbc b)
{
#define     VWBC_CLEL(A, B) WBC_ASTV(WBC_CLEL(VWBC_ASTM(A),VWBC_ASTM(B)))
    return  VWBC_CLEL(a, b);
}


INLINE(Vwhu,VWHU_CLEL) (Vwhu a, Vwhu b)
{
#define     VWHU_CLEL(A, B) WHU_ASTV(WHU_CLEL(VWHU_ASTM(A),VWHU_ASTM(B)))
    return  VWHU_CLEL(a, b);
}

INLINE(Vwhi,VWHI_CLEL) (Vwhi a, Vwhi b)
{
#define     VWHI_CLEL(A, B) WHI_ASTV(WHI_CLEL(VWHI_ASTM(A),VWHI_ASTM(B)))
    return  VWHI_CLEL(a, b);
}

INLINE(Vwhi,VWHF_CLEL) (Vwhf a, Vwhf b)
{
#define     VWHF_CLEL(A, B) WHI_ASTV(WHF_CLEL(VWHF_ASTM(A),VWHF_ASTM(B)))
    return  VWHF_CLEL(a, b);
}


INLINE(Vwwu,VWWU_CLEL) (Vwwu a, Vwwu b)
{
#define     VWWU_CLEL(A, B) WWU_ASTV(WWU_CLEL(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_CLEL(a, b);
}

INLINE(Vwwi,VWWI_CLEL) (Vwwi a, Vwwi b)
{
#define     VWWI_CLEL(A, B) WWI_ASTV(WWI_CLEL(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_CLEL(a, b);
}

INLINE(Vwwi,VWWF_CLEL) (Vwwf a, Vwwf b)
{
#define     VWWF_CLEL(A, B) WWI_ASTV(WWF_CLEL(VWWF_ASTM(A),VWWF_ASTM(B)))
    return  VWWF_CLEL(a, b);
}


INLINE(Vdbu,VDBU_CLEL) (Vdbu a, Vdbu b) {return DBU_CLEL(a, b);}
INLINE(Vdbi,VDBI_CLEL) (Vdbi a, Vdbi b) {return DBI_CLEL(a, b);}
INLINE(Vdbc,VDBC_CLEL) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(
        DBC_CLEL(
            VDBC_ASTM(a),
            VDBC_ASTM(b)
        )
    );
}

INLINE(Vdhu,VDHU_CLEL) (Vdhu a, Vdhu b) {return DHU_CLEL(a, b);}
INLINE(Vdhi,VDHI_CLEL) (Vdhi a, Vdhi b) {return DHI_CLEL(a, b);}
INLINE(Vdhi,VDHF_CLEL) (Vdhf a, Vdhf b) {return DHF_CLEL(a, b);}

INLINE(Vdwu,VDWU_CLEL) (Vdwu a, Vdwu b) {return DWU_CLEL(a, b);}
INLINE(Vdwi,VDWI_CLEL) (Vdwi a, Vdwi b) {return DWI_CLEL(a, b);}
INLINE(Vdwi,VDWF_CLEL) (Vdwf a, Vdwf b) {return DWF_CLEL(a, b);}

INLINE(Vddu,VDDU_CLEL) (Vddu a, Vddu b) {return DDU_CLEL(a, b);}
INLINE(Vddi,VDDI_CLEL) (Vddi a, Vddi b) {return DDI_CLEL(a, b);}
INLINE(Vddi,VDDF_CLEL) (Vddf a, Vddf b) {return DDF_CLEL(a, b);}


INLINE(Vqbu,VQBU_CLEL) (Vqbu a, Vqbu b) {return QBU_CLEL(a,b);}
INLINE(Vqbi,VQBI_CLEL) (Vqbi a, Vqbi b) {return QBI_CLEL(a,b);}
INLINE(Vqbc,VQBC_CLEL) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CLEL(VQBC_ASTM(a),VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CLEL) (Vqhu a, Vqhu b) {return QHU_CLEL(a,b);}
INLINE(Vqhi,VQHI_CLEL) (Vqhi a, Vqhi b) {return QHI_CLEL(a,b);}
INLINE(Vqhi,VQHF_CLEL) (Vqhf a, Vqhf b) {return QHF_CLEL(a,b);}
INLINE(Vqwu,VQWU_CLEL) (Vqwu a, Vqwu b) {return QWU_CLEL(a,b);}
INLINE(Vqwi,VQWI_CLEL) (Vqwi a, Vqwi b) {return QWI_CLEL(a,b);}
INLINE(Vqwi,VQWF_CLEL) (Vqwf a, Vqwf b) {return QWF_CLEL(a,b);}
INLINE(Vqdu,VQDU_CLEL) (Vqdu a, Vqdu b) {return QDU_CLEL(a,b);}
INLINE(Vqdi,VQDI_CLEL) (Vqdi a, Vqdi b) {return QDI_CLEL(a,b);}
INLINE(Vqdi,VQDF_CLEL) (Vqdf a, Vqdf b) {return QDF_CLEL(a,b);}

#if 0 // _LEAVE_ARM_CLEL
}
#endif


#if 0 // _ENTER_ARM_CLEY
{
#endif

INLINE(_Bool,FLT16_CLEY) (flt16_t a, flt16_t b) {return a<=b;}
INLINE(_Bool,  FLT_CLEY)   (float a,   float b) {return a<=b;}
INLINE(_Bool,  DBL_CLEY)  (double a,  double b) {return a<=b;}

INLINE(_Bool,WBU_CLEY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.B.U = vcle_u8(x.B.U, y.B.U);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WBI_CLEY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.B.U = vcle_s8(x.B.I, y.B.I);
    return  -1==vget_lane_s32(x.W.I, 0);
}

#if CHAR_MIN
#   define  WBC_CLEY  WBI_CLEY
#else
#   define  WBC_CLEY  WBU_CLEY
#endif

INLINE(_Bool,WHU_CLEY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.U = vcle_u16(x.H.U, y.H.U);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WHI_CLEY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.U = vcle_s16(x.H.I, y.H.I);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WHF_CLEY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.I = VDHF_CLES(x.H.F, y.H.F);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WWU_CLEY) (float a, float b) 
{
    WORD_TYPE x={.F=a}, y={.F=b};
    return  x.U<=y.U;
}

INLINE(_Bool,WWI_CLEY) (float a, float b) 
{
    WORD_TYPE x={.F=a}, y={.F=b};
    return  x.I<=y.I;
}

INLINE(_Bool,VWBU_CLEY) (Vwbu a, Vwbu b) {return WBU_CLEY(a.V0, b.V0);}         
INLINE(_Bool,VWBI_CLEY) (Vwbi a, Vwbi b) {return WBI_CLEY(a.V0, b.V0);}
INLINE(_Bool,VWBC_CLEY) (Vwbc a, Vwbc b) {return WBC_CLEY(a.V0, b.V0);}
INLINE(_Bool,VWHU_CLEY) (Vwhu a, Vwhu b) {return WHU_CLEY(a.V0, b.V0);}
INLINE(_Bool,VWHI_CLEY) (Vwhi a, Vwhi b) {return WHI_CLEY(a.V0, b.V0);}         
INLINE(_Bool,VWHF_CLEY) (Vwhf a, Vwhf b) {return WHF_CLEY(a.V0, b.V0);}
INLINE(_Bool,VWWU_CLEY) (Vwwu a, Vwwu b) {return WWU_CLEY(a.V0, b.V0);}         
INLINE(_Bool,VWWI_CLEY) (Vwwi a, Vwwi b) {return WWI_CLEY(a.V0, b.V0);}         
INLINE(_Bool,VWWF_CLEY) (Vwwf a, Vwwf b) {return a.V0<=b.V0;}

INLINE(_Bool,VDBU_CLEY) (Vdbu a, Vdbu b)
{
    DWRD_VTYPE x={.B.U=a}, y={.B.U=b};
    x.B.U = vcle_u8(x.B.U, y.B.U);
    return  -1==x.I;
}

INLINE(_Bool,VDBI_CLEY) (Vdbi a, Vdbi b)
{
    DWRD_VTYPE x={.B.I=a}, y={.B.I=b};
    x.B.U = vcle_s8(x.B.I, y.B.I);
    return  -1==x.I;
}

INLINE(_Bool,VDBC_CLEY) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
    return  VDBI_CLEY(a.V0, b.V0);
#else
    return  VDBU_CLEY(a.V0, b.V0);
#endif
}

INLINE(_Bool,VDHU_CLEY) (Vdhu a, Vdhu b)
{
    DWRD_VTYPE x={.H.U=a}, y={.H.U=b};
    x.H.U = vcle_u16(x.H.U, y.H.U);
    return  -1==x.I;
}

INLINE(_Bool,VDHI_CLEY) (Vdhi a, Vdhi b)
{
    DWRD_VTYPE x={.H.I=a}, y={.H.I=b};
    x.H.U = vcle_s16(x.H.I, y.H.I);
    return  -1==x.I;
}

INLINE(_Bool,VDHF_CLEY) (Vdhf a, Vdhf b)
{
    DWRD_VTYPE x={.H.F=a}, y={.H.F=b};
    x.H.I = VDHF_CLES(a, b);
    return  -1==x.I;
}

INLINE(_Bool,VDWU_CLEY) (Vdwu a, Vdwu b)
{
    DWRD_VTYPE x={.W.U=a}, y={.W.U=b};
    x.W.U = vcle_u32(x.W.U, y.W.U);
    return  -1==x.I;
}

INLINE(_Bool,VDWI_CLEY) (Vdwi a, Vdwi b)
{
    DWRD_VTYPE x={.W.I=a}, y={.W.I=b};
    x.W.U = vcle_u32(x.W.I, y.W.I);
    return  -1==x.I;
}

INLINE(_Bool,VDWF_CLEY) (Vdwf a, Vdwf b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.W.U = vcle_f32(x.W.F, y.W.F);
    return  -1==x.I;
}

INLINE(_Bool,VDDU_CLEY) (Vddu a, Vddu b)
{
    uint64x1_t  c = vcle_u64(a, b);
    return  vget_lane_u64(c, 0);
}

INLINE(_Bool,VDDI_CLEY) (Vddi a, Vddi b)
{
    uint64x1_t  c = vcle_s64(a, b);
    return  vget_lane_u64(c, 0);
}

INLINE(_Bool,VDDF_CLEY) (Vddf a, Vddf b)
{
    uint64x1_t  c = vcle_f64(a, b);
    return  vget_lane_u64(c, 0);
}

#define     MY_CLEYZ(C) \
(UINT64_MAX==(vgetq_lane_u64(C,0)&vgetq_lane_u64(C,1)))

INLINE(_Bool,VQBU_CLEY) (Vqbu a, Vqbu b)
{
    uint8x16_t  u = vcleq_u8(a, b);
    uint64x2_t  c = vreinterpretq_u64_u8(u);
    return  MY_CLEYZ(c);
}

INLINE(_Bool,VQBI_CLEY) (Vqbi a, Vqbi b)
{
    uint8x16_t  u = vcleq_s8(a, b);
    uint64x2_t  c = vreinterpretq_u64_u8(u);
    return  MY_CLEYZ(c);
}

INLINE(_Bool,VQBC_CLEY) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
    return  VQBI_CLEY(a.V0, b.V0);
#else
    return  VQBU_CLEY(a.V0, b.V0);
#endif
}

INLINE(_Bool,VQHU_CLEY) (Vqhu a, Vqhu b)
{
    uint16x8_t  u = vcleq_u16(a, b);
    uint64x2_t  c = vreinterpretq_u64_u16(u);
    return  MY_CLEYZ(c);
}

INLINE(_Bool,VQHI_CLEY) (Vqhi a, Vqhi b)
{
    uint16x8_t  u = vcleq_s16(a, b);
    uint64x2_t  c = vreinterpretq_u64_u16(u);
    return  MY_CLEYZ(c);
}

INLINE(_Bool,VQHF_CLEY) (Vqhf a, Vqhf b)
{
    QUAD_VTYPE c = {.H.I=VQHF_CLES(a, b)};
    return  MY_CLEYZ(c.D.U);
}

INLINE(_Bool,VQWU_CLEY) (Vqwu a, Vqwu b)
{
    uint32x4_t  u = vcleq_u32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CLEYZ(c);
}

INLINE(_Bool,VQWI_CLEY) (Vqwi a, Vqwi b)
{
    uint32x4_t  u = vcleq_s32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CLEYZ(c);
}

INLINE(_Bool,VQWF_CLEY) (Vqwf a, Vqwf b)
{
    uint32x4_t  u = vcleq_f32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CLEYZ(c);
}

INLINE(_Bool,VQDU_CLEY) (Vqdu a, Vqdu b)
{
    uint64x2_t  c = vcleq_u64(a, b);
    return  MY_CLEYZ(c);
}

INLINE(_Bool,VQDI_CLEY) (Vqdi a, Vqdi b)
{
    uint64x2_t  c = vcleq_s64(a, b);
    return  MY_CLEYZ(c);
}

INLINE(_Bool,VQDF_CLEY) (Vqdf a, Vqdf b)
{
    uint64x2_t  c = vcleq_f64(a, b);
    return  MY_CLEYZ(c);
}

INLINE(_Bool,VQQU_CLEY) (Vqqu a, Vqqu b)
{
    return  
    (vgetq_lane_u64(a.V0,1)==vgetq_lane_u64(b.V0,1))
    ?   (vgetq_lane_u64(a.V0,0)<=vgetq_lane_u64(b.V0,0))
    :   (vgetq_lane_u64(a.V0,1)<=vgetq_lane_u64(b.V0,1));
}

INLINE(_Bool,VQQI_CLEY) (Vqqi a, Vqqi b)
{
    return  
    (vgetq_lane_s64(a.V0,1)==vgetq_lane_s64(b.V0,1))
    ?   (vgetq_lane_s64(a.V0,0)<=vgetq_lane_s64(b.V0,0))
    :   (vgetq_lane_s64(a.V0,1)<=vgetq_lane_s64(b.V0,1));
}
INLINE(_Bool,VQQF_CLEY) (Vqqf a, Vqqf b) {return a.V0 <= b.V0;}

#if 0 // _LEAVE_ARM_CLEY
}
#endif

#if 0 // _ENTER_ARM_CGTS
{
#endif

INLINE(int16_t,  FLT16_CGTS) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcgth_f16(a, b);
#else
    return  0-(a>b);
#endif
}

INLINE(int32_t,    FLT_CGTS)   (float a,   float b) {return  vcgts_f32(a, b);}
INLINE(int64_t,    DBL_CGTS)  (double a,  double b) {return  vcgtd_f64(a, b);}

INLINE(QUAD_ITYPE,cgtsqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (QUAD_ITYPE) 0-(a>b);
}


#define     DBU_CGTS                             vcgt_u8
#define     DBI_CGTS(A, B)  vreinterpret_s16_u16(vcgt_s8(A,B))
#define     DBC_CGTS        VDBC_BASE(CGTS)
#define     DHU_CGTS                             vcgt_u16
#define     DHI_CGTS(A, B)  vreinterpret_s16_u16(vcgt_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CGTS(A, B)  vreinterpret_s16_u16(vcgt_f16(A,B))
#else
INLINE(float16x4_t,DHF_CGTS) (float16x4_t a, float16x4_t b)
{
#   define  DHF_CGTS    DHF_CGTS
    MY_NOT_IMPLEMENTED(0, __func__);
    return  DHF_VOID;
}
#endif
#define     DWU_CGTS                             vcgt_u32
#define     DWI_CGTS(A, B)  vreinterpret_s32_u32(vcgt_s32(A,B))
#define     DWF_CGTS(A, B)  vreinterpret_s32_u32(vcgt_f32(A,B))
#define     DDU_CGTS                             vcgt_u64
#define     DDI_CGTS(A, B)  vreinterpret_s64_u64(vcgt_s32(A,B))
#define     DDF_CGTS(A, B)  vreinterpret_s64_u64(vcgt_f32(A,B))


#define     QBU_CGTS                              vcgtq_u8
#define     QBI_CGTS(A, B)  vreinterpretq_s16_u16(vcgtq_s8(A,B))
#define     QBC_CGTS        VQBC_BASE(CGTS)
#define     QHU_CGTS                              vcgtq_u16
#define     QHI_CGTS(A, B)  vreinterpretq_s16_u16(vcgtq_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CGTS(A, B)  vreinterpretq_s16_u16(vcgtq_f16(A,B))
#else
INLINE(float16x8_t,QHF_CGTS) (float16x8_t a, float16x8_t b)
{
#   define  QHF_CGTS    QHF_CGTS
    MY_NOT_IMPLEMENTED(0, __func__);
    return  QHF_VOID;
}
#endif

#define     QWU_CGTS                              vcgtq_u32
#define     QWI_CGTS(A, B)  vreinterpretq_s32_u32(vcgtq_s32(A,B))
#define     QWF_CGTS(A, B)  vreinterpretq_s32_u32(vcgtq_f32(A,B))
#define     QDU_CGTS                              vcgtq_u64
#define     QDI_CGTS(A, B)  vreinterpretq_s64_u64(vcgtq_s32(A,B))
#define     QDF_CGTS(A, B)  vreinterpretq_s64_u64(vcgtq_f32(A,B))


INLINE(Vwbu,VWBU_CGTS) (Vwbu a, Vwbu b)
{
    float32x2_t l = vset_lane_f32(VWBU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWBU_ASTM(a), r, 0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CGTS(p, q)),
            0
        )
    );
}

INLINE(Vwbi,VWBI_CGTS) (Vwbi a, Vwbi b)
{
    float32x2_t l = vset_lane_f32(VWBI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWBI_ASTM(a), r, 0);
    int8x8_t    p = vreinterpret_s8_f32(l);
    int8x8_t    q = vreinterpret_s8_f32(r);
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(vcgt_s8(p, q)),
            0
        )
    );
}

INLINE(Vwbc,VWBC_CGTS) (Vwbc a, Vwbc b)
{
    float32x2_t x = vset_lane_f32(VWBC_ASTM(a), x, 0);
    float32x2_t y = vset_lane_f32(VWBC_ASTM(a), y, 0);
    float32x2_t r;
#if CHAR_MIN
    int8x8_t    p = vreinterpret_s8_f32(x);
    int8x8_t    q = vreinterpret_s8_f32(y);
    r = vreinterpret_f32_u8(vcgt_s8(p, q));
#else
    uint8x8_t   p = vreinterpret_u8_f32(x);
    uint8x8_t   q = vreinterpret_u8_f32(y);
    r = vreinterpret_f32_u8(vcgt_u8(p, q));
#endif
    return  WBC_ASTV(vget_lane_f32(r, 0));
}


INLINE(Vwhu,VWHU_CGTS) (Vwhu a, Vwhu b)
{
    float32x2_t l = vset_lane_f32(VWHU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHU_ASTM(a), r, 0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcgt_u16(p, q)),
            0
        )
    );
}

INLINE(Vwhi,VWHI_CGTS) (Vwhi a, Vwhi b)
{
    float32x2_t l = vset_lane_f32(VWHI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHI_ASTM(a), r, 0);
    int16x4_t   p = vreinterpret_s16_f32(l);
    int16x4_t   q = vreinterpret_s16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcgt_s16(p, q)),
            0
        )
    );
}

#if defined(SPC_ARM_FP16_SIMD)
INLINE(Vwhi,VWHF_CGTS) (Vwhf a, Vwhf b)
{
#   define  VWHF_CGTS VWHF_CGTS
    float32x2_t l = vset_lane_f32(VWHF_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHF_ASTM(a), r, 0);
    float16x4_t p = vreinterpret_f16_f32(l);
    float16x4_t q = vreinterpret_f16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcgt_f16(p, q)),
            0
        )
    );
}
#else
INLINE(Vwhf,VWHF_CGTS) (Vwhf a, Vwhf b)
{
#   define  VWHF_CGTS    VWHF_CGTS
    MY_NOT_IMPLEMENTED(0, __func__);
    return  VWHF_VOID;
}
#endif


INLINE(Vwwu,VWWU_CGTS) (Vwwu a, Vwwu b)
{
    float32x2_t l = vset_lane_f32(VWWU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWWU_ASTM(a), r, 0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcgt_u32(p, q)),
            0
        )
    );
}

INLINE(Vwwi,VWWI_CGTS) (Vwwi a, Vwwi b)
{
    float32x2_t l = vset_lane_f32(VWWI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWWI_ASTM(a), r, 0);
    int32x2_t   p = vreinterpret_s32_f32(l);
    int32x2_t   q = vreinterpret_s32_f32(r);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcgt_s32(p, q)),
            0
        )
    );
}

INLINE(Vwwi,VWWF_CGTS) (Vwwf a, Vwwf b)
{
    float32x2_t p = vset_lane_f32(VWWF_ASTM(a), p, 0);
    float32x2_t q = vset_lane_f32(VWWF_ASTM(a), q, 0);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcgt_f32(p, q)),
            0
        )
    );
}


INLINE(Vdbu,VDBU_CGTS) (Vdbu a, Vdbu b) {return DBU_CGTS(a, b);}
INLINE(Vdbi,VDBI_CGTS) (Vdbi a, Vdbi b) {return DBI_CGTS(a, b);}
INLINE(Vdbc,VDBC_CGTS) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(DBC_CGTS(VDBC_ASTM(b), VDBC_ASTM(b)));
}

INLINE(Vdhu,VDHU_CGTS) (Vdhu a, Vdhu b) {return DHU_CGTS(a, b);}
INLINE(Vdhi,VDHI_CGTS) (Vdhi a, Vdhi b) {return DHI_CGTS(a, b);}
INLINE(Vdhi,VDHF_CGTS) (Vdhf a, Vdhf b) {return DHF_CGTS(a, b);}
INLINE(Vdwu,VDWU_CGTS) (Vdwu a, Vdwu b) {return DWU_CGTS(a, b);}
INLINE(Vdwi,VDWI_CGTS) (Vdwi a, Vdwi b) {return DWI_CGTS(a, b);}
INLINE(Vdwi,VDWF_CGTS) (Vdwf a, Vdwf b) {return DWF_CGTS(a, b);}
INLINE(Vddu,VDDU_CGTS) (Vddu a, Vddu b) {return DDU_CGTS(a, b);}
INLINE(Vddi,VDDI_CGTS) (Vddi a, Vddi b) {return DDI_CGTS(a, b);}
INLINE(Vddi,VDDF_CGTS) (Vddf a, Vddf b) {return DDF_CGTS(a, b);}


INLINE(Vqbu,VQBU_CGTS) (Vqbu a, Vqbu b) {return QBU_CGTS(a, b);}
INLINE(Vqbi,VQBI_CGTS) (Vqbi a, Vqbi b) {return QBI_CGTS(a, b);}
INLINE(Vqbc,VQBC_CGTS) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CGTS(VQBC_ASTM(b), VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CGTS) (Vqhu a, Vqhu b) {return QHU_CGTS(a, b);}
INLINE(Vqhi,VQHI_CGTS) (Vqhi a, Vqhi b) {return QHI_CGTS(a, b);}
INLINE(Vqhi,VQHF_CGTS) (Vqhf a, Vqhf b) {return QHF_CGTS(a, b);}
INLINE(Vqwu,VQWU_CGTS) (Vqwu a, Vqwu b) {return QWU_CGTS(a, b);}
INLINE(Vqwi,VQWI_CGTS) (Vqwi a, Vqwi b) {return QWI_CGTS(a, b);}
INLINE(Vqwi,VQWF_CGTS) (Vqwf a, Vqwf b) {return QWF_CGTS(a, b);}
INLINE(Vqdu,VQDU_CGTS) (Vqdu a, Vqdu b) {return QDU_CGTS(a, b);}
INLINE(Vqdi,VQDI_CGTS) (Vqdi a, Vqdi b) {return QDI_CGTS(a, b);}
INLINE(Vqdi,VQDF_CGTS) (Vqdf a, Vqdf b) {return QDF_CGTS(a, b);}


#if 0 // _LEAVE_ARM_CGTS
}
#endif

#if 0 // _ENTER_ARM_CGTL
{
#endif

INLINE(_Bool, FLT16_CGTL) (flt16_t a, flt16_t b) {return a>b;}
INLINE(_Bool,   FLT_CGTL)   (float a,   float b) {return a>b;}
INLINE(_Bool,   DBL_CGTL)  (double a,  double b) {return a>b;}

INLINE(QUAD_ITYPE,cgtlqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (a>b);
}

INLINE( Wbu,WBU_CGTL) (Wbu a, Wbu b)
{
#define     WBU_CGTL    WBU_CGTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    uint8x8_t   r = vcgt_u8(
        vreinterpret_u8_f32(p),
        vreinterpret_u8_f32(q)
    );
    return  vget_lane_f32(
        vreinterpret_f32_u8(vshr_n_u8(r, 7)),
        0
    );
}

INLINE( Wbu,WBI_CGTL) (Wbi a, Wbi b)
{
#define     WBI_CGTL    WBI_CGTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vshr_n_u8(
                vcgt_s8(
                    vreinterpret_s8_f32(p),
                    vreinterpret_s8_f32(q)
                ),
                7
            )
        ),
        0
    );
}

#if CHAR_MIN
#   define  WBC_CGTL    WBI_CGTL
#else
#   define  WBC_CGTL    WBU_CGTL
#endif

INLINE( Whu,WHU_CGTL) (Whu a, Whu b)
{
#define     WHU_CGTL    WHU_CGTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcgt_u16(
                    vreinterpret_u16_f32(p),
                    vreinterpret_u16_f32(q)
                ),
                15
            )
        ),
        0
    );
}

INLINE( Whu,WHI_CGTL) (Whi a, Whi b)
{
#define     WHI_CGTL    WHI_CGTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcgt_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        0
    );
}

INLINE( Whu,WHF_CGTL) (Whf a, Whf b)
{
#define     WHF_CGTL    WHF_CGTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcgt_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        0
    );
#else
    float32x4_t l = vcvt_f32_f16(vreinterpret_f16_f32(p));
    float32x4_t r = vcvt_f32_f16(vreinterpret_f16_f32(q));
    uint16x4_t  v = vshr_n_u16(vmovn_u32(vcgtq_f32(l, r)), 15);
    return  vget_lane_f32(vreinterpret_f32_u16(v), 0);
#endif
}

INLINE( Wwu,WWU_CGTL) (Wwu a, Wwu b)
{
#define     WWU_CGTL    WWU_CGTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcgt_u32(
                    vreinterpret_u32_f32(p),
                    vreinterpret_u32_f32(q)
                ),
                31
            )
        ),
        0
    );
}

INLINE( Wwu,WWI_CGTL) (Wwi a, Wwi b)
{
#define     WWI_CGTL    WWI_CGTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcgt_s32(
                    vreinterpret_s32_f32(p),
                    vreinterpret_s32_f32(q)
                ),
                31
            )
        ),
        0
    );
}

INLINE( Wwu,WWF_CGTL) (Wwf a, Wwf b)
{
#define     WWF_CGTL    WWF_CGTL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(vshr_n_u32(vcgt_f32(p, q), 31)),
        0
    );
}

#define     DBU_CGTL(A, B)            vshr_n_u8(vcgt_u8(A,B),7)
#define     DBI_CGTL(A, B)  VDBU_ASTI(vshr_n_u8(vcgt_s8(A,B),7))
#if CHAR_MIN
#   define  DBC_CGTL        DBI_CGTL
#else
#   define  DBC_CGTL        DBU_CGTL
#endif

#define     DHU_CGTL(A, B)            vshr_n_u16(vcgt_u16(A,B),15)
#define     DHI_CGTL(A, B)  VDHU_ASTI(vshr_n_u16(vcgt_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CGTL(A, B)  VDHU_ASTI(vshr_n_u16(vcgt_f16(A,B),15))
#else
#   define  DHF_CGTL(A, B)      \
VDHU_ASHI(                      \
    vshr_n_u16(                 \
        vmovn_u32(              \
            vcgtq_f32(          \
                vcvt_f32_f16(a),\
                vcvt_f32_f16(b) \
            )                   \
        ),                      \
        15                      \
    )                           \
)
#endif

#define     DWU_CGTL(A, B)            vshr_n_u32(vcgt_u32(A,B),31)
#define     DWI_CGTL(A, B)  VDWU_ASTI(vshr_n_u32(vcgt_s32(A,B),31))
#define     DWF_CGTL(A, B)  VDWU_ASTI(vshr_n_u32(vcgt_f32(A,B),31))

#define     DDU_CGTL(A, B)            vshr_n_u64(vcgt_u64(A,B),63)
#define     DDI_CGTL(A, B)  VDDU_ASTI(vshr_n_u64(vcgt_s64(A,B),63))
#define     DDF_CGTL(A, B)  VDDU_ASTI(vshr_n_u64(vcgt_f64(A,B),63))


#define     QBU_CGTL(A, B)            vshrq_n_u8(vcgtq_u8(A,B),7)
#define     QBI_CGTL(A, B)  VQBU_ASTI(vshrq_n_u8(vcgtq_s8(A,B),7))
#if CHAR_MIN
#   define  QBC_CGTL        QBI_CGTL
#else
#   define  QBC_CGTL        QBU_CGTL
#endif

#define     QHU_CGTL(A, B)            vshrq_n_u16(vcgtq_u16(A,B),15)
#define     QHI_CGTL(A, B)  VQHU_ASTI(vshrq_n_u16(vcgtq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CGTL(A, B)  VQHU_ASTI(vshrq_n_u16(vcgtq_f16(A,B),15))
#else
#   define  QHF_CGTL(A, B)                          \
vreinterpretq_s16_u16(                              \
    vshrq_n_u16(                                    \
        vcombine_u16(                               \
            vmovn_u32(                              \
                vcgtq_f32(                          \
                    vcvt_f32_f16(vget_low_f16(A)),  \
                    vcvt_f32_f16(vget_low_f16(B))   \
                )                                   \
            ),                                      \
            vmovn_u32(                              \
                vcgtq_f32(                          \
                    vcvt_f32_f16(vget_high_f16(A)), \
                    vcvt_f32_f16(vget_high_f16(B))  \
                )                                   \
            )                                       \
        ),                                          \
        15                                          \
    )                                               \
)
#endif

#define     QWU_CGTL(A, B)            vshrq_n_u32(vcgtq_u32(A,B),31)
#define     QWI_CGTL(A, B)  VQWU_ASTI(vshrq_n_u32(vcgtq_s32(A,B),31))
#define     QWF_CGTL(A, B)  VQWU_ASTI(vshrq_n_u32(vcgtq_f32(A,B),31))

#define     QDU_CGTL(A, B)            vshrq_n_u64(vcgtq_u64(A,B),63)
#define     QDI_CGTL(A, B)  VQDU_ASTI(vshrq_n_u64(vcgtq_s64(A,B),63))
#define     QDF_CGTL(A, B)  VQDU_ASTI(vshrq_n_u64(vcgtq_f64(A,B),63))


INLINE(Vwyu,VWYU_CGTL) (Vwyu a, Vwyu b)
{
    uint32_t p = VWWU_ASTV(VWYU_ASWU(a));
    uint32_t q = VWWU_ASTV(VWYU_ASWU(b));
    return  VWWU_ASYU(UINT_ASTV((~q&p)));
}

INLINE(Vwbu,VWBU_CGTL) (Vwbu a, Vwbu b)
{
#define     VWBU_CGTL(A, B) WBU_ASTV(WBU_CGTL(VWBU_ASTM(A),VWBU_ASTM(B)))
    return  VWBU_CGTL(a, b);
}

INLINE(Vwbi,VWBI_CGTL) (Vwbi a, Vwbi b)
{
#define     VWBI_CGTL(A, B) WBI_ASTV(WBI_CGTL(VWBI_ASTM(A),VWBI_ASTM(B)))
    return  VWBI_CGTL(a, b);
}

INLINE(Vwbc,VWBC_CGTL) (Vwbc a, Vwbc b)
{
#define     VWBC_CGTL(A, B) WBC_ASTV(WBC_CGTL(VWBC_ASTM(A),VWBC_ASTM(B)))
    return  VWBC_CGTL(a, b);
}


INLINE(Vwhu,VWHU_CGTL) (Vwhu a, Vwhu b)
{
#define     VWHU_CGTL(A, B) WHU_ASTV(WHU_CGTL(VWHU_ASTM(A),VWHU_ASTM(B)))
    return  VWHU_CGTL(a, b);
}

INLINE(Vwhi,VWHI_CGTL) (Vwhi a, Vwhi b)
{
#define     VWHI_CGTL(A, B) WHI_ASTV(WHI_CGTL(VWHI_ASTM(A),VWHI_ASTM(B)))
    return  VWHI_CGTL(a, b);
}

INLINE(Vwhi,VWHF_CGTL) (Vwhf a, Vwhf b)
{
#define     VWHF_CGTL(A, B) WHI_ASTV(WHF_CGTL(VWHF_ASTM(A),VWHF_ASTM(B)))
    return  VWHF_CGTL(a, b);
}


INLINE(Vwwu,VWWU_CGTL) (Vwwu a, Vwwu b)
{
#define     VWWU_CGTL(A, B) WWU_ASTV(WWU_CGTL(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_CGTL(a, b);
}

INLINE(Vwwi,VWWI_CGTL) (Vwwi a, Vwwi b)
{
#define     VWWI_CGTL(A, B) WWI_ASTV(WWI_CGTL(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_CGTL(a, b);
}

INLINE(Vwwi,VWWF_CGTL) (Vwwf a, Vwwf b)
{
#define     VWWF_CGTL(A, B) WWI_ASTV(WWF_CGTL(VWWF_ASTM(A),VWWF_ASTM(B)))
    return  VWWF_CGTL(a, b);
}

INLINE(Vdyu,VDYU_CGTL) (Vdyu a, Vdyu b)
{
    uint64x1_t p = VDYU_ASDU(a);
    uint64x1_t q = VDYU_ASDU(b);
    p = vbic_u64(p, q);
    return  VDDU_ASYU(p);
}


INLINE(Vdbu,VDBU_CGTL) (Vdbu a, Vdbu b) {return DBU_CGTL(a, b);}
INLINE(Vdbi,VDBI_CGTL) (Vdbi a, Vdbi b) {return DBI_CGTL(a, b);}
INLINE(Vdbc,VDBC_CGTL) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(
        DBC_CGTL(
            VDBC_ASTM(a),
            VDBC_ASTM(b)
        )
    );
}

INLINE(Vdhu,VDHU_CGTL) (Vdhu a, Vdhu b) {return DHU_CGTL(a, b);}
INLINE(Vdhi,VDHI_CGTL) (Vdhi a, Vdhi b) {return DHI_CGTL(a, b);}
INLINE(Vdhi,VDHF_CGTL) (Vdhf a, Vdhf b) {return DHF_CGTL(a, b);}

INLINE(Vdwu,VDWU_CGTL) (Vdwu a, Vdwu b) {return DWU_CGTL(a, b);}
INLINE(Vdwi,VDWI_CGTL) (Vdwi a, Vdwi b) {return DWI_CGTL(a, b);}
INLINE(Vdwi,VDWF_CGTL) (Vdwf a, Vdwf b) {return DWF_CGTL(a, b);}

INLINE(Vddu,VDDU_CGTL) (Vddu a, Vddu b) {return DDU_CGTL(a, b);}
INLINE(Vddi,VDDI_CGTL) (Vddi a, Vddi b) {return DDI_CGTL(a, b);}
INLINE(Vddi,VDDF_CGTL) (Vddf a, Vddf b) {return DDF_CGTL(a, b);}

INLINE(Vqyu,VQYU_CGTL) (Vqyu a, Vqyu b)
{
    uint64x2_t p = VQYU_ASDU(a);
    uint64x2_t q = VQYU_ASDU(b);
    p = vbicq_u64(p, q);
    return  VQDU_ASYU(p);
}

INLINE(Vqbu,VQBU_CGTL) (Vqbu a, Vqbu b) {return QBU_CGTL(a,b);}
INLINE(Vqbi,VQBI_CGTL) (Vqbi a, Vqbi b) {return QBI_CGTL(a,b);}
INLINE(Vqbc,VQBC_CGTL) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CGTL(VQBC_ASTM(a),VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CGTL) (Vqhu a, Vqhu b) {return QHU_CGTL(a,b);}
INLINE(Vqhi,VQHI_CGTL) (Vqhi a, Vqhi b) {return QHI_CGTL(a,b);}
INLINE(Vqhi,VQHF_CGTL) (Vqhf a, Vqhf b) {return QHF_CGTL(a,b);}
INLINE(Vqwu,VQWU_CGTL) (Vqwu a, Vqwu b) {return QWU_CGTL(a,b);}
INLINE(Vqwi,VQWI_CGTL) (Vqwi a, Vqwi b) {return QWI_CGTL(a,b);}
INLINE(Vqwi,VQWF_CGTL) (Vqwf a, Vqwf b) {return QWF_CGTL(a,b);}
INLINE(Vqdu,VQDU_CGTL) (Vqdu a, Vqdu b) {return QDU_CGTL(a,b);}
INLINE(Vqdi,VQDI_CGTL) (Vqdi a, Vqdi b) {return QDI_CGTL(a,b);}
INLINE(Vqdi,VQDF_CGTL) (Vqdf a, Vqdf b) {return QDF_CGTL(a,b);}

#if 0 // _LEAVE_ARM_CGTL
}
#endif

#if 0 // _ENTER_ARM_CGTY
{
#endif

INLINE(_Bool,FLT16_CGTY) (flt16_t a, flt16_t b) {return a>b;}
INLINE(_Bool,  FLT_CGTY)   (float a,   float b) {return a>b;}
INLINE(_Bool,  DBL_CGTY)  (double a,  double b) {return a>b;}

INLINE(_Bool,WYU_CGTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    return !vget_lane_u32(y.W.U,0) && (-1==vget_lane_s32(x.W.I,0));
}

INLINE(_Bool,WBU_CGTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.B.U = vcgt_u8(x.B.U, y.B.U);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WBI_CGTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.B.U = vcgt_s8(x.B.I, y.B.I);
    return  -1==vget_lane_s32(x.W.I, 0);
}

#if CHAR_MIN
#   define  WBC_CGTY  WBI_CGTY
#else
#   define  WBC_CGTY  WBU_CGTY
#endif

INLINE(_Bool,WHU_CGTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.U = vcgt_u16(x.H.U, y.H.U);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WHI_CGTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.U = vcgt_s16(x.H.I, y.H.I);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WHF_CGTY) (float a, float b) 
{
    DWRD_VTYPE x={.W.F={a}}, y={.W.F={b}};
    x.H.I = VDHF_CGTS(x.H.F, y.H.F);
    return  -1==vget_lane_s32(x.W.I, 0);
}

INLINE(_Bool,WWU_CGTY) (float a, float b) 
{
    WORD_TYPE x={.F=a}, y={.F=b};
    return  x.U<=y.U;
}

INLINE(_Bool,WWI_CGTY) (float a, float b) 
{
    WORD_TYPE x={.F=a}, y={.F=b};
    return  x.I<=y.I;
}

INLINE(_Bool,VWYU_CGTY) (Vwyu a, Vwyu b) {return WYU_CGTY(a.V0, b.V0);}         
INLINE(_Bool,VWBU_CGTY) (Vwbu a, Vwbu b) {return WBU_CGTY(a.V0, b.V0);}         
INLINE(_Bool,VWBI_CGTY) (Vwbi a, Vwbi b) {return WBI_CGTY(a.V0, b.V0);}
INLINE(_Bool,VWBC_CGTY) (Vwbc a, Vwbc b) {return WBC_CGTY(a.V0, b.V0);}
INLINE(_Bool,VWHU_CGTY) (Vwhu a, Vwhu b) {return WHU_CGTY(a.V0, b.V0);}
INLINE(_Bool,VWHI_CGTY) (Vwhi a, Vwhi b) {return WHI_CGTY(a.V0, b.V0);}         
INLINE(_Bool,VWHF_CGTY) (Vwhf a, Vwhf b) {return WHF_CGTY(a.V0, b.V0);}
INLINE(_Bool,VWWU_CGTY) (Vwwu a, Vwwu b) {return WWU_CGTY(a.V0, b.V0);}         
INLINE(_Bool,VWWI_CGTY) (Vwwi a, Vwwi b) {return WWI_CGTY(a.V0, b.V0);}         
INLINE(_Bool,VWWF_CGTY) (Vwwf a, Vwwf b) {return a.V0>b.V0;}

INLINE(_Bool,VDYU_CGTY) (Vdyu a, Vdyu b)
{
    return (UINT64_MAX==vget_lane_u64(a.V0, 0)) && !vget_lane_u64(b.V0, 0);
}

INLINE(_Bool,VDBU_CGTY) (Vdbu a, Vdbu b)
{
    uint8x8_t   u = vcgt_u8(a, b);
    uint64x1_t  c = vreinterpret_u64_u8(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDBI_CGTY) (Vdbi a, Vdbi b)
{
    uint8x8_t   u = vcgt_s8(a, b);
    uint64x1_t  c = vreinterpret_u64_u8(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDBC_CGTY) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
    return VDBI_CGTY(a.V0, b.V0);
#else
    return VDBU_CGTY(a.V0, b.V0);
#endif
}

INLINE(_Bool,VDHU_CGTY) (Vdhu a, Vdhu b)
{
    uint16x4_t  u = vcgt_u16(a, b);
    uint64x1_t  c = vreinterpret_u64_u16(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDHI_CGTY) (Vdhi a, Vdhi b)
{
    uint16x4_t  u = vcgt_s16(a, b);
    uint64x1_t  c = vreinterpret_u64_u16(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDHF_CGTY) (Vdhf a, Vdhf b)
{
    int16x4_t   u = VDHF_CGTS(a, b);
    uint64x1_t  c = vreinterpret_u64_u16(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDWU_CGTY) (Vdwu a, Vdwu b)
{
    uint32x2_t  u = vcgt_u32(a, b);
    uint64x1_t  c = vreinterpret_u64_u32(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDWI_CGTY) (Vdwi a, Vdwi b)
{
    uint32x2_t  u = vcgt_s32(a, b);
    uint64x1_t  c = vreinterpret_u64_u32(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDWF_CGTY) (Vdwf a, Vdwf b)
{
    uint32x2_t  u = vcgt_f32(a, b);
    uint64x1_t  c = vreinterpret_u64_u32(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDDU_CGTY) (Vddu a, Vddu b)
{
    uint64x1_t  u = vcgt_u64(a, b);
    return  vget_lane_u64(u, 0);
}

INLINE(_Bool,VDDI_CGTY) (Vddi a, Vddi b)
{
    uint64x1_t  u = vcgt_s64(a, b);
    return  vget_lane_u64(u, 0);
}

INLINE(_Bool,VDDF_CGTY) (Vddf a, Vddf b)
{
    uint64x1_t  u = vcgt_f64(a, b);
    return  vget_lane_u64(u, 0);
}


INLINE(_Bool,VQYU_CGTY) (Vqyu a, Vqyu b)
{
    uint64_t x0 = vgetq_lane_u64(a.V0, 0);
    uint64_t x1 = vgetq_lane_u64(a.V0, 1);

    uint64_t x2 = vgetq_lane_u64(b.V0, 0);
    uint64_t x3 = vgetq_lane_u64(b.V0, 1);
    return  (!(x0|x1)) && (UINT64_MAX==(x2&x3));
}

#define     MY_CGTYZ(C) \
(UINT64_MAX==vget_lane_u64(vand_u64(vget_low_u64(C),vget_high_u64(C)),0))

INLINE(_Bool,VQBU_CGTY) (Vqbu a, Vqbu b)
{
    uint8x16_t  u = vcgtq_u8(a, b);
    uint64x2_t  c = vreinterpretq_u64_u8(u);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQBI_CGTY) (Vqbi a, Vqbi b)
{
    uint8x16_t  u = vcgtq_s8(a, b);
    uint64x2_t  c = vreinterpretq_u64_u8(u);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQBC_CGTY) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
    return  VQBI_CGTY(a.V0, b.V0);
#else
    return  VQBU_CGTY(a.V0, b.V0);
#endif
}

INLINE(_Bool,VQHU_CGTY) (Vqhu a, Vqhu b)
{
    uint16x8_t  u = vcgtq_u16(a, b);
    uint64x2_t  c = vreinterpretq_u64_u16(u);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQHI_CGTY) (Vqhi a, Vqhi b)
{
    uint16x8_t  u = vcgtq_s16(a, b);
    uint64x2_t  c = vreinterpretq_u64_u16(u);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQHF_CGTY) (Vqhf a, Vqhf b)
{
    int16x8_t   u = VQHF_CGTS(a, b);
    uint64x2_t  c = vreinterpretq_u64_s16(u);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQWU_CGTY) (Vqwu a, Vqwu b)
{
    uint32x4_t  u = vcgtq_u32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQWI_CGTY) (Vqwi a, Vqwi b)
{
    uint32x4_t  u = vcgtq_s32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQWF_CGTY) (Vqwf a, Vqwf b)
{
    uint32x4_t  u = vcgtq_f32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQDU_CGTY) (Vqdu a, Vqdu b)
{
    uint64x2_t  c = vcgtq_u64(a, b);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQDI_CGTY) (Vqdi a, Vqdi b)
{
    uint64x2_t  c = vcgtq_s64(a, b);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQDF_CGTY) (Vqdf a, Vqdf b)
{
    uint64x2_t  c = vcgtq_f64(a, b);
    return  MY_CGTYZ(c);
}

INLINE(_Bool,VQQU_CGTY) (Vqqu a, Vqqu b)
{
    return  
    (vgetq_lane_u64(a.V0,1)==vgetq_lane_u64(b.V0,1))
    ?   (vgetq_lane_u64(a.V0,0)>vgetq_lane_u64(b.V0,0))
    :   (vgetq_lane_u64(a.V0,1)>vgetq_lane_u64(b.V0,1));
}

INLINE(_Bool,VQQI_CGTY) (Vqqi a, Vqqi b)
{
    return  
    (vgetq_lane_s64(a.V0,1)==vgetq_lane_s64(b.V0,1))
    ?   (vgetq_lane_s64(a.V0,0)>vgetq_lane_s64(b.V0,0))
    :   (vgetq_lane_s64(a.V0,1)>vgetq_lane_s64(b.V0,1));
}
INLINE(_Bool,VQQF_CGTY) (Vqqf a, Vqqf b) {return a.V0 > b.V0;}

#if 0 // _LEAVE_ARM_CGTY
}
#endif

#if 0 // _ENTER_ARM_CGES
{
#endif

INLINE(int16_t,  FLT16_CGES) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcgeh_f16(a, b);
#else
    return  0-(a>=b);
#endif
}

INLINE(int32_t,    FLT_CGES)   (float a,   float b) {return  vcges_f32(a, b);}
INLINE(int64_t,    DBL_CGES)  (double a,  double b) {return  vcged_f64(a, b);}


INLINE(QUAD_ITYPE,cgesqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (QUAD_ITYPE) 0-(a>=b);
}


#define     DBU_CGES                             vcge_u8
#define     DBI_CGES(A, B)  vreinterpret_s16_u16(vcge_s8(A,B))
#define     DBC_CGES        VDBC_BASE(CGES)
#define     DHU_CGES                             vcge_u16
#define     DHI_CGES(A, B)  vreinterpret_s16_u16(vcge_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CGES(A, B)  vreinterpret_s16_u16(vcge_f16(A,B))
#else
INLINE(float16x4_t,DHF_CGES) (float16x4_t a, float16x4_t b)
{
#   define  DHF_CGES    DHF_CGES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  DHF_VOID;
}
#endif
#define     DWU_CGES                             vcge_u32
#define     DWI_CGES(A, B)  vreinterpret_s32_u32(vcge_s32(A,B))
#define     DWF_CGES(A, B)  vreinterpret_s32_u32(vcge_f32(A,B))
#define     DDU_CGES                             vcge_u64
#define     DDI_CGES(A, B)  vreinterpret_s64_u64(vcge_s32(A,B))
#define     DDF_CGES(A, B)  vreinterpret_s64_u64(vcge_f32(A,B))


#define     QBU_CGES                              vcgeq_u8
#define     QBI_CGES(A, B)  vreinterpretq_s16_u16(vcgeq_s8(A,B))
#define     QBC_CGES        VQBC_BASE(CGES)
#define     QHU_CGES                              vcgeq_u16
#define     QHI_CGES(A, B)  vreinterpretq_s16_u16(vcgeq_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CGES(A, B)  vreinterpretq_s16_u16(vcgeq_f16(A,B))
#else
INLINE(float16x8_t,QHF_CGES) (float16x8_t a, float16x8_t b)
{
#   define  QHF_CGES    QHF_CGES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  QHF_VOID;
}
#endif

#define     QWU_CGES                              vcgeq_u32
#define     QWI_CGES(A, B)  vreinterpretq_s32_u32(vcgeq_s32(A,B))
#define     QWF_CGES(A, B)  vreinterpretq_s32_u32(vcgeq_f32(A,B))
#define     QDU_CGES                              vcgeq_u64
#define     QDI_CGES(A, B)  vreinterpretq_s64_u64(vcgeq_s32(A,B))
#define     QDF_CGES(A, B)  vreinterpretq_s64_u64(vcgeq_f32(A,B))


INLINE(Vwbu,VWBU_CGES) (Vwbu a, Vwbu b)
{
    float32x2_t l = vset_lane_f32(VWBU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWBU_ASTM(a), r, 0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CGES(p, q)),
            0
        )
    );
}

INLINE(Vwbi,VWBI_CGES) (Vwbi a, Vwbi b)
{
    float32x2_t l = vset_lane_f32(VWBI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWBI_ASTM(a), r, 0);
    int8x8_t    p = vreinterpret_s8_f32(l);
    int8x8_t    q = vreinterpret_s8_f32(r);
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(vcge_s8(p, q)),
            0
        )
    );
}

INLINE(Vwbc,VWBC_CGES) (Vwbc a, Vwbc b)
{
    float32x2_t x = vset_lane_f32(VWBC_ASTM(a), x, 0);
    float32x2_t y = vset_lane_f32(VWBC_ASTM(a), y, 0);
    float32x2_t r;
#if CHAR_MIN
    int8x8_t    p = vreinterpret_s8_f32(x);
    int8x8_t    q = vreinterpret_s8_f32(y);
    r = vreinterpret_f32_u8(vcge_s8(p, q));
#else
    uint8x8_t   p = vreinterpret_u8_f32(x);
    uint8x8_t   q = vreinterpret_u8_f32(y);
    r = vreinterpret_f32_u8(vcge_u8(p, q));
#endif
    return  WBC_ASTV(vget_lane_f32(r, 0));
}


INLINE(Vwhu,VWHU_CGES) (Vwhu a, Vwhu b)
{
    float32x2_t l = vset_lane_f32(VWHU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHU_ASTM(a), r, 0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcge_u16(p, q)),
            0
        )
    );
}

INLINE(Vwhi,VWHI_CGES) (Vwhi a, Vwhi b)
{
    float32x2_t l = vset_lane_f32(VWHI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHI_ASTM(a), r, 0);
    int16x4_t   p = vreinterpret_s16_f32(l);
    int16x4_t   q = vreinterpret_s16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcge_s16(p, q)),
            0
        )
    );
}

#if defined(SPC_ARM_FP16_SIMD)
INLINE(Vwhi,VWHF_CGES) (Vwhf a, Vwhf b)
{
#   define  VWHF_CGES VWHF_CGES
    float32x2_t l = vset_lane_f32(VWHF_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWHF_ASTM(a), r, 0);
    float16x4_t p = vreinterpret_f16_f32(l);
    float16x4_t q = vreinterpret_f16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcge_f16(p, q)),
            0
        )
    );
}
#else
INLINE(Vwhf,VWHF_CGES) (Vwhf a, Vwhf b)
{
#   define  VWHF_CGES    VWHF_CGES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  VWHF_VOID;
}
#endif


INLINE(Vwwu,VWWU_CGES) (Vwwu a, Vwwu b)
{
    float32x2_t l = vset_lane_f32(VWWU_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWWU_ASTM(a), r, 0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcge_u32(p, q)),
            0
        )
    );
}

INLINE(Vwwi,VWWI_CGES) (Vwwi a, Vwwi b)
{
    float32x2_t l = vset_lane_f32(VWWI_ASTM(a), l, 0);
    float32x2_t r = vset_lane_f32(VWWI_ASTM(a), r, 0);
    int32x2_t   p = vreinterpret_s32_f32(l);
    int32x2_t   q = vreinterpret_s32_f32(r);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcge_s32(p, q)),
            0
        )
    );
}

INLINE(Vwwi,VWWF_CGES) (Vwwf a, Vwwf b)
{
    float32x2_t p = vset_lane_f32(VWWF_ASTM(a), p, 0);
    float32x2_t q = vset_lane_f32(VWWF_ASTM(a), q, 0);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcge_f32(p, q)),
            0
        )
    );
}


INLINE(Vdbu,VDBU_CGES) (Vdbu a, Vdbu b) {return DBU_CGES(a, b);}
INLINE(Vdbi,VDBI_CGES) (Vdbi a, Vdbi b) {return DBI_CGES(a, b);}
INLINE(Vdbc,VDBC_CGES) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(DBC_CGES(VDBC_ASTM(b), VDBC_ASTM(b)));
}

INLINE(Vdhu,VDHU_CGES) (Vdhu a, Vdhu b) {return DHU_CGES(a, b);}
INLINE(Vdhi,VDHI_CGES) (Vdhi a, Vdhi b) {return DHI_CGES(a, b);}
INLINE(Vdhi,VDHF_CGES) (Vdhf a, Vdhf b) {return DHF_CGES(a, b);}
INLINE(Vdwu,VDWU_CGES) (Vdwu a, Vdwu b) {return DWU_CGES(a, b);}
INLINE(Vdwi,VDWI_CGES) (Vdwi a, Vdwi b) {return DWI_CGES(a, b);}
INLINE(Vdwi,VDWF_CGES) (Vdwf a, Vdwf b) {return DWF_CGES(a, b);}
INLINE(Vddu,VDDU_CGES) (Vddu a, Vddu b) {return DDU_CGES(a, b);}
INLINE(Vddi,VDDI_CGES) (Vddi a, Vddi b) {return DDI_CGES(a, b);}
INLINE(Vddi,VDDF_CGES) (Vddf a, Vddf b) {return DDF_CGES(a, b);}


INLINE(Vqbu,VQBU_CGES) (Vqbu a, Vqbu b) {return QBU_CGES(a, b);}
INLINE(Vqbi,VQBI_CGES) (Vqbi a, Vqbi b) {return QBI_CGES(a, b);}
INLINE(Vqbc,VQBC_CGES) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CGES(VQBC_ASTM(b), VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CGES) (Vqhu a, Vqhu b) {return QHU_CGES(a, b);}
INLINE(Vqhi,VQHI_CGES) (Vqhi a, Vqhi b) {return QHI_CGES(a, b);}
INLINE(Vqhi,VQHF_CGES) (Vqhf a, Vqhf b) {return QHF_CGES(a, b);}
INLINE(Vqwu,VQWU_CGES) (Vqwu a, Vqwu b) {return QWU_CGES(a, b);}
INLINE(Vqwi,VQWI_CGES) (Vqwi a, Vqwi b) {return QWI_CGES(a, b);}
INLINE(Vqwi,VQWF_CGES) (Vqwf a, Vqwf b) {return QWF_CGES(a, b);}
INLINE(Vqdu,VQDU_CGES) (Vqdu a, Vqdu b) {return QDU_CGES(a, b);}
INLINE(Vqdi,VQDI_CGES) (Vqdi a, Vqdi b) {return QDI_CGES(a, b);}
INLINE(Vqdi,VQDF_CGES) (Vqdf a, Vqdf b) {return QDF_CGES(a, b);}


#if 0 // _LEAVE_ARM_CGES
}
#endif

#if 0 // _ENTER_ARM_CGEL
{
#endif

INLINE(_Bool, FLT16_CGEL) (flt16_t a, flt16_t b) {return a>=b;}
INLINE(_Bool,   FLT_CGEL)   (float a,   float b) {return a>=b;}
INLINE(_Bool,   DBL_CGEL)  (double a,  double b) {return a>=b;}

INLINE(QUAD_ITYPE,cgelqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (a>=b);
}


INLINE( Wbu,WBU_CGEL) (Wbu a, Wbu b)
{
#define     WBU_CGEL    WBU_CGEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    uint8x8_t   r = vcge_u8(
        vreinterpret_u8_f32(p),
        vreinterpret_u8_f32(q)
    );
    return  vget_lane_f32(
        vreinterpret_f32_u8(vshr_n_u8(r, 7)),
        0
    );
}

INLINE( Wbu,WBI_CGEL) (Wbi a, Wbi b)
{
#define     WBI_CGEL    WBI_CGEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vshr_n_u8(
                vcge_s8(
                    vreinterpret_s8_f32(p),
                    vreinterpret_s8_f32(q)
                ),
                7
            )
        ),
        0
    );
}

#if CHAR_MIN
#   define  WBC_CGEL    WBI_CGEL
#else
#   define  WBC_CGEL    WBU_CGEL
#endif

INLINE( Whu,WHU_CGEL) (Whu a, Whu b)
{
#define     WHU_CGEL    WHU_CGEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcge_u16(
                    vreinterpret_u16_f32(p),
                    vreinterpret_u16_f32(q)
                ),
                15
            )
        ),
        0
    );
}

INLINE( Whu,WHI_CGEL) (Whi a, Whi b)
{
#define     WHI_CGEL    WHI_CGEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcge_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        0
    );
}

INLINE( Whu,WHF_CGEL) (Whf a, Whf b)
{
#define     WHF_CGEL    WHF_CGEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcge_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        0
    );
#else
    float32x4_t l = vcvt_f32_f16(vreinterpret_f16_f32(p));
    float32x4_t r = vcvt_f32_f16(vreinterpret_f16_f32(q));
    uint16x4_t  v = vshr_n_u16(vmovn_u32(vcgeq_f32(l, r)), 15);
    return  vget_lane_f32(vreinterpret_f32_u16(v), 0);
#endif
}

INLINE( Wwu,WWU_CGEL) (Wwu a, Wwu b)
{
#define     WWU_CGEL    WWU_CGEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcge_u32(
                    vreinterpret_u32_f32(p),
                    vreinterpret_u32_f32(q)
                ),
                31
            )
        ),
        0
    );
}

INLINE( Wwu,WWI_CGEL) (Wwi a, Wwi b)
{
#define     WWI_CGEL    WWI_CGEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcge_s32(
                    vreinterpret_s32_f32(p),
                    vreinterpret_s32_f32(q)
                ),
                31
            )
        ),
        0
    );
}

INLINE( Wwu,WWF_CGEL) (Wwf a, Wwf b)
{
#define     WWF_CGEL    WWF_CGEL
    float32x2_t p = vset_lane_f32(a, p, 0);
    float32x2_t q = vset_lane_f32(b, q, 0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(vshr_n_u32(vcge_f32(p, q), 31)),
        0
    );
}

#define     DBU_CGEL(A, B)            vshr_n_u8(vcge_u8(A,B),7)
#define     DBI_CGEL(A, B)  VDBU_ASTI(vshr_n_u8(vcge_s8(A,B),7))
#if CHAR_MIN
#   define  DBC_CGEL        DBI_CGEL
#else
#   define  DBC_CGEL        DBU_CGEL
#endif

#define     DHU_CGEL(A, B)            vshr_n_u16(vcge_u16(A,B),15)
#define     DHI_CGEL(A, B)  VDHU_ASTI(vshr_n_u16(vcge_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CGEL(A, B)  VDHU_ASTI(vshr_n_u16(vcge_f16(A,B),15))
#else
#   define  DHF_CGEL(A, B)      \
VDHU_ASHI(                      \
    vshr_n_u16(                 \
        vmovn_u32(              \
            vcgeq_f32(          \
                vcvt_f32_f16(a),\
                vcvt_f32_f16(b) \
            )                   \
        ),                      \
        15                      \
    )                           \
)
#endif

#define     DWU_CGEL(A, B)            vshr_n_u32(vcge_u32(A,B),31)
#define     DWI_CGEL(A, B)  VDWU_ASTI(vshr_n_u32(vcge_s32(A,B),31))
#define     DWF_CGEL(A, B)  VDWU_ASTI(vshr_n_u32(vcge_f32(A,B),31))

#define     DDU_CGEL(A, B)            vshr_n_u64(vcge_u64(A,B),63)
#define     DDI_CGEL(A, B)  VDDU_ASTI(vshr_n_u64(vcge_s64(A,B),63))
#define     DDF_CGEL(A, B)  VDDU_ASTI(vshr_n_u64(vcge_f64(A,B),63))


#define     QBU_CGEL(A, B)            vshrq_n_u8(vcgeq_u8(A,B),7)
#define     QBI_CGEL(A, B)  VQBU_ASTI(vshrq_n_u8(vcgeq_s8(A,B),7))
#if CHAR_MIN
#   define  QBC_CGEL        QBI_CGEL
#else
#   define  QBC_CGEL        QBU_CGEL
#endif

#define     QHU_CGEL(A, B)            vshrq_n_u16(vcgeq_u16(A,B),15)
#define     QHI_CGEL(A, B)  VQHU_ASTI(vshrq_n_u16(vcgeq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CGEL(A, B)  VQHU_ASTI(vshrq_n_u16(vcgeq_f16(A,B),15))
#else
#   define  QHF_CGEL(A, B)                          \
vreinterpretq_s16_u16(                              \
    vshrq_n_u16(                                    \
        vcombine_u16(                               \
            vmovn_u32(                              \
                vcgeq_f32(                          \
                    vcvt_f32_f16(vget_low_f16(A)),  \
                    vcvt_f32_f16(vget_low_f16(B))   \
                )                                   \
            ),                                      \
            vmovn_u32(                              \
                vcgeq_f32(                          \
                    vcvt_f32_f16(vget_high_f16(A)), \
                    vcvt_f32_f16(vget_high_f16(B))  \
                )                                   \
            )                                       \
        ),                                          \
        15                                          \
    )                                               \
)
#endif

#define     QWU_CGEL(A, B)            vshrq_n_u32(vcgeq_u32(A,B),31)
#define     QWI_CGEL(A, B)  VQWU_ASTI(vshrq_n_u32(vcgeq_s32(A,B),31))
#define     QWF_CGEL(A, B)  VQWU_ASTI(vshrq_n_u32(vcgeq_f32(A,B),31))

#define     QDU_CGEL(A, B)            vshrq_n_u64(vcgeq_u64(A,B),63)
#define     QDI_CGEL(A, B)  VQDU_ASTI(vshrq_n_u64(vcgeq_s64(A,B),63))
#define     QDF_CGEL(A, B)  VQDU_ASTI(vshrq_n_u64(vcgeq_f64(A,B),63))


INLINE(Vwbu,VWBU_CGEL) (Vwbu a, Vwbu b)
{
#define     VWBU_CGEL(A, B) WBU_ASTV(WBU_CGEL(VWBU_ASTM(A),VWBU_ASTM(B)))
    return  VWBU_CGEL(a, b);
}

INLINE(Vwbi,VWBI_CGEL) (Vwbi a, Vwbi b)
{
#define     VWBI_CGEL(A, B) WBI_ASTV(WBI_CGEL(VWBI_ASTM(A),VWBI_ASTM(B)))
    return  VWBI_CGEL(a, b);
}

INLINE(Vwbc,VWBC_CGEL) (Vwbc a, Vwbc b)
{
#define     VWBC_CGEL(A, B) WBC_ASTV(WBC_CGEL(VWBC_ASTM(A),VWBC_ASTM(B)))
    return  VWBC_CGEL(a, b);
}


INLINE(Vwhu,VWHU_CGEL) (Vwhu a, Vwhu b)
{
#define     VWHU_CGEL(A, B) WHU_ASTV(WHU_CGEL(VWHU_ASTM(A),VWHU_ASTM(B)))
    return  VWHU_CGEL(a, b);
}

INLINE(Vwhi,VWHI_CGEL) (Vwhi a, Vwhi b)
{
#define     VWHI_CGEL(A, B) WHI_ASTV(WHI_CGEL(VWHI_ASTM(A),VWHI_ASTM(B)))
    return  VWHI_CGEL(a, b);
}

INLINE(Vwhi,VWHF_CGEL) (Vwhf a, Vwhf b)
{
#define     VWHF_CGEL(A, B) WHI_ASTV(WHF_CGEL(VWHF_ASTM(A),VWHF_ASTM(B)))
    return  VWHF_CGEL(a, b);
}


INLINE(Vwwu,VWWU_CGEL) (Vwwu a, Vwwu b)
{
#define     VWWU_CGEL(A, B) WWU_ASTV(WWU_CGEL(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_CGEL(a, b);
}

INLINE(Vwwi,VWWI_CGEL) (Vwwi a, Vwwi b)
{
#define     VWWI_CGEL(A, B) WWI_ASTV(WWI_CGEL(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_CGEL(a, b);
}

INLINE(Vwwi,VWWF_CGEL) (Vwwf a, Vwwf b)
{
#define     VWWF_CGEL(A, B) WWI_ASTV(WWF_CGEL(VWWF_ASTM(A),VWWF_ASTM(B)))
    return  VWWF_CGEL(a, b);
}


INLINE(Vdbu,VDBU_CGEL) (Vdbu a, Vdbu b) {return DBU_CGEL(a, b);}
INLINE(Vdbi,VDBI_CGEL) (Vdbi a, Vdbi b) {return DBI_CGEL(a, b);}
INLINE(Vdbc,VDBC_CGEL) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(
        DBC_CGEL(
            VDBC_ASTM(a),
            VDBC_ASTM(b)
        )
    );
}

INLINE(Vdhu,VDHU_CGEL) (Vdhu a, Vdhu b) {return DHU_CGEL(a, b);}
INLINE(Vdhi,VDHI_CGEL) (Vdhi a, Vdhi b) {return DHI_CGEL(a, b);}
INLINE(Vdhi,VDHF_CGEL) (Vdhf a, Vdhf b) {return DHF_CGEL(a, b);}

INLINE(Vdwu,VDWU_CGEL) (Vdwu a, Vdwu b) {return DWU_CGEL(a, b);}
INLINE(Vdwi,VDWI_CGEL) (Vdwi a, Vdwi b) {return DWI_CGEL(a, b);}
INLINE(Vdwi,VDWF_CGEL) (Vdwf a, Vdwf b) {return DWF_CGEL(a, b);}

INLINE(Vddu,VDDU_CGEL) (Vddu a, Vddu b) {return DDU_CGEL(a, b);}
INLINE(Vddi,VDDI_CGEL) (Vddi a, Vddi b) {return DDI_CGEL(a, b);}
INLINE(Vddi,VDDF_CGEL) (Vddf a, Vddf b) {return DDF_CGEL(a, b);}


INLINE(Vqbu,VQBU_CGEL) (Vqbu a, Vqbu b) {return QBU_CGEL(a,b);}
INLINE(Vqbi,VQBI_CGEL) (Vqbi a, Vqbi b) {return QBI_CGEL(a,b);}
INLINE(Vqbc,VQBC_CGEL) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CGEL(VQBC_ASTM(a),VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CGEL) (Vqhu a, Vqhu b) {return QHU_CGEL(a,b);}
INLINE(Vqhi,VQHI_CGEL) (Vqhi a, Vqhi b) {return QHI_CGEL(a,b);}
INLINE(Vqhi,VQHF_CGEL) (Vqhf a, Vqhf b) {return QHF_CGEL(a,b);}
INLINE(Vqwu,VQWU_CGEL) (Vqwu a, Vqwu b) {return QWU_CGEL(a,b);}
INLINE(Vqwi,VQWI_CGEL) (Vqwi a, Vqwi b) {return QWI_CGEL(a,b);}
INLINE(Vqwi,VQWF_CGEL) (Vqwf a, Vqwf b) {return QWF_CGEL(a,b);}
INLINE(Vqdu,VQDU_CGEL) (Vqdu a, Vqdu b) {return QDU_CGEL(a,b);}
INLINE(Vqdi,VQDI_CGEL) (Vqdi a, Vqdi b) {return QDI_CGEL(a,b);}
INLINE(Vqdi,VQDF_CGEL) (Vqdf a, Vqdf b) {return QDF_CGEL(a,b);}

#if 0 // _LEAVE_ARM_CGEL
}
#endif


#if 0 // _ENTER_ARM_CGEY
{
#endif

INLINE(_Bool,FLT16_CGEY) (flt16_t a, flt16_t b)
{
    return a>=b;
}

INLINE(_Bool,FLT_CGEY) (float a, float b)
{
    return a>=b;
}

INLINE(_Bool,DBL_CGEY) (double a, double b)
{
    return a>=b;
}

INLINE(_Bool,WBU_CGEY) (float a, float b) 
{
    float32x2_t p={a}, q={b};
    uint8x8_t   l = vreinterpret_u8_f32(p);
    uint8x8_t   r = vreinterpret_u8_f32(q);
    uint8x8_t   m = vcge_u8(l, r);
    uint32x2_t  c = vreinterpret_u32_u8(m);
    return  UINT32_MAX==vget_lane_u32(c, 0);
}

INLINE(_Bool,WBI_CGEY) (float a, float b) 
{
    float32x2_t p={a}, q={b};
    int8x8_t    l = vreinterpret_s8_f32(p);
    int8x8_t    r = vreinterpret_s8_f32(q);
    uint8x8_t   m = vcge_s8(l, r);
    uint32x2_t  c = vreinterpret_u32_u8(m);
    return  UINT32_MAX==vget_lane_u32(c, 0);
}

INLINE(_Bool,WHU_CGEY) (float a, float b) 
{
    float32x2_t p={a}, q={b};
    uint16x4_t  l = vreinterpret_u16_f32(p);
    uint16x4_t  r = vreinterpret_u16_f32(q);
    uint16x4_t  m = vcge_u16(l, r);
    uint32x2_t  c = vreinterpret_u32_u16(m);
    return  UINT32_MAX==vget_lane_u32(c, 0);
}

INLINE(_Bool,WHI_CGEY) (float a, float b) 
{
    float32x2_t p={a}, q={b};
    int16x4_t   l = vreinterpret_s16_f32(p);
    int16x4_t   r = vreinterpret_s16_f32(q);
    uint16x4_t  m = vcge_s16(l, r);
    uint32x2_t  c = vreinterpret_u32_u16(m);
    return  UINT32_MAX==vget_lane_u32(c, 0);
}

INLINE(_Bool,WHF_CGEY) (float a, float b) 
{
    float32x2_t p={a}, q={b};
    float16x4_t l = vreinterpret_f16_f32(p);
    float16x4_t r = vreinterpret_f16_f32(q);
    Vdhi        m = VDHF_CGES(l, r);
    uint32x2_t  c = vreinterpret_u32_s16(m);
    return  UINT32_MAX==vget_lane_u32(c, 0);
}

INLINE(_Bool,WWU_CGEY) (float a, float b) 
{
    WORD_TYPE p={.F=a}, q={.F=b};
    return  p.U >= q.U;
}

INLINE(_Bool,WWI_CGEY) (float a, float b) 
{
    WORD_TYPE p={.F=a}, q={.F=b};
    return  p.I >= q.I;
}

INLINE(_Bool,VWBU_CGEY) (Vwbu a, Vwbu b) {return WBU_CGEY(a.V0, b.V0);}         
INLINE(_Bool,VWBI_CGEY) (Vwbi a, Vwbi b) {return WBI_CGEY(a.V0, b.V0);}
INLINE(_Bool,VWBC_CGEY) (Vwbc a, Vwbc b) 
{
#if CHAR_MIN
    return  WBI_CGEY(a.V0, b.V0);
#else
    return  WBU_CGEY(a.V0, b.V0);
#endif
}

INLINE(_Bool,VWHU_CGEY) (Vwhu a, Vwhu b) {return WHU_CGEY(a.V0, b.V0);}
INLINE(_Bool,VWHI_CGEY) (Vwhi a, Vwhi b) {return WHI_CGEY(a.V0, b.V0);}         
INLINE(_Bool,VWHF_CGEY) (Vwhf a, Vwhf b) {return WHF_CGEY(a.V0, b.V0);}
INLINE(_Bool,VWWU_CGEY) (Vwwu a, Vwwu b) {return WWU_CGEY(a.V0, b.V0);}         
INLINE(_Bool,VWWI_CGEY) (Vwwi a, Vwwi b) {return WWI_CGEY(a.V0, b.V0);}         
INLINE(_Bool,VWWF_CGEY) (Vwwf a, Vwwf b) {return a.V0>=b.V0;}

INLINE(_Bool,VDBU_CGEY) (Vdbu a, Vdbu b)
{
    uint8x8_t   u = vcge_u8(a, b);
    uint64x1_t  c = vreinterpret_u64_u8(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDBI_CGEY) (Vdbi a, Vdbi b)
{
    uint8x8_t   u = vcge_s8(a, b);
    uint64x1_t  c = vreinterpret_u64_u8(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDBC_CGEY) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
    return VDBI_CGEY(a.V0, b.V0);
#else
    return VDBU_CGEY(a.V0, b.V0);
#endif
}

INLINE(_Bool,VDHU_CGEY) (Vdhu a, Vdhu b)
{
    uint16x4_t  u = vcge_u16(a, b);
    uint64x1_t  c = vreinterpret_u64_u16(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDHI_CGEY) (Vdhi a, Vdhi b)
{
    uint16x4_t  u = vcge_s16(a, b);
    uint64x1_t  c = vreinterpret_u64_u16(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDHF_CGEY) (Vdhf a, Vdhf b)
{
    int16x4_t   u = VDHF_CGES(a, b);
    uint64x1_t  c = vreinterpret_u64_u16(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDWU_CGEY) (Vdwu a, Vdwu b)
{
    uint32x2_t  u = vcge_u32(a, b);
    uint64x1_t  c = vreinterpret_u64_u32(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDWI_CGEY) (Vdwi a, Vdwi b)
{
    uint32x2_t  u = vcge_s32(a, b);
    uint64x1_t  c = vreinterpret_u64_u32(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDWF_CGEY) (Vdwf a, Vdwf b)
{
    uint32x2_t  u = vcge_f32(a, b);
    uint64x1_t  c = vreinterpret_u64_u32(u);
    return  UINT64_MAX==vget_lane_u64(c, 0);
}

INLINE(_Bool,VDDU_CGEY) (Vddu a, Vddu b)
{
    return vget_lane_u64(a,0) >= vget_lane_u64(b,0);
}

INLINE(_Bool,VDDI_CGEY) (Vddi a, Vddi b)
{
    return vget_lane_s64(a,0) >= vget_lane_s64(b,0);
}

INLINE(_Bool,VDDF_CGEY) (Vddf a, Vddf b)
{
    return vget_lane_f64(a,0) >= vget_lane_f64(b,0);
}

#define     MY_CGEYZ(C) \
(UINT64_MAX==vget_lane_u64(vand_u64(vget_low_u64(C),vget_high_u64(C)),0))

INLINE(_Bool,VQBU_CGEY) (Vqbu a, Vqbu b)
{
    uint8x16_t  u = vcgeq_u8(a, b);
    uint64x2_t  c = vreinterpretq_u64_u8(u);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQBI_CGEY) (Vqbi a, Vqbi b)
{
    uint8x16_t  u = vcgeq_s8(a, b);
    uint64x2_t  c = vreinterpretq_u64_u8(u);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQBC_CGEY) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
    return  VQBI_CGEY(a.V0, b.V0);
#else
    return  VQBU_CGEY(a.V0, b.V0);
#endif
}

INLINE(_Bool,VQHU_CGEY) (Vqhu a, Vqhu b)
{
    uint16x8_t  u = vcgeq_u16(a, b);
    uint64x2_t  c = vreinterpretq_u64_u16(u);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQHI_CGEY) (Vqhi a, Vqhi b)
{
    uint16x8_t  u = vcgeq_s16(a, b);
    uint64x2_t  c = vreinterpretq_u64_u16(u);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQHF_CGEY) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x8_t  u = vcgeq_f16(a, b);
#else
    float32x4_t p, q;
    uint32x4_t  m;
    uint16x4_t  l, r;
    p = vcvt_f32_f16(vget_low_f16(a));
    q = vcvt_f32_f16(vget_low_f16(b));
    m = vcgeq_f32(p, q);
    l = vmovn_u32(m);
    p = vcvt_f32_f16(vget_high_f16(a));
    q = vcvt_f32_f16(vget_high_f16(b));
    m = vcgeq_f32(p, q);
    r = vmovn_u32(m);
    uint16x8_t u = vcombine_u16(l, r);
#endif
    uint64x2_t  c = vreinterpretq_u64_u16(u);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQWU_CGEY) (Vqwu a, Vqwu b)
{
    uint32x4_t  u = vcgeq_u32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQWI_CGEY) (Vqwi a, Vqwi b)
{
    uint32x4_t  u = vcgeq_s32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQWF_CGEY) (Vqwf a, Vqwf b)
{
    uint32x4_t  u = vcgeq_f32(a, b);
    uint64x2_t  c = vreinterpretq_u64_u32(u);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQDU_CGEY) (Vqdu a, Vqdu b)
{
    uint64x2_t  c = vcgeq_u64(a, b);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQDI_CGEY) (Vqdi a, Vqdi b)
{
    uint64x2_t  c = vcgeq_s64(a, b);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQDF_CGEY) (Vqdf a, Vqdf b)
{
    uint64x2_t  c = vcgeq_f64(a, b);
    return  MY_CGEYZ(c);
}

INLINE(_Bool,VQQU_CGEY) (Vqqu a, Vqqu b)
{
    return  
    (vgetq_lane_u64(a.V0,1)==vgetq_lane_u64(b.V0,1))
    ?   (vgetq_lane_u64(a.V0,0)>=vgetq_lane_u64(b.V0,0))
    :   (vgetq_lane_u64(a.V0,1)>=vgetq_lane_u64(b.V0,1));
}

INLINE(_Bool,VQQI_CGEY) (Vqqi a, Vqqi b)
{
    return  
    (vgetq_lane_s64(a.V0,1)==vgetq_lane_s64(b.V0,1))
    ?   (vgetq_lane_s64(a.V0,0)>=vgetq_lane_s64(b.V0,0))
    :   (vgetq_lane_s64(a.V0,1)>=vgetq_lane_s64(b.V0,1));
}
INLINE(_Bool,VQQF_CGEY) (Vqqf a, Vqqf b) {return a.V0 >= b.V0;}

#if 0 // _LEAVE_ARM_CGEY
}
#endif

#if 0 // _ENTER_ARM_ZEQY
{
#endif

INLINE(_Bool,FLT16_ZEQY)    (flt16_t a) {return a == 0.0F16;}
INLINE(_Bool,  FLT_ZEQY)      (float a) {return a == 0.0F;}
INLINE(_Bool,  DBL_ZEQY)     (double a) {return a == 0.0;}
INLINE(_Bool,    zeqyqf) (QUAD_FTYPE a) {return a == 0.0L;}

#define     DYU_ZEQY(A) vreinterpret_u64_u8(vmvn_u8(vreinterpret_u8_u64(A)))
  
#define     DBU_ZEQY(A) vand_u8(vceqz_u8(A),vdup_n_u8(1))
#define     DBI_ZEQY(A) \
vreinterpret_s8_u8(vand_u8(vceqz_s8(A),vdup_n_u8(1)))

#if CHAR_MIN
#   define  DBC_ZEQY    DBI_ZEQY
#else
#   define  DBC_ZEQY    DBU_ZEQY
#endif

#define     DHU_ZEQY(A) vand_u16(vceqz_u16(A),vdup_n_u16(1))
#define     DHI_ZEQY(A) \
vreinterpret_s16_u16(vand_u16(vceqz_s16(A),vdup_n_u16(1)))

#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_ZEQY(A) vand_u16(vceqz_f16(A),vdup_n_u16(1))
#else
#   define  DHF_ZEQY(A)                         \
vreinterpret_s16_u16(                           \
    vand_u16(                                   \
        vorr_u16(                               \
            vceqz_u16(vreinterpret_u16_f16(A)), \
            vceq_u16(                           \
                vreinterpret_u16_f16(A),        \
                vdup_n_u16(0x8000)              \
            )                                   \
        ),                                      \
        vdup_n_u16(1)                           \
    )                                           \
)

#endif

#define     DWU_ZEQY(A) vand_u32(vceqz_u32(A),vdup_n_u32(1))
#define     DWI_ZEQY(A) \
vreinterpret_s32_u32(vand_u32(vceqz_s32(A),vdup_n_u32(1)))

#define     DWF_ZEQY(A) \
vreinterpret_s32_u32(vand_u32(vceqz_f32(A),vdup_n_u32(1)))

#define     DDU_ZEQY(A) vand_u64(vceqz_u64(A),vdup_n_u64(1))
#define     DDI_ZEQY(A) \
vreinterpret_s64_u64(vand_u64(vceqz_s64(A),vdup_n_u64(1)))

#define     DDF_ZEQY(A) \
vreinterpret_s64_u64(vand_u64(vceqz_f64(A),vdup_n_u64(1)))

#define     QYU_ZEQY(A) \
vreinterpretq_u64_u8(vmvnq_u8(vreinterpretq_u8_u64(A)))
  
#define     QBU_ZEQY(A) vandq_u8(vceqzq_u8(A),vdupq_n_u8(1))
#define     QBI_ZEQY(A) \
vreinterpretq_s8_u8(vandq_u8(vceqzq_s8(A),vdupq_n_u8(1)))

#if CHAR_MIN
#   define  QBC_ZEQY    QBI_ZEQY
#else
#   define  QBC_ZEQY    QBU_ZEQY
#endif

#define     QHU_ZEQY(A) vandq_u16(vceqzq_u16(A),vdupq_n_u16(1))
#define     QHI_ZEQY(A) \
vreinterpretq_s16_u16(vandq_u16(vceqzq_s16(A),vdupq_n_u16(1)))

#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_ZEQY(A) \
vreinterpretq_s16_u16(vandq_u16(vceqzq_f16(A),vdupq_n_u16(1)))

#else
#   define  QHF_ZEQY(A)                             \
vreinterpretq_s16_u16(                              \
    vandq_u16(                                      \
        vorrq_u16(                                  \
            vceqzq_u16(vreinterpretq_u16_f16(A)),   \
            vceqq_u16(                              \
                vreinterpretq_u16_f16(A),           \
                vdupq_n_u16(0x8000)                 \
            )                                       \
        ),                                          \
        vdupq_n_u16(1)                              \
    )                                               \
)

#endif

#define     QWU_ZEQY(A) vandq_u32(vceqzq_u32(A),vdupq_n_u32(1))
#define     QWI_ZEQY(A) \
vreinterpretq_s32_u32(vandq_u32(vceqzq_s32(A),vdupq_n_u32(1)))

#define     QWF_ZEQY(A) vandq_u32(vceqzq_f32(A),vdupq_n_u32(1))


#define     QDU_ZEQY(A) vandq_u64(vceqzq_u64(A),vdupq_n_u64(1))
#define     QDI_ZEQY(A) \
vreinterpretq_s64_u64(vandq_u64(vceqzq_s64(A),vdupq_n_u64(1)))

#define     QDF_ZEQY(A) \
vreinterpretq_s64_u64(vandq_u64(vceqzq_f64(A),vdupq_n_u64(1)))

INLINE(Vwyu,VWYU_ZEQY) (Vwyu a)
{
    float       f = VWYU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vmvn_u32(v);
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WYU_ASTV(f);
}

INLINE(Vwbu,VWBU_ZEQY) (Vwbu a)
{
    float       f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_ZEQY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZEQY) (Vwbc a)
{
    float       f = VWBC_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
}


INLINE(Vwhu,VWHU_ZEQY) (Vwhu a)
{
    float       f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vceqz_u16(v);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_ZEQY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vceqz_u16(v);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZEQY) (Vwhf a)
{
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    float16x4_t r = vreinterpret_f16_u16(v);
    v = DHF_ZEQY(v);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_ZEQY) (Vwwu a)
{
    float       f = VWWU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vceqz_u32(v);
    v = vand_u32(v, vdup_n_u32(1));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_ZEQY) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vceqz_u32(v);
    v = vand_u32(v, vdup_n_u32(1));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZEQY) (Vwwf a)
{
    float       f = VWWF_ASTM(a);
    return WWI_ASTV(!f);
}


INLINE(Vdyu,VDYU_ZEQY) (Vdyu a) {return VDBU_ASYU(vmvn_u8(VDYU_ASBU(a)));}
INLINE(Vdbu,VDBU_ZEQY) (Vdbu a) {return DBU_ZEQY(a);}
INLINE(Vdbi,VDBI_ZEQY) (Vdbi a) {return DBI_ZEQY(a);}
INLINE(Vdbc,VDBC_ZEQY) (Vdbc a) {return DBC_ASTV(DBC_ZEQY(VDBC_ASTM(a)));}
INLINE(Vdhu,VDHU_ZEQY) (Vdhu a) {return DHU_ZEQY(a);}
INLINE(Vdhi,VDHI_ZEQY) (Vdhi a) {return DHI_ZEQY(a);}
INLINE(Vdhf,VDHF_ZEQY) (Vdhf a) {return DHF_ZEQY(a);}
INLINE(Vdwu,VDWU_ZEQY) (Vdwu a) {return DWU_ZEQY(a);}
INLINE(Vdwu,VDWI_ZEQY) (Vdwi a) {return DWI_ZEQY(a);}
INLINE(Vdwu,VDWF_ZEQY) (Vdwf a) {return DWF_ZEQY(a);}
INLINE(Vddu,VDDU_ZEQY) (Vddu a) {return DDU_ZEQY(a);}
INLINE(Vddu,VDDI_ZEQY) (Vddi a) {return DDI_ZEQY(a);}
INLINE(Vddu,VDDF_ZEQY) (Vddf a) {return DDF_ZEQY(a);}

INLINE(Vqyu,VQYU_ZEQY) (Vqyu a) {return VQBU_ASYU(vmvnq_u8(VQYU_ASBU(a)));}
INLINE(Vqbu,VQBU_ZEQY) (Vqbu a) {return QBU_ZEQY(a);}
INLINE(Vqbi,VQBI_ZEQY) (Vqbi a) {return QBI_ZEQY(a);}
INLINE(Vqbc,VQBC_ZEQY) (Vqbc a) {return QBC_ASTV(QBC_ZEQY(VQBC_ASTM(a)));}
INLINE(Vqhu,VQHU_ZEQY) (Vqhu a) {return QHU_ZEQY(a);}
INLINE(Vqhi,VQHI_ZEQY) (Vqhi a) {return QHI_ZEQY(a);}
INLINE(Vqhi,VQHF_ZEQY) (Vqhf a) {return QHF_ZEQY(a);}
INLINE(Vqwu,VQWU_ZEQY) (Vqwu a) {return QWU_ZEQY(a);}
INLINE(Vqwi,VQWI_ZEQY) (Vqwi a) {return QWI_ZEQY(a);}
INLINE(Vqwi,VQWF_ZEQY) (Vqwf a) {return QWF_ZEQY(a);}
INLINE(Vqdu,VQDU_ZEQY) (Vqdu a) {return QDU_ZEQY(a);}
INLINE(Vqdi,VQDI_ZEQY) (Vqdi a) {return QDI_ZEQY(a);}
INLINE(Vqdi,VQDF_ZEQY) (Vqdf a) {return QDF_ZEQY(a);}

#if 0 // _LEAVE_ARM_ZEQY
}
#endif

#if 0 // _ENTER_ARM_ZEQS
{
#endif

INLINE(int16_t, FLT16_ZEQS) (flt16_t a) {return -(!a);}
INLINE(int32_t,   FLT_ZEQS)   (float a) {return -(!a);}
INLINE(int64_t,   DBL_ZEQS)  (double a) {return -(!a);}

INLINE(QUAD_ITYPE,zeqsqf) (QUAD_FTYPE a) 
{
    QUAD_TYPE c;
    c.Lo.I = -(!a);
    c.Hi.I = c.Lo.I;
    return c.I;
}


#define     DBU_ZEQS            vceqz_u8
#define     DBI_ZEQS(A)         vreinterpret_s8_u8(vceqz_s8(A))

#if CHAR_MIN
#   define  DBC_ZEQS            DBI_ZEQS
#else
#   define  DBC_ZEQS            DBU_ZEQS
#endif

#define     DHU_ZEQS            vceqz_u16
#define     DHI_ZEQS(A)         vreinterpret_s16_u16(vceqz_s16(A))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_ZEQS(A)         vreinterpret_s16_u16(vceqz_f16(A))
#else
#   define  DHF_ZEQS(A)                     \
vreinterpret_s16_u16(                       \
    vorr_u16(                               \
        vceqz_u16(vreinterpret_u16_f16(A)), \
        vceq_u16(                           \
            vreinterpret_u16_f16(A),        \
            vdup_n_u16(0x8000)              \
        )                                   \
    )                                       \
)

#endif

#define     DWU_ZEQS            vceqz_u32
#define     DWI_ZEQS(A)         vreinterpret_s32_u32(vceqz_s32(A))
#define     DWF_ZEQS(A)         vreinterpret_s32_u32(vceqz_f32(A))

#define     DDU_ZEQS            vceqz_u64
#define     DDI_ZEQS(A)         vreinterpret_s64_u64(vceqz_s64(A))
#define     DDF_ZEQS(A)         vreinterpret_s64_u64(vceqz_f64(A))

#define     QBU_ZEQS            vceqzq_u8
#define     QBI_ZEQS(A)         vreinterpretq_s8_u8(vceqzq_s8(A))
#if CHAR_MIN
#   define  QBC_ZEQS            QBI_ZEQS
#else
#   define  QBC_ZEQS            QBU_ZEQS
#endif

#define     QHU_ZEQS            vceqzq_u16
#define     QHI_ZEQS(A)         vreinterpretq_s16_u16(vceqzq_s16(A))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_ZEQS(A)         vreinterpretq_s16_u16(vceqz_f16(A))
#else
#   define  QHF_ZEQS(A)                         \
vreinterpretq_s16_u16(                          \
    vorrq_u16(                                  \
        vceqzq_u16(vreinterpretq_u16_f16(A)),   \
        vceqq_u16(                              \
            vreinterpretq_u16_f16(A),           \
            vdupq_n_u16(0x8000)                 \
        )                                       \
    )                                           \
)

#endif

#define     QWU_ZEQS            vceqzq_u32
#define     QWI_ZEQS(A)         vreinterpretq_s32_u32(vceqzq_s32(A))
#define     QWF_ZEQS(A)         vreinterpretq_s32_u32(vceqzq_f32(A))

#define     QDU_ZEQS            vceqzq_u64
#define     QDI_ZEQS(A)         vreinterpretq_s64_u64(vceqzq_s64(A))
#define     QDF_ZEQS(A)         vreinterpretq_s64_u64(vceqzq_f64(A))

INLINE(Vwbu,VWBU_ZEQS) (Vwbu a)
{
    float       f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_ZEQS) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZEQS) (Vwbc a)
{
    float       f = VWBC_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
}


INLINE(Vwhu,VWHU_ZEQS) (Vwhu a)
{
    float       f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vceqz_u16(v);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_ZEQS) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vceqz_u16(v);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZEQS) (Vwhf a)
{
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    float16x4_t r = vreinterpret_f16_u16(v);
    v = DHF_ZEQS(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_ZEQS) (Vwwu a)
{
    float       f = VWWU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vceqz_u32(v);
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_ZEQS) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vceqz_u32(v);
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZEQS) (Vwwf a)
{
    float       f = VWWF_ASTM(a);
    return WWI_ASTV(0-(!f));
}


INLINE(Vdbu,VDBU_ZEQS) (Vdbu a) {return DBU_ZEQS(a);}
INLINE(Vdbi,VDBI_ZEQS) (Vdbi a) {return DBI_ZEQS(a);}
INLINE(Vdbc,VDBC_ZEQS) (Vdbc a) {return DBC_ASTV(DBC_ZEQS(VDBC_ASTM(a)));}
INLINE(Vdhu,VDHU_ZEQS) (Vdhu a) {return DHU_ZEQS(a);}
INLINE(Vdhi,VDHI_ZEQS) (Vdhi a) {return DHI_ZEQS(a);}
INLINE(Vdhi,VDHF_ZEQS) (Vdhf a) {return DHF_ZEQS(a);}
INLINE(Vdwu,VDWU_ZEQS) (Vdwu a) {return DWU_ZEQS(a);}
INLINE(Vdwi,VDWI_ZEQS) (Vdwi a) {return DWI_ZEQS(a);}
INLINE(Vdwi,VDWF_ZEQS) (Vdwf a) {return DWF_ZEQS(a);}
INLINE(Vddu,VDDU_ZEQS) (Vddu a) {return DDU_ZEQS(a);}
INLINE(Vddi,VDDI_ZEQS) (Vddi a) {return DDI_ZEQS(a);}
INLINE(Vddi,VDDF_ZEQS) (Vddf a) {return DDF_ZEQS(a);}

INLINE(Vqbu,VQBU_ZEQS) (Vqbu a) {return QBU_ZEQS(a);}
INLINE(Vqbi,VQBI_ZEQS) (Vqbi a) {return QBI_ZEQS(a);}
INLINE(Vqbc,VQBC_ZEQS) (Vqbc a) {return QBC_ASTV(QBC_ZEQS(VQBC_ASTM(a)));}
INLINE(Vqhu,VQHU_ZEQS) (Vqhu a) {return QHU_ZEQS(a);}
INLINE(Vqhi,VQHI_ZEQS) (Vqhi a) {return QHI_ZEQS(a);}
INLINE(Vqhi,VQHF_ZEQS) (Vqhf a) {return QHF_ZEQS(a);}
INLINE(Vqwu,VQWU_ZEQS) (Vqwu a) {return QWU_ZEQS(a);}
INLINE(Vqwi,VQWI_ZEQS) (Vqwi a) {return QWI_ZEQS(a);}
INLINE(Vqwi,VQWF_ZEQS) (Vqwf a) {return QWF_ZEQS(a);}
INLINE(Vqdu,VQDU_ZEQS) (Vqdu a) {return QDU_ZEQS(a);}
INLINE(Vqdi,VQDI_ZEQS) (Vqdi a) {return QDI_ZEQS(a);}
INLINE(Vqdi,VQDF_ZEQS) (Vqdf a) {return QDF_ZEQS(a);}

#if 0 // _LEAVE_ARM_ZEQS
}
#endif


#if 0 // _ENTER_ARM_ZNEY
{
#endif

INLINE(_Bool,FLT16_ZNEY)    (flt16_t a) {return a != 0.0F16;}
INLINE(_Bool,  FLT_ZNEY)      (float a) {return a != 0.0F;}
INLINE(_Bool,  DBL_ZNEY)     (double a) {return a != 0.0;}
INLINE(_Bool,    zneyqf) (QUAD_FTYPE a) {return a != 0.0L;}

#define     DBU_ZNEY(A) vshr_n_u8(vtst_u8(A,vdup_n_u8(UINT8_MAX)),7)
#define     DBI_ZNEY(A) \
vreinterpret_s8_u8(vand_u8(vtst_s8(A,vdup_n_s8(-1)),vdup_n_u8(1)))

#if CHAR_MIN
#   define  DBC_ZNEY    DBI_ZNEY
#else
#   define  DBC_ZNEY    DBU_ZNEY
#endif

#define     DHU_ZNEY(A)  \
vand_u16(vtst_u16(A,vdup_n_u16(UINT16_MAX)),vdup_n_u16(1))

#define     DHI_ZNEY(A) \
vreinterpret_s16_u16(vand_u16(vtst_s16(A,vdup_n_s16(-1)),vdup_n_u16(1)))

#define     DHF_ZNEY(A)                         \
vreinterpret_s16_u16(                           \
   vand_u16(                                    \
        vorr_u16(                               \
            vtst_u16(                           \
                vreinterpret_u16_f16(A),        \
                vdup_n_u16(UINT16_C(0x7fff))    \
            ),                                  \
            vceq_u16(                           \
                vreinterpret_u16_f16(A),        \
                vdup_n_u16(0x8000)              \
            )                                   \
        ),                                      \
        vdup_n_u16(1)                           \
    )                                           \
)


#define     DWU_ZNEY(A) \
vand_u32(vtst_u32(A,vdup_n_u32(UINT32_MAX)),vdup_n_u32(1))

#define     DWI_ZNEY(A) \
vreinterpret_s32_u32(vand_u32(vtst_s32(A,vdup_n_s32(-1)),vdup_n_u32(1)))

#define     DWF_ZNEY(A) \
vreinterpret_s32_u32(vand_u32(vmvn_u32(vceqz_f32(A)),vdup_n_u32(1)))

#define     DDU_ZNEY(A) \
vand_u64(vtst_u64(A,vdup_n_u64(UINT64_MAX)),vdup_n_u64(1))

#define     DDI_ZNEY(A) \
vreinterpret_s64_u64(vand_u64(vtst_s64(A,vdup_n_s64(-1)),vdup_n_u64(1)))

#define     DDF_ZNEY(A) \
vreinterpret_s64_u8(\
    vmvn_u8(vreinterpret_u8_u64(vand_u64(vceqz_f64(A),vdup_n_u64(1))))   \
)


#define     QBU_ZNEY(A) \
vandq_u8(vtstq_u8(A,vdupq_n_u8(UINT8_MAX)),vdupq_n_u8(1))

#define     QBI_ZNEY(A) \
vreinterpretq_s8_u8(vandq_u8(vtstq_s8(A,vdupq_n_s8(-1)),vdupq_n_u8(1)))

#if CHAR_MIN
#   define  QBC_ZNEY    QBI_ZNEY
#else
#   define  QBC_ZNEY    QBU_ZNEY
#endif

#define     QHU_ZNEY(A) \
vandq_u16(vtstq_u16(A,vdupq_n_u16(UINT16_MAX)),vdupq_n_u16(1))

#define     QHI_ZNEY(A) \
vreinterpretq_s16_u16(vandq_u16(vtstq_s16(A,vdupq_n_s16(-1)),vdupq_n_u16(1)))

#define     QHF_ZNEY(A)                         \
vreinterpretq_s16_u16(                          \
   vshrq_n_u16(                                 \
        vorrq_u16(                              \
            vtstq_u16(                          \
                vreinterpretq_u16_f16(A),       \
                vdupq_n_u16(UINT16_C(0x7fff))   \
            ),                                  \
            vceqq_u16(                          \
                vreinterpretq_u16_f16(A),       \
                vdupq_n_u16(0x8000)             \
            )                                   \
        ),                                      \
        15                                      \
    )                                           \
)

#define     QWU_ZNEY(A) \
vandq_u32(vtstq_u32(A,vdupq_n_u32(UINT32_MAX)),vdupq_n_u32(1))

#define     QWI_ZNEY(A) \
vreinterpretq_s32_u32(vandq_u32(vtstq_s32(A,vdupq_n_s32(-1)),vdupq_n_u32(1)))

#define     QWF_ZNEY(A) \
vreinterpretq_s32_u32(vandq_u32(vmvnq_u32(vceqzq_f32(A)),vdupq_n_u32(1)))


#define     QDU_ZNEY(A) vshrq_n_u64(vtstq_u64(A,vdupq_n_u64(UINT64_MAX)),63)
#define     QDI_ZNEY(A) \
vreinterpretq_s64_u64(vshrq_n_u64(vtstq_s64(A,vdupq_n_s64(-1)),63))

#define     QDF_ZNEY(A) \
vreinterpretq_s64_u8(\
    vmvnq_u8(vreinterpretq_u8_u64(vandq_u64(vceqzq_f64(A),vdupq_n_u64(1))))   \
)

INLINE(Vwyu,VWYU_ZNEY) (Vwyu a) {return a;}

INLINE(Vwbu,VWBU_ZNEY) (Vwbu a)
{
    float       f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vtst_u8(v, vdup_n_u8(UINT8_MAX));
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_ZNEY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vtst_u8(v, vdup_n_u8(UINT8_MAX));
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZNEY) (Vwbc a)
{
    return  VWBU_ASBC(VWBU_ZNEY(VWBC_ASBU(a)));
}


INLINE(Vwhu,VWHU_ZNEY) (Vwhu a)
{
    float       f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vtst_u16(v, vdup_n_u16(UINT16_MAX));
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_ZNEY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vtst_u16(v, vdup_n_u16(UINT16_MAX));
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZNEY) (Vwhf a)
{
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vorr_u16(
        vceqz_u16(v),
        vceq_u16(v, vdup_n_u16(UINT16_C(0x8000)))
    );
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_ZNEY) (Vwwu a)
{
    float       f = VWWU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vtst_u32(v, vdup_n_u32(UINT32_MAX));
    v = vand_u32(v, vdup_n_u32(1));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_ZNEY) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vtst_u32(v, vdup_n_u32(UINT32_MAX));
    v = vand_u32(v, vdup_n_u32(1));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZNEY) (Vwwf a)
{
    return VWWF_ASTM(a) ? WWI_ASTV(0) : WWI_ASTV(1);
}



INLINE(Vdyu,VDYU_ZNEY) (Vdyu a) {return a;}
INLINE(Vdbu,VDBU_ZNEY) (Vdbu a) {return DBU_ZNEY(a);}
INLINE(Vdbi,VDBI_ZNEY) (Vdbi a) {return DBI_ZNEY(a);}
INLINE(Vdbc,VDBC_ZNEY) (Vdbc a) {return DBC_ASTV(DBC_ZNEY(VDBC_ASTM(a)));}
INLINE(Vdhu,VDHU_ZNEY) (Vdhu a) {return DHU_ZNEY(a);}
INLINE(Vdhi,VDHI_ZNEY) (Vdhi a) {return DHI_ZNEY(a);}
INLINE(Vdhi,VDHF_ZNEY) (Vdhf a) {return DHF_ZNEY(a);}
INLINE(Vdwu,VDWU_ZNEY) (Vdwu a) {return DWU_ZNEY(a);}
INLINE(Vdwi,VDWI_ZNEY) (Vdwi a) {return DWI_ZNEY(a);}
INLINE(Vdwi,VDWF_ZNEY) (Vdwf a) {return DWF_ZNEY(a);}
INLINE(Vddu,VDDU_ZNEY) (Vddu a) {return DDU_ZNEY(a);}
INLINE(Vddi,VDDI_ZNEY) (Vddi a) {return DDI_ZNEY(a);}
INLINE(Vddi,VDDF_ZNEY) (Vddf a) {return DDF_ZNEY(a);}

INLINE(Vqbu,VQBU_ZNEY) (Vqbu a) {return QBU_ZNEY(a);}
INLINE(Vqbi,VQBI_ZNEY) (Vqbi a) {return QBI_ZNEY(a);}
INLINE(Vqbc,VQBC_ZNEY) (Vqbc a) {return QBC_ASTV(QBC_ZNEY(VQBC_ASTM(a)));}
INLINE(Vqhu,VQHU_ZNEY) (Vqhu a) {return QHU_ZNEY(a);}
INLINE(Vqhi,VQHI_ZNEY) (Vqhi a) {return QHI_ZNEY(a);}
INLINE(Vqhi,VQHF_ZNEY) (Vqhf a) {return QHF_ZNEY(a);}
INLINE(Vqwu,VQWU_ZNEY) (Vqwu a) {return QWU_ZNEY(a);}
INLINE(Vqwi,VQWI_ZNEY) (Vqwi a) {return QWI_ZNEY(a);}
INLINE(Vqwi,VQWF_ZNEY) (Vqwf a) {return QWF_ZNEY(a);}
INLINE(Vqdu,VQDU_ZNEY) (Vqdu a) {return QDU_ZNEY(a);}
INLINE(Vqdi,VQDI_ZNEY) (Vqdi a) {return QDI_ZNEY(a);}
INLINE(Vqdi,VQDF_ZNEY) (Vqdf a) {return QDF_ZNEY(a);}

#if 0 // _LEAVE_ARM_ZNEY
}
#endif

#if 0 // _ENTER_ARM_ZNES
{
#endif

INLINE(int16_t, FLT16_ZNES) (flt16_t a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  -vceqzh_f16(a);
#else
    return  -(a!=0.0f16);
#endif
}

INLINE(int32_t,   FLT_ZNES)   (float a) {return -(a!=0.0f);}
INLINE(int64_t,   DBL_ZNES)  (double a) {return -(a!=0.0);}

INLINE(QUAD_ITYPE,znesqf) (QUAD_FTYPE a)
{
    QUAD_TYPE c;
    c.Lo.I = -(0.0L != a);
    c.Hi.I = c.Lo.I;
    return c.I;
}



INLINE(Vwbu,VWBU_ZNES) (Vwbu a)
{
    float       f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vtst_u8(v, vdup_n_u8(UINT8_MAX));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_ZNES) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vtst_u8(v, vdup_n_u8(UINT8_MAX));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZNES) (Vwbc a)
{
    return  VWBU_ASBC(VWBU_ZNES(VWBC_ASBU(a)));
}


INLINE(Vwhu,VWHU_ZNES) (Vwhu a)
{
    float       f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vtst_u16(v, vdup_n_u16(UINT16_MAX));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_ZNES) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vtst_u16(v, vdup_n_u16(UINT16_MAX));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZNES) (Vwhf a)
{
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vorr_u16(
        vceqz_u16(v),
        vceq_u16(v, vdup_n_u16(UINT16_C(0x8000)))
    );
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_ZNES) (Vwwu a)
{
    float       f = VWWU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vtst_u32(v, vdup_n_u32(UINT32_MAX));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_ZNES) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vtst_u32(v, vdup_n_u32(UINT32_MAX));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZNES) (Vwwf a)
{
    return VWWF_ASTM(a) ? WWI_ASTV(0) : WWI_ASTV(-1);
}


#define     DBU_ZNES(A)                    vtst_u8(A,vdup_n_u8(UINT8_MAX))
#define     DBI_ZNES(A) vreinterpret_s8_u8(vtst_s8(A,vdup_n_s8(-1)))
#if CHAR_MIN
#   define  DBC_ZNES    DBI_ZNES
#else
#   define  DBC_ZNES    DBU_ZNES
#endif

#define     DHU_ZNES(A)                      vtst_u16(A,vdup_n_u16(UINT16_MAX))
#define     DHI_ZNES(A) vreinterpret_s16_u16(vtst_s16(A,vdup_n_s16(-1)))
#define     DHF_ZNES(A)                     \
vreinterpret_s16_u16(                       \
    vorr_u16(                               \
        vtst_u16(                           \
            vreinterpret_u16_f16(A),        \
            vdup_n_u16(UINT16_C(0x7fff))    \
        ),                                  \
        vceq_u16(                           \
            vreinterpret_u16_f16(A),        \
            vdup_n_u16(0x8000)              \
        )                                   \
    )                                       \
)


#define     DWU_ZNES(A)                      vtst_u32(A,vdup_n_u32(UINT32_MAX))
#define     DWI_ZNES(A) vreinterpret_s32_u32(vtst_s32(A,vdup_n_s32(-1)))
#define     DWF_ZNES(A) vmvn_s32(vreinterpret_s32_u32(vceqz_f32(A)))

#define     DDU_ZNES(A)                      vtst_u64(A,vdup_n_u64(UINT64_MAX))
#define     DDI_ZNES(A) vreinterpret_s64_u64(vtst_s64(A,vdup_n_s64(-1)))
#define     DDF_ZNES(A) \
vreinterpret_f64_u8(vmvn_u8(vreinterpret_u8_u64(vceqz_f64(A))))

#define     QBU_ZNES(A) vtstq_u8(A,vdupq_n_u8(UINT8_MAX))
#define     QBI_ZNES(A) vreinterpretq_s8_u8(vtstq_s8(A,vdupq_n_s8(-1)))
#if CHAR_MIN
#   define  QBC_ZNES    QBI_ZNES
#else
#   define  QBC_ZNES    QBU_ZNES
#endif

#define     QHU_ZNES(A) vtstq_u16(A,vdupq_n_u16(UINT16_MAX))
#define     QHI_ZNES(A) vreinterpretq_s16_u16(vtstq_s16(A,vdupq_n_s16(-1)))
#define     QHF_ZNES(A)                     \
vreinterpretq_s16_u16(                      \
    vorrq_u16(                              \
        vtstq_u16(                          \
            vreinterpretq_u16_f16(A),       \
            vdupq_n_u16(UINT16_C(0x7fff))   \
        ),                                  \
        vceqq_u16(                          \
            vreinterpretq_u16_f16(A),       \
            vdupq_n_u16(0x8000)             \
        )                                   \
    )                                       \
)


#define     QWU_ZNES(A) vtstq_u32(A,vdupq_n_u32(UINT32_MAX))
#define     QWI_ZNES(A) vreinterpretq_s32_u32(vtstq_s32(A,vdupq_n_s32(-1)))
#define     QWF_ZNES(A) vmvnq_s32(vreinterpretq_s32_u32(vceqzq_f32(A)))

#define     QDU_ZNES(A) vtstq_u64(A,vdupq_n_u64(UINT64_MAX))
#define     QDI_ZNES(A) vreinterpretq_s64_u64(vtstq_s64(A,vdupq_n_s64(-1)))
#define     QDF_ZNES(A) \
vreinterpretq_f64_u8(vmvnq_u8(vreinterpretq_u8_u64(vceqzq_f64(A))))


INLINE(Vdbu,VDBU_ZNES) (Vdbu a) {return DBU_ZNES(a);}
INLINE(Vdbi,VDBI_ZNES) (Vdbi a) {return DBI_ZNES(a);}
INLINE(Vdbc,VDBC_ZNES) (Vdbc a) {return DBC_ASTV(DBC_ZNES(VDBC_ASTM(a)));}
INLINE(Vdhu,VDHU_ZNES) (Vdhu a) {return DHU_ZNES(a);}
INLINE(Vdhi,VDHI_ZNES) (Vdhi a) {return DHI_ZNES(a);}
INLINE(Vdhi,VDHF_ZNES) (Vdhf a) {return DHF_ZNES(a);}
INLINE(Vdwu,VDWU_ZNES) (Vdwu a) {return DWU_ZNES(a);}
INLINE(Vdwi,VDWI_ZNES) (Vdwi a) {return DWI_ZNES(a);}
INLINE(Vdwi,VDWF_ZNES) (Vdwf a) {return DWF_ZNES(a);}
INLINE(Vddu,VDDU_ZNES) (Vddu a) {return DDU_ZNES(a);}
INLINE(Vddi,VDDI_ZNES) (Vddi a) {return DDI_ZNES(a);}
INLINE(Vddi,VDDF_ZNES) (Vddf a) {return DDF_ZNES(a);}

INLINE(Vqbu,VQBU_ZNES) (Vqbu a) {return QBU_ZNES(a);}
INLINE(Vqbi,VQBI_ZNES) (Vqbi a) {return QBI_ZNES(a);}
INLINE(Vqbc,VQBC_ZNES) (Vqbc a) {return QBC_ASTV(QBC_ZNES(VQBC_ASTM(a)));}
INLINE(Vqhu,VQHU_ZNES) (Vqhu a) {return QHU_ZNES(a);}
INLINE(Vqhi,VQHI_ZNES) (Vqhi a) {return QHI_ZNES(a);}
INLINE(Vqhi,VQHF_ZNES) (Vqhf a) {return QHF_ZNES(a);}
INLINE(Vqwu,VQWU_ZNES) (Vqwu a) {return QWU_ZNES(a);}
INLINE(Vqwi,VQWI_ZNES) (Vqwi a) {return QWI_ZNES(a);}
INLINE(Vqwi,VQWF_ZNES) (Vqwf a) {return QWF_ZNES(a);}
INLINE(Vqdu,VQDU_ZNES) (Vqdu a) {return QDU_ZNES(a);}
INLINE(Vqdi,VQDI_ZNES) (Vqdi a) {return QDI_ZNES(a);}
INLINE(Vqdi,VQDF_ZNES) (Vqdf a) {return QDF_ZNES(a);}

#if 0 // _LEAVE_ARM_ZNES
}
#endif

#if 0 // _ENTER_ARM_ZLTY
{
#endif

INLINE(_Bool,FLT16_ZLTY)    (flt16_t a) {return 0.0F16< a;}
INLINE(_Bool,  FLT_ZLTY)      (float a) {return 0.0F  < a;}
INLINE(_Bool,  DBL_ZLTY)     (double a) {return 0.0   < a;}
INLINE(_Bool,    zltyqf) (QUAD_FTYPE a) {return 0.0L  < a;}

INLINE(Vwbu,VWBU_ZLTY) (Vwbu a)
{
    float       m = VWBU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint8x8_t   z = vreinterpret_u8_f32(f);
    z = vclt_u8(vdup_n_u8(0), z);
    z = vand_u8(vdup_n_u8(1), z);
    f = vreinterpret_f32_u8(z);
    m = vget_lane_f32(f, 0);
    return  WBU_ASTV(m);
}


INLINE(Vwbi,VWBI_ZLTY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcgtz_s8(v);
    u = vand_u8(u, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZLTY) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZLTY(VWBC_ASBI(a)));
#else
    return  VWBU_ASBC(VWBU_ZLTY(VWBC_ASBU(a)));
#endif
}


INLINE(Vwhu,VWHU_ZLTY) (Vwhu a)
{
    float       m = VWHU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint16x4_t  z = vreinterpret_u16_f32(f);
    z = vclt_u16(vdup_n_u16(0), z);
    z = vand_u16(vdup_n_u16(1), z);
    f = vreinterpret_f32_u16(z);
    m = vget_lane_f32(f, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_ZLTY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcgtz_s16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZLTY) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcgtz_f16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcgtzq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}


INLINE(Vwwu,VWWU_ZLTY) (Vwwu a)
{
    return  UINT32_ASTV((0u < VWWU_ASTV(a)));
}

INLINE(Vwwi,VWWI_ZLTY) (Vwwi a)
{
    return  INT32_ASTV((0 < VWWI_ASTV(a)));
}

INLINE(Vwwi,VWWF_ZLTY) (Vwwf a)
{
    return  INT32_ASTV((0.0f < VWWF_ASTV(a)));
}


INLINE(Vdbu,VDBU_ZLTY) (Vdbu a) 
{
    return  vand_u8(
        vclt_u8(vdup_n_u8(0), a), 
        vdup_n_u8(1)
    );
}

INLINE(Vdbi,VDBI_ZLTY) (Vdbi a) 
{
    return  vreinterpret_s8_u8(
        vand_u8(vdup_n_u8(1), vcgtz_s8(a))
    );
}

INLINE(Vdbc,VDBC_ZLTY) (Vdbc a)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_ZLTY(VDBC_ASBI(a)));
#else
    return  VDBU_ASBC(VDBU_ZLTY(VDBC_ASBU(a)));
#endif
}


INLINE(Vdhu,VDHU_ZLTY) (Vdhu a) 
{
    return  vand_u16(
        vclt_u16(vdup_n_u16(0), a), 
        vdup_n_u16(1)
    );
}

INLINE(Vdhi,VDHI_ZLTY) (Vdhi a) 
{
    return  vreinterpret_s16_u16(
        vand_u16(vdup_n_u16(1), vcgtz_s16(a))
    );
}

INLINE(Vdhi,VDHF_ZLTY) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vand_u16(vcgtz_f16(a),vdup_n_u16(1)));
#else
    uint32x4_t q = vcgtzq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vand_u16(vmovn_u32(q), vdup_n_u16(1)));
#endif
}


INLINE(Vdwu,VDWU_ZLTY) (Vdwu a) 
{
    return  vand_u32(
        vclt_u32(vdup_n_u32(0), a), 
        vdup_n_u32(1)
    );
}

INLINE(Vdwi,VDWI_ZLTY) (Vdwi a) 
{
    return vreinterpret_s32_u32(vand_u32(vcgtz_s32(a), vdup_n_u32(1)));
}

INLINE(Vdwi,VDWF_ZLTY) (Vdwf a) 
{
    return vreinterpret_s32_u32(vand_u32(vcgtz_f32(a), vdup_n_u32(1)));
}



INLINE(Vddu,VDDU_ZLTY) (Vddu a) 
{
    return  vand_u64(
        vclt_u64(vdup_n_u64(0), a), 
        vdup_n_u64(1)
    );
}

INLINE(Vddi,VDDI_ZLTY) (Vddi a) 
{
    return vreinterpret_s64_u64(vand_u64(vcgtz_s64(a), vdup_n_u64(1)));
}

INLINE(Vddi,VDDF_ZLTY) (Vddf a) 
{
    return vreinterpret_s64_u64(vand_u64(vcgtz_f64(a), vdup_n_u64(1)));
}


INLINE(Vqbu,VQBU_ZLTY) (Vqbu a) 
{
    return  vandq_u8(
        vcltq_u8(vdupq_n_u8(0), a), 
        vdupq_n_u8(1)
    );
}

INLINE(Vqbi,VQBI_ZLTY) (Vqbi a) 
{
    return vreinterpretq_s8_u8(vandq_u8(vcgtzq_s8(a), vdupq_n_u8(1)));
}

INLINE(Vqbc,VQBC_ZLTY) (Vqbc a)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_ZLTY(VQBC_ASBI(a)));
#else
    return  VQBU_ASBC(VQBU_ZLTY(VQBC_ASBU(a)));
#endif
}


INLINE(Vqhu,VQHU_ZLTY) (Vqhu a) 
{
    return  vandq_u16(
        vcltq_u16(vdupq_n_u16(0), a), 
        vdupq_n_u16(1)
    );
}


INLINE(Vqhi,VQHI_ZLTY) (Vqhi a) 
{
    return vreinterpretq_s16_u16(vandq_u16(vcgtzq_s16(a), vdupq_n_u16(1)));
}

INLINE(Vqhi,VQHF_ZLTY) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vandq_u16(vcgtzq_f16(a), vdupq_n_u16(1)));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcgtzq_f32(wl);
    uint32x4_t  ur = vcgtzq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    uint16x8_t  qq = vcombine_u16(vl, vr);
    qq = vandq_u16(qq, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(qq);
#endif
}


INLINE(Vqwu,VQWU_ZLTY) (Vqwu a) 
{
    return  vandq_u32(
        vcltq_u32(vdupq_n_u32(0), a), 
        vdupq_n_u32(1)
    );
}

INLINE(Vqwi,VQWI_ZLTY) (Vqwi a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcgtzq_s32(a), vdupq_n_u32(1)));
}

INLINE(Vqwi,VQWF_ZLTY) (Vqwf a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcgtzq_f32(a), vdupq_n_u32(1)));
}


INLINE(Vqdu,VQDU_ZLTY) (Vqdu a) 
{
    return  vandq_u64(
        vcltq_u64(vdupq_n_u64(0), a), 
        vdupq_n_u64(1)
    );
}

INLINE(Vqdi,VQDI_ZLTY) (Vqdi a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcgtzq_s64(a), vdupq_n_u64(1)));
}

INLINE(Vqdi,VQDF_ZLTY) (Vqdf a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcgtzq_f64(a), vdupq_n_u64(1)));
}

#if 0 // _LEAVE_ARM_ZLTY
}
#endif

#if 0 // _ENTER_ARM_ZLTS
{
#endif

INLINE(int16_t, FLT16_ZLTS) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  vcgtzh_f16(a);
#else
    return  0.0f16 < a ? -1 : 0;
#endif
}

INLINE(int32_t,   FLT_ZLTS)   (float a) {return vcgtzs_f32(a);}
INLINE(int64_t,   DBL_ZLTS)  (double a) {return vcgtzd_f64(a);}

INLINE(QUAD_ITYPE,zltsqf) (QUAD_FTYPE a) 
{
    QUAD_TYPE c;
    c.Lo.I = -(0 < a);
    c.Hi.I = c.Lo.I;
    return c.I;
}

INLINE(Vwbu,VWBU_ZLTS) (Vwbu a)
{
    float       m = VWBU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint8x8_t   z = vreinterpret_u8_f32(f);
    z = vclt_u8(vdup_n_u8(0), z);
    f = vreinterpret_f32_u8(z);
    m = vget_lane_f32(f, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbi,VWBI_ZLTS) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcgtz_s8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZLTS) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZLTS(VWBC_ASBI(a)));
#else
    return  VWBU_ASBC(VWBU_ZLTS(VWBC_ASBU(a)));
#endif
}


INLINE(Vwhu,VWHU_ZLTS) (Vwhu a)
{
    float       m = VWHU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint16x4_t  z = vreinterpret_u16_f32(f);
    z = vclt_u16(vdup_n_u16(0), z);
    f = vreinterpret_f32_u16(z);
    m = vget_lane_f32(f, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_ZLTS) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcgtz_s16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZLTS) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcgtz_f16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcgtzq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}


INLINE(Vwwu,VWWU_ZLTS) (Vwwu a)
{
    return  UINT32_ASTV((0u < VWWU_ASTV(a) ? -1 : 0));
}

INLINE(Vwwi,VWWI_ZLTS) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vcgtz_s32(v);
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZLTS) (Vwwf a)
{
    return  WWI_ASTV(0.0f < VWWF_ASTM(a) ? -1 : 0);
}


INLINE(Vdbu,VDBU_ZLTS) (Vdbu a) {return  vclt_u8(vdup_n_u8(0), a);}
INLINE(Vdbi,VDBI_ZLTS) (Vdbi a) {return vreinterpret_s8_u8(vcgtz_s8(a));}
INLINE(Vdbc,VDBC_ZLTS) (Vdbc a)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_ZLTS(VDBC_ASBI(a)));
#else
    return  VDBU_ASBC(VDBU_ZLTS(VDBC_ASBU(a)));
#endif
}

INLINE(Vdhu,VDHU_ZLTS) (Vdhu a) {return  vclt_u16(vdup_n_u16(0), a);}
INLINE(Vdhi,VDHI_ZLTS) (Vdhi a) {return vreinterpret_s16_u16(vcgtz_s16(a));}
INLINE(Vdhi,VDHF_ZLTS) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vcgtz_f16(a));
#else
    uint32x4_t q = vcgtzq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vmovn_u32(q));
#endif
}

INLINE(Vdwu,VDWU_ZLTS) (Vdwu a) {return  vclt_u32(vdup_n_u32(0), a);}
INLINE(Vdwi,VDWI_ZLTS) (Vdwi a) {return vreinterpret_s32_u32(vcgtz_s32(a));}            
INLINE(Vdwi,VDWF_ZLTS) (Vdwf a) {return vreinterpret_s32_u32(vcgtz_f32(a));}

INLINE(Vddu,VDDU_ZLTS) (Vddu a) {return  vclt_u64(vdup_n_u64(0), a);}
INLINE(Vddi,VDDI_ZLTS) (Vddi a) {return vreinterpret_s64_u64(vcgtz_s64(a));}            
INLINE(Vddi,VDDF_ZLTS) (Vddf a) {return vreinterpret_s64_u64(vcgtz_f64(a));}

INLINE(Vqbu,VQBU_ZLTS) (Vqbu a) {return  vcltq_u8(vdupq_n_u8(0), a);}
INLINE(Vqbi,VQBI_ZLTS) (Vqbi a) {return vreinterpretq_u8_s8(vcgtzq_s8(a));}
INLINE(Vqbc,VQBC_ZLTS) (Vqbc a)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_ZLTS(VQBC_ASBI(a)));
#else
    return  VQBU_ASBC(VQBU_ZLTS(VQBC_ASBU(a)));
#endif
}

INLINE(Vqhu,VQHU_ZLTS) (Vqhu a) {return  vcltq_u16(vdupq_n_u16(0), a);}
INLINE(Vqhi,VQHI_ZLTS) (Vqhi a) {return vreinterpretq_s16_u16(vcgtzq_s16(a));}
INLINE(Vqhi,VQHF_ZLTS) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vcgtzq_f16(a));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcgtzq_f32(wl);
    uint32x4_t  ur = vcgtzq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    return  vreinterpretq_s16_u16(vcombine_u16(vl, vr));
#endif
}

INLINE(Vqwu,VQWU_ZLTS) (Vqwu a) {return  vcltq_u32(vdupq_n_u32(0), a);}
INLINE(Vqwi,VQWI_ZLTS) (Vqwi a) {return vreinterpretq_s32_u32(vcgtzq_s32(a));}            
INLINE(Vqwi,VQWF_ZLTS) (Vqwf a) {return vreinterpretq_s32_u32(vcgtzq_f32(a));}

INLINE(Vqdu,VQDU_ZLTS) (Vqdu a) {return  vcltq_u64(vdupq_n_u64(0), a);}
INLINE(Vqdi,VQDI_ZLTS) (Vqdi a) {return vreinterpretq_s64_u64(vcgtzq_s64(a));}            
INLINE(Vqdi,VQDF_ZLTS) (Vqdf a) {return vreinterpretq_s64_u64(vcgtzq_f64(a));}

#if 0 // _LEAVE_ARM_ZLTS
}
#endif

#if 0 // _ENTER_ARM_ZLEY
{
#endif

INLINE(_Bool,FLT16_ZLEY)    (flt16_t a) {return 0.0F16 <= a;}
INLINE(_Bool,  FLT_ZLEY)      (float a) {return 0.0F   <= a;}
INLINE(_Bool,  DBL_ZLEY)     (double a) {return 0.0    <= a;}
INLINE(_Bool,    zleyqf) (QUAD_FTYPE a) {return 0.0L   <= a;}

INLINE(Vwbi,VWBI_ZLEY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcgez_s8(v);
    u = vand_u8(u, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZLEY) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZLEY(VWBC_ASBI(a)));
#else
    uint8x8_t v = vdup_n_u8(1);
    float32x2_t m = vreinterpret_f32_u8(v);
    float f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
#endif
}

INLINE(Vwhi,VWHI_ZLEY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcgez_s16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZLEY) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcgez_f16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcgezq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}

INLINE(Vwwi,VWWI_ZLEY) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vcgez_s32(v);
    r = vand_u32(r, vdup_n_u32(1));
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZLEY) (Vwwf a)
{
    return  WWI_ASTV(0.0f <= VWWF_ASTM(a));
}

INLINE(Vdbi,VDBI_ZLEY) (Vdbi a) 
{
    return vreinterpret_s8_u8(vand_u8(vcgez_s8(a), vdup_n_u8(1)));
}

INLINE(Vdbc,VDBC_ZLEY) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vand_u8(vcgez_s8(VDBC_ASBI(a)), vdup_n_u8(1)));
#else
    return  VDBU_ASBC(vdup_n_u8(1));
#endif
}


INLINE(Vdhi,VDHI_ZLEY) (Vdhi a) 
{
    return vreinterpret_s16_u16(vand_u16(vcgez_s16(a), vdup_n_u16(1)));
}

INLINE(Vdhi,VDHF_ZLEY) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vand_u16(vcgez_f16(a),vdup_n_u16(1)));
#else
    uint32x4_t q = vcgezq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vand_u16(vmovn_u32(q), vdup_n_u16(1)));
#endif
}


INLINE(Vdwi,VDWI_ZLEY) (Vdwi a) 
{
    return vreinterpret_s32_u32(vand_u32(vcgez_s32(a), vdup_n_u32(1)));
}

INLINE(Vdwi,VDWF_ZLEY) (Vdwf a) 
{
    return vreinterpret_s32_u32(vand_u32(vcgez_f32(a), vdup_n_u32(1)));
}


INLINE(Vddi,VDDI_ZLEY) (Vddi a) 
{
    return vreinterpret_s64_u64(vand_u64(vcgez_s64(a), vdup_n_u64(1)));
}

INLINE(Vddi,VDDF_ZLEY) (Vddf a) 
{
    return vreinterpret_s64_u64(vand_u64(vcgez_f64(a), vdup_n_u64(1)));
}


INLINE(Vqbi,VQBI_ZLEY) (Vqbi a) 
{
    return vreinterpretq_s8_u8(vandq_u8(vcgezq_s8(a), vdupq_n_u8(1)));
}

INLINE(Vqbc,VQBC_ZLEY) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vandq_u8(vcgezq_s8(VQBC_ASBI(a)), vdupq_n_u8(1)));
#else
    return  VQBU_ASBC(vdupq_n_u8(1));
#endif
}


INLINE(Vqhi,VQHI_ZLEY) (Vqhi a) 
{
    return vreinterpretq_s16_u16(vandq_u16(vcgezq_s16(a), vdupq_n_u16(1)));
}

INLINE(Vqhi,VQHF_ZLEY) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vandq_u16(vcgezq_f16(a), vdupq_n_u16(1)));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcgezq_f32(wl);
    uint32x4_t  ur = vcgezq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    uint16x8_t  qq = vcombine_u16(vl, vr);
    qq = vandq_u16(qq, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(qq);
#endif
}


INLINE(Vqwi,VQWI_ZLEY) (Vqwi a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcgezq_s32(a), vdupq_n_u32(1)));
}

INLINE(Vqwi,VQWF_ZLEY) (Vqwf a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcgezq_f32(a), vdupq_n_u32(1)));
}


INLINE(Vqdi,VQDI_ZLEY) (Vqdi a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcgezq_s64(a), vdupq_n_u64(1)));
}

INLINE(Vqdi,VQDF_ZLEY) (Vqdf a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcgezq_f64(a), vdupq_n_u64(1)));
}


#if 0 // _LEAVE_ARM_ZLEY
}
#endif

#if 0 // _ENTER_ARM_ZLES
{
#endif

INLINE(int16_t, FLT16_ZLES) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  vcgezh_f16(a);
#else
    return  0.0f16 <= a ? -1 : 0;
#endif
}

INLINE(int32_t,   FLT_ZLES)   (float a) {return vcgezs_f32(a);}
INLINE(int64_t,   DBL_ZLES)  (double a) {return vcgezd_f64(a);}

INLINE(QUAD_ITYPE,zlesqf) (QUAD_FTYPE a) {return 0 <= a ? -1 : 0;}

INLINE(Vwbi,VWBI_ZLES) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcgez_s8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZLES) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZLES(VWBC_ASBI(a)));
#else
    uint8x8_t   r = vdup_n_u8(UINT8_MAX);
    float32x2_t m = vreinterpret_f32_u8(r);
    float       f = vget_lane_f32(m, 0);
    return WBC_ASTV(f);
#endif
}

INLINE(Vwhi,VWHI_ZLES) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcgez_s16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZLES) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcgez_f16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcgezq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}

INLINE(Vwwi,VWWI_ZLES) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vcgez_s32(v);
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZLES) (Vwwf a)
{
    return  WWI_ASTV(0.0f <= VWWF_ASTM(a) ? -1 : 0);
}

INLINE(Vdbi,VDBI_ZLES) (Vdbi a) {return vreinterpret_s8_u8(vcgez_s8(a));}
INLINE(Vdbc,VDBC_ZLES) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vcgez_s8(VDBC_ASBI(a)));
#else
    return  VDBU_ASBC(vdup_n_u8(UINT8_MAX));
#endif
}

INLINE(Vdhi,VDHI_ZLES) (Vdhi a) {return vreinterpret_s16_u16(vcgez_s16(a));}
INLINE(Vdhi,VDHF_ZLES) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vcgez_f16(a));
#else
    uint32x4_t q = vcgezq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vmovn_u32(q));
#endif
}

INLINE(Vdwi,VDWI_ZLES) (Vdwi a) {return vreinterpret_s32_u32(vcgez_s32(a));}            
INLINE(Vdwi,VDWF_ZLES) (Vdwf a) {return vreinterpret_s32_u32(vcgez_f32(a));}

INLINE(Vddi,VDDI_ZLES) (Vddi a) {return vreinterpret_s64_u64(vcgez_s64(a));}            
INLINE(Vddi,VDDF_ZLES) (Vddf a) {return vreinterpret_s64_u64(vcgez_f64(a));}

INLINE(Vqbi,VQBI_ZLES) (Vqbi a) {return vreinterpretq_u8_s8(vcgezq_s8(a));}
INLINE(Vqbc,VQBC_ZLES) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vcgezq_s8(VQBC_ASBI(a)));
#else
    return  VQBU_ASBC(vdupq_n_u8(UINT8_MAX));
#endif
}

INLINE(Vqhi,VQHI_ZLES) (Vqhi a) {return vreinterpretq_s16_u16(vcgezq_s16(a));}
INLINE(Vqhi,VQHF_ZLES) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vcgezq_f16(a));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcgezq_f32(wl);
    uint32x4_t  ur = vcgezq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    return  vreinterpretq_s16_u16(vcombine_u16(vl, vr));
#endif
}

INLINE(Vqwi,VQWI_ZLES) (Vqwi a) {return vreinterpretq_s32_u32(vcgezq_s32(a));}            
INLINE(Vqwi,VQWF_ZLES) (Vqwf a) {return vreinterpretq_s32_u32(vcgezq_f32(a));}

INLINE(Vqdi,VQDI_ZLES) (Vqdi a) {return vreinterpretq_s64_u64(vcgezq_s64(a));}            
INLINE(Vqdi,VQDF_ZLES) (Vqdf a) {return vreinterpretq_s64_u64(vcgezq_f64(a));}


#if 0 // _LEAVE_ARM_ZLES
}
#endif

#if 0 // _ENTER_ARM_ZGTY
{
#endif

INLINE(_Bool,FLT16_ZGTY)    (flt16_t a) {return 0.0F16> a;}
INLINE(_Bool,  FLT_ZGTY)      (float a) {return 0.0F  > a;}
INLINE(_Bool,  DBL_ZGTY)     (double a) {return 0.0   > a;}
INLINE(_Bool,    zgtyqf) (QUAD_FTYPE a) {return 0.0L  > a;}


INLINE(Vwbi,VWBI_ZGTY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcltz_s8(v);
    u = vand_u8(u, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZGTY) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZGTY(VWBC_ASBI(a)));
#else
    return  WBC_ASTV(0);
#endif
}

INLINE(Vwhi,VWHI_ZGTY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcltz_s16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZGTY) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcltz_f16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcltzq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}

INLINE(Vwwi,VWWI_ZGTY) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vcltz_s32(v);
    r = vand_u32(r, vdup_n_u32(1));
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZGTY) (Vwwf a)
{
    return  WWI_ASTV(0.0f <= VWWF_ASTM(a));
}

INLINE(Vdbi,VDBI_ZGTY) (Vdbi a) 
{
    return vreinterpret_s8_u8(vand_u8(vcltz_s8(a), vdup_n_u8(1)));
}

INLINE(Vdbc,VDBC_ZGTY) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vand_u8(vcltz_s8(VDBC_ASBI(a)), vdup_n_u8(1)));
#else
    return  VDBU_ASBC(vdup_n_u8(1));
#endif
}


INLINE(Vdhi,VDHI_ZGTY) (Vdhi a) 
{
    return vreinterpret_s16_u16(vand_u16(vcltz_s16(a), vdup_n_u16(1)));
}

INLINE(Vdhi,VDHF_ZGTY) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vand_u16(vcltz_f16(a),vdup_n_u16(1)));
#else
    uint32x4_t q = vcltzq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vand_u16(vmovn_u32(q), vdup_n_u16(1)));
#endif
}


INLINE(Vdwi,VDWI_ZGTY) (Vdwi a) 
{
    return vreinterpret_s32_u32(vand_u32(vcltz_s32(a), vdup_n_u32(1)));
}

INLINE(Vdwi,VDWF_ZGTY) (Vdwf a) 
{
    return vreinterpret_s32_u32(vand_u32(vcltz_f32(a), vdup_n_u32(1)));
}


INLINE(Vddi,VDDI_ZGTY) (Vddi a) 
{
    return vreinterpret_s64_u64(vand_u64(vcltz_s64(a), vdup_n_u64(1)));
}

INLINE(Vddi,VDDF_ZGTY) (Vddf a) 
{
    return vreinterpret_s64_u64(vand_u64(vcltz_f64(a), vdup_n_u64(1)));
}


INLINE(Vqbi,VQBI_ZGTY) (Vqbi a) 
{
    return vreinterpretq_s8_u8(vandq_u8(vcltzq_s8(a), vdupq_n_u8(1)));
}

INLINE(Vqbc,VQBC_ZGTY) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vandq_u8(vcltzq_s8(VQBC_ASBI(a)), vdupq_n_u8(1)));
#else
    return  VQBU_ASBC(vdupq_n_u8(1));
#endif
}


INLINE(Vqhi,VQHI_ZGTY) (Vqhi a) 
{
    return vreinterpretq_s16_u16(vandq_u16(vcltzq_s16(a), vdupq_n_u16(1)));
}

INLINE(Vqhi,VQHF_ZGTY) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vandq_u16(vcltzq_f16(a), vdupq_n_u16(1)));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcltzq_f32(wl);
    uint32x4_t  ur = vcltzq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    uint16x8_t  qq = vcombine_u16(vl, vr);
    qq = vandq_u16(qq, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(qq);
#endif
}


INLINE(Vqwi,VQWI_ZGTY) (Vqwi a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcltzq_s32(a), vdupq_n_u32(1)));
}

INLINE(Vqwi,VQWF_ZGTY) (Vqwf a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcltzq_f32(a), vdupq_n_u32(1)));
}


INLINE(Vqdi,VQDI_ZGTY) (Vqdi a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcltzq_s64(a), vdupq_n_u64(1)));
}

INLINE(Vqdi,VQDF_ZGTY) (Vqdf a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcltzq_f64(a), vdupq_n_u64(1)));
}


#if 0 // _LEAVE_ARM_ZGTY
}
#endif

#if 0 // _ENTER_ARM_ZGTS
{
#endif

INLINE(int16_t, FLT16_ZGTS) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  vcltzh_f16(a);
#else
    return  0.0f16 > a ? -1 : 0;
#endif
}

INLINE(int32_t,   FLT_ZGTS)   (float a) {return vcltzs_f32(a);}
INLINE(int64_t,   DBL_ZGTS)  (double a) {return vcltzd_f64(a);}

INLINE(QUAD_ITYPE,zgtsqf) (QUAD_FTYPE a) {return 0 > a ? -1 : 0;}

INLINE(Vwbi,VWBI_ZGTS) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcltz_s8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZGTS) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZGTS(VWBC_ASBI(a)));
#else
    return  WBC_ASTV(0.0f);
#endif
}

INLINE(Vwhi,VWHI_ZGTS) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcltz_s16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZGTS) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcltz_f16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcltzq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}

INLINE(Vwwi,VWWI_ZGTS) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vcltz_s32(v);
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZGTS) (Vwwf a)
{
    return  WWI_ASTV(0.0f > VWWF_ASTM(a) ? -1 : 0);
}

INLINE(Vdbi,VDBI_ZGTS) (Vdbi a) {return vreinterpret_s8_u8(vcltz_s8(a));}
INLINE(Vdbc,VDBC_ZGTS) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vcltz_s8(VDBC_ASBI(a)));
#else
    return  VDBU_ASBC(vdup_n_u8(0));
#endif
}

INLINE(Vdhi,VDHI_ZGTS) (Vdhi a) {return vreinterpret_s16_u16(vcltz_s16(a));}
INLINE(Vdhi,VDHF_ZGTS) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vcltz_f16(a));
#else
    uint32x4_t q = vcltzq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vmovn_u32(q));
#endif
}

INLINE(Vdwi,VDWI_ZGTS) (Vdwi a) {return vreinterpret_s32_u32(vcltz_s32(a));}            
INLINE(Vdwi,VDWF_ZGTS) (Vdwf a) {return vreinterpret_s32_u32(vcltz_f32(a));}

INLINE(Vddi,VDDI_ZGTS) (Vddi a) {return vreinterpret_s64_u64(vcltz_s64(a));}            
INLINE(Vddi,VDDF_ZGTS) (Vddf a) {return vreinterpret_s64_u64(vcltz_f64(a));}

INLINE(Vqbi,VQBI_ZGTS) (Vqbi a) {return vreinterpretq_u8_s8(vcltzq_s8(a));}
INLINE(Vqbc,VQBC_ZGTS) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vcltzq_s8(VQBC_ASBI(a)));
#else
    return  VQBU_ASBC(vdupq_n_u8(0));
#endif
}

INLINE(Vqhi,VQHI_ZGTS) (Vqhi a) {return vreinterpretq_s16_u16(vcltzq_s16(a));}
INLINE(Vqhi,VQHF_ZGTS) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vcltzq_f16(a));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcltzq_f32(wl);
    uint32x4_t  ur = vcltzq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    return  vreinterpretq_s16_u16(vcombine_u16(vl, vr));
#endif
}

INLINE(Vqwi,VQWI_ZGTS) (Vqwi a) {return vreinterpretq_s32_u32(vcltzq_s32(a));}            
INLINE(Vqwi,VQWF_ZGTS) (Vqwf a) {return vreinterpretq_s32_u32(vcltzq_f32(a));}

INLINE(Vqdi,VQDI_ZGTS) (Vqdi a) {return vreinterpretq_s64_u64(vcltzq_s64(a));}            
INLINE(Vqdi,VQDF_ZGTS) (Vqdf a) {return vreinterpretq_s64_u64(vcltzq_f64(a));}


#if 0 // _LEAVE_ARM_ZGTS
}
#endif

#if 0 // _ENTER_ARM_ZGEY
{
#endif

INLINE(_Bool,FLT16_ZGEY)    (flt16_t a) {return 0.0F16>= a;}
INLINE(_Bool,  FLT_ZGEY)      (float a) {return 0.0F  >= a;}
INLINE(_Bool,  DBL_ZGEY)     (double a) {return 0.0   >= a;}
INLINE(_Bool,    zgeyqf) (QUAD_FTYPE a) {return 0.0L  >= a;}

INLINE(Vwbi,VWBI_ZGEY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vclez_s8(v);
    u = vand_u8(u, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZGEY) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZGEY(VWBC_ASBI(a)));
#else
    float       f = VWBC_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return WBC_ASTV(f);
#endif
}


INLINE(Vwhi,VWHI_ZGEY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vclez_s16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZGEY) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vclez_f16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vclezq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}

INLINE(Vwwi,VWWI_ZGEY) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vclez_s32(v);
    r = vand_u32(r, vdup_n_u32(1));
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZGEY) (Vwwf a)
{
    return  WWI_ASTV(0.0f <= VWWF_ASTM(a));
}


INLINE(Vdbi,VDBI_ZGEY) (Vdbi a) 
{
    return vreinterpret_s8_u8(vand_u8(vclez_s8(a), vdup_n_u8(1)));
}

INLINE(Vdbc,VDBC_ZGEY) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vand_u8(vclez_s8(VDBC_ASBI(a)), vdup_n_u8(1)));
#else
    uint8x8_t   m = VDBC_ASBU(a);
    m = vceqz_u8(m);
    m = vand_u8(m, vdup_n_u8(1));
    return  VDBU_ASBC(m);
#endif
}


INLINE(Vdhi,VDHI_ZGEY) (Vdhi a) 
{
    return vreinterpret_s16_u16(vand_u16(vclez_s16(a), vdup_n_u16(1)));
}

INLINE(Vdhi,VDHF_ZGEY) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vand_u16(vclez_f16(a),vdup_n_u16(1)));
#else
    uint32x4_t q = vclezq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vand_u16(vmovn_u32(q), vdup_n_u16(1)));
#endif
}


INLINE(Vdwi,VDWI_ZGEY) (Vdwi a) 
{
    return vreinterpret_s32_u32(vand_u32(vclez_s32(a), vdup_n_u32(1)));
}

INLINE(Vdwi,VDWF_ZGEY) (Vdwf a) 
{
    return vreinterpret_s32_u32(vand_u32(vclez_f32(a), vdup_n_u32(1)));
}


INLINE(Vddi,VDDI_ZGEY) (Vddi a) 
{
    return vreinterpret_s64_u64(vand_u64(vclez_s64(a), vdup_n_u64(1)));
}

INLINE(Vddi,VDDF_ZGEY) (Vddf a) 
{
    return vreinterpret_s64_u64(vand_u64(vclez_f64(a), vdup_n_u64(1)));
}


INLINE(Vqbi,VQBI_ZGEY) (Vqbi a) 
{
    return vreinterpretq_s8_u8(vandq_u8(vclezq_s8(a), vdupq_n_u8(1)));
}

INLINE(Vqbc,VQBC_ZGEY) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vandq_u8(vclezq_s8(VQBC_ASBI(a)), vdupq_n_u8(1)));
#else
    uint8x16_t  m = VQBC_ASBU(a);
    m = vceqzq_u8(m);
    m = vandq_u8(m, vdupq_n_u8(1));
    return  VQBU_ASBC(m);
#endif
}


INLINE(Vqhi,VQHI_ZGEY) (Vqhi a) 
{
    return vreinterpretq_s16_u16(vandq_u16(vclezq_s16(a), vdupq_n_u16(1)));
}

INLINE(Vqhi,VQHF_ZGEY) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vandq_u16(vclezq_f16(a), vdupq_n_u16(1)));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vclezq_f32(wl);
    uint32x4_t  ur = vclezq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    uint16x8_t  qq = vcombine_u16(vl, vr);
    qq = vandq_u16(qq, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(qq);
#endif
}


INLINE(Vqwi,VQWI_ZGEY) (Vqwi a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vclezq_s32(a), vdupq_n_u32(1)));
}

INLINE(Vqwi,VQWF_ZGEY) (Vqwf a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vclezq_f32(a), vdupq_n_u32(1)));
}


INLINE(Vqdi,VQDI_ZGEY) (Vqdi a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vclezq_s64(a), vdupq_n_u64(1)));
}

INLINE(Vqdi,VQDF_ZGEY) (Vqdf a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vclezq_f64(a), vdupq_n_u64(1)));
}


#if 0 // _LEAVE_ARM_ZGEY
}
#endif

#if 0 // _ENTER_ARM_ZGES
{
#endif

INLINE(int16_t, FLT16_ZGES) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  vclezh_f16(a);
#else
    return  0.0f16 >= a ? -1 : 0;
#endif
}

INLINE(int32_t,   FLT_ZGES)   (float a) {return vclezs_f32(a);}
INLINE(int64_t,   DBL_ZGES)  (double a) {return vclezd_f64(a);}

INLINE(QUAD_ITYPE,zgesqf) (QUAD_FTYPE a) {return 0 >= a ? -1 : 0;}

INLINE(Vwbi,VWBI_ZGES) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vclez_s8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZGES) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZGES(VWBC_ASBI(a)));
#else

    float       f = VWBC_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   r = vreinterpret_f32_u8(m);
    r = vceqz_u8(r);
    m = vreinterpret_u8_f32(r);
    f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
#endif
}


INLINE(Vwhi,VWHI_ZGES) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vclez_s16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZGES) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vclez_f16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vclezq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}


INLINE(Vwwi,VWWI_ZGES) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vclez_s32(v);
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZGES) (Vwwf a)
{
    return  WWI_ASTV(0.0f >= VWWF_ASTM(a) ? -1 : 0);
}


INLINE(Vdbi,VDBI_ZGES) (Vdbi a) {return vreinterpret_s8_u8(vclez_s8(a));}
INLINE(Vdbc,VDBC_ZGES) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vclez_s8(VDBC_ASBI(a)));
#else
    return  VDBU_ASBC(vceqz_u8(VDBC_ASBU(a)));
#endif
}

INLINE(Vdhi,VDHI_ZGES) (Vdhi a) {return vreinterpret_s16_u16(vclez_s16(a));}
INLINE(Vdhi,VDHF_ZGES) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vclez_f16(a));
#else
    uint32x4_t q = vclezq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vmovn_u32(q));
#endif
}

INLINE(Vdwi,VDWI_ZGES) (Vdwi a) {return vreinterpret_s32_u32(vclez_s32(a));}            
INLINE(Vdwi,VDWF_ZGES) (Vdwf a) {return vreinterpret_s32_u32(vclez_f32(a));}

INLINE(Vddi,VDDI_ZGES) (Vddi a) {return vreinterpret_s64_u64(vclez_s64(a));}            
INLINE(Vddi,VDDF_ZGES) (Vddf a) {return vreinterpret_s64_u64(vclez_f64(a));}

INLINE(Vqbi,VQBI_ZGES) (Vqbi a) {return vreinterpretq_u8_s8(vclezq_s8(a));}
INLINE(Vqbc,VQBC_ZGES) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vclezq_s8(VQBC_ASBI(a)));
#else
    return  VQBU_ASBC(vceqzq_u8(VQBC_ASBU(a)));
#endif
}

INLINE(Vqhi,VQHI_ZGES) (Vqhi a) {return vreinterpretq_s16_u16(vclezq_s16(a));}
INLINE(Vqhi,VQHF_ZGES) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vclezq_f16(a));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vclezq_f32(wl);
    uint32x4_t  ur = vclezq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    return  vreinterpretq_s16_u16(vcombine_u16(vl, vr));
#endif
}

INLINE(Vqwi,VQWI_ZGES) (Vqwi a) {return vreinterpretq_s32_u32(vclezq_s32(a));}            
INLINE(Vqwi,VQWF_ZGES) (Vqwf a) {return vreinterpretq_s32_u32(vclezq_f32(a));}

INLINE(Vqdi,VQDI_ZGES) (Vqdi a) {return vreinterpretq_s64_u64(vclezq_s64(a));}            
INLINE(Vqdi,VQDF_ZGES) (Vqdf a) {return vreinterpretq_s64_u64(vclezq_f64(a));}


#if 0 // _LEAVE_ARM_ZGES
}
#endif


#if 0 // _ENTER_ARM_VEQS
{
#endif
#if CHAR_MIN
#   define DBC_VEQS DBI_VEQS
#   define QBC_VEQS QBI_VEQS
#else
#   define DBC_VEQS DBU_VEQS
#   define QBC_VEQS QBU_VEQS
#endif

INLINE(float,WYU_VEQS) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.B.U = vmvn_u8(x.B.U);
    z.W.U = vceq_u32(y.W.U, z.W.U);
    z.B.U = vmvn_u8(z.B.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WBZ_VEQS) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.B.U = vceq_u8(x.B.U, y.B.U);
    x.W.U = vtst_u32(x.W.U, vcreate_u32(UINT32_MAX));
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WHZ_VEQS) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.H.U = vceq_u16(x.H.U, y.H.U);
    x.W.U = vtst_u32(x.W.U, vcreate_u32(UINT32_MAX));
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WHF_VEQS) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.H.I = DHF_CEQS(x.H.F, y.H.F);
    x.W.U = vtst_u32(x.W.U, vcreate_u32(UINT32_MAX));
    return  vget_lane_f32(x.W.F, 0);
}


INLINE(uint64x1_t,DYU_VEQS)  (uint64x1_t a,  uint64x1_t b)
{
    DWRD_VTYPE x={.D.U=a}, y={.D.U=b}, z;
    z.B.U = vmvn_u8(x.B.U);
    z.D.U = vceq_u64(y.W.U, z.W.U);
    z.B.U = vmvn_u8(z.B.U);
    return  z.D.U;
}

INLINE( uint8x8_t,DBU_VEQS)   (uint8x8_t a,   uint8x8_t b)
{
    DWRD_VTYPE c = {.B.U=vceq_u8(a, b)};
    c.D.U = vtst_u64(c.D.U, vdup_n_u64(-1));
    return  c.B.U;
}

INLINE(  int8x8_t,DBI_VEQS)    (int8x8_t a,    int8x8_t b)
{
    DWRD_VTYPE c = {.B.U=vceq_s8(a, b)};
    c.D.U = vtst_u64(c.D.U, vdup_n_u64(-1));
    return  c.B.I;
}

INLINE(uint16x4_t,DHU_VEQS)  (uint16x4_t a,  uint16x4_t b)
{
    DWRD_VTYPE c = {.H.U=vceq_u16(a, b)};
    c.D.U = vtst_u64(c.D.U, vdup_n_u64(-1));
    return  c.H.U;
}

INLINE( int16x4_t,DHI_VEQS)   (int16x4_t a,   int16x4_t b)
{
    DWRD_VTYPE c = {.H.U=vceq_s16(a, b)};
    c.D.U = vtst_u64(c.D.U, vdup_n_u64(-1));
    return  c.H.I;
}

INLINE( int16x4_t,DHF_VEQS) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE c = {.H.I=DHF_CEQS(a, b)};
    c.D.U = vtst_u64(c.D.U, vdup_n_u64(-1));
    return  c.H.I;
}

INLINE(uint32x2_t,DWU_VEQS)  (uint32x2_t a,  uint32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_u32(a, b)};
    c.D.U = vtst_u64(c.D.U, vdup_n_u64(-1));
    return  c.W.U;
}

INLINE( int32x2_t,DWI_VEQS)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_s32(a, b)};
    c.D.U = vtst_u64(c.D.U, vdup_n_u64(-1));
    return  c.W.I;
}

INLINE( int32x2_t,DWF_VEQS) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_f32(a, b)};
    c.D.U = vtst_u64(c.D.U, vdup_n_u64(-1));
    return  c.W.F;
}

INLINE(uint64x2_t,QYU_VEQS)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE x={.D.U=a}, y={.D.U=b}, z;
    z.B.U = vmvnq_u8(x.B.U);
    z.D.U = vceqq_u64(y.W.U, z.W.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(z.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(z.D.U)};
    l.B.U = vand_u8(l.B.U, r.B.U);
    l.B.U = vmvn_u8(l.B.U);
    return  vdupq_lane_u64(l.D.U, 0);
}

INLINE(uint8x16_t,QBU_VEQS)  (uint8x16_t a,  uint8x16_t b)
{
    QUAD_VTYPE c = {.B.U=vceqq_u8(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    l.D.U = vtst_u64(l.D.U, vdup_n_u64(-1));
    c.D.U = vdupq_lane_u64(l.D.U, 0);
    return  c.B.U;
}

INLINE( int8x16_t,QBI_VEQS)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE c = {.B.U=vceqq_s8(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    l.D.U = vtst_u64(l.D.U, vdup_n_u64(-1));
    c.D.U = vdupq_lane_u64(l.D.U, 0);
    return  c.B.I;
}

INLINE(uint16x8_t,QHU_VEQS)  (uint16x8_t a,  uint16x8_t b)
{
    QUAD_VTYPE c = {.H.U=vceqq_u16(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    l.D.U = vtst_u64(l.D.U, vdup_n_u64(-1));
    c.D.U = vdupq_lane_u64(l.D.U, 0);
    return  c.H.U;
}

INLINE( int16x8_t,QHI_VEQS)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE c = {.H.U=vceqq_s16(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    l.D.U = vtst_u64(l.D.U, vdup_n_u64(-1));
    c.D.U = vdupq_lane_u64(l.D.U, 0);
    return  c.H.I;
}


INLINE( int16x8_t,QHF_VEQS) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE c = {.H.I=QHF_CEQS(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    l.D.U = vtst_u64(l.D.U, vdup_n_u64(-1));
    c.D.U = vdupq_lane_u64(l.D.U, 0);
    return  c.H.I;
}

INLINE(uint32x4_t,QWU_VEQS)  (uint32x4_t a,  uint32x4_t b)
{
    QUAD_VTYPE c = {.W.U=vceqq_u32(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    l.D.U = vtst_u64(l.D.U, vdup_n_u64(-1));
    c.D.U = vdupq_lane_u64(l.D.U, 0);
    return  c.W.U;
}

INLINE( int32x4_t,QWI_VEQS)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE c = {.W.U=vceqq_s32(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    l.D.U = vtst_u64(l.D.U, vdup_n_u64(-1));
    c.D.U = vdupq_lane_u64(l.D.U, 0);
    return  c.W.I;
}

INLINE( int32x4_t,QWF_VEQS) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE c = {.W.U=vceqq_f32(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    l.D.U = vtst_u64(l.D.U, vdup_n_u64(-1));
    c.D.U = vdupq_lane_u64(l.D.U, 0);
    return  c.W.F;
}


INLINE(uint64x2_t,QDU_VEQS)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_u64(a,b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    return  vdupq_lane_u64(l.D.U, 0);
}

INLINE( int64x2_t,QDI_VEQS)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_s64(a,b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    return  vdupq_lane_s64(l.D.I, 0);
}

INLINE( int64x2_t,QDF_VEQS) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_f64(a,b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vorr_u8(l.B.U, r.B.U);
    return  vdupq_lane_s64(l.D.I, 0);
}

INLINE(Vwyu,VWYU_VEQS) (Vwyu a, Vwyu b) {a.V0=WYU_VEQS(a.V0,b.V0); return a;}
INLINE(Vwbu,VWBU_VEQS) (Vwbu a, Vwbu b) {a.V0=WBZ_VEQS(a.V0,b.V0); return a;}
INLINE(Vwbi,VWBI_VEQS) (Vwbi a, Vwbi b) {a.V0=WBZ_VEQS(a.V0,b.V0); return a;}
INLINE(Vwbc,VWBC_VEQS) (Vwbc a, Vwbc b) {a.V0=WBZ_VEQS(a.V0,b.V0); return a;}
INLINE(Vwhu,VWHU_VEQS) (Vwhu a, Vwhu b) {a.V0=WHZ_VEQS(a.V0,b.V0); return a;}
INLINE(Vwhi,VWHI_VEQS) (Vwhi a, Vwhi b) {a.V0=WHZ_VEQS(a.V0,b.V0); return a;}
INLINE(Vwhi,VWHF_VEQS) (Vwhi a, Vwhi b) {return ((Vwhi){WHZ_VEQS(a.V0,b.V0)});}

INLINE(Vdyu,VDYU_VEQS) (Vdyu a, Vdyu b) {a.V0=DYU_VEQS(a.V0,b.V0); return a;}
INLINE(Vdbu,VDBU_VEQS) (Vdbu a, Vdbu b) {return DBU_VEQS(a, b);}
INLINE(Vdbi,VDBI_VEQS) (Vdbi a, Vdbi b) {return DBI_VEQS(a, b);}
INLINE(Vdbc,VDBC_VEQS) (Vdbc a, Vdbc b) {a.V0=DBC_VEQS(a.V0,b.V0); return a;}
INLINE(Vdhu,VDHU_VEQS) (Vdhu a, Vdhu b) {return DHU_VEQS(a, b);}
INLINE(Vdhi,VDHI_VEQS) (Vdhi a, Vdhi b) {return DHI_VEQS(a, b);}
INLINE(Vdhi,VDHF_VEQS) (Vdhf a, Vdhf b) {return ((Vdhi){DHF_VEQS(a, b)});}
INLINE(Vdwu,VDWU_VEQS) (Vdwu a, Vdwu b) {return DWU_VEQS(a, b);}
INLINE(Vdwi,VDWI_VEQS) (Vdwi a, Vdwi b) {return DWI_VEQS(a, b);}
INLINE(Vdwi,VDWF_VEQS) (Vdwf a, Vdwf b) {return DWF_VEQS(a, b);}

INLINE(Vqyu,VQYU_VEQS) (Vqyu a, Vqyu b) {a.V0=QYU_VEQS(a.V0,b.V0); return a;}
INLINE(Vqbu,VQBU_VEQS) (Vqbu a, Vqbu b) {return QBU_VEQS(a, b);}
INLINE(Vqbi,VQBI_VEQS) (Vqbi a, Vqbi b) {return QBI_VEQS(a, b);}
INLINE(Vqbc,VQBC_VEQS) (Vqbc a, Vqbc b) {a.V0=QBC_VEQS(a.V0,b.V0); return a;}
INLINE(Vqhu,VQHU_VEQS) (Vqhu a, Vqhu b) {return QHU_VEQS(a, b);}
INLINE(Vqhi,VQHI_VEQS) (Vqhi a, Vqhi b) {return QHI_VEQS(a, b);}
INLINE(Vqhi,VQHF_VEQS) (Vqhf a, Vqhf b) {return ((Vqhi){QHF_VEQS(a, b)});}
INLINE(Vqwu,VQWU_VEQS) (Vqwu a, Vqwu b) {return QWU_VEQS(a, b);}
INLINE(Vqwi,VQWI_VEQS) (Vqwi a, Vqwi b) {return QWI_VEQS(a, b);}
INLINE(Vqwi,VQWF_VEQS) (Vqwf a, Vqwf b) {return QWF_VEQS(a, b);}
INLINE(Vqdu,VQDU_VEQS) (Vqdu a, Vqdu b) {return QDU_VEQS(a, b);}
INLINE(Vqdi,VQDI_VEQS) (Vqdi a, Vqdi b) {return QDI_VEQS(a, b);}
INLINE(Vqdi,VQDF_VEQS) (Vqdf a, Vqdf b) {return QDF_VEQS(a, b);}

#if 0 // _LEAVE_ARM_VEQS
}
#endif

#if 0 // _ENTER_ARM_VEQY
{
#endif

#if CHAR_MIN
#   define  DBC_VEQY DBI_VEQY
#   define  QBC_VEQY QBI_VEQY
#else
#   define  DBC_VEQY DBU_VEQY
#   define  QBC_VEQY QBU_VEQY
#endif

INLINE(uint32_t,WYU_VEQY) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b}, z;
    z.B.U = vmvn_u8(x.B.U);
    z.W.U = vceq_u32(y.W.U, z.W.U);
    return  !vget_lane_u32(x.W.U, 0);
}

INLINE(uint32_t,WBZ_VEQY) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.B.U = vceq_u8(x.B.U, y.B.U);
    return  vget_lane_u32(x.W.U, 0);
}

INLINE(uint32_t,WHZ_VEQY) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.H.U = vceq_u16(x.H.U, y.H.U);
    return  vget_lane_u32(x.W.U, 0);
}

INLINE(uint32_t,WHF_VEQY) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.H.I = DHF_CEQS(x.H.F, y.H.F);
    return  vget_lane_u32(x.W.U, 0);
}


INLINE(uint64_t,DYU_VEQY)  (uint64x1_t a,  uint64x1_t b)
{
    DWRD_VTYPE x={.D.U=a}, y={.D.U=b}, z;
    return x.U^~y.U;
}

INLINE(uint64_t,DBU_VEQY)   (uint8x8_t a,   uint8x8_t b)
{
    return  ((DWRD_VTYPE){.B.U=vceq_u8(a,b)}).U;
}

INLINE(uint64_t,DBI_VEQY)    (int8x8_t a,    int8x8_t b)
{
    return  ((DWRD_VTYPE){.B.U=vceq_s8(a,b)}).U;
}

INLINE(uint64_t,DHU_VEQY)  (uint16x4_t a,  uint16x4_t b)
{
    return  ((DWRD_VTYPE){.H.U=vceq_u16(a,b)}).U;
}

INLINE(uint64_t,DHI_VEQY)   (int16x4_t a,   int16x4_t b)
{
    return  ((DWRD_VTYPE){.H.U=vceq_s16(a,b)}).U;
}

INLINE(uint64_t,DHF_VEQY) (float16x4_t a, float16x4_t b)
{
    return  ((DWRD_VTYPE){.H.I=DHF_CEQS(a,b)}).U;
}

INLINE(uint64_t,DWU_VEQY)  (uint32x2_t a,  uint32x2_t b)
{
    return  ((DWRD_VTYPE){.W.U=vceq_u32(a,b)}).U;
}

INLINE(uint64_t,DWI_VEQY)   (int32x2_t a,   int32x2_t b)
{
    return  ((DWRD_VTYPE){.W.U=vceq_s32(a,b)}).U;
}

INLINE(uint64_t,DWF_VEQY) (float32x2_t a, float32x2_t b)
{
    return  ((DWRD_VTYPE){.W.U=vceq_f32(a,b)}).U;
}

INLINE(uint64_t,QYU_VEQY)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE x={.D.U=a}, y={.D.U=b};
    y.B.U = vmvnq_u8(y.B.U);
    x.B.U = veorq_u8(x.B.U, y.B.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(x.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(x.D.U)};
    return  l.U|r.U;
}

INLINE(uint64_t,QBU_VEQY)  (uint8x16_t a,  uint8x16_t b)
{
    QUAD_VTYPE c = {.B.U=vceqq_u8(a, b)};
    return  vgetq_lane_u64(c.D.U, 0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(uint64_t,QBI_VEQY)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE c = {.B.U=vceqq_s8(a, b)};
    return  vgetq_lane_u64(c.D.U,0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(uint64_t,QHU_VEQY)  (uint16x8_t a,  uint16x8_t b)
{
    QUAD_VTYPE c = {.H.U=vceqq_u16(a, b)};
    return  vgetq_lane_u64(c.D.U,0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(uint64_t,QHI_VEQY)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE c = {.H.U=vceqq_s16(a, b)};
    return  vgetq_lane_u64(c.D.U,0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(uint64_t,QHF_VEQY) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE c = {.H.I=QHF_CEQS(a, b)};
    return  vgetq_lane_u64(c.D.U,0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(uint64_t,QWU_VEQY)  (uint32x4_t a,  uint32x4_t b)
{
    QUAD_VTYPE c = {.W.U=vceqq_u32(a, b)};
    return  vgetq_lane_u64(c.D.U,0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(uint64_t,QWI_VEQY)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE c = {.W.U=vceqq_s32(a, b)};
    return  vgetq_lane_u64(c.D.U,0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(uint64_t,QWF_VEQY) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE c = {.W.U=vceqq_f32(a, b)};
    return  vgetq_lane_u64(c.D.U,0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(uint64_t,QDU_VEQY)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_u64(a, b)};
    return  vgetq_lane_u64(c.D.U,0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(uint64_t,QDI_VEQY)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_s64(a, b)};
    return  vgetq_lane_u64(c.D.U,0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(uint64_t,QDF_VEQY) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_f64(a, b)};
    return  vgetq_lane_u64(c.D.U,0)|vgetq_lane_u64(c.D.U,1);
}

INLINE(_Bool,VWYU_VEQY) (Vwyu a, Vwyu b) {return WYU_VEQY(a.V0, b.V0);}
INLINE(_Bool,VWBU_VEQY) (Vwbu a, Vwbu b) {return WBZ_VEQY(a.V0, b.V0);}
INLINE(_Bool,VWBI_VEQY) (Vwbi a, Vwbi b) {return WBZ_VEQY(a.V0, b.V0);}
INLINE(_Bool,VWBC_VEQY) (Vwbc a, Vwbc b) {return WBZ_VEQY(a.V0, b.V0);}
INLINE(_Bool,VWHU_VEQY) (Vwhu a, Vwhu b) {return WHZ_VEQY(a.V0, b.V0);}
INLINE(_Bool,VWHI_VEQY) (Vwhi a, Vwhi b) {return WHZ_VEQY(a.V0, b.V0);}
INLINE(_Bool,VWHF_VEQY) (Vwhf a, Vwhf b) {return WHF_VEQY(a.V0, b.V0);}

INLINE(_Bool,VDYU_VEQY) (Vdyu a, Vdyu b) {return DYU_VEQY(a.V0, b.V0);}
INLINE(_Bool,VDBU_VEQY) (Vdbu a, Vdbu b) {return DBU_VEQY(a, b);}
INLINE(_Bool,VDBI_VEQY) (Vdbi a, Vdbi b) {return DBI_VEQY(a, b);}
INLINE(_Bool,VDBC_VEQY) (Vdbc a, Vdbc b) {return DBC_VEQY(a.V0, b.V0);}
INLINE(_Bool,VDHU_VEQY) (Vdhu a, Vdhu b) {return DHU_VEQY(a, b);}
INLINE(_Bool,VDHI_VEQY) (Vdhi a, Vdhi b) {return DHI_VEQY(a, b);}
INLINE(_Bool,VDHF_VEQY) (Vdhf a, Vdhf b) {return DHF_VEQY(a, b);}
INLINE(_Bool,VDWU_VEQY) (Vdwu a, Vdwu b) {return DWU_VEQY(a, b);}
INLINE(_Bool,VDWI_VEQY) (Vdwi a, Vdwi b) {return DWI_VEQY(a, b);}
INLINE(_Bool,VDWF_VEQY) (Vdwf a, Vdwf b) {return DWF_VEQY(a, b);}

INLINE(_Bool,VQYU_VEQY) (Vqyu a, Vqyu b) {return QYU_VEQY(a.V0, b.V0);}
INLINE(_Bool,VQBU_VEQY) (Vqbu a, Vqbu b) {return QBU_VEQY(a, b);}
INLINE(_Bool,VQBI_VEQY) (Vqbi a, Vqbi b) {return QBI_VEQY(a, b);}
INLINE(_Bool,VQBC_VEQY) (Vqbc a, Vqbc b) {return QBC_VEQY(a.V0, b.V0);}
INLINE(_Bool,VQHU_VEQY) (Vqhu a, Vqhu b) {return QHU_VEQY(a, b);}
INLINE(_Bool,VQHI_VEQY) (Vqhi a, Vqhi b) {return QHI_VEQY(a, b);}
INLINE(_Bool,VQHF_VEQY) (Vqhf a, Vqhf b) {return QHF_VEQY(a, b);}
INLINE(_Bool,VQWU_VEQY) (Vqwu a, Vqwu b) {return QWU_VEQY(a, b);}
INLINE(_Bool,VQWI_VEQY) (Vqwi a, Vqwi b) {return QWI_VEQY(a, b);}
INLINE(_Bool,VQWF_VEQY) (Vqwf a, Vqwf b) {return QWF_VEQY(a, b);}
INLINE(_Bool,VQDU_VEQY) (Vqdu a, Vqdu b) {return QDU_VEQY(a, b);}
INLINE(_Bool,VQDI_VEQY) (Vqdi a, Vqdi b) {return QDI_VEQY(a, b);}
INLINE(_Bool,VQDF_VEQY) (Vqdf a, Vqdf b) {return QDF_VEQY(a, b);}

#if 0 // _LEAVE_ARM_VEQY
}
#endif

#if 0 // _ENTER_ARM_VEQN
{
#endif

#if CHAR_MIN
#   define DBC_VEQN DBI_VEQN
#   define QBC_VEQN QBI_VEQN
#else
#   define DBC_VEQN DBU_VEQN
#   define QBC_VEQN QBU_VEQN
#endif

INLINE( uint8x8_t,DBU_VEQN)   (uint8x8_t a,   uint8x8_t b)
{
    DWRD_VTYPE  c;
    c.B.U   = vceq_u8(a, b);
    c.B.U   = vshr_n_u8(c.B.U, 7);
    return  ((uint8x8_t){vaddv_u8(c.B.U)});
}

INLINE(  int8x8_t,DBI_VEQN)    (int8x8_t a,    int8x8_t b)
{
    DWRD_VTYPE  c;
    c.B.U   = vceq_s8(a, b);
    c.B.U   = vshr_n_u8(c.B.U, 7);
    return  ((int8x8_t){vaddv_s8(c.B.I)});
}

INLINE(uint16x4_t,DHU_VEQN)  (uint16x4_t a,  uint16x4_t b)
{
    DWRD_VTYPE  c;
    c.H.U   = vceq_u16(a, b);
    c.H.U   = vshr_n_u16(c.H.U, 15);
    return  ((uint16x4_t){vaddv_u16(c.H.U)});
}

INLINE( int16x4_t,DHI_VEQN)   (int16x4_t a,   int16x4_t b)
{
    DWRD_VTYPE  c;
    c.H.U   = vceq_s16(a, b);
    c.H.U   = vshr_n_u16(c.H.U, 15);
    return  ((int16x4_t){vaddv_s16(c.H.I)});
}

INLINE( int16x4_t,DHF_VEQN) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE  c;
    c.H.I   = DHF_CEQS(a,b);
    c.H.U   = vshr_n_u16(c.H.U, 15);
    return  ((int16x4_t){vaddv_s16(c.H.I)});
}

INLINE(uint32x2_t,DWU_VEQN)  (uint32x2_t a,  uint32x2_t b)
{
    DWRD_VTYPE  c;
    c.W.U   = vceq_u32(a, b);
    c.W.U   = vshr_n_u32(c.W.U, 31);
    return  ((uint32x2_t){vaddv_u32(c.W.U)});
}

INLINE( int32x2_t,DWI_VEQN)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE  c;
    c.W.U   = vceq_s32(a, b);
    c.W.U   = vshr_n_u32(c.W.U, 31);
    return  ((int32x2_t){vaddv_s32(c.W.I)});
}

INLINE( int32x2_t,DWF_VEQN) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE  c;
    c.W.U   = vceq_f32(a, b);
    c.W.U   = vshr_n_u32(c.W.U, 31);
    return  ((int32x2_t){vaddv_s32(c.W.I)});
}


INLINE(uint8x16_t,QBU_VEQN)  (uint8x16_t a,  uint8x16_t b)
{
    QUAD_VTYPE  c;
    c.B.U   = vceqq_u8(a, b);
    c.B.U   = vshrq_n_u8(c.B.U, 7);
    return  ((uint8x16_t){vaddvq_u8(c.B.U)});
}

INLINE( int8x16_t,QBI_VEQN)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE  c;
    c.B.U   = vceqq_s8(a, b);
    c.B.U   = vshrq_n_u8(c.B.U, 7);
    return  ((int8x16_t){vaddvq_s8(c.B.I)});
}


INLINE(uint16x8_t,QHU_VEQN)  (uint16x8_t a,  uint16x8_t b)
{
    QUAD_VTYPE  c;
    c.H.U   = vceqq_u16(a, b);
    c.H.U   = vshrq_n_u16(c.H.U, 15);
    return  ((uint16x8_t){vaddvq_u16(c.H.U)});
}

INLINE( int16x8_t,QHI_VEQN)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE  c;
    c.H.U   = vceqq_s16(a, b);
    c.H.U   = vshrq_n_u16(c.H.U, 15);
    return  ((int16x8_t){vaddvq_s16(c.H.I)});
}

INLINE( int16x8_t,QHF_VEQN) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE  c;
    c.H.I   = QHF_CEQS(a,b);
    c.H.U   = vshrq_n_u16(c.H.U, 15);
    return  ((int16x8_t){vaddvq_s16(c.H.I)});
}


INLINE(uint32x4_t,QWU_VEQN)  (uint32x4_t a,  uint32x4_t b)
{
    QUAD_VTYPE  c;
    c.W.U   = vceqq_u32(a, b);
    c.W.U   = vshrq_n_u32(c.W.U, 31);
    return  ((uint32x4_t){vaddvq_u32(c.W.U)});
}

INLINE( int32x4_t,QWI_VEQN)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE  c;
    c.W.U   = vceqq_s32(a, b);
    c.W.U   = vshrq_n_u32(c.W.U, 31);
    return  ((int32x4_t){vaddvq_s32(c.W.I)});
}

INLINE( int32x4_t,QWF_VEQN) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE  c;
    c.W.U   = vceqq_f32(a, b);
    c.W.U   = vshrq_n_u32(c.W.U, 31);
    return  ((int32x4_t){vaddvq_s32(c.W.I)});
}


INLINE(uint64x2_t,QDU_VEQN)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE  c;
    c.D.U   = vceqq_u64(a, b);
    c.D.U   = vshrq_n_u64(c.D.U, 63);
    return  ((uint64x2_t){vaddvq_u64(c.D.U)});
}

INLINE( int64x2_t,QDI_VEQN)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE  c;
    c.D.U   = vceqq_s64(a, b);
    c.D.U   = vshrq_n_u64(c.D.U, 63);
    return  ((int64x2_t){vaddvq_s64(c.D.I)});
}

INLINE( int64x2_t,QDF_VEQN) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE  c;
    c.W.U   = vceqq_f64(a, b);
    c.W.U   = vshrq_n_u64(c.D.U, 63);
    return  ((int64x2_t){vaddvq_s64(c.D.I)});
}


INLINE(Vwbu,VWBU_VEQN) (Vwbu a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.U = DBU_VEQN(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbi,VWBI_VEQN) (Vwbi a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.I = DBI_VEQN(x.B.I, y.B.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbc,VWBC_VEQN) (Vwbc a, Vwbc b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.U = DBU_VEQN(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}


INLINE(Vwhu,VWHU_VEQN) (Vwhu a, Vwhu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.U = DHU_VEQN(x.H.U, y.H.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHI_VEQN) (Vwhi a, Vwhi b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHI_VEQN(x.H.I, y.H.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHF_VEQN) (Vwhf a, Vwhf b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHF_VEQN(x.H.F, y.H.F);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return ((Vwhi){a.V0});
}

INLINE(Vdbu,VDBU_VEQN) (Vdbu a, Vdbu b) {return DBU_VEQN(a,b);}
INLINE(Vdbi,VDBI_VEQN) (Vdbi a, Vdbi b) {return DBI_VEQN(a,b);}
INLINE(Vdbc,VDBC_VEQN) (Vdbc a, Vdbc b) {a.V0 = DBC_VEQN(a.V0,b.V0); return a;}
INLINE(Vdhu,VDHU_VEQN) (Vdhu a, Vdhu b) {return DHU_VEQN(a,b);}
INLINE(Vdhi,VDHI_VEQN) (Vdhi a, Vdhi b) {return DHI_VEQN(a,b);}
INLINE(Vdhi,VDHF_VEQN) (Vdhf a, Vdhf b) {return DHF_VEQN(a,b);}
INLINE(Vdwu,VDWU_VEQN) (Vdwu a, Vdwu b) {return DWU_VEQN(a,b);}
INLINE(Vdwi,VDWI_VEQN) (Vdwi a, Vdwi b) {return DWI_VEQN(a,b);}
INLINE(Vdwi,VDWF_VEQN) (Vdwf a, Vdwf b) {return DWF_VEQN(a,b);}

INLINE(Vqbu,VQBU_VEQN) (Vqbu a, Vqbu b) {return QBU_VEQN(a,b);}
INLINE(Vqbi,VQBI_VEQN) (Vqbi a, Vqbi b) {return QBI_VEQN(a,b);}
INLINE(Vqbc,VQBC_VEQN) (Vqbc a, Vqbc b) {a.V0 = QBC_VEQN(a.V0,b.V0); return a;}
INLINE(Vqhu,VQHU_VEQN) (Vqhu a, Vqhu b) {return QHU_VEQN(a,b);}
INLINE(Vqhi,VQHI_VEQN) (Vqhi a, Vqhi b) {return QHI_VEQN(a,b);}
INLINE(Vqhi,VQHF_VEQN) (Vqhf a, Vqhf b) {return QHF_VEQN(a,b);}
INLINE(Vqwu,VQWU_VEQN) (Vqwu a, Vqwu b) {return QWU_VEQN(a,b);}
INLINE(Vqwi,VQWI_VEQN) (Vqwi a, Vqwi b) {return QWI_VEQN(a,b);}
INLINE(Vqwi,VQWF_VEQN) (Vqwf a, Vqwf b) {return QWF_VEQN(a,b);}
INLINE(Vqdu,VQDU_VEQN) (Vqdu a, Vqdu b) {return QDU_VEQN(a,b);}
INLINE(Vqdi,VQDI_VEQN) (Vqdi a, Vqdi b) {return QDI_VEQN(a,b);}
INLINE(Vqdi,VQDF_VEQN) (Vqdf a, Vqdf b) {return QDF_VEQN(a,b);}

#if 0 // _LEAVE_ARM_VEQN
}
#endif

#if 0 // _ENTER_ARM_VEQL
{
#endif

#if CHAR_MIN
#   define DBC_VEQL DBI_VEQL
#   define QBC_VEQL QBI_VEQL
#else
#   define DBC_VEQL DBU_VEQL
#   define QBC_VEQL QBU_VEQL
#endif
/*

    [0] cmeq    v0.8b, v0.8b, v1.8b
    [1] fmov    x8, d0
    [2] rev     x8, x8
    [3] clz     x8, x8
    [4] lsl     x8, x8, #3
    [5] fmov    d0, x8
    
The difference between vrev64_u8+__builtin_clzll and just
__builtin_ctzll is [2] is replaced with rbit x8, x8 for 
the latter. Not sure which is better 

*/
INLINE( uint8x8_t,DBU_VEQL)   (uint8x8_t a,   uint8x8_t b)
{
    DWRD_VTYPE  c = {.B.U=vceq_u8(a, b)};
    c.B.U = vrev64_u8(c.B.U);
    c.U = __builtin_clzll(c.U)/8;
    return  c.B.U;
}

INLINE(  int8x8_t,DBI_VEQL)    (int8x8_t a,    int8x8_t b)
{
    DWRD_VTYPE  c = {.B.U=vceq_s8(a, b)};
    c.B.U = vrev64_u8(c.B.U);
    c.U = __builtin_clzll(c.U)/8;
    //c.U = __builtin_ctzll(c.U)/8;
    return  c.B.I;
}

INLINE(uint16x4_t,DHU_VEQL)  (uint16x4_t a,  uint16x4_t b)
{
    DWRD_VTYPE  c = {.H.U=vceq_u16(a, b)};
    c.U = __builtin_ctzll(c.U)/16;
    return  c.H.U;
}

INLINE( int16x4_t,DHI_VEQL)   (int16x4_t a,   int16x4_t b)
{
    DWRD_VTYPE  c = {.H.U=vceq_s16(a, b)};
    c.U = __builtin_ctzll(c.U)/16;
    return  c.H.I;
}

INLINE( int16x4_t,DHF_VEQL) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE  c = {.H.I=DHF_CEQS(a, b)};
    c.U = __builtin_ctzll(c.U)/16;
    return  c.H.I;
}

INLINE(uint32x2_t,DWU_VEQL)  (uint32x2_t a,  uint32x2_t b)
{
    DWRD_VTYPE  c = {.W.U=vceq_u32(a, b)};
    c.U = __builtin_ctzll(c.U)/32;
    return  c.W.U;
}

INLINE( int32x2_t,DWI_VEQL)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE  c = {.W.I=vceq_s32(a, b)};
    c.U = __builtin_ctzll(c.U)/32;
    return  c.W.I;
}

INLINE( int32x2_t,DWF_VEQL) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE  c = {.W.U=vceq_f32(a, b)};
    c.U = __builtin_ctzll(c.U)/32;
    return  c.W.I;
}


INLINE(uint64_t,MY_VEQLQ_CSZL) (QUAD_UTYPE x)
{
    QUAD_TYPE   c = {.U=x};
    c.Lo.U = __builtin_ctzll(c.Lo.U);
    if (c.Lo.U == 64)
        c.Lo.U += __builtin_ctzll(c.Hi.U);
    return  c.Lo.U;
}

/*  Without the inline MY_VEQLQ_CSZL, clang will use a 
    branch for the 128 bit veql ops.
    TODO: optimize 
*/
INLINE(uint8x16_t,QBU_VEQL)  (uint8x16_t a,  uint8x16_t b)
{
    QUAD_VTYPE q = {.B.U=vceqq_u8(a, b)};
    return  ((uint8x16_t){MY_VEQLQ_CSZL(q.U)/8});
}


INLINE( int8x16_t,QBI_VEQL)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE q = {.B.U=vceqq_s8(a, b)};
    return  ((int8x16_t){MY_VEQLQ_CSZL(q.U)/8});
}


INLINE(uint16x8_t,QHU_VEQL)  (uint16x8_t a,  uint16x8_t b)
{
    QUAD_VTYPE q = {.H.U=vceqq_u16(a, b)};
    return  ((uint16x8_t){MY_VEQLQ_CSZL(q.U)/16});
}

INLINE( int16x8_t,QHI_VEQL)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE q = {.H.U=vceqq_s16(a, b)};
    return  ((int16x8_t){MY_VEQLQ_CSZL(q.U)/16});
}

INLINE( int16x8_t,QHF_VEQL) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE q = {.H.I=QHF_CEQS(a, b)};
    return  ((int16x8_t){MY_VEQLQ_CSZL(q.U)/16});
}

INLINE(uint32x4_t,QWU_VEQL)  (uint32x4_t a,  uint32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_s32(a, b)};
    return  ((uint32x4_t){MY_VEQLQ_CSZL(q.U)/32});
}

INLINE( int32x4_t,QWI_VEQL)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_s32(a, b)};
    return  ((int32x4_t){MY_VEQLQ_CSZL(q.U)/32});
}

INLINE( int32x4_t,QWF_VEQL) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_f32(a, b)};
    return  ((int32x4_t){MY_VEQLQ_CSZL(q.U)/32});
}

INLINE(uint64x2_t,QDU_VEQL)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE q = {.D.U=vceqq_u64(a, b)};
    return  ((uint64x2_t){MY_VEQLQ_CSZL(q.U)/64});
}

INLINE( int64x2_t,QDI_VEQL)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE q = {.D.U=vceqq_s64(a, b)};
    return  ((uint64x2_t){MY_VEQLQ_CSZL(q.U)/64});
}

INLINE( int64x2_t,QDF_VEQL) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE q = {.D.U=vceqq_f64(a, b)};
    return  ((uint64x2_t){MY_VEQLQ_CSZL(q.U)/64});
}


INLINE(Vwbu,VWBU_VEQL) (Vwbu a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.U = DBU_VEQL(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbi,VWBI_VEQL) (Vwbi a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.I = DBI_VEQL(x.B.I, y.B.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbc,VWBC_VEQL) (Vwbc a, Vwbc b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
#if CHAR_MIN
    x.B.I = DBI_VEQL(x.B.I, y.B.I);
#else
    x.B.U = DBU_VEQL(x.B.U, y.B.U);
#endif
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}


INLINE(Vwhu,VWHU_VEQL) (Vwhu a, Vwhu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.U = DHU_VEQL(x.H.U, y.H.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHI_VEQL) (Vwhi a, Vwhi b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHI_VEQL(x.H.I, y.H.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHF_VEQL) (Vwhf a, Vwhf b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHF_VEQL(x.H.F, y.H.F);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return ((Vwhi){a.V0});
}

INLINE(Vdbu,VDBU_VEQL) (Vdbu a, Vdbu b) {return DBU_VEQL(a,b);}
INLINE(Vdbi,VDBI_VEQL) (Vdbi a, Vdbi b) {return DBI_VEQL(a,b);}
INLINE(Vdbc,VDBC_VEQL) (Vdbc a, Vdbc b) {a.V0 = DBC_VEQL(a.V0,b.V0); return a;}
INLINE(Vdhu,VDHU_VEQL) (Vdhu a, Vdhu b) {return DHU_VEQL(a,b);}
INLINE(Vdhi,VDHI_VEQL) (Vdhi a, Vdhi b) {return DHI_VEQL(a,b);}
INLINE(Vdhi,VDHF_VEQL) (Vdhf a, Vdhf b) {return DHF_VEQL(a,b);}
INLINE(Vdwu,VDWU_VEQL) (Vdwu a, Vdwu b) {return DWU_VEQL(a,b);}
INLINE(Vdwi,VDWI_VEQL) (Vdwi a, Vdwi b) {return DWI_VEQL(a,b);}
INLINE(Vdwi,VDWF_VEQL) (Vdwf a, Vdwf b) {return DWF_VEQL(a,b);}

INLINE(Vqbu,VQBU_VEQL) (Vqbu a, Vqbu b) {return QBU_VEQL(a,b);}
INLINE(Vqbi,VQBI_VEQL) (Vqbi a, Vqbi b) {return QBI_VEQL(a,b);}
INLINE(Vqbc,VQBC_VEQL) (Vqbc a, Vqbc b) {a.V0 = QBC_VEQL(a.V0,b.V0); return a;}
INLINE(Vqhu,VQHU_VEQL) (Vqhu a, Vqhu b) {return QHU_VEQL(a,b);}
INLINE(Vqhi,VQHI_VEQL) (Vqhi a, Vqhi b) {return QHI_VEQL(a,b);}
INLINE(Vqhi,VQHF_VEQL) (Vqhf a, Vqhf b) {return QHF_VEQL(a,b);}
INLINE(Vqwu,VQWU_VEQL) (Vqwu a, Vqwu b) {return QWU_VEQL(a,b);}
INLINE(Vqwi,VQWI_VEQL) (Vqwi a, Vqwi b) {return QWI_VEQL(a,b);}
INLINE(Vqwi,VQWF_VEQL) (Vqwf a, Vqwf b) {return QWF_VEQL(a,b);}
INLINE(Vqdu,VQDU_VEQL) (Vqdu a, Vqdu b) {return QDU_VEQL(a,b);}
INLINE(Vqdi,VQDI_VEQL) (Vqdi a, Vqdi b) {return QDI_VEQL(a,b);}
INLINE(Vqdi,VQDF_VEQL) (Vqdf a, Vqdf b) {return QDF_VEQL(a,b);}

#if 0 // _LEAVE_ARM_VEQL
}
#endif

#if 0 // _ENTER_ARM_VEQR
{
#endif
/*
DHU_VEQR:
    cmeq        v0.4h,  v0.4h, v1.4h
    mov         w8,     #48
    mov         w10,    #128
    fmov        x9,     d0
    movi        d0,     #0
    cmp         x9,     #0
    clz         x9,     x9
    csel        w8,     w10,    w8,     eq
    sub         w8,     w8,     w9
    lsr         w8,     w8,     #4
    mov         v0.4[0],w8
    ret

11 instructions to find the rightmost matching vector lane
seems way too steep a cos but w/e.
    
*/
#if CHAR_MIN
#   define DBC_VEQR DBI_VEQR
#   define QBC_VEQR QBI_VEQR
#else
#   define DBC_VEQR DBU_VEQR
#   define QBC_VEQR QBU_VEQR
#endif

INLINE( uint8x8_t,DBU_VEQR)   (uint8x8_t a,   uint8x8_t b)
{
    DWRD_VTYPE  c = {.B.U=vceq_u8(a, b)};
    uint64_t n = 128-(c.U ? (64+8) : 0);
    return  ((uint8x8_t){(n-__builtin_clzll(c.U))/8});
}

INLINE(  int8x8_t,DBI_VEQR)    (int8x8_t a,    int8x8_t b)
{
    DWRD_VTYPE  c = {.B.U=vceq_s8(a, b)};
    uint64_t    n = c.U ? 56 : 128;
    return  ((int8x8_t){(n-__builtin_clzll(c.U))/8});
}


INLINE(uint16x4_t,DHU_VEQR)  (uint16x4_t a,  uint16x4_t b)
{
    DWRD_VTYPE  c = {.H.U=vceq_u16(a, b)};
    uint64_t n = 128-(c.U ? (64+16) : 0);
    return  ((uint16x4_t){(n-__builtin_clzll(c.U))/16});
}

INLINE( int16x4_t,DHI_VEQR)   (int16x4_t a,   int16x4_t b)
{
    DWRD_VTYPE  c = {.H.U=vceq_s16(a, b)};
    uint64_t n = 128-(c.U ? (64+16) : 0);
    return  ((int16x4_t){(n-__builtin_clzll(c.U))/16});
}

INLINE( int16x4_t,DHF_VEQR) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE  c = {.H.I=DHF_CEQS(a, b)};
    uint64_t n = 128-(c.U ? (64+16) : 0);
    return  ((int16x4_t){(n-__builtin_clzll(c.U))/16});
}

INLINE(uint32x2_t,DWU_VEQR)  (uint32x2_t a,  uint32x2_t b)
{
    DWRD_VTYPE  c = {.W.U=vceq_u32(a, b)};
    uint64_t n = 128-(c.U ? (64+32) : 0);
    return  ((uint32x2_t){(n-__builtin_clzll(c.U))/32});
}

INLINE( int32x2_t,DWI_VEQR)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE  c = {.W.U=vceq_s32(a, b)};
    uint64_t n = 128-(c.U ? (64+32) : 0);
    return  ((int32x2_t){(n-__builtin_clzll(c.U))/32});
}

INLINE( int32x2_t,DWF_VEQR) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE  c = {.W.U=vceq_f32(a, b)};
    uint64_t n = 128-(c.U ? (64+32) : 0);
    return  ((int32x2_t){(n-__builtin_clzll(c.U))/32});
}


INLINE(uint8x16_t,QBU_VEQR)  (uint8x16_t a,  uint8x16_t b)
{
/*
Compare 16 corresponding bytes of two 128 bit vectors for
equality and set the lowest 4 bits of the result vector to 
the rightmost lane that was equal, or 16 if no comparison
was true. All other bits in the result are set to zero.

    cmeq        v0.16b,  v0.16b, v1.16b
    mov         w8,     #56
    mov         w11,    #192
    mov         x9,     v0.d[1]
    fmov        x10,    d0

    movi        v0.2d,  #0
    cmp         x10,    #0
    csel        w8,     w11,    w8,     eq
    cmp         x9,     #0
    mov         w11,    #120

    csel        x9,     x10,    x9,     eq
    csel        w8,     w8,    w11,     eq
    clz         x9,     x9
    sub         w8,     w8,     w9
    lsr         w8,     w8,     #3

    mov         v0.b[0],w8
    ret
    
*/
    QUAD_VTYPE  c = {.B.U=vceqq_u8(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r;
    if (l)
        r = (128-8);
    else 
    {
        l = vgetq_lane_u64(c.D.U, 0);
        r = l ? (64-8) : 192;
    }
    return  ((uint8x16_t){(r-__builtin_clzll(l))/8});
}


INLINE( int8x16_t,QBI_VEQR)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE  c = {.B.U=vceqq_s8(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r;
    if (l)
        r = (128-8);
    else 
    {
        l = vgetq_lane_u64(c.D.U, 0);
        r = l ? (64-8) : 192;
    }
    return  ((int8x16_t){(r-__builtin_clzll(l))/8});
}

INLINE(uint16x8_t,QHU_VEQR)  (uint16x8_t a,  uint16x8_t b)
{
    QUAD_VTYPE  c = {.H.U=vceqq_u16(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r;
    if (l)
        r = (128-16);
    else 
    {
        l = vgetq_lane_u64(c.D.U, 0);
        r = l ? (64-16) : 192;
    }
    return  ((uint16x8_t){(r-__builtin_clzll(l))/16});
}

INLINE( int16x8_t,QHI_VEQR)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE  c = {.H.U=vceqq_s16(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r;
    if (l)
        r = (128-16);
    else 
    {
        l = vgetq_lane_u64(c.D.U, 0);
        r = l ? (64-16) : 192;
    }
    return  ((int16x8_t){(r-__builtin_clzll(l))/16});
}

INLINE( int16x8_t,QHF_VEQR) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE  c = {.H.I=QHF_CEQS(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r;
    if (l)
        r = (128-16);
    else 
    {
        l = vgetq_lane_u64(c.D.U, 0);
        r = l ? (64-16) : 192;
    }
    return  ((uint16x8_t){(r-__builtin_clzll(l))/16});
}

INLINE(uint32x4_t,QWU_VEQR)  (uint32x4_t a,  uint32x4_t b)
{
    QUAD_VTYPE  c = {.W.U=vceqq_u32(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r;
    if (l)
        r = (128-32);
    else 
    {
        l = vgetq_lane_u64(c.D.U, 0);
        r = l ? (64-32) : 192;
    }
    return  ((uint32x4_t){(r-__builtin_clzll(l))/32});
}

INLINE( int32x4_t,QWI_VEQR)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE  c = {.W.U=vceqq_s32(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r;
    if (l)
        r = (128-32);
    else 
    {
        l = vgetq_lane_u64(c.D.U, 0);
        r = l ? (64-32) : 192;
    }
    return  ((int32x4_t){(r-__builtin_clzll(l))/32});
}

INLINE( int32x4_t,QWF_VEQR) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE  c = {.W.U=vceqq_f32(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r;
    if (l)
        r = (128-32);
    else 
    {
        l = vgetq_lane_u64(c.D.U, 0);
        r = l ? (64-32) : 192;
    }
    return  ((uint32x4_t){(r-__builtin_clzll(l))/32});
}

INLINE(uint64x2_t,QDU_VEQR)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE  c = {.D.U=vceqq_u64(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r = vgetq_lane_u64(c.D.U, 0);
    r = l ? 1 : r ? 0 : 2;
    return  ((uint64x2_t){r});
}

INLINE( int64x2_t,QDI_VEQR)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE  c = {.D.U=vceqq_s64(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r = vgetq_lane_u64(c.D.U, 0);
    r = l ? 1 : r ? 0 : 2;
    return  ((int64x2_t){r});
}

INLINE( int64x2_t,QDF_VEQR) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE  c = {.D.U=vceqq_f64(a, b)};
    uint64_t l = vgetq_lane_u64(c.D.U, 1);
    uint64_t r = vgetq_lane_u64(c.D.U, 0);
    r = l ? 1 : r ? 0 : 2;
    return  ((int64x2_t){r});
}


INLINE(Vwbu,VWBU_VEQR) (Vwbu a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.U = DBU_VEQR(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbi,VWBI_VEQR) (Vwbi a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.I = DBI_VEQR(x.B.I, y.B.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbc,VWBC_VEQR) (Vwbc a, Vwbc b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
#if CHAR_MIN
    x.B.I = DBI_VEQR(x.B.I, y.B.I);
#else
    x.B.U = DBU_VEQR(x.B.U, y.B.U);
#endif
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}


INLINE(Vwhu,VWHU_VEQR) (Vwhu a, Vwhu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.U = DHU_VEQR(x.H.U, y.H.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHI_VEQR) (Vwhi a, Vwhi b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHI_VEQR(x.H.I, y.H.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHF_VEQR) (Vwhf a, Vwhf b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHF_VEQR(x.H.F, y.H.F);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return ((Vwhi){a.V0});
}

INLINE(Vdbu,VDBU_VEQR) (Vdbu a, Vdbu b) {return DBU_VEQR(a,b);}
INLINE(Vdbi,VDBI_VEQR) (Vdbi a, Vdbi b) {return DBI_VEQR(a,b);}
INLINE(Vdbc,VDBC_VEQR) (Vdbc a, Vdbc b) {a.V0 = DBC_VEQR(a.V0,b.V0); return a;}
INLINE(Vdhu,VDHU_VEQR) (Vdhu a, Vdhu b) {return DHU_VEQR(a,b);}
INLINE(Vdhi,VDHI_VEQR) (Vdhi a, Vdhi b) {return DHI_VEQR(a,b);}
INLINE(Vdhi,VDHF_VEQR) (Vdhf a, Vdhf b) {return DHF_VEQR(a,b);}
INLINE(Vdwu,VDWU_VEQR) (Vdwu a, Vdwu b) {return DWU_VEQR(a,b);}
INLINE(Vdwi,VDWI_VEQR) (Vdwi a, Vdwi b) {return DWI_VEQR(a,b);}
INLINE(Vdwi,VDWF_VEQR) (Vdwf a, Vdwf b) {return DWF_VEQR(a,b);}

INLINE(Vqbu,VQBU_VEQR) (Vqbu a, Vqbu b) {return QBU_VEQR(a,b);}
INLINE(Vqbi,VQBI_VEQR) (Vqbi a, Vqbi b) {return QBI_VEQR(a,b);}
INLINE(Vqbc,VQBC_VEQR) (Vqbc a, Vqbc b) {a.V0 = QBC_VEQR(a.V0,b.V0); return a;}
INLINE(Vqhu,VQHU_VEQR) (Vqhu a, Vqhu b) {return QHU_VEQR(a,b);}
INLINE(Vqhi,VQHI_VEQR) (Vqhi a, Vqhi b) {return QHI_VEQR(a,b);}
INLINE(Vqhi,VQHF_VEQR) (Vqhf a, Vqhf b) {return QHF_VEQR(a,b);}
INLINE(Vqwu,VQWU_VEQR) (Vqwu a, Vqwu b) {return QWU_VEQR(a,b);}
INLINE(Vqwi,VQWI_VEQR) (Vqwi a, Vqwi b) {return QWI_VEQR(a,b);}
INLINE(Vqwi,VQWF_VEQR) (Vqwf a, Vqwf b) {return QWF_VEQR(a,b);}
INLINE(Vqdu,VQDU_VEQR) (Vqdu a, Vqdu b) {return QDU_VEQR(a,b);}
INLINE(Vqdi,VQDI_VEQR) (Vqdi a, Vqdi b) {return QDI_VEQR(a,b);}
INLINE(Vqdi,VQDF_VEQR) (Vqdf a, Vqdf b) {return QDF_VEQR(a,b);}

#if 0 // _LEAVE_ARM_VEQR
}
#endif


#if 0 // _ENTER_ARM_VNES
{
#endif

#if CHAR_MIN
#   define DBC_VNES DBI_VNES
#   define QBC_VNES QBI_VNES
#else
#   define DBC_VNES DBU_VNES
#   define QBC_VNES QBU_VNES
#endif

INLINE(float,WYU_VNES) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.W.U = vceq_u32(x.W.U, y.W.U);
    x.W.U = vmvn_u32(x.W.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WBZ_VNES) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b}, z={0};
    x.B.U = vceq_u8(x.B.U, y.B.U);
    x.B.U = vmvn_u8(x.B.U);
    x.B.U = vcgt_u8(x.B.U, z.B.U);
    x.W.U = vcgt_u32(x.W.U, z.W.U);
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WHZ_VNES) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.H.U = vceq_u16(x.H.U, y.H.U);
    x.W.U = vtst_u32(x.W.U, vcreate_u32(UINT32_MAX));
    return  vget_lane_f32(x.W.F, 0);
}

INLINE(float,WHF_VNES) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.H.I = DHF_CEQS(x.H.F, y.H.F);
    x.W.U = vtst_u32(x.W.U, vcreate_u32(UINT32_MAX));
    return  vget_lane_f32(x.W.F, 0);
}


INLINE(uint64x1_t,DYU_VNES)  (uint64x1_t a,  uint64x1_t b)
{
    DWRD_VTYPE x={.D.U=a}, y={.D.U=b}, z;
    z.D.U = vceq_u64(y.W.U, z.W.U);
    z.B.U = vmvn_u8(z.B.U);
    return  z.D.U;
}

INLINE( uint8x8_t,DBU_VNES)   (uint8x8_t a,   uint8x8_t b)
{
/*
	cmeq	v0.8b, v0.8b, v1.8b
	    // v0 = {a[i]==b[i] for i in [0..a.nel-1]}

	fmov	x8, d0
	cmn	    x8, #1
	    // set pstate.NZCV = 
	        mi|ne if x8 < -1 else 
	        gt|ne if x8 > -1 else
	        eq
	csetm	x8, ne
	    x8 = -1 if eq in pstate.NZCV else 0
	    
	fmov	d0, x8
	ret
*/
    DWRD_VTYPE c = {.B.U=vceq_u8(a, b)};
    c.U = -(c.U != UINT64_MAX);
    return  c.B.U;
}

INLINE(  int8x8_t,DBI_VNES)    (int8x8_t a,    int8x8_t b)
{
    DWRD_VTYPE c = {.B.U=vceq_s8(a, b)};
    c.U = -(c.U != UINT64_MAX);
    return  c.B.I;
}

INLINE(uint16x4_t,DHU_VNES)  (uint16x4_t a,  uint16x4_t b)
{
    DWRD_VTYPE c = {.H.U=vceq_u16(a, b)};
    c.U = -(c.U != UINT64_MAX);
    return  c.H.U;
}

INLINE( int16x4_t,DHI_VNES)   (int16x4_t a,   int16x4_t b)
{
    DWRD_VTYPE c = {.H.U=vceq_s16(a, b)};
    c.U = -(c.U != UINT64_MAX);
    return  c.H.I;
}

INLINE( int16x4_t,DHF_VNES) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE c = {.H.I=DHF_CEQS(a, b)};
    c.U = -(c.U != UINT64_MAX);
    return  c.H.U;
}

INLINE(uint32x2_t,DWU_VNES)  (uint32x2_t a,  uint32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_u32(a, b)};
    c.U = -(c.U != UINT64_MAX);
    return  c.W.U;
}

INLINE( int32x2_t,DWI_VNES)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_s32(a, b)};
    c.U = -(c.U != UINT64_MAX);
    return  c.W.I;
}

INLINE( int32x2_t,DWF_VNES) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_f32(a, b)};
    c.U = -(c.U != UINT64_MAX);
    return  c.W.I;
}

INLINE(uint64x2_t,QYU_VNES)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_u64(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vand_u64(l.B.U, r.B.U);
    l.B.U = vmvn_u8(l.B.U);
    return  vdupq_lane_u64(l.D.U, 0);
}

INLINE(uint8x16_t,QBU_VNES)  (uint8x16_t a,  uint8x16_t b)
{
    QUAD_VTYPE q = {.B.U=vceqq_u8(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    r.U = -(r.U != UINT64_MAX);
    q.D.U = vdupq_lane_u64(r.D.U, 0);
    return  q.B.U;
}

INLINE( int8x16_t,QBI_VNES)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE q = {.B.U=vceqq_s8(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    r.U = -(r.U != UINT64_MAX);
    q.D.U = vdupq_lane_u64(r.D.U, 0);
    return  q.B.I;
}

INLINE(uint16x8_t,QHU_VNES)  (uint16x8_t a,  uint16x8_t b)
{
    QUAD_VTYPE q = {.H.U=vceqq_u16(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    r.U = -(r.U != UINT64_MAX);
    q.D.U = vdupq_lane_u64(r.D.U, 0);
    return  q.H.U;
}

INLINE( int16x8_t,QHI_VNES)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE q = {.H.U=vceqq_s16(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    r.U = -(r.U != UINT64_MAX);
    q.D.U = vdupq_lane_u64(r.D.U, 0);
    return  q.H.I;
}

INLINE( int16x8_t,QHF_VNES) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE q = {.H.I=QHF_CEQS(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    r.U = -(r.U != UINT64_MAX);
    q.D.U = vdupq_lane_u64(r.D.U, 0);
    return  q.H.I;
}

INLINE(uint32x4_t,QWU_VNES)  (uint32x4_t a,  uint32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_u32(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    r.U = -(r.U != UINT64_MAX);
    q.D.U = vdupq_lane_u64(r.D.U, 0);
    return  q.W.U;
}

INLINE( int32x4_t,QWI_VNES)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_s32(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    r.U = -(r.U != UINT64_MAX);
    q.D.U = vdupq_lane_u64(r.D.U, 0);
    return  q.W.I;
}

INLINE( int32x4_t,QWF_VNES) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_f32(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    r.U = -(r.U != UINT64_MAX);
    q.D.U = vdupq_lane_u64(r.D.U, 0);
    return  q.W.I;
}


INLINE(uint64x2_t,QDU_VNES)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_u64(a,b)};
    c.B.U = vmvnq_u8(c.B.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vand_u8(l.B.U, r.B.U);
    return  vdupq_lane_u64(l.D.U, 0);
}

INLINE( int64x2_t,QDI_VNES)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_s64(a,b)};
    c.B.U = vmvnq_u8(c.B.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vand_u8(l.B.U, r.B.U);
    return  vdupq_lane_s64(l.D.U, 0);
}

INLINE( int64x2_t,QDF_VNES) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_f64(a,b)};
    c.B.U = vmvnq_u8(c.B.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vand_u8(l.B.U, r.B.U);
    return  vdupq_lane_s64(l.D.I, 0);
}

INLINE(Vwyu,VWYU_VNES) (Vwyu a, Vwyu b) {a.V0=WYU_VNES(a.V0,b.V0); return a;}
INLINE(Vwbu,VWBU_VNES) (Vwbu a, Vwbu b) {a.V0=WBZ_VNES(a.V0,b.V0); return a;}
INLINE(Vwbi,VWBI_VNES) (Vwbi a, Vwbi b) {a.V0=WBZ_VNES(a.V0,b.V0); return a;}
INLINE(Vwbc,VWBC_VNES) (Vwbc a, Vwbc b) {a.V0=WBZ_VNES(a.V0,b.V0); return a;}
INLINE(Vwhu,VWHU_VNES) (Vwhu a, Vwhu b) {a.V0=WHZ_VNES(a.V0,b.V0); return a;}
INLINE(Vwhi,VWHI_VNES) (Vwhi a, Vwhi b) {a.V0=WHZ_VNES(a.V0,b.V0); return a;}
INLINE(Vwhi,VWHF_VNES) (Vwhi a, Vwhi b) {return ((Vwhi){WHZ_VNES(a.V0,b.V0)});}

INLINE(Vdyu,VDYU_VNES) (Vdyu a, Vdyu b) {a.V0=DYU_VNES(a.V0,b.V0); return a;}
INLINE(Vdbu,VDBU_VNES) (Vdbu a, Vdbu b) {return DBU_VNES(a, b);}
INLINE(Vdbi,VDBI_VNES) (Vdbi a, Vdbi b) {return DBI_VNES(a, b);}
INLINE(Vdbc,VDBC_VNES) (Vdbc a, Vdbc b) {a.V0=DBC_VNES(a.V0,b.V0); return a;}
INLINE(Vdhu,VDHU_VNES) (Vdhu a, Vdhu b) {return DHU_VNES(a, b);}
INLINE(Vdhi,VDHI_VNES) (Vdhi a, Vdhi b) {return DHI_VNES(a, b);}
INLINE(Vdhi,VDHF_VNES) (Vdhf a, Vdhf b) {return ((Vdhi){DHF_VNES(a, b)});}
INLINE(Vdwu,VDWU_VNES) (Vdwu a, Vdwu b) {return DWU_VNES(a, b);}
INLINE(Vdwi,VDWI_VNES) (Vdwi a, Vdwi b) {return DWI_VNES(a, b);}
INLINE(Vdwi,VDWF_VNES) (Vdwf a, Vdwf b) {return DWF_VNES(a, b);}

INLINE(Vqyu,VQYU_VNES) (Vqyu a, Vqyu b) {a.V0=QYU_VNES(a.V0,b.V0); return a;}
INLINE(Vqbu,VQBU_VNES) (Vqbu a, Vqbu b) {return QBU_VNES(a, b);}
INLINE(Vqbi,VQBI_VNES) (Vqbi a, Vqbi b) {return QBI_VNES(a, b);}
INLINE(Vqbc,VQBC_VNES) (Vqbc a, Vqbc b) {a.V0=QBC_VNES(a.V0,b.V0); return a;}
INLINE(Vqhu,VQHU_VNES) (Vqhu a, Vqhu b) {return QHU_VNES(a, b);}
INLINE(Vqhi,VQHI_VNES) (Vqhi a, Vqhi b) {return QHI_VNES(a, b);}
INLINE(Vqhi,VQHF_VNES) (Vqhf a, Vqhf b) {return ((Vqhi){QHF_VNES(a, b)});}
INLINE(Vqwu,VQWU_VNES) (Vqwu a, Vqwu b) {return QWU_VNES(a, b);}
INLINE(Vqwi,VQWI_VNES) (Vqwi a, Vqwi b) {return QWI_VNES(a, b);}
INLINE(Vqwi,VQWF_VNES) (Vqwf a, Vqwf b) {return QWF_VNES(a, b);}
INLINE(Vqdu,VQDU_VNES) (Vqdu a, Vqdu b) {return QDU_VNES(a, b);}
INLINE(Vqdi,VQDI_VNES) (Vqdi a, Vqdi b) {return QDI_VNES(a, b);}
INLINE(Vqdi,VQDF_VNES) (Vqdf a, Vqdf b) {return QDF_VNES(a, b);}

#if 0 // _LEAVE_ARM_VNES
}
#endif

#if 0 // _ENTER_ARM_VNEY
{
#endif

#if CHAR_MIN
#   define DBC_VNEY DBI_VNEY
#   define QBC_VNEY QBI_VNEY
#else
#   define DBC_VNEY DBU_VNEY
#   define QBC_VNEY QBU_VNEY
#endif

INLINE(uint32_t,WYU_VNEY) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.W.U = vceq_u32(x.W.U, y.W.U);
    return  UINT32_MAX != vget_lane_u32(x.W.U, 0);
}

INLINE(uint32_t,WBZ_VNEY) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b}, z={0};
    x.B.U = vceq_u8(x.B.U, y.B.U);
    return  UINT32_MAX != vget_lane_u32(x.W.U, 0);
}

INLINE(uint32_t,WHZ_VNEY) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.B.U = vceq_u16(x.H.U, y.H.U);
    return  UINT32_MAX != vget_lane_u32(x.W.U, 0);
}

INLINE(uint32_t,WHF_VNEY) (float a, float b)
{
    DWRD_VTYPE x={.W.F=a}, y={.W.F=b};
    x.H.I = DHF_CEQS(x.H.F, y.H.F);
    return  UINT32_MAX != vget_lane_u32(x.W.U, 0);
}


INLINE(uint64_t,DYU_VNEY)  (uint64x1_t a,  uint64x1_t b)
{
    DWRD_VTYPE c = {.D.U=vceq_u64(a, b)};
    return  UINT64_MAX != vget_lane_u64(c.D.U, 0);
}

INLINE(uint64_t,DBU_VNEY)   (uint8x8_t a,   uint8x8_t b)
{
    DWRD_VTYPE c = {.B.U=vceq_u8(a, b)};
    return  UINT64_MAX != c.U;
}

INLINE(uint64_t,DBI_VNEY)    (int8x8_t a,    int8x8_t b)
{
    DWRD_VTYPE c = {.B.U=vceq_s8(a, b)};
    return  UINT64_MAX != c.U;
}

INLINE(uint64_t,DHU_VNEY)  (uint16x4_t a,  uint16x4_t b)
{
    DWRD_VTYPE c = {.H.U=vceq_u16(a, b)};
    return  UINT64_MAX != c.U;
}

INLINE(uint64_t,DHI_VNEY)   (int16x4_t a,   int16x4_t b)
{
    DWRD_VTYPE c = {.H.U=vceq_s16(a, b)};
    return  UINT64_MAX != c.U;
}

INLINE(uint64_t,DHF_VNEY) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE c = {.H.I=DHF_CEQS(a, b)};
    return  UINT64_MAX != c.U;
}

INLINE(uint64_t,DWU_VNEY)  (uint32x2_t a,  uint32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_u32(a, b)};
    return  UINT64_MAX != c.U;
}

INLINE(uint64_t,DWI_VNEY)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_s32(a, b)};
    return  UINT64_MAX != c.U;
}

INLINE(uint64_t,DWF_VNEY) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE c = {.W.U=vceq_f32(a, b)};
    return  UINT64_MAX != c.U;
}

INLINE(uint64_t,QYU_VNEY)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_u64(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.B.U = vand_u64(l.B.U, r.B.U);
    return  UINT64_MAX != l.U;
}

INLINE(uint64_t,QBU_VNEY)  (uint8x16_t a,  uint8x16_t b)
{
    QUAD_VTYPE q = {.B.U=vceqq_u8(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    return  UINT64_MAX != r.U;
}

INLINE(uint64_t,QBI_VNEY)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE q = {.B.U=vceqq_s8(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    return  UINT64_MAX != r.U;
}

INLINE(uint64_t,QHU_VNEY)  (uint16x8_t a,  uint16x8_t b)
{
    QUAD_VTYPE q = {.H.U=vceqq_u16(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    return  UINT64_MAX != r.U;
}

INLINE(uint64_t,QHI_VNEY)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE q = {.H.U=vceqq_s16(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    return  UINT64_MAX != r.U;
}

INLINE(uint64_t,QHF_VNEY) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE q = {.H.I=QHF_CEQS(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    return  UINT64_MAX != r.U;
}

INLINE(uint64_t,QWU_VNEY)  (uint32x4_t a,  uint32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_u32(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    return  UINT64_MAX != r.U;
}

INLINE(uint64_t,QWI_VNEY)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_s32(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    return  UINT64_MAX != r.U;
}

INLINE(uint64_t,QWF_VNEY) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_f32(a, b)};
    DWRD_VTYPE l = {.D.U=vget_low_u64(q.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(q.D.U)};
    r.B.U = vand_u8(l.B.U, r.B.U);
    return  UINT64_MAX != r.U;
}


INLINE(uint64_t,QDU_VNEY)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_u64(a,b)};
    c.B.U = vmvnq_u8(c.B.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    return !(l.U&r.U);
}

INLINE(uint64_t,QDI_VNEY)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_s64(a,b)};
    c.B.U = vmvnq_u8(c.B.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    return !(l.U&r.U);
}

INLINE(uint64_t,QDF_VNEY) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE c = {.D.U=vceqq_f64(a,b)};
    c.B.U = vmvnq_u8(c.B.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    return !(l.U&r.U);
}

INLINE(_Bool,VWYU_VNEY) (Vwyu a, Vwyu b) {return WYU_VNEY(a.V0, b.V0);}
INLINE(_Bool,VWBU_VNEY) (Vwbu a, Vwbu b) {return WBZ_VNEY(a.V0, b.V0);}
INLINE(_Bool,VWBI_VNEY) (Vwbi a, Vwbi b) {return WBZ_VNEY(a.V0, b.V0);}
INLINE(_Bool,VWBC_VNEY) (Vwbc a, Vwbc b) {return WBZ_VNEY(a.V0, b.V0);}
INLINE(_Bool,VWHU_VNEY) (Vwhu a, Vwhu b) {return WHZ_VNEY(a.V0, b.V0);}
INLINE(_Bool,VWHI_VNEY) (Vwhi a, Vwhi b) {return WHZ_VNEY(a.V0, b.V0);}
INLINE(_Bool,VWHF_VNEY) (Vwhf a, Vwhf b) {return WHF_VNEY(a.V0, b.V0);}

INLINE(_Bool,VDYU_VNEY) (Vdyu a, Vdyu b) {return DYU_VNEY(a.V0, b.V0);}
INLINE(_Bool,VDBU_VNEY) (Vdbu a, Vdbu b) {return DBU_VNEY(a, b);}
INLINE(_Bool,VDBI_VNEY) (Vdbi a, Vdbi b) {return DBI_VNEY(a, b);}
INLINE(_Bool,VDBC_VNEY) (Vdbc a, Vdbc b) {return DBC_VNEY(a.V0, b.V0);}
INLINE(_Bool,VDHU_VNEY) (Vdhu a, Vdhu b) {return DHU_VNEY(a, b);}
INLINE(_Bool,VDHI_VNEY) (Vdhi a, Vdhi b) {return DHI_VNEY(a, b);}
INLINE(_Bool,VDHF_VNEY) (Vdhf a, Vdhf b) {return DHF_VNEY(a, b);}
INLINE(_Bool,VDWU_VNEY) (Vdwu a, Vdwu b) {return DWU_VNEY(a, b);}
INLINE(_Bool,VDWI_VNEY) (Vdwi a, Vdwi b) {return DWI_VNEY(a, b);}
INLINE(_Bool,VDWF_VNEY) (Vdwf a, Vdwf b) {return DWF_VNEY(a, b);}

INLINE(_Bool,VQYU_VNEY) (Vqyu a, Vqyu b) {return QYU_VNEY(a.V0, b.V0);}
INLINE(_Bool,VQBU_VNEY) (Vqbu a, Vqbu b) {return QBU_VNEY(a, b);}
INLINE(_Bool,VQBI_VNEY) (Vqbi a, Vqbi b) {return QBI_VNEY(a, b);}
INLINE(_Bool,VQBC_VNEY) (Vqbc a, Vqbc b) {return QBC_VNEY(a.V0, b.V0);}
INLINE(_Bool,VQHU_VNEY) (Vqhu a, Vqhu b) {return QHU_VNEY(a, b);}
INLINE(_Bool,VQHI_VNEY) (Vqhi a, Vqhi b) {return QHI_VNEY(a, b);}
INLINE(_Bool,VQHF_VNEY) (Vqhf a, Vqhf b) {return QHF_VNEY(a, b);}
INLINE(_Bool,VQWU_VNEY) (Vqwu a, Vqwu b) {return QWU_VNEY(a, b);}
INLINE(_Bool,VQWI_VNEY) (Vqwi a, Vqwi b) {return QWI_VNEY(a, b);}
INLINE(_Bool,VQWF_VNEY) (Vqwf a, Vqwf b) {return QWF_VNEY(a, b);}
INLINE(_Bool,VQDU_VNEY) (Vqdu a, Vqdu b) {return QDU_VNEY(a, b);}
INLINE(_Bool,VQDI_VNEY) (Vqdi a, Vqdi b) {return QDI_VNEY(a, b);}
INLINE(_Bool,VQDF_VNEY) (Vqdf a, Vqdf b) {return QDF_VNEY(a, b);}

#if 0 // _LEAVE_ARM_VNEY
}
#endif

#if 0 // _ENTER_ARM_VNEN
{
#endif

#if CHAR_MIN
#   define DBC_VNEN DBI_VNEN
#   define QBC_VNEN QBI_VNEN
#else
#   define DBC_VNEN DBU_VNEN
#   define QBC_VNEN QBU_VNEN
#endif

INLINE( uint8x8_t,DBU_VNEN)   (uint8x8_t a,   uint8x8_t b)
{
    DWRD_VTYPE  c = {.B.U=vceq_u8(a, b)};
    c.B.U   = vmvn_u8(c.B.U);
    c.B.U   = vshr_n_u8(c.B.U, 7);
    return  ((uint8x8_t){vaddv_u8(c.B.U)});
}

INLINE(  int8x8_t,DBI_VNEN)    (int8x8_t a,    int8x8_t b)
{
    DWRD_VTYPE  c = {.B.U=vceq_s8(a, b)};
    c.B.U   = vmvn_u8(c.B.U);
    c.B.U   = vshr_n_u8(c.B.U, 7);
    return  ((int8x8_t){vaddv_s8(c.B.I)});
}

INLINE(uint16x4_t,DHU_VNEN)  (uint16x4_t a,  uint16x4_t b)
{
    DWRD_VTYPE  c = {.H.U=vceq_u16(a, b)};
    c.B.U   = vmvn_u8(c.B.U);
    c.H.U   = vshr_n_u16(c.H.U, 15);
    return  ((uint16x4_t){vaddv_u16(c.H.U)});
}

INLINE( int16x4_t,DHI_VNEN)   (int16x4_t a,   int16x4_t b)
{
    DWRD_VTYPE  c = {.H.U=vceq_s16(a, b)};
    c.B.U   = vmvn_u8(c.B.U);
    c.H.U   = vshr_n_u16(c.H.U, 15);
    return  ((int16x4_t){vaddv_s16(c.H.I)});
}

INLINE( int16x4_t,DHF_VNEN) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE  c = {.H.I=DHF_CEQS(a,b)};
    c.B.U = vmvn_u8(c.B.U);
    c.H.U = vshr_n_u16(c.H.U, 15);
    return  ((int16x4_t){vaddv_s16(c.H.I)});
}

INLINE(uint32x2_t,DWU_VNEN)  (uint32x2_t a,  uint32x2_t b)
{
    DWRD_VTYPE  c = {.W.U=vceq_u32(a, b)};
    c.B.U   = vmvn_u8(c.B.U);
    c.W.U   = vshr_n_u32(c.W.U, 31);
    return  ((uint32x2_t){vaddv_u32(c.W.U)});
}

INLINE( int32x2_t,DWI_VNEN)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE  c = {.W.U=vceq_s32(a, b)};
    c.B.U   = vmvn_u8(c.B.U);
    c.W.U   = vshr_n_u32(c.W.U, 31);
    return  ((int32x2_t){vaddv_s32(c.W.I)});
}

INLINE( int32x2_t,DWF_VNEN) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE  c = {.W.U=vceq_f32(a, b)};
    c.B.U   = vmvn_u8(c.B.U);
    c.W.U   = vshr_n_u32(c.W.U, 31);
    return  ((int32x2_t){vaddv_s32(c.W.I)});
}


INLINE(uint8x16_t,QBU_VNEN)  (uint8x16_t a,  uint8x16_t b)
{
    QUAD_VTYPE  c = {.B.U=vceqq_u8(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.B.U   = vshrq_n_u8(c.B.U, 7);
    return  ((uint8x16_t){vaddvq_u8(c.B.U)});
}

INLINE( int8x16_t,QBI_VNEN)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE  c = {.B.U=vceqq_s8(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.B.U   = vshrq_n_u8(c.B.U, 7);
    return  ((int8x16_t){vaddvq_s8(c.B.I)});
}


INLINE(uint16x8_t,QHU_VNEN)  (uint16x8_t a,  uint16x8_t b)
{
    QUAD_VTYPE  c = {.H.U=vceqq_u16(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.H.U   = vshrq_n_u16(c.H.U, 15);
    return  ((uint16x8_t){vaddvq_u16(c.H.U)});
}

INLINE( int16x8_t,QHI_VNEN)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE  c = {.H.U=vceqq_s16(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.H.U   = vshrq_n_u16(c.H.U, 15);
    return  ((int16x8_t){vaddvq_s16(c.H.I)});
}

INLINE( int16x8_t,QHF_VNEN) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE  c = {.H.U=QHF_CEQS(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.H.U   = vshrq_n_u16(c.H.U, 15);
    return  ((int16x8_t){vaddvq_s16(c.H.I)});
}


INLINE(uint32x4_t,QWU_VNEN)  (uint32x4_t a,  uint32x4_t b)
{
    QUAD_VTYPE  c = {.W.U=vceqq_u32(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.W.U   = vshrq_n_u32(c.W.U, 31);
    return  ((uint32x4_t){vaddvq_u32(c.W.U)});
}

INLINE( int32x4_t,QWI_VNEN)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE  c = {.W.U=vceqq_s32(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.W.U   = vshrq_n_u32(c.W.U, 31);
    return  ((int32x4_t){vaddvq_s32(c.W.I)});
}

INLINE( int32x4_t,QWF_VNEN) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE  c = {.W.U=vceqq_f32(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.W.U   = vshrq_n_u32(c.W.U, 31);
    return  ((int32x4_t){vaddvq_s32(c.W.I)});
}


INLINE(uint64x2_t,QDU_VNEN)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE  c = {.D.U=vceqq_u64(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.D.U   = vshrq_n_u64(c.D.U, 63);
    return  ((uint64x2_t){vaddvq_u64(c.D.U)});
}

INLINE( int64x2_t,QDI_VNEN)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE  c = {.D.U=vceqq_s64(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.D.U   = vshrq_n_u64(c.D.U, 63);
    return  ((int64x2_t){vaddvq_s64(c.D.I)});
}

INLINE( int64x2_t,QDF_VNEN) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE  c = {.D.U=vceqq_f64(a, b)};
    c.B.U   = vmvnq_u8(c.B.U);
    c.D.U   = vshrq_n_u64(c.D.U, 63);
    return  ((int64x2_t){vaddvq_s64(c.D.I)});
}


INLINE(Vwbu,VWBU_VNEN) (Vwbu a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.U = DBU_VNEN(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbi,VWBI_VNEN) (Vwbi a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.I = DBI_VNEN(x.B.I, y.B.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbc,VWBC_VNEN) (Vwbc a, Vwbc b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.U = DBU_VNEN(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}


INLINE(Vwhu,VWHU_VNEN) (Vwhu a, Vwhu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.U = DHU_VNEN(x.H.U, y.H.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHI_VNEN) (Vwhi a, Vwhi b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHI_VNEN(x.H.I, y.H.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHF_VNEN) (Vwhf a, Vwhf b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHF_VNEN(x.H.F, y.H.F);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return ((Vwhi){a.V0});
}

INLINE(Vdbu,VDBU_VNEN) (Vdbu a, Vdbu b) {return DBU_VNEN(a,b);}
INLINE(Vdbi,VDBI_VNEN) (Vdbi a, Vdbi b) {return DBI_VNEN(a,b);}
INLINE(Vdbc,VDBC_VNEN) (Vdbc a, Vdbc b) {a.V0 = DBC_VNEN(a.V0,b.V0); return a;}
INLINE(Vdhu,VDHU_VNEN) (Vdhu a, Vdhu b) {return DHU_VNEN(a,b);}
INLINE(Vdhi,VDHI_VNEN) (Vdhi a, Vdhi b) {return DHI_VNEN(a,b);}
INLINE(Vdhi,VDHF_VNEN) (Vdhf a, Vdhf b) {return DHF_VNEN(a,b);}
INLINE(Vdwu,VDWU_VNEN) (Vdwu a, Vdwu b) {return DWU_VNEN(a,b);}
INLINE(Vdwi,VDWI_VNEN) (Vdwi a, Vdwi b) {return DWI_VNEN(a,b);}
INLINE(Vdwi,VDWF_VNEN) (Vdwf a, Vdwf b) {return DWF_VNEN(a,b);}

INLINE(Vqbu,VQBU_VNEN) (Vqbu a, Vqbu b) {return QBU_VNEN(a,b);}
INLINE(Vqbi,VQBI_VNEN) (Vqbi a, Vqbi b) {return QBI_VNEN(a,b);}
INLINE(Vqbc,VQBC_VNEN) (Vqbc a, Vqbc b) {a.V0 = QBC_VNEN(a.V0,b.V0); return a;}
INLINE(Vqhu,VQHU_VNEN) (Vqhu a, Vqhu b) {return QHU_VNEN(a,b);}
INLINE(Vqhi,VQHI_VNEN) (Vqhi a, Vqhi b) {return QHI_VNEN(a,b);}
INLINE(Vqhi,VQHF_VNEN) (Vqhf a, Vqhf b) {return QHF_VNEN(a,b);}
INLINE(Vqwu,VQWU_VNEN) (Vqwu a, Vqwu b) {return QWU_VNEN(a,b);}
INLINE(Vqwi,VQWI_VNEN) (Vqwi a, Vqwi b) {return QWI_VNEN(a,b);}
INLINE(Vqwi,VQWF_VNEN) (Vqwf a, Vqwf b) {return QWF_VNEN(a,b);}
INLINE(Vqdu,VQDU_VNEN) (Vqdu a, Vqdu b) {return QDU_VNEN(a,b);}
INLINE(Vqdi,VQDI_VNEN) (Vqdi a, Vqdi b) {return QDI_VNEN(a,b);}
INLINE(Vqdi,VQDF_VNEN) (Vqdf a, Vqdf b) {return QDF_VNEN(a,b);}

#if 0 // _LEAVE_ARM_VNEN
}
#endif


#if 0 // _ENTER_ARM_VNEL
{
#endif

#if CHAR_MIN
#   define DBC_VNEL DBI_VNEL
#   define QBC_VNEL QBI_VNEL
#else
#   define DBC_VNEL DBU_VNEL
#   define QBC_VNEL QBU_VNEL
#endif

INLINE( uint8x8_t,DBU_VNEL)   (uint8x8_t a,   uint8x8_t b)
{
    DWRD_VTYPE  c = {.B.U=vceq_u8(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.B.U = vrev64_u8(c.B.U);
    c.U = __builtin_clzll(c.U)/8;
    return  c.B.U;
}

INLINE(  int8x8_t,DBI_VNEL)    (int8x8_t a,    int8x8_t b)
{
    DWRD_VTYPE  c = {.B.U=vceq_s8(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.B.U = vrev64_u8(c.B.U);
    c.U = __builtin_clzll(c.U)/8;
    return  c.B.I;
}

INLINE(uint16x4_t,DHU_VNEL)  (uint16x4_t a,  uint16x4_t b)
{
    DWRD_VTYPE  c = {.H.U=vceq_u16(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.U = __builtin_ctzll(c.U)/16;
    return  c.H.U;
}

INLINE( int16x4_t,DHI_VNEL)   (int16x4_t a,   int16x4_t b)
{
    DWRD_VTYPE  c = {.H.U=vceq_s16(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.U = __builtin_ctzll(c.U)/16;
    return  c.H.I;
}

INLINE( int16x4_t,DHF_VNEL) (float16x4_t a, float16x4_t b)
{
    DWRD_VTYPE  c = {.H.I=DHF_CEQS(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.U = __builtin_ctzll(c.U)/16;
    return  c.H.I;
}

INLINE(uint32x2_t,DWU_VNEL)  (uint32x2_t a,  uint32x2_t b)
{
    DWRD_VTYPE  c = {.W.U=vceq_u32(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.U = __builtin_ctzll(c.U)/32;
    return  c.W.U;
}

INLINE( int32x2_t,DWI_VNEL)   (int32x2_t a,   int32x2_t b)
{
    DWRD_VTYPE  c = {.W.I=vceq_s32(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.U = __builtin_ctzll(c.U)/32;
    return  c.W.I;
}

INLINE( int32x2_t,DWF_VNEL) (float32x2_t a, float32x2_t b)
{
    DWRD_VTYPE  c = {.W.U=vceq_f32(a, b)};
    c.B.U = vmvn_u8(c.B.U);
    c.U = __builtin_ctzll(c.U)/32;
    return  c.W.I;
}


INLINE(uint64_t,MY_VNELQ_CSZL) (QUAD_UTYPE x)
{
    QUAD_TYPE   c = {.U=x};
    c.Lo.U = __builtin_ctzll(c.Lo.U);
    if (c.Lo.U == 64)
        c.Lo.U += __builtin_ctzll(c.Hi.U);
    return  c.Lo.U;
}

INLINE(uint8x16_t,QBU_VNEL)  (uint8x16_t a,  uint8x16_t b)
{
    QUAD_VTYPE q = {.B.U=vceqq_u8(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((uint8x16_t){MY_VNELQ_CSZL(q.U)/8});
}


INLINE( int8x16_t,QBI_VNEL)   (int8x16_t a,   int8x16_t b)
{
    QUAD_VTYPE q = {.B.U=vceqq_s8(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((int8x16_t){MY_VNELQ_CSZL(q.U)/8});
}


INLINE(uint16x8_t,QHU_VNEL)  (uint16x8_t a,  uint16x8_t b)
{
    QUAD_VTYPE q = {.H.U=vceqq_u16(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((uint16x8_t){MY_VNELQ_CSZL(q.U)/16});
}

INLINE( int16x8_t,QHI_VNEL)   (int16x8_t a,   int16x8_t b)
{
    QUAD_VTYPE q = {.H.U=vceqq_s16(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((int16x8_t){MY_VNELQ_CSZL(q.U)/16});
}

INLINE( int16x8_t,QHF_VNEL) (float16x8_t a, float16x8_t b)
{
    QUAD_VTYPE q = {.H.I=QHF_CEQS(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((int16x8_t){MY_VNELQ_CSZL(q.U)/16});
}

INLINE(uint32x4_t,QWU_VNEL)  (uint32x4_t a,  uint32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_s32(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((uint32x4_t){MY_VNELQ_CSZL(q.U)/32});
}

INLINE( int32x4_t,QWI_VNEL)   (int32x4_t a,   int32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_s32(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((int32x4_t){MY_VNELQ_CSZL(q.U)/32});
}

INLINE( int32x4_t,QWF_VNEL) (float32x4_t a, float32x4_t b)
{
    QUAD_VTYPE q = {.W.U=vceqq_f32(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((int32x4_t){MY_VNELQ_CSZL(q.U)/32});
}

INLINE(uint64x2_t,QDU_VNEL)  (uint64x2_t a,  uint64x2_t b)
{
    QUAD_VTYPE q = {.D.U=vceqq_u64(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((uint64x2_t){MY_VNELQ_CSZL(q.U)/64});
}

INLINE( int64x2_t,QDI_VNEL)   (int64x2_t a,   int64x2_t b)
{
    QUAD_VTYPE q = {.D.U=vceqq_s64(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((uint64x2_t){MY_VNELQ_CSZL(q.U)/64});
}

INLINE( int64x2_t,QDF_VNEL) (float64x2_t a, float64x2_t b)
{
    QUAD_VTYPE q = {.D.U=vceqq_f64(a, b)};
    q.B.U = vmvnq_u8(q.B.U);
    return  ((uint64x2_t){MY_VNELQ_CSZL(q.U)/64});
}


INLINE(Vwbu,VWBU_VNEL) (Vwbu a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.U = DBU_VNEL(x.B.U, y.B.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbi,VWBI_VNEL) (Vwbi a, Vwbu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.B.I = DBI_VNEL(x.B.I, y.B.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwbc,VWBC_VNEL) (Vwbc a, Vwbc b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
#if CHAR_MIN
    x.B.I = DBI_VNEL(x.B.I, y.B.I);
#else
    x.B.U = DBU_VNEL(x.B.U, y.B.U);
#endif
    a.V0 = vget_lane_f32(x.W.F, 0);
    return  a;
}


INLINE(Vwhu,VWHU_VNEL) (Vwhu a, Vwhu b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.U = DHU_VNEL(x.H.U, y.H.U);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHI_VNEL) (Vwhi a, Vwhi b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHI_VNEL(x.H.I, y.H.I);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return a;
}

INLINE(Vwhi,VWHF_VNEL) (Vwhf a, Vwhf b)
{
    DWRD_VTYPE x={.W.F=a.V0}, y={.W.F=b.V0};
    x.H.I = DHF_VNEL(x.H.F, y.H.F);
    a.V0 = vget_lane_f32(x.W.F, 0);
    return ((Vwhi){a.V0});
}

INLINE(Vdbu,VDBU_VNEL) (Vdbu a, Vdbu b) {return DBU_VNEL(a,b);}
INLINE(Vdbi,VDBI_VNEL) (Vdbi a, Vdbi b) {return DBI_VNEL(a,b);}
INLINE(Vdbc,VDBC_VNEL) (Vdbc a, Vdbc b) {a.V0 = DBC_VNEL(a.V0,b.V0); return a;}
INLINE(Vdhu,VDHU_VNEL) (Vdhu a, Vdhu b) {return DHU_VNEL(a,b);}
INLINE(Vdhi,VDHI_VNEL) (Vdhi a, Vdhi b) {return DHI_VNEL(a,b);}
INLINE(Vdhi,VDHF_VNEL) (Vdhf a, Vdhf b) {return DHF_VNEL(a,b);}
INLINE(Vdwu,VDWU_VNEL) (Vdwu a, Vdwu b) {return DWU_VNEL(a,b);}
INLINE(Vdwi,VDWI_VNEL) (Vdwi a, Vdwi b) {return DWI_VNEL(a,b);}
INLINE(Vdwi,VDWF_VNEL) (Vdwf a, Vdwf b) {return DWF_VNEL(a,b);}

INLINE(Vqbu,VQBU_VNEL) (Vqbu a, Vqbu b) {return QBU_VNEL(a,b);}
INLINE(Vqbi,VQBI_VNEL) (Vqbi a, Vqbi b) {return QBI_VNEL(a,b);}
INLINE(Vqbc,VQBC_VNEL) (Vqbc a, Vqbc b) {a.V0 = QBC_VNEL(a.V0,b.V0); return a;}
INLINE(Vqhu,VQHU_VNEL) (Vqhu a, Vqhu b) {return QHU_VNEL(a,b);}
INLINE(Vqhi,VQHI_VNEL) (Vqhi a, Vqhi b) {return QHI_VNEL(a,b);}
INLINE(Vqhi,VQHF_VNEL) (Vqhf a, Vqhf b) {return QHF_VNEL(a,b);}
INLINE(Vqwu,VQWU_VNEL) (Vqwu a, Vqwu b) {return QWU_VNEL(a,b);}
INLINE(Vqwi,VQWI_VNEL) (Vqwi a, Vqwi b) {return QWI_VNEL(a,b);}
INLINE(Vqwi,VQWF_VNEL) (Vqwf a, Vqwf b) {return QWF_VNEL(a,b);}
INLINE(Vqdu,VQDU_VNEL) (Vqdu a, Vqdu b) {return QDU_VNEL(a,b);}
INLINE(Vqdi,VQDI_VNEL) (Vqdi a, Vqdi b) {return QDI_VNEL(a,b);}
INLINE(Vqdi,VQDF_VNEL) (Vqdf a, Vqdf b) {return QDF_VNEL(a,b);}

#if 0 // _LEAVE_ARM_VNEL
}
#endif

#if 0 // _ENTER_ARM_CBNS
{
#endif

INLINE(int16_t,  FLT16_CBNS) (flt16_t x, flt16_t l, flt16_t r)
{
#if defined(SPC_ARM_FP16)
    return  -((l <= x) && (x <= r));
#else
    return 0;
#endif
}

INLINE(int32_t,    FLT_CBNS)   (float a,   float l,   float r) 
{
    return  vcles_f32(l, a)&vcles_f32(a, r);
}

INLINE(int64_t,    DBL_CBNS)  (double a,  double l, double r) 
{
    return  vcltd_f64(l, a)&vcltd_f64(a, r);
}


#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cbnsqu) (QUAD_UTYPE a, QUAD_UTYPE l, QUAD_UTYPE r)
{
    return  -((l <= a) || (a <= r));
}

INLINE(QUAD_ITYPE,cbnsqi) (QUAD_ITYPE a, QUAD_ITYPE l, QUAD_ITYPE r)
{
    return  -((l <= a) || (a <= r));
}

INLINE(QUAD_ITYPE,cbnsqf) (QUAD_FTYPE a, QUAD_FTYPE l, QUAD_FTYPE r)
{
    return  -((l <= a) || (a <= r));
}

#endif


INLINE(Vwbu,VWBU_CBNS) (Vwbu a,  uint8_t b,  uint8_t c)
{
    float       m = VWBU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   x = vreinterpret_u8_f32(v);
    uint8x8_t   l = vcge_u8(x, vdup_n_u8(b));
    uint8x8_t   r = vcle_u8(x, vdup_n_u8(c));
    x = vand_u8(l, r);
    v = vreinterpret_f32_u8(x);
    m = vget_lane_f32(v, 0);
    return WBU_ASTV(m);
}

INLINE(Vwbi,VWBI_CBNS) (Vwbi a,   int8_t b,   int8_t c)
{
    float       m = VWBI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int8x8_t    x = vreinterpret_s8_f32(v);
    uint8x8_t   l = vcge_s8(x, vdup_n_s8(b));
    uint8x8_t   r = vcle_s8(x, vdup_n_s8(c));
    x = vand_s8(
        vreinterpret_s8_u8(l),
        vreinterpret_s8_u8(r)
    );
    v = vreinterpret_f32_s8(x);
    m = vget_lane_f32(v, 0);
    return  WBI_ASTV(m);
}

INLINE(Vwbc,VWBC_CBNS) (Vwbc a,     char b,     char c)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_CBNS(VWBC_ASBI(a), b, c));
#else
    return  VWBU_ASBC(VWBU_CBNS(VWBC_ASBU(a), b, c));
#endif
}


INLINE(Vwhu,VWHU_CBNS) (Vwhu a, uint16_t b, uint16_t c)
{
    float       m = VWHU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint16x4_t  x = vreinterpret_u16_f32(v);
    uint16x4_t  l = vcge_u16(x, vdup_n_u16(b));
    uint16x4_t  r = vcle_u16(x, vdup_n_u16(c));
    x = vand_u16(l, r);
    v = vreinterpret_f32_u16(x);
    m = vget_lane_f32(v, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_CBNS) (Vwhi a,  int16_t b,  int16_t c)
{
    float       m = VWHI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int16x4_t   x = vreinterpret_s16_f32(v);
    uint16x4_t  l = vcge_s16(x, vdup_n_s16(b));
    uint16x4_t  r = vcle_s16(x, vdup_n_s16(c));
    x = vand_s16(
        vreinterpret_s16_u16(l),
        vreinterpret_s16_u16(r)
    );
    v = vreinterpret_f32_s16(x);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vwhi,VWHF_CBNS) (Vwhf a,  flt16_t b,  flt16_t c)
{
#if defined(SPC_ARM_FP16_SIMD)
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t x = vreinterpret_f16_f32(v);
    uint16x4_t  l = vcge_f16(x, vdup_n_s16(b));
    uint16x4_t  r = vcle_f16(x, vdup_n_s16(c));
    l = vand_u16(
        vreinterpret_s16_u16(l),
        vreinterpret_s16_u16(r)
    );
    v = vreinterpret_f32_u16(l);
#else
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t t = vreinterpret_f16_f32(v);
    float32x4_t x = vcvt_f32_f16(t);
    uint32x4_t  l = vcgeq_f32(x, vdupq_n_f32(b));
    uint32x4_t  r = vcgeq_f32(x, vdupq_n_f32(c));
    l = vorrq_u32(l, r);
    uint16x4_t  n = vmovn_u32(l);
    v = vreinterpret_f32_u16(n);
#endif
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}


INLINE(Vwwu,VWWU_CBNS) (Vwwu a, uint32_t b, uint32_t c)
{
    float       m = VWWU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint32x2_t  x = vreinterpret_u32_f32(v);
    uint32x2_t  l = vcge_u32(x, vdup_n_u32(b));
    uint32x2_t  r = vcle_u32(x, vdup_n_u32(c));
    x = vand_u32(l, r);
    v = vreinterpret_f32_u32(x);
    m = vget_lane_f32(v, 0);
    return  WWU_ASTV(m);
}

INLINE(Vwwi,VWWI_CBNS) (Vwwi a,  int32_t b,  int32_t c)
{
    float       m = VWWI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int32x2_t   x = vreinterpret_s32_f32(v);
    uint32x2_t  l = vcge_s32(x, vdup_n_s32(b));
    uint32x2_t  r = vcle_s32(x, vdup_n_s32(c));
    x = vand_s16(
        vreinterpret_s32_u32(l),
        vreinterpret_s32_u32(r)
    );
    v = vreinterpret_f32_s32(x);
    m = vget_lane_f32(v, 0);
    return  WWI_ASTV(m);
}

INLINE(Vwwi,VWWF_CBNS) (Vwwf a,    float l,    float r)
{
    float       m = VWWF_ASTM(a);
    uint32_t    c = vcles_f32(l, m)&vcles_f32(m, r);
    m = UINT_ASTF(c);
    return  WWI_ASTV(m);
}


INLINE(Vdbu,VDBU_CBNS) (Vdbu a,  uint8_t l,  uint8_t r)
{
    return  vand_u8(
        vcge_u8(a, vdup_n_u8(l)),
        vcle_u8(a, vdup_n_u8(r))
    );
}

INLINE(Vdbi,VDBI_CBNS) (Vdbi a,   int8_t l,   int8_t r)
{
    uint8x8_t p = vcge_s8(a, vdup_n_s8(l));
    uint8x8_t q = vcle_s8(a, vdup_n_s8(r));
    return  vand_s8(
        vreinterpret_s8_u8(p),
        vreinterpret_s8_u8(q)
    );
}

INLINE(Vdbc,VDBC_CBNS) (Vdbc a,     char l,     char r)
{
#if CHAR_MIN
    Vdbi m = VDBC_ASBI(a);
    m = VDBI_CBNS(m, l, r);
    return  VDBI_ASBC(m);
#else
    Vdbu m = VDBC_ASBU(a);
    m = VDBU_CBNS(m, l, r);
    return  VDBU_ASBC(m);
#endif

}


INLINE(Vdhu,VDHU_CBNS) (Vdhu a, uint16_t l, uint16_t r) 
{
    return  vand_u16(
        vcge_u16(a, vdup_n_u16(l)),
        vcle_u16(a, vdup_n_u16(r))
    );
}

INLINE(Vdhi,VDHI_CBNS) (Vdhi a,  int16_t l,  int16_t r)
{
    uint16x4_t p = vcge_s16(a, vdup_n_s16(l));
    uint16x4_t q = vcle_s16(a, vdup_n_s16(r));
    return  vand_s16(
        vreinterpret_s16_u16(p),
        vreinterpret_s16_u16(q)
    );
}

INLINE(Vdhi,VDHF_CBNS) (Vdhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x4_t p = vcge_f16(a, vdup_n_f16(l));
    uint16x4_t q = vcle_f16(a, vdup_n_f16(r));
    return  vand_s16(
        vreinterpret_s16_u16(p),
        vreinterpret_s16_u16(q)
    );
#else
    float32x4_t v = vcvt_f32_f16(a);
    uint32x4_t  p = vcgeq_f32(v, vdupq_n_f32(l));
    uint32x4_t  q = vcleq_f32(v, vdupq_n_f32(r));
    p = vorrq_u32(p, q);
    uint16x4_t  d = vmovn_u32(p);
    return  vreinterpret_s16_u16(d);
#endif
}


INLINE(Vdwu,VDWU_CBNS) (Vdwu a, uint32_t l, uint32_t r)
{
    return  vand_u32(
        vcge_u32(a, vdup_n_u32(l)),
        vcle_u32(a, vdup_n_u32(r))
    );
}

INLINE(Vdwi,VDWI_CBNS) (Vdwi a,  int32_t l,  int32_t r)
{
    uint32x2_t p = vcge_s32(a, vdup_n_s32(l));
    uint32x2_t q = vcle_s32(a, vdup_n_s32(r));
    return  vand_s32(
        vreinterpret_s32_u32(p),
        vreinterpret_s32_u32(q)
    );
}

INLINE(Vdwi,VDWF_CBNS) (Vdwf a,    float l,    float r)
{
    uint32x2_t p = vcge_f32(a, vdup_n_f32(l));
    uint32x2_t q = vcle_f32(a, vdup_n_f32(r));
    return  vand_s32(
        vreinterpret_s32_u32(p),
        vreinterpret_s32_u32(q)
    );
}


INLINE(Vddu,VDDU_CBNS) (Vddu a, uint64_t l, uint64_t r)
{
    return  vand_u64(
        vcge_u64(a, vdup_n_u64(l)),
        vcle_u64(a, vdup_n_u64(r))
    );
}

INLINE(Vddi,VDDI_CBNS) (Vddi a,  int64_t l,  int64_t r)
{
    uint64x1_t p = vcge_s64(a, vdup_n_s64(l));
    uint64x1_t q = vcle_s64(a, vdup_n_s64(r));
    return  vand_s64(
        vreinterpret_s64_u64(p),
        vreinterpret_s64_u64(q)
    );
}

INLINE(Vddi,VDDF_CBNS) (Vddf a,   double l,   double r)
{
    uint64x1_t p = vcge_f64(a, vdup_n_f64(l));
    uint64x1_t q = vcle_f64(a, vdup_n_f64(r));
    return  vand_s64(
        vreinterpret_s64_u64(p),
        vreinterpret_s64_u64(q)
    );
}


INLINE(Vqbu,VQBU_CBNS) (Vqbu a,  uint8_t l,  uint8_t r)
{
    return vandq_u8(
        vcgeq_u8(a, vdupq_n_u8(l)),
        vcleq_u8(a, vdupq_n_u8(r))
    );
}

INLINE(Vqbi,VQBI_CBNS) (Vqbi a,   int8_t l,   int8_t r)
{
    uint8x16_t  p = vcgeq_s8(a, vdupq_n_s8(l));
    uint8x16_t  q = vcleq_s8(a, vdupq_n_s8(r));
    return  vandq_s8(
        vreinterpretq_s8_u8(p),
        vreinterpretq_s8_u8(q)
    );
}

INLINE(Vqbc,VQBC_CBNS) (Vqbc a,     char l,     char r)
{
#if CHAR_MIN
    Vqbi m = VQBC_ASBI(a);
    m = VQBI_CBNS(m, l, r);
    return  VQBI_ASBC(m);
#else
    Vqbu m = VQBC_ASBU(a);
    m = VQBU_CBNS(m, l, r);
    return  VQBU_ASBC(m);
#endif

}


INLINE(Vqhu,VQHU_CBNS) (Vqhu a, uint16_t l, uint16_t r) 
{
    return  vandq_u16(
        vcgeq_u16(a, vdupq_n_u16(l)),
        vcleq_u16(a, vdupq_n_u16(r))
    );
}

INLINE(Vqhi,VQHI_CBNS) (Vqhi a,  int16_t l,  int16_t r)
{
    uint16x8_t p = vcgeq_s16(a, vdupq_n_s16(l));
    uint16x8_t q = vcleq_s16(a, vdupq_n_s16(r));
    return  vandq_s16(
        vreinterpretq_s16_u16(p),
        vreinterpretq_s16_u16(q)
    );
}

INLINE(Vqhi,VQHF_CBNS) (Vqhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x8_t p = vcgeq_f16(a, vdupq_n_f16(l));
    uint16x8_t q = vcleq_f16(a, vdupq_n_f16(r));
    return  vandq_s16(
        vreinterpretq_s16_u16(p),
        vreinterpretq_s16_u16(q)
    );
#else
    float16x4_t p = vget_low_f16(a);
    float16x4_t q = vget_high_f16(a);
    return vcombine_s16(
        VDHF_CBNS(p, l, r),
        VDHF_CBNS(q, l, r)
    );
#endif
}


INLINE(Vqwu,VQWU_CBNS) (Vqwu a, uint32_t l, uint32_t r)
{
    return  vandq_u32(
        vcgeq_u32(a, vdupq_n_u32(l)),
        vcleq_u32(a, vdupq_n_u32(r))
    );
}

INLINE(Vqwi,VQWI_CBNS) (Vqwi a,  int32_t l,  int32_t r)
{
    uint32x4_t p = vcgeq_s32(a, vdupq_n_s32(l));
    uint32x4_t q = vcleq_s32(a, vdupq_n_s32(r));
    return  vandq_s32(
        vreinterpretq_s32_u32(p),
        vreinterpretq_s32_u32(q)
    );
}

INLINE(Vqwi,VQWF_CBNS) (Vqwf a,    float l,    float r)
{
    uint32x4_t p = vcgeq_f32(a, vdupq_n_f32(l));
    uint32x4_t q = vcleq_f32(a, vdupq_n_f32(r));
    return  vandq_s32(
        vreinterpretq_s32_u32(p),
        vreinterpretq_s32_u32(q)
    );
}


INLINE(Vqdu,VQDU_CBNS) (Vqdu a, uint64_t l, uint64_t r)
{
    return  vandq_u64(
        vcgeq_u64(a, vdupq_n_u64(l)),
        vcleq_u64(a, vdupq_n_u64(r))
    );
}

INLINE(Vqdi,VQDI_CBNS) (Vqdi a,  int64_t l,  int64_t r)
{
    uint64x2_t p = vcgeq_s64(a, vdupq_n_s64(l));
    uint64x2_t q = vcleq_s64(a, vdupq_n_s64(r));
    return  vandq_s64(
        vreinterpretq_s64_u64(p),
        vreinterpretq_s64_u64(q)
    );
}

INLINE(Vqdi,VQDF_CBNS) (Vqdf a,   double l,   double r)
{
    uint64x2_t p = vcgeq_f64(a, vdupq_n_f64(l));
    uint64x2_t q = vcleq_f64(a, vdupq_n_f64(r));
    return  vandq_s64(
        vreinterpretq_s64_u64(p),
        vreinterpretq_s64_u64(q)
    );
}


#if 0 // _LEAVE_ARM_CBNS
}
#endif

#if 0 // _ENTER_ARM_CBNY
{
#endif

INLINE(int16_t,  FLT16_CBNY) (flt16_t a, flt16_t l, flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  1&(vcleh_f16(l, a)|vcleh_f16(a, r));
#else
    return  1&(vcles_f32(l, a)|vcles_f32(a, r));
#endif
}

INLINE(int32_t,    FLT_CBNY)   (float a,   float l,   float r) 
{
    return  1&(vcles_f32(l, a)|vcles_f32(a, r));
}

INLINE(int64_t,    DBL_CBNY)  (double a,  double l,  double r) 
{
    return  1&(vcled_f64(l, a)|vcled_f64(a, r));
}


#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cbnyqu) (QUAD_UTYPE a, QUAD_UTYPE l, QUAD_UTYPE r)
{
    return  (l <= a) && (a <= r);
}

INLINE(QUAD_ITYPE,cbnyqi) (QUAD_ITYPE a, QUAD_ITYPE l, QUAD_ITYPE r)
{
    return  (l <= a) && (a <= r);
}

INLINE(QUAD_ITYPE,cbnyqf) (QUAD_FTYPE a, QUAD_FTYPE l, QUAD_FTYPE r)
{
    return  (l <= a) && (a <= r);
}

#endif


INLINE(Vwbu,VWBU_CBNY) (Vwbu a,  uint8_t l,  uint8_t r)
{
    float       m = VWBU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   x = vreinterpret_u8_f32(v);
    uint8x8_t   b = vcle_u8(vdup_n_u8(l), x);
    uint8x8_t   c = vcge_u8(vdup_n_u8(r), x);
    c = vand_u8(c, b);
    c = vand_u8(c, vdup_n_u8(1));
    v = vreinterpret_f32_u8(c);
    m = vget_lane_f32(v, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbi,VWBI_CBNY) (Vwbi a,   int8_t l,   int8_t r)
{
    float       m = VWBI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int8x8_t    x = vreinterpret_s8_f32(v);
    uint8x8_t   b = vcle_s8(vdup_n_s8(l), x);
    uint8x8_t   c = vcge_s8(vdup_n_s8(r), x);
    c = vand_u8(c, b);
    c = vand_u8(c, vdup_n_u8(1));
    v = vreinterpret_f32_u8(c);
    m = vget_lane_f32(v, 0);
    return  WBI_ASTV(m);
}

INLINE(Vwbc,VWBC_CBNY) (Vwbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_CBNY(VWBC_ASBI(a), l, r));
#else
    return  VWBU_ASBC(VWBU_CBNY(VWBC_ASBU(a), l, r));
#endif
}


INLINE(Vwhu,VWHU_CBNY) (Vwhu a, uint16_t l, uint16_t r)
{
    float       m = VWHU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint16x4_t  x = vreinterpret_u16_f32(v);
    uint16x4_t  b = vcle_u16(vdup_n_u16(l), x);
    uint16x4_t  c = vcge_u16(vdup_n_u16(r), x);
    c = vand_u16(c, b);
    c = vand_u16(c, vdup_n_u16(1));
    v = vreinterpret_f32_u16(c);
    m = vget_lane_f32(v, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_CBNY) (Vwhi a,  int16_t l,  int16_t r)
{
    float       m = VWHI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int16x4_t   x = vreinterpret_s16_f32(v);
    uint16x4_t  b = vcle_s16(vdup_n_s16(l), x);
    uint16x4_t  c = vcge_s16(vdup_n_s16(r), x);
    c = vand_u16(c, b);
    c = vand_u16(c, vdup_n_u16(1));
    v = vreinterpret_f32_u16(c);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vwhi,VWHF_CBNY) (Vwhf a,  flt16_t l,  flt16_t r)
{
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);

#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t x = vreinterpret_f16_f32(v);
    uint16x4_t  b = vcle_f16(vdup_n_f16(l), x);
    uint16x4_t  c = vcge_s16(vdup_n_f16(r), x);
    c = vand_u16(c, b);
    c = vand_u16(c, vdup_n_u16(1));
    v = vreinterpret_f32_u16(c);
#else
    float16x4_t t = vreinterpret_f16_f32(v);
    float32x4_t x = vcvt_f32_f16(t);
    uint32x4_t  b = vcleq_f32(vdupq_n_f32(l), x);
    uint32x4_t  c = vcgeq_f32(vdupq_n_f32(r), x);
    c = vandq_u32(c, b);
    uint16x4_t  n = vmovn_u32(c);
    n = vand_u16(n, vdup_n_u16(1));
    v = vreinterpret_f32_u16(n);
#endif

    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}


INLINE(Vwwu,VWWU_CBNY) (Vwwu a, uint32_t l, uint32_t r)
{
    float       m = VWWU_ASTM(a);
    uint32_t    c =  FLT_ASTU(m);
    c = (l <= c) && (c <= r);
    m = UINT32_ASTF(c);
    return  WWU_ASTV(m);
}

INLINE(Vwwi,VWWI_CBNY) (Vwwi a,  int32_t l,  int32_t r)
{
    float       m = VWWI_ASTM(a);
    int32_t     c =  FLT_ASTI(m);
    c = (l <= c) && (c <= r);
    m = INT32_ASTF(c);
    return  WWI_ASTV(m);
}

INLINE(Vwwi,VWWF_CBNY) (Vwwf a,    float l,    float r)
{
    float       m = VWWF_ASTM(a);
    uint32_t    c = 1&(vcles_f32(l, m)|vcges_f32(m, r));
    m = UINT32_ASTF(c);
    return  WWI_ASTV(m);
}


INLINE(Vdbu,VDBU_CBNY) (Vdbu a,  uint8_t l,  uint8_t r)
{
    a = vand_u8(
        vcge_u8(a, vdup_n_u8(l)),
        vcle_u8(a, vdup_n_u8(r))
    );
    return  vand_u8(a, vdup_n_u8(1));
}

INLINE(Vdbi,VDBI_CBNY) (Vdbi a,   int8_t l,   int8_t r)
{
    uint8x8_t c = vand_u8(
        vcge_s8(a, vdup_n_s8(l)),
        vcle_s8(a, vdup_n_s8(r))
    );
    c = vand_u8(a, vdup_n_u8(1));
    return  vreinterpret_s8_u8(c);
}

INLINE(Vdbc,VDBC_CBNY) (Vdbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_CBNY(VDBC_ASBI(a), l, r));
#else
    return  VDBI_ASBC(VDBU_CBNY(VDBC_ASBU(a), l, r));
#endif

}


INLINE(Vdhu,VDHU_CBNY) (Vdhu a, uint16_t l, uint16_t r) 
{
    a =  vand_u16(
        vcge_u16(a, vdup_n_u16(l)),
        vcle_u16(a, vdup_n_u16(r))
    );
    return vand_u16(a, vdup_n_u16(1));
}

INLINE(Vdhi,VDHI_CBNY) (Vdhi a,  int16_t l,  int16_t r)
{
    uint16x4_t c = vand_u16(
        vcge_s16(a, vdup_n_s16(l)),
        vcle_s16(a, vdup_n_s16(r))
    );
    c = vand_u16(c, vdup_n_u16(1));
    return  vreinterpret_s16_u16(c);
}

INLINE(Vdhi,VDHF_CBNY) (Vdhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x4_t c = vand_u16(
        vcge_f16(a, vdup_n_f16(l)),
        vcle_f16(a, vdup_n_f16(r))
    );
#else
    float32x4_t q = vcvt_f32_f16(a);
    uint32x4_t  v = vandq_u32(
        vcltq_f32(q, vdupq_n_f32(l)),
        vcgtq_f32(q, vdupq_n_f32(r))
    );
    uint16x4_t  c = vmovn_u32(v);
#endif
    c = vand_u16(c, vdup_n_u16(1));
    return  vreinterpret_s16_u16(c);
}


INLINE(Vdwu,VDWU_CBNY) (Vdwu a, uint32_t l, uint32_t r)
{
    a =  vand_u32(
        vcge_u32(a, vdup_n_u32(l)),
        vcle_u32(a, vdup_n_u32(r))
    );
    return vand_u32(a, vdup_n_u32(1));
}

INLINE(Vdwi,VDWI_CBNY) (Vdwi a,  int32_t l,  int32_t r)
{
    uint32x2_t c = vand_u32(
        vcge_s32(a, vdup_n_s32(l)),
        vcle_s32(a, vdup_n_s32(r))
    );
    c = vand_u32(c, vdup_n_u32(1));
    return  vreinterpret_s32_u32(c);
}

INLINE(Vdwi,VDWF_CBNY) (Vdwf a,    float l,    float r)
{
    uint32x2_t c = vand_u32(
        vcge_f32(a, vdup_n_f32(l)),
        vcle_f32(a, vdup_n_f32(r))
    );
    c = vand_u32(c, vdup_n_u32(1));
    return  vreinterpret_s32_u32(c);
}


INLINE(Vddu,VDDU_CBNY) (Vddu a, uint64_t l, uint64_t r)
{
    a =  vand_u64(
        vcge_u64(a, vdup_n_u64(l)),
        vcle_u64(a, vdup_n_u64(r))
    );
    a = vand_u64(a, vdup_n_u64(1));
    return  a;
}

INLINE(Vddi,VDDI_CBNY) (Vddi a,  int64_t l,  int64_t r)
{
    uint64x1_t c = vand_u64(
        vcge_s64(a, vdup_n_s64(l)),
        vcle_s64(a, vdup_n_s64(r))
    );
    c = vand_u64(c, vdup_n_u64(1));
    return  vreinterpret_s64_u64(c);
}

INLINE(Vddi,VDDF_CBNY) (Vddf a,   double l,   double r)
{
    uint64x1_t c = vand_u64(
        vcge_f64(a, vdup_n_f64(l)),
        vcle_f64(a, vdup_n_f64(r))
    );
    c = vand_u64(c, vdup_n_u64(1));
    return  vreinterpret_s64_u64(c);
}



INLINE(Vqbu,VQBU_CBNY) (Vqbu a,  uint8_t l,  uint8_t r)
{
    a = vandq_u8(
        vcgeq_u8(a, vdupq_n_u8(l)),
        vcleq_u8(a, vdupq_n_u8(r))
    );
    return  vandq_u8(a, vdupq_n_u8(1));
}

INLINE(Vqbi,VQBI_CBNY) (Vqbi a,   int8_t l,   int8_t r)
{
    uint8x16_t c = vandq_u8(
        vcgeq_s8(a, vdupq_n_s8(l)),
        vcleq_s8(a, vdupq_n_s8(r))
    );
    c = vandq_u8(a, vdupq_n_u8(1));
    return  vreinterpretq_s8_u8(c);
}

INLINE(Vqbc,VQBC_CBNY) (Vqbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_CBNY(VQBC_ASBI(a), l, r));
#else
    return  VQBI_ASBC(VQBU_CBNY(VQBC_ASBU(a), l, r));
#endif

}


INLINE(Vqhu,VQHU_CBNY) (Vqhu a, uint16_t l, uint16_t r) 
{
    a =  vandq_u16(
        vcgeq_u16(a, vdupq_n_u16(l)),
        vcleq_u16(a, vdupq_n_u16(r))
    );
    return vandq_u16(a, vdupq_n_u16(1));
}

INLINE(Vqhi,VQHI_CBNY) (Vqhi a,  int16_t l,  int16_t r)
{
    uint16x8_t c = vandq_u16(
        vcgeq_s16(a, vdupq_n_s16(l)),
        vcleq_s16(a, vdupq_n_s16(r))
    );
    c = vandq_u16(c, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(c);
}

INLINE(Vqhi,VQHF_CBNY) (Vqhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x8_t c = vandq_u16(
        vcgeq_f16(a, vdupq_n_f16(l)),
        vcleq_f16(a, vdupq_n_f16(r))
    );
    c = vandq_u16(c, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(c);
#else
    return vcombine_s16(
        VDHF_CBNY(vget_low_f16(a), l, r),
        VDHF_CBNY(vget_high_f16(a),l, r)
    );
#endif
}


INLINE(Vqwu,VQWU_CBNY) (Vqwu a, uint32_t l, uint32_t r)
{
    a =  vandq_u32(
        vcgeq_u32(a, vdupq_n_u32(l)),
        vcleq_u32(a, vdupq_n_u32(r))
    );
    return  vandq_u32(a, vdupq_n_u32(1));
}

INLINE(Vqwi,VQWI_CBNY) (Vqwi a,  int32_t l,  int32_t r)
{
    uint32x4_t c = vandq_u32(
        vcgeq_s32(a, vdupq_n_s32(l)),
        vcleq_s32(a, vdupq_n_s32(r))
    );
    c = vandq_u32(c, vdupq_n_u32(1));
    return  vreinterpretq_s32_u32(c);
}

INLINE(Vqwi,VQWF_CBNY) (Vqwf a,    float l,    float r)
{
    uint32x4_t c = vandq_u32(
        vcgeq_f32(a, vdupq_n_f32(l)),
        vcleq_f32(a, vdupq_n_f32(r))
    );
    c = vandq_u32(c, vdupq_n_u32(1));
    return  vreinterpretq_s32_u32(c);
}


INLINE(Vqdu,VQDU_CBNY) (Vqdu a, uint64_t l, uint64_t r)
{
    a =  vandq_u64(
        vcgeq_u64(a, vdupq_n_u64(l)),
        vcleq_u64(a, vdupq_n_u64(r))
    );
    a = vandq_u64(a, vdupq_n_u64(1));
    return  a;
}

INLINE(Vqdi,VQDI_CBNY) (Vqdi a,  int64_t l,  int64_t r)
{
    uint64x2_t c = vandq_u64(
        vcgeq_s64(a, vdupq_n_s64(l)),
        vcleq_s64(a, vdupq_n_s64(r))
    );
    c = vandq_u64(c, vdupq_n_u64(1));
    return  vreinterpretq_s64_u64(c);
}

INLINE(Vqdi,VQDF_CBNY) (Vqdf a,   double l,   double r)
{
    uint64x2_t c = vandq_u64(
        vcgeq_f64(a, vdupq_n_f64(l)),
        vcleq_f64(a, vdupq_n_f64(r))
    );
    c = vandq_u64(c, vdupq_n_u64(1));
    return  vreinterpretq_s64_u64(c);
}


#if 0 // _LEAVE_ARM_CBNY
}
#endif


#if 0 // _ENTER_ARM_CNBS
{
#endif

INLINE(int16_t,  FLT16_CNBS) (flt16_t a, flt16_t l, flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vclth_f16(a, l)|vclth_f16(r, a);
#else
    return  vclts_f32(a, l)|vclts_f32(r, a);
#endif
}

INLINE(int32_t,    FLT_CNBS)   (float a,   float l,   float r) 
{
    return  vclts_f32(a, l)|vclts_f32(r, a);
}

INLINE(int64_t,    DBL_CNBS)  (double a,  double l,  double r) 
{
    return  vcltd_f64(a, l)|vcltd_f64(r, a);
}


#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cnbsqu) (QUAD_UTYPE a, QUAD_UTYPE l, QUAD_UTYPE r)
{
    return  -((a < l) || (r < a));
}

INLINE(QUAD_ITYPE,cnbsqi) (QUAD_ITYPE a, QUAD_ITYPE l, QUAD_ITYPE r)
{
    return  -((a < l) || (r < a));
}

INLINE(QUAD_ITYPE,cnbsqf) (QUAD_FTYPE a, QUAD_FTYPE l, QUAD_FTYPE r)
{
    return  -((a < l) || (r < a));
}

#endif


INLINE(Vwbu,VWBU_CNBS) (Vwbu a,  uint8_t l,  uint8_t r)
{
    float       m = VWBU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   x = vreinterpret_u8_f32(v);
    uint8x8_t   b = vclt_u8(x, vdup_n_u8(l));
    uint8x8_t   c = vcgt_u8(x, vdup_n_u8(r));
    c = vorr_u8(b, c);
    v = vreinterpret_f32_u8(c);
    m = vget_lane_f32(v, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbi,VWBI_CNBS) (Vwbi a,   int8_t l,   int8_t r)
{
    float       m = VWBI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int8x8_t    x = vreinterpret_s8_f32(v);
    uint8x8_t   b = vclt_s8(x, vdup_n_s8(l));
    uint8x8_t   c = vcgt_s8(x, vdup_n_s8(r));
    c = vorr_u8(b, c);
    v = vreinterpret_f32_u8(c);
    m = vget_lane_f32(v, 0);
    return  WBI_ASTV(m);
}

INLINE(Vwbc,VWBC_CNBS) (Vwbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_CNBS(VWBC_ASBI(a), l, r));
#else
    return  VWBU_ASBC(VWBU_CNBS(VWBC_ASBU(a), l, r));
#endif
}


INLINE(Vwhu,VWHU_CNBS) (Vwhu a, uint16_t l, uint16_t r)
{
    float       m = VWHU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint16x4_t  x = vreinterpret_u16_f32(v);
    uint16x4_t  b = vclt_u16(x, vdup_n_u16(l));
    uint16x4_t  c = vcgt_u16(x, vdup_n_u16(r));
    c = vorr_u16(b, c);
    v = vreinterpret_f32_u16(c);
    m = vget_lane_f32(v, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_CNBS) (Vwhi a,  int16_t l,  int16_t r)
{
    float       m = VWHI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int16x4_t   x = vreinterpret_s16_f32(v);
    uint16x4_t  b = vclt_s16(x, vdup_n_s16(l));
    uint16x4_t  c = vcgt_s16(x, vdup_n_s16(r));
    c = vorr_u16(b, c);
    v = vreinterpret_f32_u16(c);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vwhi,VWHF_CNBS) (Vwhf a,  flt16_t l,  flt16_t r)
{
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);

#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t x = vreinterpret_s16_f32(v);
    uint16x4_t  b = vclt_f16(x, vdup_n_f16(l));
    uint16x4_t  c = vcgt_s16(x, vdup_n_f16(r));
    c = vorr_u16(b, c);
    v = vreinterpret_f32_u16(c);
#else
    float16x4_t t = vreinterpret_s16_f32(v);
    float32x4_t x = vcvt_f32_f16(t);
    uint32x4_t  b = vcltq_f32(x, vdupq_n_f32(l));
    uint32x4_t  c = vcgtq_f32(x, vdupq_n_f32(r));
    c = vorrq_u32(b, c);
    uint16x4_t  n = vmovn_u32(c);
    v = vreinterpret_f32_u16(n);
#endif
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);

}


INLINE(Vwwu,VWWU_CNBS) (Vwwu a, uint32_t l, uint32_t r)
{
    float       m = VWWU_ASTM(a);
    uint32_t    c =  FLT_ASTU(m);
    c = ((c < l) || (r < c)) ? UINT32_MAX : 0;
    m = UINT32_ASTF(c);
    return  WWU_ASTV(m);
}

INLINE(Vwwi,VWWI_CNBS) (Vwwi a,  int32_t l,  int32_t r)
{
    float       m = VWWI_ASTM(a);
    int32_t     c =  FLT_ASTI(m);
    c = ((c < l) || (r < c)) ? -1 : 0;
    m = INT32_ASTF(c);
    return  WWI_ASTV(m);
}

INLINE(Vwwi,VWWF_CNBS) (Vwwf a,    float l,    float r)
{
    float       m = VWWF_ASTM(a);
    uint32_t    c = vclts_f32(m, l)|vclts_f32(r, m);
    m = UINT32_ASTF(c);
    return  WWI_ASTV(m);
}


INLINE(Vdbu,VDBU_CNBS) (Vdbu a,  uint8_t l,  uint8_t r)
{
    return  vorr_u8(
        vclt_u8(a, vdup_n_u8(l)),
        vcgt_u8(a, vdup_n_u8(r))
    );
}

INLINE(Vdbi,VDBI_CNBS) (Vdbi a,   int8_t l,   int8_t r)
{
    uint8x8_t c = vorr_u8(
        vclt_s8(a, vdup_n_s8(l)),
        vcgt_s8(a, vdup_n_s8(r))
    );
    return  vreinterpret_s8_u8(c);
}

INLINE(Vdbc,VDBC_CNBS) (Vdbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_CNBS(VDBC_ASBI(a), l, r));
#else
    return  VDBI_ASBC(VDBU_CNBS(VDBC_ASBU(a), l, r));
#endif

}


INLINE(Vdhu,VDHU_CNBS) (Vdhu a, uint16_t l, uint16_t r) 
{
    return  vorr_u16(
        vclt_u16(a, vdup_n_u16(l)),
        vcgt_u16(a, vdup_n_u16(r))
    );
}

INLINE(Vdhi,VDHI_CNBS) (Vdhi a,  int16_t l,  int16_t r)
{
    uint16x4_t c = vorr_u16(
        vclt_s16(a, vdup_n_s16(l)),
        vcgt_s16(a, vdup_n_s16(r))
    );
    return  vreinterpret_s16_u16(c);
}

INLINE(Vdhi,VDHF_CNBS) (Vdhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x4_t c = vorr_u16(
        vclt_f16(a, vdup_n_f16(l)),
        vcgt_f16(a, vdup_n_f16(r))
    );
#else
    float32x4_t q = vcvt_f32_f16(a);
    uint32x4_t  v = vorrq_u32(
        vcltq_f32(q, vdupq_n_f32(l)),
        vcgtq_f32(q, vdupq_n_f32(r))
    );
    uint16x4_t  c = vmovn_u32(v);
#endif
    return  vreinterpret_s16_u16(c);
}


INLINE(Vdwu,VDWU_CNBS) (Vdwu a, uint32_t l, uint32_t r)
{
    return  vorr_u32(
        vclt_u32(a, vdup_n_u32(l)),
        vcgt_u32(a, vdup_n_u32(r))
    );
}

INLINE(Vdwi,VDWI_CNBS) (Vdwi a,  int32_t l,  int32_t r)
{
    uint32x2_t c = vorr_u32(
        vclt_s32(a, vdup_n_s32(l)),
        vcgt_s32(a, vdup_n_s32(r))
    );
    return  vreinterpret_s32_u32(c);
}

INLINE(Vdwi,VDWF_CNBS) (Vdwf a,    float l,    float r)
{
    uint32x2_t c = vorr_u32(
        vclt_f32(a, vdup_n_f32(l)),
        vcgt_f32(a, vdup_n_f32(r))
    );
    return  vreinterpret_s32_u32(c);
}


INLINE(Vddu,VDDU_CNBS) (Vddu a, uint64_t l, uint64_t r)
{
    return  vorr_u64(
        vclt_u64(a, vdup_n_u64(l)),
        vcgt_u64(a, vdup_n_u64(r))
    );
}

INLINE(Vddi,VDDI_CNBS) (Vddi a,  int64_t l,  int64_t r)
{
    uint64x1_t c = vorr_u64(
        vclt_s64(a, vdup_n_s64(l)),
        vcgt_s64(a, vdup_n_s64(r))
    );
    return  vreinterpret_s64_u64(c);
}

INLINE(Vddi,VDDF_CNBS) (Vddf a,   double l,   double r)
{
    uint64x1_t c = vorr_u64(
        vclt_f64(a, vdup_n_f64(l)),
        vcgt_f64(a, vdup_n_f64(r))
    );
    return  vreinterpret_s64_u64(c);
}



INLINE(Vqbu,VQBU_CNBS) (Vqbu a,  uint8_t l,  uint8_t r)
{
    return  vorrq_u8(
        vcltq_u8(a, vdupq_n_u8(l)),
        vcgtq_u8(a, vdupq_n_u8(r))
    );
}

INLINE(Vqbi,VQBI_CNBS) (Vqbi a,   int8_t l,   int8_t r)
{
    uint8x16_t c = vorrq_u8(
        vcltq_s8(a, vdupq_n_s8(l)),
        vcgtq_s8(a, vdupq_n_s8(r))
    );
    return  vreinterpretq_s8_u8(c);
}

INLINE(Vqbc,VQBC_CNBS) (Vqbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_CNBS(VQBC_ASBI(a), l, r));
#else
    return  VQBI_ASBC(VQBU_CNBS(VQBC_ASBU(a), l, r));
#endif

}


INLINE(Vqhu,VQHU_CNBS) (Vqhu a, uint16_t l, uint16_t r) 
{
    return  vorrq_u16(
        vcltq_u16(a, vdupq_n_u16(l)),
        vcgtq_u16(a, vdupq_n_u16(r))
    );
}

INLINE(Vqhi,VQHI_CNBS) (Vqhi a,  int16_t l,  int16_t r)
{
    uint16x8_t c = vorrq_u16(
        vcltq_s16(a, vdupq_n_s16(l)),
        vcgtq_s16(a, vdupq_n_s16(r))
    );
    return  vreinterpretq_s16_u16(c);
}

INLINE(Vqhi,VQHF_CNBS) (Vqhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x8_t c = vorrq_u16(
        vcltq_f16(a, vdupq_n_f16(l)),
        vcgtq_f16(a, vdupq_n_f16(r))
    );
    return  vreinterpretq_s16_u16(c);
#else
    return  vcombine_s16(
        VDHF_CNBS(vget_low_f16(a), l, r),
        VDHF_CNBS(vget_high_f16(a),l, r)
    );
#endif
}


INLINE(Vqwu,VQWU_CNBS) (Vqwu a, uint32_t l, uint32_t r)
{
    return  vorrq_u32(
        vcltq_u32(a, vdupq_n_u32(l)),
        vcgtq_u32(a, vdupq_n_u32(r))
    );
}

INLINE(Vqwi,VQWI_CNBS) (Vqwi a,  int32_t l,  int32_t r)
{
    uint32x4_t c = vorrq_u32(
        vcltq_s32(a, vdupq_n_s32(l)),
        vcgtq_s32(a, vdupq_n_s32(r))
    );
    return  vreinterpretq_s32_u32(c);
}

INLINE(Vqwi,VQWF_CNBS) (Vqwf a,    float l,    float r)
{
    uint32x4_t c = vorrq_u32(
        vcltq_f32(a, vdupq_n_f32(l)),
        vcgtq_f32(a, vdupq_n_f32(r))
    );
    return  vreinterpretq_s32_u32(c);
}


INLINE(Vqdu,VQDU_CNBS) (Vqdu a, uint64_t l, uint64_t r)
{
    return  vorrq_u64(
        vcltq_u64(a, vdupq_n_u64(l)),
        vcgtq_u64(a, vdupq_n_u64(r))
    );
}

INLINE(Vqdi,VQDI_CNBS) (Vqdi a,  int64_t l,  int64_t r)
{
    uint64x2_t c = vorrq_u64(
        vcltq_s64(a, vdupq_n_s64(l)),
        vcgtq_s64(a, vdupq_n_s64(r))
    );
    return  vreinterpretq_s64_u64(c);
}

INLINE(Vqdi,VQDF_CNBS) (Vqdf a,   double l,   double r)
{
    uint64x2_t c = vorrq_u64(
        vcltq_f64(a, vdupq_n_f64(l)),
        vcgtq_f64(a, vdupq_n_f64(r))
    );
    return  vreinterpretq_s64_u64(c);
}


#if 0 // _LEAVE_ARM_CNBS
}
#endif

#if 0 // _ENTER_ARM_CNBY
{
#endif

INLINE(flt16_t,  FLT16_CNBY) (flt16_t x, flt16_t l, flt16_t r)
{
    return  ((l > x) || (x > r));
}

INLINE(float,      FLT_CNBY)   (float x,   float l,   float r) 
{
    WORD_TYPE b = {.U=vcgts_f32(l, x)};
    WORD_TYPE c = {.U=vcgts_f32(x, r)};
    b.U |= c.U;
    c.F = 1.0f;
    c.U &= b.U;
    return c.F;
}

INLINE(int64_t,    DBL_CNBY)  (double x,  double l,  double r) 
{
    return  (x < l) || (r < x);
}


#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cnbyqu) (QUAD_UTYPE a, QUAD_UTYPE l, QUAD_UTYPE r)
{
    return  (a < l) || (r < a);
}

INLINE(QUAD_ITYPE,cnbyqi) (QUAD_ITYPE a, QUAD_ITYPE l, QUAD_ITYPE r)
{
    return  (a < l) || (r < a);
}

#endif


INLINE(Vwbu,VWBU_CNBY) 
(
    Vwbu                x,
    Jc(0, UINT8_MAX)    l,
    Jc(0, UINT8_MAX)    r
)
{
    float32x2_t v = {x.V0};
    uint8x8_t   a = vreinterpret_u8_f32(v);
    uint8x8_t   b = vclt_u8(a, vdup_n_u8(l));
    uint8x8_t   c = vcgt_u8(a, vdup_n_u8(r));
    c = vorr_u8(b, c);
    c = vshr_n_u8(c, 7);
    v = vreinterpret_f32_u8(c);
    x.V0 = vget_lane_f32(v, 0);
    return x;
}

INLINE(Vwbi,VWBI_CNBY) 
(
    Vwbi                    x,
    Jc(INT8_MIN, INT8_MAX)  l,
    Jc(INT8_MIN, INT8_MAX)  r
)
{
    float32x2_t v = {x.V0};
    int8x8_t    a = vreinterpret_s8_f32(v);
    uint8x8_t   b = vclt_s8(a, vdup_n_s8(l));
    uint8x8_t   c = vcgt_s8(a, vdup_n_s8(r));
    c = vorr_u8(b, c);
    c = vshr_n_u8(c, 7);
    v = vreinterpret_f32_s8(c);
    x.V0 = vget_lane_f32(v, 0);
    return x;
}

INLINE(Vwbc,VWBC_CNBY) 
(
    Vwbc                    x,
    Jc(CHAR_MIN, CHAR_MAX)  l,
    Jc(CHAR_MIN, CHAR_MAX)  r
)
{
    float32x2_t v = vdup_n_f32(x.V0);
    uint8x8_t   b, c;
#if CHAR_MIN
    int8x8_t    a = vreinterpret_s8_f32(v);
    b = vclt_s8(a, vdup_n_s8(l));
    c = vcgt_s8(a, vdup_n_s8(r));
#else
    uint8x8_t   a = vreinterpret_u8_f32(v);
    b = vclt_u8(a, vdup_n_u8(l));
    c = vcgt_u8(a, vdup_n_u8(r));
#endif
    c = vorr_u8(b, c);
    c = vshr_n_u8(c, 7);
    v = vreinterpret_f32_s8(c);
    x.V0 = vget_lane_f32(v, 0);
    return x;
}

INLINE(Vwhu,VWHU_CNBY) 
(
    Vwhu                x,
    Jc(0, UINT16_MAX)   l,
    Jc(0, UINT16_MAX)   r
)
{
    float32x2_t v = vdup_n_f32(x.V0);
    uint16x4_t  a = vreinterpret_u16_f32(v);
    uint16x4_t  b = vclt_u16(a, vdup_n_u16(l));
    uint16x4_t  c = vcgt_u16(a, vdup_n_u16(r));
    c = vorr_u16(b, c);
    c = vshr_n_u16(c, 15);
    v = vreinterpret_f32_u16(c);
    x.V0 = vget_lane_f32(v, 0);
    return x;
}

INLINE(Vwhi,VWHI_CNBY) 
(
    Vwhi                        x,
    Jc(INT16_MIN, INT16_MAX)    l,
    Jc(INT16_MIN, INT16_MAX)    r
)
{
    float32x2_t v = vdup_n_f32(x.V0);
    int16x4_t   a = vreinterpret_s16_f32(v);
    uint16x4_t  b = vclt_s16(a, vdup_n_s16(l));
    uint16x4_t  c = vcgt_s16(a, vdup_n_s16(r));
    c = vorr_u16(b, c);
    c = vshr_n_u16(c, 15);
    v = vreinterpret_f32_s16(c);
    x.V0 = vget_lane_f32(v, 0);
    return x;
}

INLINE(Vwhf,VWHF_CNBY) (Vwhf x, flt16_t l, flt16_t r)
{
    float32x2_t v = vdup_n_f32(x.V0);
    uint16x4_t  b, c;
    float16x4_t a = vreinterpret_s16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    b = vclt_f16(a, vdup_n_f16(l));
    c = vcgt_f16(a, vdup_n_f16(r));
    c = vorr_u16(b, c);
#else
    float32x4_t t = vcvt_f32_f16(a);
    uint32x4_t  p = vcltq_f32(t, vdupq_n_f32(l));
    uint32x4_t  q = vcgtq_f32(t, vdupq_n_f32(r));
    p = vorrq_u32(p, q);
    c = vmovn_u32(p);
#endif
    a = vdup_n_f16(1.0f16);
    b = vreinterpret_u16_f16(a);
    c = vand_u16(b, c);
    v = vreinterpret_f32_u16(c);
    x.V0 = vget_lane_f32(v, 0);
    return  x;
}


INLINE(Vwwu,VWWU_CNBY) 
(
    Vwwu                        x,
    Jc(UINT32_MIN, UINT32_MAX)  l,
    Jc(UINT32_MIN, UINT32_MAX)  r
)
{
    WORD_TYPE   c = {.F=x.V0};
    c.U = (l > c.U) || (c.U > r);
    x.V0 = c.F;
    return  x;
}

INLINE(Vwwi,VWWI_CNBY)
(
    Vwwi                        x,
    Jc(INT32_MIN, INT32_MAX)    l,
    Jc(INT32_MIN, INT32_MAX)    r
)
{
    WORD_TYPE   c = {.F=x.V0};
    c.I = (l > c.I) || (c.I > r);
    x.V0 = c.F;
    return  x;
}

INLINE(Vwwi,VWWF_CNBY) (Vwwf a,    float l,    float r)
{
    WORD_TYPE   c = {.F=a.V0};
    c.I = (l > c.I) || (c.I > r);
    return  ((Vwwi){c.F});
}


INLINE(Vdbu,VDBU_CNBY) (Vdbu a,  uint8_t l,  uint8_t r)
{
    a = vorr_u8(
        vclt_u8(a, vdup_n_u8(l)),
        vcgt_u8(a, vdup_n_u8(r))
    );
    return  vshr_n_u8(a, 1);
}

INLINE(Vdbi,VDBI_CNBY) (Vdbi a,   int8_t l,   int8_t r)
{
    uint8x8_t c = vorr_u8(
        vclt_s8(a, vdup_n_s8(l)),
        vcgt_s8(a, vdup_n_s8(r))
    );
    c = vshr_n_u8(c, 7);
    return  vreinterpret_s8_u8(c);
}

INLINE(Vdbc,VDBC_CNBY) (Vdbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_CNBY(VDBC_ASBI(a), l, r));
#else
    return  VDBI_ASBC(VDBU_CNBY(VDBC_ASBU(a), l, r));
#endif

}


INLINE(Vdhu,VDHU_CNBY) (Vdhu a, uint16_t l, uint16_t r) 
{
    a =  vorr_u16(
        vclt_u16(a, vdup_n_u16(l)),
        vcgt_u16(a, vdup_n_u16(r))
    );
    return vshr_n_u16(a, 15);
}

INLINE(Vdhi,VDHI_CNBY) (Vdhi a,  int16_t l,  int16_t r)
{
    uint16x4_t c = vorr_u16(
        vclt_s16(a, vdup_n_s16(l)),
        vcgt_s16(a, vdup_n_s16(r))
    );
    c = vshr_n_u16(c, 15);
    return  vreinterpret_s16_u16(c);
}

INLINE(Vdhi,VDHF_CNBY) (Vdhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x4_t c = vorr_u16(
        vclt_f16(a, vdup_n_f16(l)),
        vcgt_f16(a, vdup_n_f16(r))
    );
#else
    float32x4_t q = vcvt_f32_f16(a);
    uint32x4_t  v = vorrq_u32(
        vcltq_f32(q, vdupq_n_f32(l)),
        vcgtq_f32(q, vdupq_n_f32(r))
    );
    uint16x4_t  c = vmovn_u32(v);
#endif
    c = vshr_n_u16(c, 15);
    return  vreinterpret_s16_u16(c);
}


INLINE(Vdwu,VDWU_CNBY) (Vdwu a, uint32_t l, uint32_t r)
{
    a =  vorr_u32(
        vclt_u32(a, vdup_n_u32(l)),
        vcgt_u32(a, vdup_n_u32(r))
    );
    return vshr_n_u32(a, 31);
}

INLINE(Vdwi,VDWI_CNBY) (Vdwi a,  int32_t l,  int32_t r)
{
    uint32x2_t c = vorr_u32(
        vclt_s32(a, vdup_n_s32(l)),
        vcgt_s32(a, vdup_n_s32(r))
    );
    c = vshr_n_u32(c, 31);
    return  vreinterpret_s32_u32(c);
}

INLINE(Vdwi,VDWF_CNBY) (Vdwf a,    float l,    float r)
{
    uint32x2_t c = vorr_u32(
        vclt_f32(a, vdup_n_f32(l)),
        vcgt_f32(a, vdup_n_f32(r))
    );
    c = vshr_n_u32(c, 31);
    return  vreinterpret_s32_u32(c);
}


INLINE(Vddu,VDDU_CNBY) (Vddu a, uint64_t l, uint64_t r)
{
    a =  vorr_u64(
        vclt_u64(a, vdup_n_u64(l)),
        vcgt_u64(a, vdup_n_u64(r))
    );
    a = vshr_n_u64(a, 63);
    return  a;
}

INLINE(Vddi,VDDI_CNBY) (Vddi a,  int64_t l,  int64_t r)
{
    uint64x1_t c = vorr_u64(
        vclt_s64(a, vdup_n_s64(l)),
        vcgt_s64(a, vdup_n_s64(r))
    );
    c = vshr_n_u64(c, 63);
    return  vreinterpret_s64_u64(c);
}

INLINE(Vddi,VDDF_CNBY) (Vddf a,   double l,   double r)
{
    uint64x1_t c = vorr_u64(
        vclt_f64(a, vdup_n_f64(l)),
        vcgt_f64(a, vdup_n_f64(r))
    );
    c = vshr_n_u64(c, 63);
    return  vreinterpret_s64_u64(c);
}



INLINE(Vqbu,VQBU_CNBY) (Vqbu a,  uint8_t l,  uint8_t r)
{
    a =  vorrq_u8(
        vcltq_u8(a, vdupq_n_u8(l)),
        vcgtq_u8(a, vdupq_n_u8(r))
    );
    return vshrq_n_u8(a, 7);
}

INLINE(Vqbi,VQBI_CNBY) (Vqbi a,   int8_t l,   int8_t r)
{
    uint8x16_t c = vorrq_u8(
        vcltq_s8(a, vdupq_n_s8(l)),
        vcgtq_s8(a, vdupq_n_s8(r))
    );
    c = vshrq_n_u8(c, 7);
    return  vreinterpretq_s8_u8(c);
}

INLINE(Vqbc,VQBC_CNBY) (Vqbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_CNBY(VQBC_ASBI(a), l, r));
#else
    return  VQBI_ASBC(VQBU_CNBY(VQBC_ASBU(a), l, r));
#endif

}


INLINE(Vqhu,VQHU_CNBY) (Vqhu a, uint16_t l, uint16_t r) 
{
    a = vorrq_u16(
        vcltq_u16(a, vdupq_n_u16(l)),
        vcgtq_u16(a, vdupq_n_u16(r))
    );
    return vshrq_n_u16(a,  15);
}

INLINE(Vqhi,VQHI_CNBY) (Vqhi a,  int16_t l,  int16_t r)
{
    uint16x8_t c = vorrq_u16(
        vcltq_s16(a, vdupq_n_s16(l)),
        vcgtq_s16(a, vdupq_n_s16(r))
    );
    c = vshrq_n_u16(c, 15);
    return  vreinterpretq_s16_u16(c);
}

INLINE(Vqhi,VQHF_CNBY) (Vqhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x8_t c = vorrq_u16(
        vcltq_f16(a, vdupq_n_f16(l)),
        vcgtq_f16(a, vdupq_n_f16(r))
    );
    c = vshrq_n_u16(c, 15);
    return  vreinterpretq_s16_u16(c);
#else
    return  vcombine_s16(
        VDHF_CNBY(vget_low_f16(a), l, r),
        VDHF_CNBY(vget_high_f16(a),l, r)
    );
#endif
}


INLINE(Vqwu,VQWU_CNBY) (Vqwu a, uint32_t l, uint32_t r)
{
    a = vorrq_u32(
        vcltq_u32(a, vdupq_n_u32(l)),
        vcgtq_u32(a, vdupq_n_u32(r))
    );
    return vshrq_n_u32(a, 31);
}

INLINE(Vqwi,VQWI_CNBY) (Vqwi a,  int32_t l,  int32_t r)
{
    uint32x4_t c = vorrq_u32(
        vcltq_s32(a, vdupq_n_s32(l)),
        vcgtq_s32(a, vdupq_n_s32(r))
    );
    c = vshrq_n_u32(c, 31);
    return  vreinterpretq_s32_u32(c);
}

INLINE(Vqwi,VQWF_CNBY) (Vqwf a,    float l,    float r)
{
    uint32x4_t c = vorrq_u32(
        vcltq_f32(a, vdupq_n_f32(l)),
        vcgtq_f32(a, vdupq_n_f32(r))
    );
    c = vshrq_n_u32(c, 31);
    return  vreinterpretq_s32_u32(c);
}


INLINE(Vqdu,VQDU_CNBY) (Vqdu a, uint64_t l, uint64_t r)
{
    a =  vorrq_u64(
        vcltq_u64(a, vdupq_n_u64(l)),
        vcgtq_u64(a, vdupq_n_u64(r))
    );
    return vshrq_n_u64(a, 63);
}

INLINE(Vqdi,VQDI_CNBY) (Vqdi a,  int64_t l,  int64_t r)
{
    uint64x2_t c = vorrq_u64(
        vcltq_s64(a, vdupq_n_s64(l)),
        vcgtq_s64(a, vdupq_n_s64(r))
    );
    c = vshrq_n_u64(c, 63);
    return  vreinterpretq_s64_u64(c);
}

INLINE(Vqdi,VQDF_CNBY) (Vqdf a,   double l,   double r)
{
    uint64x2_t c = vorrq_u64(
        vcltq_f64(a, vdupq_n_f64(l)),
        vcgtq_f64(a, vdupq_n_f64(r))
    );
    c = vshrq_n_u64(c, 63);
    return  vreinterpretq_s64_u64(c);
}


#if 0 // _LEAVE_ARM_CNBY
}
#endif

#if 0 // _ENTER_ARM_RAZB
{
#endif

/*  Round each floating point element in the operand to
    the nearest integer then convert the result to int8_t.
    Ties are rounded away from zero, i.e. towards ±∞.

    If rounded values are greater than INT8_MAX or less
    than INT8_MIN, the result is implementation defined.

*/

INLINE(int8_t,FLT16_RAZB) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtah_s16_f16(x);
#else
    return  vcvtas_s32_f32(x);
#endif
}

INLINE(int8_t,  FLT_RAZB)   (float x) {return vcvtas_s32_f32(x);}
INLINE(int8_t,  DBL_RAZB)  (double x) {return vcvtad_s64_f64(x);}
INLINE(int8_t,razbqf)  (QUAD_FTYPE x) {return vcvtad_s64_f64(x);}

INLINE(Vwbi,VDHF_RAZB) (Vdhf x)
{
// TODO
    return  VWBI_VOID;
}

INLINE(Vdbi,VQHF_RAZB) (Vqhf x)
{
// TODO
    return  VDBI_VOID;
}

INLINE(Vwbi,VQWF_RAZB) (Vqwf x)
{
    return  VQWI_CVBI(vcvtaq_s32_f32(x));
}

#if 0 // _LEAVE_ARM_RAZB
}
#endif

#if 0 // _ENTER_ARM_RAZH
{
#endif

/*  Round each floating point element in the operand to
    the nearest integer then convert the result to int16_t.
    Ties round away from zero, i.e. towards ±∞.

    If rounded values are greater than INT16_MAX or less
    than INT16_MIN, the result is implementation defined.

*/

INLINE(int16_t,FLT16_RAZH) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtah_s16_f16(x);
#else
    return  vcvtas_s32_f32(x);
#endif
}

INLINE(int16_t,  FLT_RAZH)   (float x) {return vcvtas_s32_f32(x);}
INLINE(int16_t,  DBL_RAZH)  (double x) {return vcvtad_s64_f64(x);}
INLINE(int16_t,razhqf)  (QUAD_FTYPE x) {return vcvtad_s64_f64(x);}

INLINE(Vwhi,VWHF_RAZH) (Vwhf x) {return VWHI_VOID;}

INLINE(Vdhi,VDHF_RAZH) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvta_s16_f16(x);
#else
    return  vmovn_s32(vcvtaq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vwhi,VDWF_RAZH) (Vdwf x)
{
    return  VDWI_CVHI(vcvta_s32_f32(x));
}

INLINE(Vqhi,VQHF_RAZH) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtaq_s16_f16(x);
#else
    float32x4_t f;
    int32x4_t   w;
    f = vcvt_f32_f16(vget_low_f16(x));
    w = vcvtaq_s32_f32(f);
    int16x4_t   p = vmovn_s16(w);
    f = vcvt_f32_f16(vget_high_f16(x));
    w = vcvtaq_s32_f32(f);
    int16x4_t   q = vmovn_s16(w);
    return  vcombine_s16(p, q);
#endif
}

INLINE(Vdhi,VQWF_RAZH) (Vqwf x)
{
    return  vmovn_s32(vcvtaq_s32_f32(x));
}

INLINE(Vwhi,VQDF_RAZH) (Vqdf x)
{
    return  VQDI_CVHI(vcvtaq_s64_f64(x));
}

#if 0 // _LEAVE_ARM_RAZH
}
#endif

#if 0 // _ENTER_ARM_RAZW
{
#endif

/*  Round each floating point element in the operand to
    the nearest integer then convert the result to int32_t.

    Ties are rounded away from zero, i.e. towards ±∞.

    If rounded values are greater than INT32_MAX or less
    than INT32_MIN, the result is implementation defined.

*/


INLINE(int32_t,FLT16_RAZW) (flt16_t x) {return x;}
INLINE(int32_t,  FLT_RAZW)   (float x) {return vcvtas_s32_f32(x);}
INLINE(int32_t,  DBL_RAZW)  (double x) {return vcvtad_s64_f64(x);}
INLINE(int32_t,razwqf)  (QUAD_FTYPE x) {return vcvtad_s64_f64(x);}


INLINE(Vdwi,VWHF_RAZW) (Vwhf x) 
{
    float m = VWHF_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
    return  vcvta_s32_f32(vget_low_f32(vcvt_f32_f16(f)));
}

INLINE(Vwwi,VWWF_RAZW) (Vwwf x)
{
    return  INT_ASTV(vcvtas_s32_f32(VWWF_ASTM(x)));
}


INLINE(Vqwi,VDHF_RAZW) (Vdhf x)
{
    return  vcvtaq_s32_f32(vcvt_f32_f16(x));
}

INLINE(Vdwi,VDWF_RAZW) (Vdwf x) {return vcvta_s32_f32(x);}

INLINE(Vwwi,VDDF_RAZW) (Vddf x)
{
    return  INT_ASTV(vcvtad_s64_f64(vget_lane_f64(x, 0)));
}

INLINE(Vqwi,VQWF_RAZW) (Vqwf x) {return vcvtaq_s32_f32(x);}
INLINE(Vdwi,VQDF_RAZW) (Vqdf x) {return vmovn_s64(vcvtaq_s64_f64(x));}

#if 0 // _LEAVE_ARM_RAZW
}
#endif

#if 0 // _ENTER_ARM_RAZD
{
#endif

/*  Round each floating point element in the operand to
    the nearest integer then convert the result to int64_t.
    Ties round away from zero, i.e. towards ±infinity.

    If rounded values are greater than INT64_MAX or less
    than INT64_MIN, the result is implementation defined.

*/

INLINE(int64_t,FLT16_RAZD) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtah_s64_f16(x);
#else
    return  vcvtas_s32_f32(x);
#endif
}

INLINE(int64_t,  FLT_RAZD)   (float x) {return vcvtas_s32_f32(x);}
INLINE(int64_t,  DBL_RAZD)  (double x) {return vcvtad_s64_f64(x);}
INLINE(int64_t,razdqf)  (QUAD_FTYPE x) {return vcvtad_s64_f64(x);}

// TODO
INLINE(Vqdi,VWHF_RAZD) (Vwhf x) {return VQDI_VOID;}
INLINE(Vddi,VWWF_RAZD) (Vwwf x)
{
    return  vdup_n_s64(vcvtas_s32_f32(VWWF_ASTM(x)));
}

INLINE(Vqdi,VDWF_RAZD) (Vdwf x)
{
    return  vcvtaq_s64_f64(vcvt_f64_f32(x));
}
INLINE(Vddi,VDDF_RAZD) (Vddf x)
{
    return  vcvta_s64_f64(x);
}

INLINE(Vqdi,VQDF_RAZD) (Vqdf x) {return vcvtaq_s64_f64(x);}

#if 0 // _LEAVE_ARM_RAZD
}
#endif

#if 0 // _ENTER_ARM_RAZF
{
#endif


INLINE(flt16_t,FLT16_RAZF) (flt16_t x)
{
#if defined(SPC_ARM_FRINT)
    return  vrndah_f16(x);
#else
/*
TODO: reimplement this??
*/
    return 0;
#endif
}


INLINE(float, FLT_RAZF) (float x)
{
#if defined(SPC_ARM_FRINT) && defined(SPC_ANY_ASM)
    __asm__ volatile("frinta %s0, s0" : "=w"(x) : :);
    return x;
#elif defined(SPC_ARM_FRINT)
    return  vget_lane_f64(
        vrnda_f64(vdup_n_f64(x)),
        0
    );
#else
    union {
        float Value;
        struct {uint32_t Mant: 23, Expo: 8, Sign: 1,: 0;};
    } f = {x};
    if (f.Expo > 149)
        return x;
    if (f.Expo <= 126)
        return  (f.Expo == 126)
        ?   (f.Sign ? -1.0f : 1.0f)
        :   (f.Sign ? -0.0f : 0.0f);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    unsigned r = 23-(f.Expo-126);
    unsigned h = f.Mant>>r;
    unsigned m = f.Mant-(h<<r);
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5f) : (x+0.5f))
        :   x;
    }
    if (h&1)
        h+=1;
    m = h<<r;
    if (m <= 0x7fffff)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(double,DBL_RAZF) (double x)
{
#if defined(SPC_ARM_FRINT)
    float64x1_t v = vdup_n_f64(x); 
    v = vrnda_f64(v);
    return  vget_lane_f64(v, 0);
#else
    union {
        double Value;
        struct {
            uint64_t
                Mant:   52,
                Expo:   11,
                Sign:   1,
                :       0;
        };
    } f = {x};

    if (f.Expo > 1074)
        return x;
    if (f.Expo <= 1022)
        return  (f.Expo == 1022)
        ?   (f.Sign ? -1.0 : 1.0)
        :   (f.Sign ? -0.0 : 0.0);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    uint64_t r = 52-(f.Expo-1022);
    uint64_t h = f.Mant>>r;
    uint64_t m = f.Mant-(h<<r);
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5) : (x+0.5))
        :   x;
    }
    if (h&1)
        h+=1;
    m = h<<r;
    if (m <= 0xfffffffffffffULL)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}


INLINE(Vwhf,VWHF_RAZF) (Vwhf x)
{
#if defined(SPC_ARM_FRINT)
    float32x2_t w = vdup_n_f32(x.V0);
    float16x4_t h = vreinterpret_f16_f32(w);
    h = vrnda_f16(h);
    w = vreinterpret_f32_f16(h);
    x.V0 = vget_lane_f32(w, 0);
    return x;
#else
/*  TODO 
*/ 
    return x;
#endif
}

INLINE(Vwwf,VWWF_RAZF) (Vwwf x)
{
    x.V0 = FLT_RAZF(x.V0);
    return x;
}

INLINE(Vdhf,VDHF_RAZF) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f16(x);
#else
    x = vset_lane_f16(FLT16_RAZF(vget_lane_f16(x, 0)), x, 0);
    x = vset_lane_f16(FLT16_RAZF(vget_lane_f16(x, 1)), x, 1);
    x = vset_lane_f16(FLT16_RAZF(vget_lane_f16(x, 2)), x, 2);
    x = vset_lane_f16(FLT16_RAZF(vget_lane_f16(x, 3)), x, 3);
    return  x;
#endif
}

INLINE(Vdwf,VDWF_RAZF) (Vdwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f32(x);
#else
    x = vset_lane_f32(FLT_RAZF(vget_lane_f32(x, 0)), x, 0);
    x = vset_lane_f32(FLT_RAZF(vget_lane_f32(x, 1)), x, 1);
    return  x;
#endif
}


INLINE(Vddf,VDDF_RAZF) (Vddf x)
{
#if defined(SPC_ARM_FRINT)
    return  vrnda_f64(x);
#else
    return  vdup_n_f64(DBL_RAZF(vget_lane_f64(x, 0)));
#endif
}


INLINE(Vqhf,VQHF_RAZF) (Vqhf x)
{
#if defined(SPC_ARM_FRINT)
    return  vrndaq_f16(x);
#else
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, 0)), x, 0);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, 1)), x, 1);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, 2)), x, 2);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, 3)), x, 3);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, 4)), x, 4);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, 5)), x, 5);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, 6)), x, 6);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, 7)), x, 7);
    return  x;
#endif
}

INLINE(Vqwf,VQWF_RAZF) (Vqwf x)
{
#if defined(SPC_ARM_FRINT)
    return  vrndaq_f32(x);
#else
    x = vsetq_lane_f32(FLT_RAZF(vgetq_lane_f32(x, 0)), x, 0);
    x = vsetq_lane_f32(FLT_RAZF(vgetq_lane_f32(x, 1)), x, 1);
    x = vsetq_lane_f32(FLT_RAZF(vgetq_lane_f32(x, 2)), x, 2);
    x = vsetq_lane_f32(FLT_RAZF(vgetq_lane_f32(x, 3)), x, 3);
    return  x;
#endif
}

INLINE(Vqdf,VQDF_RAZF) (Vqdf x)
{
#if defined(SPC_ARM_FRINT)
    return  vrndaq_f64(x);
#else
    x = vsetq_lane_f64(DBL_RAZF(vgetq_lane_f64(x, 0)), x, 0);
    x = vsetq_lane_f64(DBL_RAZF(vgetq_lane_f64(x, 1)), x, 1);
    return  x;
#endif
}

#if 0 // _LEAVE_ARM_RAZF
}
#endif


#if 0 // _ENTER_ARM_RTNB
{
#endif

INLINE(int8_t,FLT16_RTNB) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtmh_s16_f16(x);
#else
    return  vcvtms_s32_f32(x);
#endif
}

INLINE(int8_t,  FLT_RTNB)   (float x) {return vcvtms_s32_f32(x);}
INLINE(int8_t,  DBL_RTNB)  (double x) {return vcvtmd_s64_f64(x);}

INLINE(Vwbi,VDHF_RTNB) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  VDHI_CVBI(vcvtm_s16_f16(x));
#else
    return  VQWI_CVBI(vcvtmq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vdbi,VQHF_RTNB) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmovn_s16(vcvtmq_s16_f16(x));
#else
    return  VDBI_VOID;
/*
    float32x4_t r = vcvt_f32_f16(vget_high_f16(x));
    return  vmovn_s16(
        vcombine_s16(
            vmovn_s32(vcvtmq_s32_f32(l)),
            vmovn_s32(vcvtmq_s32_f32(r)),
        )
    );
*/    float32x4_t l = vcvt_f32_f16(vget_low_f16(x));
#endif
}

INLINE(Vwbi,VQWF_RTNB) (Vqwf x)
{
    return  VQWI_CVBI(vcvtmq_s32_f32(x));
}

#if 0 // _LEAVE_ARM_RTNB
}
#endif

#if 0 // _ENTER_ARM_RTNH
{
#endif

INLINE(int16_t,FLT16_RTNH) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtmh_s16_f16(x);
#else
    return  vcvtms_s32_f32(x);
#endif
}

INLINE(int16_t,  FLT_RTNH)   (float x) {return vcvtms_s32_f32(x);}
INLINE(int16_t,  DBL_RTNH)  (double x) {return vcvtmd_s64_f64(x);}

INLINE(Vwhi,VWHF_RTNH) (Vwhf x) 
{
    float       m = VWHF_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    int16x4_t   z = vcvtm_s16_f16(f);
#else
    float32x4_t q = vcvt_f32_f16(f);
    int16x4_t   z = vmovn_s32(vcvtmq_s32_f32(q));
#endif
    v = vreinterpret_f32_s16(z);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vdhi,VDHF_RTNH) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtm_s16_f16(x);
#else
    return  vmovn_s32(vcvtmq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vwhi,VDWF_RTNH) (Vdwf x)
{
    return  VDWI_CVHI(vcvtm_s32_f32(x));
}

INLINE(Vqhi,VQHF_RTNH) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtmq_s16_f16(x);
#else
    float32x4_t f;
    int32x4_t   w;
    f = vcvt_f32_f16(vget_low_f16(x));
    w = vcvtmq_s32_f32(f);
    int16x4_t   p = vmovn_s16(w);
    f = vcvt_f32_f16(vget_high_f16(x));
    w = vcvtmq_s32_f32(f);
    int16x4_t   q = vmovn_s16(w);
    return  vcombine_s16(p, q);
#endif
}

INLINE(Vdhi,VQWF_RTNH) (Vqwf x)
{
    return  vmovn_s32(vcvtmq_s32_f32(x));
}

INLINE(Vwhi,VQDF_RTNH) (Vqdf x)
{
    return  VQDI_CVHI(vcvtmq_s64_f64(x));
}

#if 0 // _LEAVE_ARM_RTNH
}
#endif

#if 0 // _ENTER_ARM_RTNW
{
#endif

INLINE(int32_t,FLT16_RTNW) (flt16_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtmh_s16_f16(x);
#else
    return  vcvtms_s32_f32(x);
#endif
}

INLINE(int32_t,  FLT_RTNW)   (float x) {return vcvtms_s32_f32(x);}
INLINE(int32_t,  DBL_RTNW)  (double x) {return vcvtmd_s64_f64(x);}

INLINE(Vdwi,VWHF_RTNW) (Vwhf x) 
{
    float       m = VWHF_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    int16x4_t   z = vcvtm_s16_f16(f);
    return  vget_low_s32(vmovl_s16(z));
#else
    float32x4_t q = vcvt_f32_f16(f);
    int32x4_t   z = vcvtmq_s32_f32(q);
    return vget_low_s32(z);
#endif
}

INLINE(Vwwi,VWWF_RTNW) (Vwwf x)
{
    return  INT_ASTV(vcvtms_s32_f32(VWWF_ASTM(x)));
}

INLINE(Vqwi,VDHF_RTNW) (Vdhf x)
{
    return  vcvtmq_s32_f32(vcvt_f32_f16(x));
}

INLINE(Vdwi,VDWF_RTNW) (Vdwf x) {return vcvtm_s32_f32(x);}
INLINE(Vwwi,VDDF_RTNW) (Vddf x)
{
    return  INT_ASTV(vcvtmd_s64_f64(vget_lane_f64(x, 0)));
}

INLINE(Vqwi,VQWF_RTNW) (Vqwf x) {return vcvtmq_s32_f32(x);}
INLINE(Vdwi,VQDF_RTNW) (Vqdf x) {return vmovn_s64(vcvtmq_s64_f64(x));}

#if 0 // _LEAVE_ARM_RTNW
}
#endif

#if 0 // _ENTER_ARM_RTND
{
#endif

INLINE(int64_t,FLT16_RTND) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtmh_s64_f16(x);
#else
    return  vcvtms_s32_f32(x);
#endif
}

INLINE(int64_t,  FLT_RTND)   (float x) {return vcvtms_s32_f32(x);}
INLINE(int64_t,  DBL_RTND)  (double x) {return vcvtmd_s64_f64(x);}

INLINE(Vqdi,VWHF_RTND) (Vwhf x) 
{
    return  vcvtmq_s64_f64(VWHF_CVDF(x));
}

INLINE(Vddi,VWWF_RTND) (Vwwf x)
{
    return  vcvtm_s64_f64(vdup_n_f64(VWWF_ASTV(x)));
}

INLINE(Vqdi,VDWF_RTND) (Vdwf x)
{
    return  vcvtmq_s64_f64(vcvt_f64_f32(x));
}

INLINE(Vddi,VDDF_RTND) (Vddf x)
{
    return  vcvtm_s64_f64(x);
}

INLINE(Vqdi,VQDF_RTND) (Vqdf x) {return vcvtmq_s64_f64(x);}

#if 0 // _LEAVE_ARM_RTND
}
#endif

#if 0 // _ENTER_ARM_RTNF
{
#endif
// TODO: fix this

INLINE(double,DBL_RTNF) (double x)
{
#if defined(SPC_ARM_FRINT)
    float64x1_t v = vdup_n_f64(x); // nsta vrndad_f64?...
    v = vrnda_f64(v);
    return  vget_lane_f64(v, 0);
#else
    union {
        double Value;
        struct {
            uint64_t
                Mant:   52,
                Expo:   11,
                Sign:   1,
                :       0;
        };
    } f = {x};
    // 
    if (f.Expo > 1074)
        return x;

    if (f.Expo <= 1022)
        return  (f.Expo == 1022)
        ?   (f.Sign ? -1.0 : 1.0)
        :   (f.Sign ? -0.0 : 0.0);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    uint64_t r = 52-(f.Expo-1022);
    uint64_t h = f.Mant>>r;
    uint64_t m = f.Mant-(h<<r);
    printf(
        "r=%" UINT64_DFMT ", "
        "h=%" UINT64_DFMT ", "
        "m=%" UINT64_DFMT "\n",
        r,h,m
    );
    // x=+1.99: r=51, h=1, m=2206763817411543
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5) : (x+0.5))
        :   x;
    }
    if (h&1)
        h-=1;
    m = h<<r;
    if (m <= 0xfffffffffffffULL)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(float,FLT_RTNF) (float x)
{
#if defined(SPC_ARM_FRINT)
    float32x2_t v = vdup_n_f32(x); // nsta vrndas_f32?...
    v = vrnda_f32(v);
    return  vget_lane_f32(v, 0);
#else
    union {
        float Value;
        struct {
            uint32_t
                Mant:   23,
                Expo:   8,
                Sign:   1,
                :       0;
        };
    } f = {x};

    if (f.Expo > 149)
        return x;
    if (f.Expo <= 126)
        return  (f.Expo == 126)
        ?   (f.Sign ? -1.0f : 1.0f)
        :   (f.Sign ? -0.0f : 0.0f);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    unsigned r = 23-(f.Expo-126);
    unsigned h = f.Mant>>r;
    unsigned m = f.Mant-(h<<r);
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5f) : (x+0.5f))
        :   x;
    }
    if (h&1)
        h+=1;
    m = h<<r;
    if (m <= 0x7fffff)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(flt16_t,FLT16_RTNF) (flt16_t x)
{
#if defined(SPC_ARM_FRINT)
    return  vrndah_f16(x);
#else
    return  FLT_RTNF(x);
#endif
}

INLINE(Vwhf,VWHF_RTNF) (Vwhf x)
{
    return  VWHF_VOID; // TODO
}

INLINE(Vwwf,VWWF_RTNF) (Vwwf x)
{
    return  WWF_ASTV(FLT_RTNF(VWWF_ASTM(x)));
}

INLINE(Vdhf,VDHF_RTNF) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f16(x);
#else
    x = vset_lane_f16(FLT16_RTNF(vget_lane_f16(x, 0)), x, 0);
    x = vset_lane_f16(FLT16_RTNF(vget_lane_f16(x, 1)), x, 1);
    x = vset_lane_f16(FLT16_RTNF(vget_lane_f16(x, 2)), x, 2);
    x = vset_lane_f16(FLT16_RTNF(vget_lane_f16(x, 3)), x, 3);
    return  x;
#endif
}

INLINE(Vdwf,VDWF_RTNF) (Vdwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f32(x);
#else
    x = vset_lane_f32(FLT_RTNF(vget_lane_f32(x, 0)), x, 0);
    x = vset_lane_f32(FLT_RTNF(vget_lane_f32(x, 1)), x, 1);
    return  x;
#endif
}

INLINE(Vddf,VDDF_RTNF) (Vddf x)
{
    return  DBL_ASTV(DBL_RTNF(vget_lane_f64(x, 0)));
}


INLINE(Vqhf,VQHF_RTNF) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f16(x);
#else
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 0)), x, 0);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 1)), x, 1);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 2)), x, 2);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 3)), x, 3);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 4)), x, 4);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 5)), x, 5);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 6)), x, 6);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 7)), x, 7);
    return  x;
#endif
}

INLINE(Vqwf,VQWF_RTNF) (Vqwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f32(x);
#else
    x = vsetq_lane_f32(FLT_RTNF(vgetq_lane_f32(x, 0)), x, 0);
    x = vsetq_lane_f32(FLT_RTNF(vgetq_lane_f32(x, 1)), x, 1);
    x = vsetq_lane_f32(FLT_RTNF(vgetq_lane_f32(x, 2)), x, 2);
    x = vsetq_lane_f32(FLT_RTNF(vgetq_lane_f32(x, 3)), x, 3);
    return  x;
#endif
}

INLINE(Vqdf,VQDF_RTNF) (Vqdf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f64(x);
#else
    x = vsetq_lane_f64(DBL_RTNF(vgetq_lane_f64(x, 0)), x, 0);
    x = vsetq_lane_f64(DBL_RTNF(vgetq_lane_f64(x, 1)), x, 1);
    return  x;
#endif
}

#if 0 // _LEAVE_ARM_RTNF
}
#endif


#if 0 // _ENTER_ARM_RTPB
{
#endif

INLINE(int8_t,FLT16_RTPB) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtph_s16_f16(x);
#else
    return  vcvtps_s32_f32(x);
#endif
}

INLINE(int8_t,  FLT_RTPB)   (float x) {return vcvtps_s32_f32(x);}
INLINE(int8_t,  DBL_RTPB)  (double x) {return vcvtpd_s64_f64(x);}

INLINE(Vwbi,VDHF_RTPB) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  VDHI_CVBI(vcvtp_s16_f16(x));
#else
    return  VQWI_CVBI(vcvtpq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vdbi,VQHF_RTPB) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmovn_s16(vcvtpq_s16_f16(x));
#else
    return  VDBI_VOID;
/*
    float32x4_t r = vcvt_f32_f16(vget_high_f16(x));
    return  vmovn_s16(
        vcombine_s16(
            vmovn_s32(vcvtpq_s32_f32(l)),
            vmovn_s32(vcvtpq_s32_f32(r)),
        )
    );
*/    float32x4_t l = vcvt_f32_f16(vget_low_f16(x));
#endif
}

INLINE(Vwbi,VQWF_RTPB) (Vqwf x)
{
    return  VQWI_CVBI(vcvtpq_s32_f32(x));
}

#if 0 // _LEAVE_ARM_RTPB
}
#endif

#if 0 // _ENTER_ARM_RTPH
{
#endif

INLINE(int16_t,FLT16_RTPH) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtph_s16_f16(x);
#else
    return  vcvtps_s32_f32(x);
#endif
}

INLINE(int16_t,  FLT_RTPH)   (float x) {return vcvtps_s32_f32(x);}
INLINE(int16_t,  DBL_RTPH)  (double x) {return vcvtpd_s64_f64(x);}

INLINE(Vwhi,VWHF_RTPH) (Vwhf x) 
{
    float       m = VWHF_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    int16x4_t   z = vcvtp_s16_f16(f);
#else
    float32x4_t q = vcvt_f32_f16(f);
    int16x4_t   z = vmovn_s32(vcvtpq_s32_f32(q));
#endif
    v = vreinterpret_f32_s16(z);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vdhi,VDHF_RTPH) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtp_s16_f16(x);
#else
    return  vmovn_s32(vcvtpq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vwhi,VDWF_RTPH) (Vdwf x)
{
    return  VDWI_CVHI(vcvtp_s32_f32(x));
}

INLINE(Vqhi,VQHF_RTPH) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtpq_s16_f16(x);
#else
    float32x4_t f;
    int32x4_t   w;
    f = vcvt_f32_f16(vget_low_f16(x));
    w = vcvtpq_s32_f32(f);
    int16x4_t   p = vmovn_s16(w);
    f = vcvt_f32_f16(vget_high_f16(x));
    w = vcvtpq_s32_f32(f);
    int16x4_t   q = vmovn_s16(w);
    return  vcombine_s16(p, q);
#endif
}

INLINE(Vdhi,VQWF_RTPH) (Vqwf x)
{
    return  vmovn_s32(vcvtpq_s32_f32(x));
}

INLINE(Vwhi,VQDF_RTPH) (Vqdf x)
{
    return  VQDI_CVHI(vcvtpq_s64_f64(x));
}

#if 0 // _LEAVE_ARM_RTPH
}
#endif

#if 0 // _ENTER_ARM_RTPW
{
#endif

INLINE(int32_t,FLT16_RTPW) (flt16_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtph_s16_f16(x);
#else
    return  vcvtps_s32_f32(x);
#endif
}

INLINE(int32_t,  FLT_RTPW)   (float x) {return vcvtps_s32_f32(x);}
INLINE(int32_t,  DBL_RTPW)  (double x) {return vcvtpd_s64_f64(x);}

INLINE(Vdwi,VWHF_RTPW) (Vwhf x) 
{
    float       m = VWHF_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    int16x4_t   z = vcvtp_s16_f16(f);
    return  vget_low_s32(vmovl_s16(z));
#else
    float32x4_t q = vcvt_f32_f16(f);
    int32x4_t   z = vcvtpq_s32_f32(q);
    return vget_low_s32(z);
#endif
}

INLINE(Vwwi,VWWF_RTPW) (Vwwf x)
{
    return  INT_ASTV(vcvtps_s32_f32(VWWF_ASTM(x)));
}

INLINE(Vqwi,VDHF_RTPW) (Vdhf x)
{
    return  vcvtpq_s32_f32(vcvt_f32_f16(x));
}

INLINE(Vdwi,VDWF_RTPW) (Vdwf x) {return vcvtp_s32_f32(x);}
INLINE(Vwwi,VDDF_RTPW) (Vddf x)
{
    return  INT_ASTV(vcvtpd_s64_f64(vget_lane_f64(x, 0)));
}

INLINE(Vqwi,VQWF_RTPW) (Vqwf x) {return vcvtpq_s32_f32(x);}
INLINE(Vdwi,VQDF_RTPW) (Vqdf x) {return vmovn_s64(vcvtpq_s64_f64(x));}

#if 0 // _LEAVE_ARM_RTPW
}
#endif

#if 0 // _ENTER_ARM_RTPD
{
#endif

INLINE(int64_t,FLT16_RTPD) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtph_s64_f16(x);
#else
    return  vcvtps_s32_f32(x);
#endif
}

INLINE(int64_t,  FLT_RTPD)   (float x) {return vcvtps_s32_f32(x);}
INLINE(int64_t,  DBL_RTPD)  (double x) {return vcvtpd_s64_f64(x);}

INLINE(Vqdi,VWHF_RTPD) (Vwhf x) 
{
    return  vcvtpq_s64_f64(VWHF_CVDF(x));
}

INLINE(Vddi,VWWF_RTPD) (Vwwf x)
{
    return  vcvtp_s64_f64(vdup_n_f64(VWWF_ASTV(x)));
}

INLINE(Vqdi,VDWF_RTPD) (Vdwf x)
{
    return  vcvtpq_s64_f64(vcvt_f64_f32(x));
}

INLINE(Vddi,VDDF_RTPD) (Vddf x)
{
    return  vcvtp_s64_f64(x);
}

INLINE(Vqdi,VQDF_RTPD) (Vqdf x) {return vcvtpq_s64_f64(x);}

#if 0 // _LEAVE_ARM_RTPD
}
#endif

#if 0 // _ENTER_ARM_RTPF
{
#endif
// TODO: fix this

INLINE(double,DBL_RTPF) (double x)
{
#if defined(SPC_ARM_FRINT)
    float64x1_t v = vdup_n_f64(x); // nsta vrndad_f64?...
    v = vrnda_f64(v);
    return  vget_lane_f64(v, 0);
#else
    union {
        double Value;
        struct {
            uint64_t
                Mant:   52,
                Expo:   11,
                Sign:   1,
                :       0;
        };
    } f = {x};
    // 
    if (f.Expo > 1074)
        return x;

    if (f.Expo <= 1022)
        return  (f.Expo == 1022)
        ?   (f.Sign ? -1.0 : 1.0)
        :   (f.Sign ? -0.0 : 0.0);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    uint64_t r = 52-(f.Expo-1022);
    uint64_t h = f.Mant>>r;
    uint64_t m = f.Mant-(h<<r);
    printf(
        "r=%" UINT64_DFMT ", "
        "h=%" UINT64_DFMT ", "
        "m=%" UINT64_DFMT "\n",
        r,h,m
    );
    // x=+1.99: r=51, h=1, m=2206763817411543
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5) : (x+0.5))
        :   x;
    }
    if (h&1)
        h-=1;
    m = h<<r;
    if (m <= 0xfffffffffffffULL)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(float,FLT_RTPF) (float x)
{
#if defined(SPC_ARM_FRINT)
    float32x2_t v = vdup_n_f32(x); // nsta vrndas_f32?...
    v = vrnda_f32(v);
    return  vget_lane_f32(v, 0);
#else
    union {
        float Value;
        struct {
            uint32_t
                Mant:   23,
                Expo:   8,
                Sign:   1,
                :       0;
        };
    } f = {x};

    if (f.Expo > 149)
        return x;
    if (f.Expo <= 126)
        return  (f.Expo == 126)
        ?   (f.Sign ? -1.0f : 1.0f)
        :   (f.Sign ? -0.0f : 0.0f);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    unsigned r = 23-(f.Expo-126);
    unsigned h = f.Mant>>r;
    unsigned m = f.Mant-(h<<r);
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5f) : (x+0.5f))
        :   x;
    }
    if (h&1)
        h+=1;
    m = h<<r;
    if (m <= 0x7fffff)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(flt16_t,FLT16_RTPF) (flt16_t x)
{
#if defined(SPC_ARM_FRINT)
    return  vrndah_f16(x);
#else
    return  FLT_RTPF(x);
#endif
}

INLINE(Vwhf,VWHF_RTPF) (Vwhf x)
{
    return  VWHF_VOID; // TODO
}

INLINE(Vwwf,VWWF_RTPF) (Vwwf x)
{
    return  WWF_ASTV(FLT_RTPF(VWWF_ASTM(x)));
}

INLINE(Vdhf,VDHF_RTPF) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f16(x);
#else
    x = vset_lane_f16(FLT16_RTPF(vget_lane_f16(x, 0)), x, 0);
    x = vset_lane_f16(FLT16_RTPF(vget_lane_f16(x, 1)), x, 1);
    x = vset_lane_f16(FLT16_RTPF(vget_lane_f16(x, 2)), x, 2);
    x = vset_lane_f16(FLT16_RTPF(vget_lane_f16(x, 3)), x, 3);
    return  x;
#endif
}

INLINE(Vdwf,VDWF_RTPF) (Vdwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f32(x);
#else
    x = vset_lane_f32(FLT_RTPF(vget_lane_f32(x, 0)), x, 0);
    x = vset_lane_f32(FLT_RTPF(vget_lane_f32(x, 1)), x, 1);
    return  x;
#endif
}

INLINE(Vddf,VDDF_RTPF) (Vddf x)
{
    return  DBL_ASTV(DBL_RTPF(vget_lane_f64(x, 0)));
}


INLINE(Vqhf,VQHF_RTPF) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f16(x);
#else
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 0)), x, 0);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 1)), x, 1);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 2)), x, 2);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 3)), x, 3);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 4)), x, 4);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 5)), x, 5);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 6)), x, 6);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 7)), x, 7);
    return  x;
#endif
}

INLINE(Vqwf,VQWF_RTPF) (Vqwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f32(x);
#else
    x = vsetq_lane_f32(FLT_RTPF(vgetq_lane_f32(x, 0)), x, 0);
    x = vsetq_lane_f32(FLT_RTPF(vgetq_lane_f32(x, 1)), x, 1);
    x = vsetq_lane_f32(FLT_RTPF(vgetq_lane_f32(x, 2)), x, 2);
    x = vsetq_lane_f32(FLT_RTPF(vgetq_lane_f32(x, 3)), x, 3);
    return  x;
#endif
}

INLINE(Vqdf,VQDF_RTPF) (Vqdf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f64(x);
#else
    x = vsetq_lane_f64(DBL_RTPF(vgetq_lane_f64(x, 0)), x, 0);
    x = vsetq_lane_f64(DBL_RTPF(vgetq_lane_f64(x, 1)), x, 1);
    return  x;
#endif
}

#if 0 // _LEAVE_ARM_RTPF
}
#endif


#if 0 // _ENTER_ARM_CLTR
{
#endif

INLINE(flt16_t, FLT16_CLTR) (flt16_t a, flt16_t b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmaxh_f16(a, b);
#else
    return a < b ? b : a;
#endif
}

INLINE(  float,   FLT_CLTR)   (float a,   float b) {return a < b ? b : a;}
INLINE( double,   DBL_CLTR)  (double a,  double b) {return a < b ? b : a;}

INLINE(Vwyu,VWYU_CLTR) (Vwyu a, Vwyu b)
{
#define     VWYU_CLTR(A, B) \
WYU_ASTV(((VWWU_ASTV(VWYU_ASWU(A)))|(VWWU_ASTV(VWYU_ASWU(B)))))
    return  VWYU_CLTR(a, b);
}

INLINE(Vwbu,VWBU_CLTR) (Vwbu a, Vwbu b) 
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    float       bm = VWBU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    az = vmax_u8(az, bz);
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_CLTR) (Vwbi a, Vwbi b) 
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    float       bm = VWBI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    az = vmax_s8(az, bz);
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_CLTR) (Vwbc a, Vwbc b) 
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_CLTR(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return  VWBU_ASBC(VWBU_CLTR(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_CLTR) (Vwhu a, Vwhu b) 
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    float       bm = VWHU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    az = vmax_u16(az, bz);
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_CLTR) (Vwhi a, Vwhi b) 
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    float       bm = VWHI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    az = vmax_s16(az, bz);
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}

INLINE(Vwhf,VWHF_CLTR) (Vwhf a, Vwhf b) 
{
    float       am = VWHF_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    float16x4_t az = vreinterpret_f16_f32(af);
    float       bm = VWHF_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    float16x4_t bz = vreinterpret_f16_f32(bf);
#if defined(SPC_ARM_FP16_SIMD)
    az = vmax_f16(az, bz);
#else
    float32x4_t aq = vcvt_f32_f16(az);
    float32x4_t bq = vcvt_f32_f16(bz);
    aq = vmaxq_f32(aq, bq);
    az = vcvt_f16_f32(aq);
#endif
    af = vreinterpret_f32_f16(az);
    am = vget_lane_f32(af, 0);
    return  WHF_ASTV(am);
}


INLINE(Vwwu,VWWU_CLTR) (Vwwu a, Vwwu b) 
{
    float       am = VWWU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint32x2_t  az = vreinterpret_u32_f32(af);
    float       bm = VWWU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint32x2_t  bz = vreinterpret_u32_f32(bf);
    az = vmax_u32(az, bz);
    af = vreinterpret_f32_u32(az);
    am = vget_lane_f32(af, 0);
    return  WWU_ASTV(am);
}

INLINE(Vwwi,VWWI_CLTR) (Vwwi a, Vwwi b) 
{
    float       am = VWWI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int32x2_t   az = vreinterpret_s32_f32(af);
    float       bm = VWWI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int32x2_t   bz = vreinterpret_s32_f32(bf);
    az = vmax_s32(az, bz);
    af = vreinterpret_f32_s32(az);
    am = vget_lane_f32(af, 0);
    return  WWI_ASTV(am);
}

INLINE(Vwwf,VWWF_CLTR) (Vwwf a, Vwwf b) 
{
    float p = VWWF_ASTM(a);
    float q = VWWF_ASTM(a);
    p = p < q ? q : p;
    return  WWF_ASTV(p);
}


INLINE(Vdyu,VDYU_CLTR) (Vdyu a, Vdyu b)
{
#define     VDYU_CLTR(A, B) DYU_ASTV(vorr_u64(VDYU_ASTM(A), VDYU_ASTM(B)))
    return  VDYU_CLTR(a, b);
}

INLINE(Vdbu,VDBU_CLTR) (Vdbu a, Vdbu b) {return vmax_u8(a, b);}
INLINE(Vdbi,VDBI_CLTR) (Vdbi a, Vdbi b) {return vmax_s8(a, b);}
INLINE(Vdbc,VDBC_CLTR) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_CLTR(A, B) VDBI_ASBC(vmax_s8(VDBC_ASBI(A),VDBC_ASBI(B)))
#else
#   define  VDBC_CLTR(A, B) VDBU_ASBC(vmax_u8(VDBC_ASBU(A),VDBC_ASBU(B)))
#endif
    return  VDBC_CLTR(a, b);
}

INLINE(Vdhu,VDHU_CLTR) (Vdhu a, Vdhu b) {return vmax_u16(a, b);}
INLINE(Vdhi,VDHI_CLTR) (Vdhi a, Vdhi b) {return vmax_s16(a, b);}
INLINE(Vdhf,VDHF_CLTR) (Vdhf a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vmax_f16(a, b);
#else
    return  vcvt_f16_f32(
        vmaxq_f32(
            vcvt_f32_f16(a),
            vcvt_f32_f16(b)
        )
    );
#endif
}

INLINE(Vdwu,VDWU_CLTR) (Vdwu a, Vdwu b) {return vmax_u32(a, b);}
INLINE(Vdwi,VDWI_CLTR) (Vdwi a, Vdwi b) {return vmax_s32(a, b);}
INLINE(Vdwf,VDWF_CLTR) (Vdwf a, Vdwf b) {return vmax_f32(a, b);}

INLINE(Vddu,VDDU_CLTR) (Vddu a, Vddu b)
{
    b = vand_u64(
        veor_u64(a, b),
        vclt_u64(a, b)
    );
    return  veor_u64(a, b);
}

INLINE(Vddi,VDDI_CLTR) (Vddi a, Vddi b)
{
    b = vand_s64(
        veor_s64(a, b),
        vclt_s64(a, b)
    );
    return  veor_s64(a, b);
}

INLINE(Vddf,VDDF_CLTR) (Vddf a, Vddf b) {return vmax_f64(a, b);}


INLINE(Vqyu,VQYU_CLTR) (Vqyu a, Vqyu b)
{
#define     VQYU_CLTR(A, B) QYU_ASTV(vandq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_CLTR(a, b);
}
INLINE(Vqbu,VQBU_CLTR) (Vqbu a, Vqbu b) {return vmaxq_u8(a, b);}
INLINE(Vqbi,VQBI_CLTR) (Vqbi a, Vqbi b) {return vmaxq_s8(a, b);}
INLINE(Vqbc,VQBC_CLTR) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_CLTR(A, B) VQBI_ASBC(vmaxq_s8(VQBC_ASBI(A),VQBC_ASBI(B)))
#else
#   define  VQBC_CLTR(A, B) VQBU_ASBC(vmaxq_u8(VQBC_ASBU(A),VQBC_ASBU(B)))
#endif
    return  VQBC_CLTR(a, b);
}

INLINE(Vqhu,VQHU_CLTR) (Vqhu a, Vqhu b) {return vmaxq_u16(a, b);}
INLINE(Vqhi,VQHI_CLTR) (Vqhi a, Vqhi b) {return vmaxq_s16(a, b);}
INLINE(Vqhf,VQHF_CLTR) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmaxq_f16(a, b);
#else
    float32x4_t al = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t bl = vcvt_f32_f16(vget_low_f16(b));
    float32x4_t ar = vcvt_f32_f16(vget_high_f16(a));
    float32x4_t br = vcvt_f32_f16(vget_high_f16(b));
    return  vcombine_f16(
        vcvt_f16_f32(vmaxq_f32(al, bl)),
        vcvt_f16_f32(vmaxq_f32(ar, br))
    );
#endif
}

INLINE(Vqwu,VQWU_CLTR) (Vqwu a, Vqwu b) {return vmaxq_u32(a, b);}
INLINE(Vqwi,VQWI_CLTR) (Vqwi a, Vqwi b) {return vmaxq_s32(a, b);}
INLINE(Vqwf,VQWF_CLTR) (Vqwf a, Vqwf b) {return vmaxq_f32(a, b);}

INLINE(Vqdu,VQDU_CLTR) (Vqdu a, Vqdu b)
{
/*  TODO: verify that this is superior to simply performing
    two VDDU_CLTR ops
*/
    b = vandq_u64(
        veorq_u64(a, b),
        vcltq_u64(a, b)
    );
    return veorq_u64(a, b);
}

INLINE(Vqdi,VQDI_CLTR) (Vqdi a, Vqdi b)
{
// use a = vandq_s64.. for CGTR
    b = vandq_s64(
        veorq_s64(a, b),
        vcltq_s64(a, b)
    );
    return  veorq_s64(a, b);
}

INLINE(Vqdf,VQDF_CLTR) (Vqdf a, Vqdf b) {return vmaxq_f64(a, b);}

#if 0 // _LEAVE_ARM_CLTR
}
#endif

#if 0 // _ENTER_ARM_CGTR
{
#endif

INLINE(flt16_t, FLT16_CGTR) (flt16_t a, flt16_t b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vminh_f16(a, b);
#else
    return a > b ? b : a;
#endif
}

INLINE(  float,   FLT_CGTR)   (float a,   float b) {return a > b ? b : a;}
INLINE( double,   DBL_CGTR)  (double a,  double b) {return a > b ? b : a;}
INLINE(Vwyu,VWYU_CGTR) (Vwyu a, Vwyu b)
{
#define     VWYU_CGTR(A, B) \
WYU_ASTV(((VWWU_ASTV(VWYU_ASWU(A)))&(VWWU_ASTV(VWYU_ASWU(B)))))
    return  VWYU_CGTR(a, b);
}

INLINE(Vwbu,VWBU_CGTR) (Vwbu a, Vwbu b) 
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    float       bm = VWBU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    az = vmin_u8(az, bz);
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_CGTR) (Vwbi a, Vwbi b) 
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    float       bm = VWBI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    az = vmin_s8(az, bz);
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_CGTR) (Vwbc a, Vwbc b) 
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_CGTR(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return  VWBU_ASBC(VWBU_CGTR(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_CGTR) (Vwhu a, Vwhu b) 
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    float       bm = VWHU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    az = vmin_u16(az, bz);
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_CGTR) (Vwhi a, Vwhi b) 
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    float       bm = VWHI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    az = vmin_s16(az, bz);
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}

INLINE(Vwhf,VWHF_CGTR) (Vwhf a, Vwhf b) 
{
    float       am = VWHF_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    float16x4_t az = vreinterpret_f16_f32(af);
    float       bm = VWHF_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    float16x4_t bz = vreinterpret_f16_f32(bf);
#if defined(SPC_ARM_FP16_SIMD)
    az = vmin_f16(az, bz);
#else
    float32x4_t aq = vcvt_f32_f16(az);
    float32x4_t bq = vcvt_f32_f16(bz);
    aq = vminq_f32(aq, bq);
    az = vcvt_f16_f32(aq);
#endif
    af = vreinterpret_f32_f16(az);
    am = vget_lane_f32(af, 0);
    return  WHF_ASTV(am);
}


INLINE(Vwwu,VWWU_CGTR) (Vwwu a, Vwwu b) 
{
    float       am = VWWU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint32x2_t  az = vreinterpret_u32_f32(af);
    float       bm = VWWU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint32x2_t  bz = vreinterpret_u32_f32(bf);
    az = vmin_u32(az, bz);
    af = vreinterpret_f32_u32(az);
    am = vget_lane_f32(af, 0);
    return  WWU_ASTV(am);
}

INLINE(Vwwi,VWWI_CGTR) (Vwwi a, Vwwi b) 
{
    float       am = VWWI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int32x2_t   az = vreinterpret_s32_f32(af);
    float       bm = VWWI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int32x2_t   bz = vreinterpret_s32_f32(bf);
    az = vmin_s32(az, bz);
    af = vreinterpret_f32_s32(az);
    am = vget_lane_f32(af, 0);
    return  WWI_ASTV(am);
}

INLINE(Vwwf,VWWF_CGTR) (Vwwf a, Vwwf b) 
{
    return  WWF_ASTV(FLT_CGTR(VWWF_ASTM(a), VWWF_ASTM(b)));
}

INLINE(Vdyu,VDYU_CGTR) (Vdyu a, Vdyu b)
{
#define     VDYU_CGTR(A, B) DYU_ASTV(vand_u64(VDYU_ASTM(A), VDYU_ASTM(B)))
    return  VDYU_CGTR(a, b);
}

INLINE(Vdbu,VDBU_CGTR) (Vdbu a, Vdbu b) {return vmin_u8(a, b);}
INLINE(Vdbi,VDBI_CGTR) (Vdbi a, Vdbi b) {return vmin_s8(a, b);}
INLINE(Vdbc,VDBC_CGTR) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_CGTR(A, B) VDBI_ASBC(vmin_s8(VDBC_ASBI(A),VDBC_ASBI(B)))
#else
#   define  VDBC_CGTR(A, B) VDBU_ASBC(vmin_u8(VDBC_ASBU(A),VDBC_ASBU(B)))
#endif
    return  VDBC_CGTR(a, b);
}

INLINE(Vdhu,VDHU_CGTR) (Vdhu a, Vdhu b) {return vmin_u16(a, b);}
INLINE(Vdhi,VDHI_CGTR) (Vdhi a, Vdhi b) {return vmin_s16(a, b);}
INLINE(Vdhf,VDHF_CGTR) (Vdhf a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vmin_f16(a, b);
#else
    return  vcvt_f16_f32(
        vminq_f32(
            vcvt_f32_f16(a),
            vcvt_f32_f16(b)
        )
    );
#endif
}

INLINE(Vdwu,VDWU_CGTR) (Vdwu a, Vdwu b) {return vmin_u32(a, b);}
INLINE(Vdwi,VDWI_CGTR) (Vdwi a, Vdwi b) {return vmin_s32(a, b);}
INLINE(Vdwf,VDWF_CGTR) (Vdwf a, Vdwf b) {return vmin_f32(a, b);}

INLINE(Vddu,VDDU_CGTR) (Vddu a, Vddu b)
{
    a = vand_u64(
        veor_u64(a, b),
        vclt_u64(a, b)
    );
    return  veor_u64(a, b);
}

INLINE(Vddi,VDDI_CGTR) (Vddi a, Vddi b)
{
    a = vand_s64(
        veor_s64(a, b),
        vclt_s64(a, b)
    );
    return  veor_s64(a, b);
}

INLINE(Vddf,VDDF_CGTR) (Vddf a, Vddf b) {return vmin_f64(a, b);}


INLINE(Vqyu,VQYU_CGTR) (Vqyu a, Vqyu b)
{
#define     VQYU_CGTR(A, B) QYU_ASTV(vandq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_CGTR(a, b);
}
INLINE(Vqbu,VQBU_CGTR) (Vqbu a, Vqbu b) {return vminq_u8(a, b);}
INLINE(Vqbi,VQBI_CGTR) (Vqbi a, Vqbi b) {return vminq_s8(a, b);}
INLINE(Vqbc,VQBC_CGTR) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_CGTR(A, B) VQBI_ASBC(vminq_s8(VQBC_ASBI(A),VQBC_ASBI(B)))
#else
#   define  VQBC_CGTR(A, B) VQBU_ASBC(vminq_u8(VQBC_ASBU(A),VQBC_ASBU(B)))
#endif
    return  VQBC_CGTR(a, b);
}

INLINE(Vqhu,VQHU_CGTR) (Vqhu a, Vqhu b) {return vminq_u16(a, b);}
INLINE(Vqhi,VQHI_CGTR) (Vqhi a, Vqhi b) {return vminq_s16(a, b);}
INLINE(Vqhf,VQHF_CGTR) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vminq_f16(a, b);
#else
    float32x4_t al = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t bl = vcvt_f32_f16(vget_low_f16(b));
    float32x4_t ar = vcvt_f32_f16(vget_high_f16(a));
    float32x4_t br = vcvt_f32_f16(vget_high_f16(b));
    return  vcombine_f16(
        vcvt_f16_f32(vminq_f32(al, bl)),
        vcvt_f16_f32(vminq_f32(ar, br))
    );
#endif
}

INLINE(Vqwu,VQWU_CGTR) (Vqwu a, Vqwu b) {return vminq_u32(a, b);}
INLINE(Vqwi,VQWI_CGTR) (Vqwi a, Vqwi b) {return vminq_s32(a, b);}
INLINE(Vqwf,VQWF_CGTR) (Vqwf a, Vqwf b) {return vminq_f32(a, b);}

INLINE(Vqdu,VQDU_CGTR) (Vqdu a, Vqdu b)
{
/*  TODO: verify that this is superior to simply performing
    two VDDU_CGTR ops
*/
    a = vandq_u64(
        veorq_u64(a, b),
        vcltq_u64(a, b)
    );
    return veorq_u64(a, b);
}

INLINE(Vqdi,VQDI_CGTR) (Vqdi a, Vqdi b)
{
// use b = vandq_s64.. for CLTR
    a = vandq_s64(
        veorq_s64(a, b),
        vcltq_s64(a, b)
    );
    return  veorq_s64(a, b);
}

INLINE(Vqdf,VQDF_CGTR) (Vqdf a, Vqdf b) {return vminq_f64(a, b);}

#if 0 // _LEAVE_ARM_CGTR
}
#endif

#if 0 // _ENTER_ARM_MAXV
{
#endif

INLINE(   _Bool,VWYU_MAXV) (Vwyu a)
{
    return  VWWU_ASTV(VWYU_ASWU(a)) != 0;
}


INLINE( uint8_t,VWBU_MAXV) (Vwbu a) 
{
    float32x2_t c = vdup_n_f32(VWBU_ASTM(a));
    uint8x8_t   b = vreinterpret_u8_f32(c);
    return  vmaxv_u8(b);
}

INLINE(  int8_t,VWBI_MAXV) (Vwbi a) 
{
    float32x2_t c = vdup_n_f32(VWBI_ASTM(a));
    int8x8_t    b = vreinterpret_s8_f32(c);
    return  vmaxv_s8(b);
}

INLINE(    char,VWBC_MAXV) (Vwbc a) 
{
#if CHAR_MAX
    return  VWBI_MAXV(VWBC_ASBI(a));
#else
    return  VWBU_MAXV(VWBC_ASBU(a));
#endif
}


INLINE(uint16_t,VWHU_MAXV) (Vwhu a) 
{
    float32x2_t c = vdup_n_f32(VWHU_ASTM(a));
    uint16x4_t  b = vreinterpret_u16_f32(c);
    return  vmaxv_u16(b);
}

INLINE( int16_t,VWHI_MAXV) (Vwhi a) 
{
    float32x2_t c = vdup_n_f32(VWHI_ASTM(a));
    int16x4_t   b = vreinterpret_s16_f32(c);
    return  vmaxv_s16(b);
}

INLINE( flt16_t,VWHF_MAXV) (Vwhf a) 
{
    float32x2_t c = vdup_n_f32(VWHF_ASTM(a));
    float16x4_t b = vreinterpret_f16_f32(c);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t b = vreinterpret_f16_f32(c);
    return  vmaxv_f16(b);
#else
    float32x4_t v = vcvt_f32_f16(b);
    return  vmaxvq_f32(v);
#endif
}


INLINE(   _Bool,VDYU_MAXV) (Vdyu a)
{
#define     VDYU_MAXV(A)        \
(                               \
    (_Bool)                     \
    vtstd_u64(                  \
        VDDU_ASTV(VDYU_ASDU(A)),\
        vdup_n_u64(UINT64_MAX)  \
    )                           \
)
    return  VDYU_MAXV(a);
}

INLINE( uint8_t,VDBU_MAXV) (Vdbu a) {return vmaxv_u8(a);}
INLINE(  int8_t,VDBI_MAXV) (Vdbi a) {return vmaxv_s8(a);}
INLINE(    char,VDBC_MAXV) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_MAXV(A) vmaxv_s8(VDBC_ASBI(A))
#else
#   define  VDBC_MAXV(A) vmaxv_u8(VDBC_ASBU(A))
#endif
    return  VDBC_MAXV(a);
}
INLINE( uint16_t,VDHU_MAXV) (Vdhu a) {return vmaxv_u16(a);}
INLINE(  int16_t,VDHI_MAXV) (Vdhi a) {return vmaxv_s16(a);}
INLINE(  flt16_t,VDHF_MAXV) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
// no such thing as vmaxv_f16
#endif
    return  vmaxvq_f32(vcvt_f32_f16(a));
}

INLINE(uint32_t,VDWU_MAXV) (Vdwu a) {return vmaxv_u32(a);}
INLINE( int32_t,VDWI_MAXV) (Vdwi a) {return vmaxv_s32(a);}
INLINE(   float,VDWF_MAXV) (Vdwf a) {return vmaxv_f32(a);}

INLINE(   _Bool,VQYU_MAXV) (Vqyu a)
{
    uint64x2_t  m = VQYU_ASTM(a);
    m = vtstq_u64(m, vdupq_n_u64(UINT64_MAX));
    return  (vgetq_lane_u64(m, 0)|vgetq_lane_u64(m, 1))>>63;
}

INLINE( uint8_t,VQBU_MAXV) (Vqbu a) {return vmaxvq_u8(a);}
INLINE(  int8_t,VQBI_MAXV) (Vqbi a) {return vmaxvq_s8(a);}
INLINE(    char,VQBC_MAXV) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_MAXV(A, B) vmaxvq_s8(VQBC_ASBI(A))
#else
#   define  VQBC_MAXV(A, B) vmaxvq_u8(VQBC_ASBU(A))
#endif
    return  VQBC_MAXV(a, b);
}

INLINE(uint16_t,VQHU_MAXV) (Vqhu a) {return vmaxvq_u16(a);}
INLINE( int16_t,VQHI_MAXV) (Vqhi a) {return vmaxvq_s16(a);}
INLINE( flt16_t,VQHF_MAXV) (Vqhf a)
{

    float32x4_t lo = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t hi = vcvt_f32_f16(vget_high_f16(a));
    lo = vsetq_lane_f32(
        vmaxvq_f32(lo),
        lo,
        0
    );
    lo = vsetq_lane_f32(
        vmaxvq_f32(hi),
        lo,
        0
    );
    return  vmaxv_f32(vget_low_f32(lo));
}

INLINE(uint32_t,VQWU_MAXV) (Vqwu a) {return vmaxvq_u32(a);}
INLINE( int32_t,VQWI_MAXV) (Vqwi a) {return vmaxvq_s32(a);}
INLINE(   float,VQWF_MAXV) (Vqwf a) {return vmaxvq_f32(a);}
INLINE(uint64_t,VQDU_MAXV) (Vqdu a)
{
    return  vget_lane_u64(
        VDDU_CLTR(
            vget_low_u64(a),
            vget_high_u64(a)
        ),
        0
    );
}

INLINE( int64_t,VQDI_MAXV) (Vqdi a)
{
    return  vget_lane_s64(
        VDDI_CLTR(
            vget_low_s64(a),
            vget_high_s64(a)
        ),
        0
    );
}

INLINE(  double,VQDF_MAXV) (Vqdf a) {return vmaxvq_f64(a);}

#if 0 // _LEAVE_ARM_MAXV
}
#endif

#if 0 // _ENTER_ARM_MINV
{
#endif

INLINE(   _Bool,VWYU_MINV) (Vwyu a)
{
    return  VWWU_ASTV(VWYU_ASWU(a)) == UINT32_MAX;
}


INLINE( uint8_t,VWBU_MINV) (Vwbu a) 
{
    float32x2_t c = vdup_n_f32(VWBU_ASTM(a));
    uint8x8_t   b = vreinterpret_u8_f32(c);
    return  vminv_u8(b);
}

INLINE(  int8_t,VWBI_MINV) (Vwbi a) 
{
    float32x2_t c = vdup_n_f32(VWBI_ASTM(a));
    int8x8_t    b = vreinterpret_s8_f32(c);
    return  vminv_s8(b);
}

INLINE(    char,VWBC_MINV) (Vwbc a) 
{
#if CHAR_MIN
    return  VWBI_MINV(VWBC_ASBI(a));
#else
    return  VWBU_MINV(VWBC_ASBU(a));
#endif
}


INLINE(uint16_t,VWHU_MINV) (Vwhu a) 
{
    float32x2_t c = vdup_n_f32(VWHU_ASTM(a));
    uint16x4_t  b = vreinterpret_u16_f32(c);
    return  vminv_u16(b);
}

INLINE( int16_t,VWHI_MINV) (Vwhi a) 
{
    float32x2_t c = vdup_n_f32(VWHI_ASTM(a));
    int16x4_t   b = vreinterpret_s16_f32(c);
    return  vminv_s16(b);
}

INLINE( flt16_t,VWHF_MINV) (Vwhf a) 
{
    float32x2_t c = vdup_n_f32(VWHF_ASTM(a));
    float16x4_t b = vreinterpret_f16_f32(c);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t b = vreinterpret_f16_f32(c);
    return  vminv_f16(b);
#else
    float32x4_t v = vcvt_f32_f16(b);
    return  vminvq_f32(v);
#endif
}


INLINE(   _Bool,VDYU_MINV) (Vdyu a)
{
#define     VDYU_MINV(A) ((_Bool)(UINT64_MAX==vget_lane_u64(VDYU_ASDU(A),0)))
    return  VDYU_MINV(a);
}

INLINE( uint8_t,VDBU_MINV) (Vdbu a) {return vminv_u8(a);}
INLINE(  int8_t,VDBI_MINV) (Vdbi a) {return vminv_s8(a);}
INLINE(    char,VDBC_MINV) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_MINV(A) vminv_s8(VDBC_ASBI(A))
#else
#   define  VDBC_MINV(A) vminv_u8(VDBC_ASBU(A))
#endif
    return  VDBC_MINV(a);
}

INLINE( uint16_t,VDHU_MINV) (Vdhu a) {return vminv_u16(a);}
INLINE(  int16_t,VDHI_MINV) (Vdhi a) {return vminv_s16(a);}
INLINE(  flt16_t,VDHF_MINV) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
// no such thing as vminv_f16
#endif
    return  vminvq_f32(vcvt_f32_f16(a));
}

INLINE(uint32_t,VDWU_MINV) (Vdwu a) {return vminv_u32(a);}
INLINE( int32_t,VDWI_MINV) (Vdwi a) {return vminv_s32(a);}
INLINE(   float,VDWF_MINV) (Vdwf a) {return vminv_f32(a);}

INLINE(   _Bool,VQYU_MINV) (Vqyu a)
{
#define     VQYU_MINV(A) ((_Bool)(128==vaddvq_u8(vcntq_u8(VQYU_ASBU(A)))))
    return  VQYU_MINV(a);
}

INLINE( uint8_t,VQBU_MINV) (Vqbu a) {return vminvq_u8(a);}
INLINE(  int8_t,VQBI_MINV) (Vqbi a) {return vminvq_s8(a);}
INLINE(    char,VQBC_MINV) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_MINV(A, B) vminvq_s8(VQBC_ASBI(A))
#else
#   define  VQBC_MINV(A, B) vminvq_u8(VQBC_ASBU(A))
#endif
    return  VQBC_MINV(a, b);
}

INLINE(uint16_t,VQHU_MINV) (Vqhu a) {return vminvq_u16(a);}
INLINE( int16_t,VQHI_MINV) (Vqhi a) {return vminvq_s16(a);}
INLINE( flt16_t,VQHF_MINV) (Vqhf a)
{

    float32x4_t lo = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t hi = vcvt_f32_f16(vget_high_f16(a));
    lo = vsetq_lane_f32(
        vminvq_f32(lo),
        lo,
        0
    );
    lo = vsetq_lane_f32(
        vminvq_f32(hi),
        lo,
        0
    );
    return  vminv_f32(vget_low_f32(lo));
}

INLINE(uint32_t,VQWU_MINV) (Vqwu a) {return vminvq_u32(a);}
INLINE( int32_t,VQWI_MINV) (Vqwi a) {return vminvq_s32(a);}
INLINE(   float,VQWF_MINV) (Vqwf a) {return vminvq_f32(a);}
INLINE(uint64_t,VQDU_MINV) (Vqdu a)
{
    return  vget_lane_u64(
        VDDU_CGTR(
            vget_low_u64(a),
            vget_high_u64(a)
        ),
        0
    );
}

INLINE( int64_t,VQDI_MINV) (Vqdi a)
{
    return  vget_lane_s64(
        VDDI_CGTR(
            vget_low_s64(a),
            vget_high_s64(a)
        ),
        0
    );
}

INLINE(  double,VQDF_MINV) (Vqdf a) {return vminvq_f64(a);}

#if 0 // _LEAVE_ARM_MINV
}
#endif

#if 0 // _ENTER_ARM_CNT1
{
#endif

INLINE(ptrdiff_t,ADDR_CNT1) (void const *x)
{
    uintptr_t p = (uintptr_t) p;
    return  __builtin_popcountll((UINTPTR_MAX&p));
}

INLINE( uchar, UCHAR_CNT1) (unsigned x) 
{
    return  __builtin_popcount((UCHAR_MAX&x));
}

INLINE( schar, SCHAR_CNT1)   (signed x) 
{
    return  __builtin_popcount((UCHAR_MAX&x));
}

INLINE(  char,  CHAR_CNT1)      (int x) 
{
    return  __builtin_popcount((UCHAR_MAX&x));
}

INLINE(ushort, USHRT_CNT1) (unsigned x) 
{
    return __builtin_popcount((USHRT_MAX&x));
}

INLINE( short,  SHRT_CNT1)   (signed x) 
{
    return __builtin_popcount((USHRT_MAX&x));
}

INLINE(  uint,  UINT_CNT1)     (uint x) {return __builtin_popcount(x);}
INLINE(   int,   INT_CNT1)      (int x) {return __builtin_popcount(x);}
INLINE( ulong, ULONG_CNT1)    (ulong x) {return __builtin_popcountl(x);}
INLINE(  long,  LONG_CNT1)     (long x) {return __builtin_popcountl(x);}
INLINE(ullong,ULLONG_CNT1)   (ullong x) {return __builtin_popcountll(x);}
INLINE( llong, LLONG_CNT1)    (llong x) {return __builtin_popcountll(x);}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,cnt1qu) (QUAD_UTYPE x) 
{
    QUAD_TYPE   c, z={.U=x};
    c.Lo.U=__builtin_popcountll(z.Lo.U)+__builtin_popcountll(z.Hi.U);
    return  c.U;
}

INLINE(QUAD_ITYPE,cnt1qi) (QUAD_ITYPE x) 
{
    QUAD_TYPE   c, z={.I=x};
    c.Lo.U=__builtin_popcountll(z.Lo.U)+__builtin_popcountll(z.Hi.U);
    return  c.I;
}

#endif

#define     DBU_CNT1 vcnt_u8
#define     DBI_CNT1 vcnt_s8
#if CHAR_MIN 
#   define  DBC_CNT1 DBI_CNT1
#else
#   define  DBC_CNT1 DBI_CNT1
#endif

INLINE(uint16x4_t,DHU_CNT1) (uint16x4_t x)
{
    DWRD_VTYPE c = {.H.U=x};
    c.B.U = vcnt_u8(c.B.U);
    return  vpaddl_u8(c.B.U);
}

INLINE(int16x4_t,DHI_CNT1) (int16x4_t x)
{
    DWRD_VTYPE c = {.H.I=x};
    c.H.U = DHU_CNT1(c.H.U);
    return  c.H.I;
}

INLINE(int16x4_t,DHF_CNT1) (float16x4_t x)
{
    DWRD_VTYPE c = {.H.F=x};
    c.H.U = DHU_CNT1(c.H.U);
    return  c.H.I;
}

INLINE(uint32x2_t, DWU_CNT1) (uint32x2_t x)
{
    DWRD_VTYPE c = {.W.U=x};
    c.B.U = vcnt_u8(c.B.U);
    c.H.U = vpaddl_u8(c.B.U);
    return  vpaddl_u16(c.H.U);
}

INLINE(int32x2_t, DWI_CNT1) (int32x2_t x)
{
    DWRD_VTYPE c = {.W.I=x};
    c.W.U = DWU_CNT1(c.W.U);
    return  c.W.I;
}

INLINE(int32x2_t, DWF_CNT1) (float32x2_t x)
{
    DWRD_VTYPE c = {.W.F=x};
    c.W.U = DWU_CNT1(c.W.U);
    return  c.W.I;
}

INLINE(uint64x1_t, DDU_CNT1) (uint64x1_t x)
{
    DWRD_VTYPE c = {.D.U=x};
    c.U = vaddv_u8(c.B.U);
    return  c.D.U;
}

INLINE(int64x1_t, DDI_CNT1) (int64x1_t x)
{
    DWRD_VTYPE c = {.D.I=x};
    c.D.U = DDU_CNT1(c.D.U);
    return  c.D.I;
}

INLINE(float64x1_t, DDF_CNT1) (float64x1_t x)
{
    DWRD_VTYPE c = {.D.F=x};
    c.D.U = DDU_CNT1(c.D.U);
    return  c.D.I;
}

#define     QBU_CNT1 vcntq_u8
#define     QBI_CNT1 vcntq_s8
#if CHAR_MIN 
#   define  QBC_CNT1 QBI_CNT1
#else
#   define  QBC_CNT1 QBI_CNT1
#endif

INLINE(uint16x8_t,QHU_CNT1)  (uint16x8_t x)
{
    QUAD_VTYPE c = {.H.U=x};
    c.B.U = vcntq_u8(c.B.U);
    return  vpaddlq_u8(c.B.U);
}

INLINE( int16x8_t,QHI_CNT1)   (int16x8_t x)
{
    QUAD_VTYPE c = {.H.I=x};
    c.H.U = QHU_CNT1(c.H.U);
    return  c.H.I;
}

INLINE( int16x8_t,QHF_CNT1) (float16x8_t x)
{
    QUAD_VTYPE c = {.H.F=x};
    c.H.U = QHU_CNT1(c.H.U);
    return  c.H.I;
}

INLINE(uint32x4_t,QWU_CNT1)  (uint32x4_t x)
{
    QUAD_VTYPE c = {.W.U=x};
    c.B.U = vcntq_u8(c.B.U);
    c.H.U = vpaddlq_u8(c.B.U);
    return  vpaddlq_u16(c.H.U);
}

INLINE( int32x4_t,QWI_CNT1)   (int32x4_t x)
{
    QUAD_VTYPE c = {.W.I=x};
    c.W.U = QWU_CNT1(c.W.U);
    return  c.W.I;
}

INLINE( int32x4_t,QWF_CNT1) (float32x4_t x)
{
    QUAD_VTYPE c = {.W.F=x};
    c.W.U = QWU_CNT1(c.W.U);
    return  c.W.I;
}

INLINE(uint64x2_t,QDU_CNT1)  (uint64x2_t x)
{
    QUAD_VTYPE c = {.D.U=x};
    c.B.U = vcntq_u8(c.B.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    l.U = vaddv_u8(l.D.U);
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    r.U = vaddv_u8(r.D.U);
    return  vcombine_u64(l.D.U, r.D.U);
}

INLINE( int64x2_t,QDI_CNT1)   (int64x2_t x)
{
    QUAD_VTYPE c = {.D.I=x};
    c.D.U = QDU_CNT1(c.D.U);
    return  c.D.I;
}

INLINE( int64x2_t,QDF_CNT1) (float64x2_t x)
{
    QUAD_VTYPE c = {.D.F=x};
    c.D.U = QDU_CNT1(c.D.U);
    return  c.D.I;
}

INLINE(uint64x2_t,QQU_CNT1)  (uint64x2_t x)
{
    QUAD_VTYPE c = {.D.U=x};
    c.B.U = vcntq_u8(c.B.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    l.U = vaddv_u8(l.D.U);
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    r.U = vaddv_u8(r.D.U);
    l.D.U = vadd_u64(l.D.U, r.D.U);
    return vcombine_u64(l.D.U, vdup_n_u64(0));
}

INLINE( int64x2_t,QQI_CNT1)   (int64x2_t x)
{
    QUAD_VTYPE c = {.D.I=x};
    c.D.U = QQU_CNT1(c.D.U);
    return  c.D.I;
}

INLINE( int64x2_t,QQF_CNT1) (QUAD_FTYPE x)
{
    QUAD_VTYPE c = {.F=x};
    c.D.U = QQU_CNT1(c.D.U);
    return  c.D.I;
}

INLINE(int16_t,FLT16_CNT1) (flt16_t x)
{
    DWRD_VTYPE c = {.H.F={x}};
    c.H.U = DHU_CNT1(c.H.U);
    return  vget_lane_s16(c.H.I, 0);
}

INLINE(int32_t, FLT_CNT1) (float x)
{
    DWRD_VTYPE c = {.W.F={x}};
    c.W.U = DWU_CNT1(c.W.U);
    return  vget_lane_s32(c.W.I, 0);
}

INLINE(int64_t, DBL_CNT1) (double x)
{
    DWRD_VTYPE c = {.F=x};
    c.D.U = DDU_CNT1(c.D.U);
    return  c.I;
}

INLINE(Vwbu,VWBU_CNT1) (Vwbu x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.B.U = DBU_CNT1(c.B.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwbi,VWBI_CNT1) (Vwbi x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.B.U = DBU_CNT1(c.B.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwbc,VWBC_CNT1) (Vwbc x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.B.U = DBU_CNT1(c.B.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwhu,VWHU_CNT1) (Vwhu x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.H.U = DHU_CNT1(c.H.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwhi,VWHI_CNT1) (Vwhi x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.H.U = DHU_CNT1(c.H.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwhi,VWHF_CNT1) (Vwhf x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    Vwhi n;
    c.H.U = DHU_CNT1(c.H.U);
    n.V0 = vget_lane_f32(c.W.F, 0);
    return  n;
}

INLINE(Vwwu,VWWU_CNT1) (Vwwu x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.W.U = DWU_CNT1(c.W.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwwi,VWWI_CNT1) (Vwwi x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.W.U = DWU_CNT1(c.W.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwwi,VWWF_CNT1) (Vwwf x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    Vwwi n;
    c.H.U = DWU_CNT1(c.W.U);
    n.V0 = vget_lane_f32(c.W.F, 0);
    return  n;
}

INLINE(Vdbu,VDBU_CNT1) (Vdbu x) {return DBU_CNT1(x);}
INLINE(Vdbi,VDBI_CNT1) (Vdbi x) {return DBI_CNT1(x);}
INLINE(Vdbc,VDBC_CNT1) (Vdbc x) {x.V0 = DBC_CNT1(x.V0); return x;}
INLINE(Vdhu,VDHU_CNT1) (Vdhu x) {return DHU_CNT1(x);}
INLINE(Vdhi,VDHI_CNT1) (Vdhi x) {return DHI_CNT1(x);}
INLINE(Vdhi,VDHF_CNT1) (Vdhf x) {return DHF_CNT1(x);}
INLINE(Vdwu,VDWU_CNT1) (Vdwu x) {return DWU_CNT1(x);}
INLINE(Vdwi,VDWI_CNT1) (Vdwi x) {return DWI_CNT1(x);}
INLINE(Vdwi,VDWF_CNT1) (Vdwf x) {return DWF_CNT1(x);}
INLINE(Vddu,VDDU_CNT1) (Vddu x) {return DDU_CNT1(x);}
INLINE(Vddi,VDDI_CNT1) (Vddi x) {return DDI_CNT1(x);}
INLINE(Vddi,VDDF_CNT1) (Vddf x) {return DDF_CNT1(x);}

INLINE(Vqbu,VQBU_CNT1) (Vqbu x) {return QBU_CNT1(x);}
INLINE(Vqbi,VQBI_CNT1) (Vqbi x) {return QBI_CNT1(x);}
INLINE(Vqbc,VQBC_CNT1) (Vqbc x) {x.V0 = QBC_CNT1(x.V0); return x;}
INLINE(Vqhu,VQHU_CNT1) (Vqhu x) {return QHU_CNT1(x);}
INLINE(Vqhi,VQHI_CNT1) (Vqhi x) {return QHI_CNT1(x);}
INLINE(Vqhi,VQHF_CNT1) (Vqhf x) {return QHF_CNT1(x);}
INLINE(Vqwu,VQWU_CNT1) (Vqwu x) {return QWU_CNT1(x);}
INLINE(Vqwi,VQWI_CNT1) (Vqwi x) {return QWI_CNT1(x);}
INLINE(Vqwi,VQWF_CNT1) (Vqwf x) {return QWF_CNT1(x);}
INLINE(Vqdu,VQDU_CNT1) (Vqdu x) {return QDU_CNT1(x);}
INLINE(Vqdi,VQDI_CNT1) (Vqdi x) {return QDI_CNT1(x);}
INLINE(Vqdi,VQDF_CNT1) (Vqdf x) {return QDF_CNT1(x);}
INLINE(Vqqu,VQQU_CNT1) (Vqqu x) {x.V0 = QQU_CNT1(x.V0); return x;}
INLINE(Vqqi,VQQI_CNT1) (Vqqi x) {x.V0 = QQI_CNT1(x.V0); return x;}
INLINE(Vqqi,VQQF_CNT1) (Vqqf x) {return ((Vqqi){QQF_CNT1(x.V0)});}

#if 0 // _LEAVE_ARM_CNT1
}
#endif

#if 0 // _ENTER_ARM_CNTS
{
#endif

INLINE( uchar, UCHAR_CNTS)  (uchar x)
{
#define     UCHAR_CNTS(X)       \
vget_lane_u8(                   \
    vreinterpret_u8_s8(         \
        vcls_u8(vdup_n_u8(X))   \
    ),                          \
    0                           \
)
    return  UCHAR_CNTS(x);
}

INLINE( schar, SCHAR_CNTS)  (schar x)
{
#define     SCHAR_CNTS(X)   \
vget_lane_s8(               \
    vcls_s8(vdup_n_s8(X)),  \
    0                       \
)
    return  SCHAR_CNTS(x);
}

INLINE(  char,  CHAR_CNTS)   (char x)
{
#if CHAR_MIN
    return  SCHAR_CNTS(x);
#else
    return  UCHAR_CNTS(x);
#endif
}

INLINE(ushort, USHRT_CNTS) (ushort x)
{
#define     USHRT_CNTS(X)       \
vget_lane_u16(                  \
    vreinterpret_u16_s16(       \
        vcls_u16(vdup_n_u16(X)) \
    ),                          \
    0                           \
)
    return  USHRT_CNTS(x);
}

INLINE(short, SHRT_CNTS) (short x)
{
#define     SHRT_CNTS(X)    \
vget_lane_s16(              \
    vcls_s16(vdup_n_s16(X)),\
    0                       \
)
    return  SHRT_CNTS(x);
}

INLINE(  uint,  UINT_CNTS)   (uint x) {return __cls(x);}
INLINE(   int,   INT_CNTS)    (int x) {return __cls(x);}
INLINE( ulong, ULONG_CNTS)  (ulong x) {return __clsl(x);}
INLINE(  long,  LONG_CNTS)   (long x) {return __clsl(x);}
INLINE(ullong,ULLONG_CNTS) (ullong x) {return __clsll(x);}
INLINE( llong, LLONG_CNTS)  (llong x) {return __clsll(x);}

INLINE(Vwbu,VWBU_CNTS) (Vwbu x)
{
    float32x2_t v = vdup_n_f32(VWBU_ASTM(x));
    int8x8_t    n = vcls_u8(vreinterpret_u8_f32(v));
    v = vreinterpret_f32_s8(n);
    return  WBU_ASTV(vget_lane_f32(v, 0));
}

INLINE(Vwbi,VWBI_CNTS) (Vwbi x)
{
    float32x2_t v = vdup_n_f32(VWBI_ASTM(x));
    int8x8_t    n = vcls_s8(vreinterpret_s8_f32(v));
    v = vreinterpret_f32_s8(n);
    return  WBI_ASTV(vget_lane_f32(v, 0));
}

INLINE(Vwbc,VWBC_CNTS) (Vwbc x)
{
#if CHAR_MIN
#   define  VWBC_CNTS(X)    VWBI_ASBC(VWBI_CNTS(VWBC_ASBI(X)))
#else
#   define  VWBC_CNTS(X)    VWBU_ASBC(VWBU_CNTS(VWBC_ASBU(X)))
#endif
    return  VWBC_CNTS(x);
}


INLINE(Vwhu,VWHU_CNTS) (Vwhu x)
{
    float32x2_t v = vdup_n_f32(VWHU_ASTM(x));
    int16x4_t   n = vcls_u16(vreinterpret_u16_f32(v));
    v = vreinterpret_f32_s16(n);
    return  WHU_ASTV(vget_lane_f32(v, 0));
}

INLINE(Vwhi,VWHI_CNTS) (Vwhi x)
{
    float32x2_t v = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   n = vcls_s16(vreinterpret_s16_f32(v));
    v = vreinterpret_f32_s16(n);
    return  WHI_ASTV(vget_lane_f32(v, 0));
}


INLINE(Vwwu,VWWU_CNTS) (Vwwu x)
{
#define     VWWU_CNTS(X)    UINT_ASTV(__cls(VWWU_ASTV(X)))
    return  VWWU_CNTS(x);
}

INLINE(Vwwi,VWWI_CNTS) (Vwwi x)
{
#define     VWWI_CNTS(X)    INT_ASTV(__cls(VWWI_ASTV(X)))
    return  VWWI_CNTS(x);
}


INLINE(Vdbu,VDBU_CNTS) (Vdbu x) {return vreinterpret_u8_s8(vcls_u8(x));}
INLINE(Vdbi,VDBI_CNTS) (Vdbi x) {return vcls_s8(x);}
INLINE(Vdbc,VDBC_CNTS) (Vdbc x)
{
#if CHAR_MIN
#   define  VDBC_CNTS(X)    VDBI_ASBC(VDBI_CNTS(VDBC_ASBI(X)))
#else
#   define  VDBC_CNTS(X)    VDBU_ASBC(VDBU_CNTS(VDBC_ASBU(X)))
#endif
    return  VDBC_CNTS(x);
}

INLINE(Vdhu,VDHU_CNTS) (Vdhu x)
{
    return  vreinterpret_u16_s16(vcls_u16(x));
}

INLINE(Vdhi,VDHI_CNTS) (Vdhi x)
{
    return  vcls_s16(x);
}


INLINE(Vdwu,VDWU_CNTS) (Vdwu x)
{
    return  vreinterpret_u32_s32(vcls_u32(x));
}

INLINE(Vdwi,VDWI_CNTS) (Vdwi x)
{
    return  vcls_s32(x);
}



INLINE(Vddu,VDDU_CNTS) (Vddu x)
{
    return  vdup_n_u64(__clsll(vget_lane_u64(x, 0)));
}

INLINE(Vddi,VDDI_CNTS) (Vddi x)
{
    return  vdup_n_s64(__clsll(vget_lane_s64(x, 0)));
}

INLINE(Vqbu,VQBU_CNTS) (Vqbu x) {return vreinterpretq_u8_s8(vclsq_u8(x));}
INLINE(Vqbi,VQBI_CNTS) (Vqbi x) {return vclsq_s8(x);}
INLINE(Vqbc,VQBC_CNTS) (Vqbc x)
{
#if CHAR_MIN
#   define  VQBC_CNTS(X)    VQBI_ASBC(VQBI_CNTS(VQBC_ASBI(X)))
#else
#   define  VQBC_CNTS(X)    VQBU_ASBC(VQBU_CNTS(VQBC_ASBU(X)))
#endif
    return  VQBC_CNTS(x);
}

INLINE(Vqhu,VQHU_CNTS) (Vqhu x)
{
    return  vreinterpretq_u16_s16(vclsq_u16(x));
}

INLINE(Vqhi,VQHI_CNTS) (Vqhi x)
{
    return  vclsq_s16(x);
}


INLINE(Vqwu,VQWU_CNTS) (Vqwu x)
{
    return  vreinterpretq_u32_s32(vclsq_u32(x));
}

INLINE(Vqwi,VQWI_CNTS) (Vqwi x)
{
    return  vclsq_s32(x);
}

INLINE(Vqdu,VQDU_CNTS) (Vqdu x)
{
    return vcombine_u64(
        VDDU_CNTS(vget_low_u64(x)),
        VDDU_CNTS(vget_high_u64(x))
    );
}

INLINE(Vqdi,VQDI_CNTS) (Vqdi x)
{
    return vcombine_s64(
        VDDU_CNTS(vget_low_u64(x)),
        VDDU_CNTS(vget_high_u64(x))
    );
}

#if 0 // _LEAVE_ARM_CNTS
}
#endif

#if 0 // _ENTER_ARM_CSZL
{
#endif

INLINE( uchar, UCHAR_CSZL) (unsigned x) 
{
    return  __builtin_ctz(((UINT_MAX&~UCHAR_MAX)|x));
}

INLINE( schar, SCHAR_CSZL)   (signed x) {return UCHAR_CSZL(x);}
INLINE(  char,  CHAR_CSZL)      (int x) {return UCHAR_CSZL(x);}

INLINE(ushort, USHRT_CSZL) (unsigned x)
{
    return  __builtin_ctz(((UINT_MAX&~USHRT_MAX)|x));
}

INLINE( short,  SHRT_CSZL)   (signed x) {return USHRT_CSZL(x);}
INLINE(  uint,  UINT_CSZL)     (uint x) {return __builtin_ctz(x);}
INLINE(   int,   INT_CSZL)      (int x) {return UINT_CSZL(x);}
INLINE( ulong, ULONG_CSZL)    (ulong x) {return __builtin_ctz(x);}
INLINE(  long,  LONG_CSZL)     (long x) {return ULONG_CSZL(x);}
INLINE(ullong,ULLONG_CSZL)   (ullong x) {return __builtin_ctzll(x);}
INLINE( llong, LLONG_CSZL)    (llong x) {return ULLONG_CSZL(x);}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,cszlqu) (QUAD_UTYPE x) 
{
    QUAD_TYPE   c, z={.U=x};
    c.Lo.U = UINT64_CSZL(z.Lo.U);
    if (c.Lo.U == 64)
        c.Lo.U += UINT64_CSZL(c.Hi.U);
    return  c.U;
}

INLINE(QUAD_ITYPE,cszlqi) (QUAD_ITYPE x) 
{
    QUAD_TYPE c = {.I=x};
    c.U = cszlqu(c.U);
    return  c.I;
}

#endif

INLINE(ptrdiff_t,ADDR_CSZL) (void const *x)
{
    return  ADDR_U(CSZL)(((uintptr_t) x));
}

INLINE(int16_t,FLT16_CSZL) (flt16_t x)
{
    DWRD_VTYPE c = {.H.F={x}};
    c.B.U = vrbit_u8(c.B.U);
    c.B.U = vrev16_u8(c.B.U);
    c.H.U = vclz_u16(c.H.U);
    return  vget_lane_s16(c.H.I, 0);
}

int32_t FLT_CSZL(float x)
{
    DWRD_VTYPE c = {.W.F={x}};
    c.B.U = vrbit_u8(c.B.U);
    c.B.U = vrev32_u8(c.B.U);
    return  __builtin_clz(c.U);
}

int64_t DBL_CSZL(double x)
{
    DWRD_VTYPE c = {.F=x};
    c.B.U = vrbit_u8(c.B.U);
    c.B.U = vrev64_u8(c.B.U);
    return  __builtin_clzll(c.U);
}

INLINE( uint8x8_t,DBU_CSZL)   (uint8x8_t x)
{
    x = vrbit_u8(x);
    return  vclz_u8(x);
}

INLINE(  int8x8_t,DBI_CSZL)    (int8x8_t x)
{
    x = vrbit_s8(x);
    return  vclz_s8(x);
}

#if CHAR_MIN 
#   define  DBC_CSZL DBI_CSZL
#else
#   define  DBC_CSZL DBI_CSZL
#endif


INLINE(uint16x4_t,DHU_CSZL)  (uint16x4_t x)
{
    DWRD_VTYPE c = {.H.U=x};
    c.B.U = vrbit_u8(c.B.U);
    c.B.U = vrev16_u8(c.B.U);
    return  vclz_u16(c.H.U);
}

INLINE( int16x4_t,DHI_CSZL)   (int16x4_t x)
{
    DWRD_VTYPE c = {.H.I=x};
    c.H.U = DHU_CSZL(c.H.U);
    return  c.H.I;
}

INLINE( int16x4_t,DHF_CSZL) (float16x4_t x)
{
    DWRD_VTYPE c = {.H.F=x};
    c.H.U = DHU_CSZL(c.H.U);
    return  c.H.I;
}

INLINE(uint32x2_t,DWU_CSZL)  (uint32x2_t x)
{
    DWRD_VTYPE c = {.W.U=x};
    c.B.U = vrbit_u8(c.B.U);
    c.B.U = vrev32_u8(c.W.U);
    return  vclz_u32(c.W.U);
}

INLINE( int32x2_t,DWI_CSZL)   (int32x2_t x)
{
    DWRD_VTYPE c = {.W.I=x};
    c.W.U = DWU_CSZL(c.W.U);
    return  c.W.I;
}

INLINE( int32x2_t,DWF_CSZL) (float32x2_t x)
{
    DWRD_VTYPE c = {.W.F=x};
    c.W.U = DWU_CSZL(c.W.U);
    return  c.W.I;
}

INLINE(uint64x1_t,DDU_CSZL)  (uint64x1_t x)
{
    DWRD_VTYPE v = {.D.U=x};
    v.B.U = vrbit_u8(v.B.U);
    v.B.U = vrev64_u8(v.B.U);
    v.U = UINT64_CSZL(v.U);
    return  v.D.U;
}

INLINE( int64x1_t,DDI_CSZL)   (int64x1_t x)
{
    DWRD_VTYPE c = {.D.I=x};
    c.D.U = DDU_CSZL(c.D.I);
    return  c.D.I;
}

INLINE( int64x1_t,DDF_CSZL) (float64x1_t x)
{
    DWRD_VTYPE c = {.D.F=x};
    c.D.U = DDU_CSZL(c.D.I);
    return  c.D.I;
}


INLINE(uint8x16_t,QBU_CSZL)   (uint8x16_t x)
{
    x = vrbitq_u8(x);
    return  vclzq_u8(x);
}

INLINE( int8x16_t,QBI_CSZL)    (int8x16_t x)
{
    x = vrbitq_s8(x);
    return  vclzq_s8(x);
}

#if CHAR_MIN 
#   define  QBC_CSZL QBI_CSZL
#else
#   define  QBC_CSZL QBI_CSZL
#endif


INLINE(uint16x8_t,QHU_CSZL)  (uint16x8_t x)
{
    QUAD_VTYPE c = {.H.U=x};
    c.B.U = vrbitq_u8(c.B.U);
    c.B.U = vrev16q_u8(c.B.U);
    return  vclzq_u16(c.H.U);
}

INLINE( int16x8_t,QHI_CSZL)   (int16x8_t x)
{
    QUAD_VTYPE c = {.H.I=x};
    c.H.U = QHU_CSZL(c.H.U);
    return  c.H.I;
}

INLINE( int16x8_t,QHF_CSZL) (float16x8_t x)
{
    QUAD_VTYPE c = {.H.F=x};
    c.H.U = QHU_CSZL(c.H.U);
    return  c.H.I;
}

INLINE(uint32x4_t,QWU_CSZL)  (uint32x4_t x)
{
    QUAD_VTYPE c = {.W.U=x};
    c.B.U = vrbitq_u8(c.B.U);
    c.B.U = vrev32q_u8(c.B.U);
    return  vclzq_u32(c.W.U);
}

INLINE( int32x4_t,QWI_CSZL)   (int32x4_t x)
{
    QUAD_VTYPE c = {.W.I=x};
    c.W.U = QWU_CSZL(c.W.U);
    return  c.W.I;
}

INLINE( int32x4_t,QWF_CSZL) (float32x4_t x)
{
    QUAD_VTYPE c = {.W.F=x};
    c.W.U = QWU_CSZL(c.W.U);
    return  c.W.I;
}

INLINE(uint64x2_t,QDU_CSZL)  (uint64x2_t x)
{
    QUAD_VTYPE c = {.D.U=x};
    c.B.U = vrbitq_u8(c.B.U);
    c.B.U = vrev64q_u8(c.B.U);
    DWRD_VTYPE l = {.D.U=vget_low_u64(c.D.U)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(c.D.U)};
    l.U = UINT64_CSZL(l.U);
    r.U = UINT64_CSZL(r.U);
    return  vcombine_u64(l.D.U, r.D.U);
}

INLINE( int64x2_t,QDI_CSZL)   (int64x2_t x)
{
    QUAD_VTYPE c = {.D.I=x};
    c.D.U = QDU_CSZL(c.D.I);
    return  c.D.I;
}

INLINE( int64x2_t,QDF_CSZL) (float64x2_t x)
{
    QUAD_VTYPE c = {.D.F=x};
    c.D.U = QDU_CSZL(c.D.I);
    return  c.D.I;
}


INLINE(uint64x2_t,QQU_CSZL)  (uint64x2_t x)
{
    QUAD_VTYPE c = {.D.U=x};
    c.B.U = vrbitq_u8(c.B.U);
    c.B.U = vrev64q_u8(c.B.U);
    DWRD_VTYPE n = {.D.U=vget_high_u64(c.D.U)};
    n.U = n.U
    ?   UINT64_CSZL(n.U)
    :   (64+UINT64_CSZL(vgetq_lane_u64(c.D.U, 0)));
    return  vcombine_u64(n.D.U, vdup_n_u64(0));
}

INLINE( int64x2_t,QQI_CSZL)   (int64x2_t x)
{
    QUAD_VTYPE c = {.D.I=x};
    c.D.U = QQU_CSZL(c.D.U);
    return  c.D.I;
}

INLINE( int64x2_t,QQF_CSZL)   (QUAD_FTYPE x)
{
    QUAD_VTYPE c = {.F=x};
    c.D.U = QQU_CSZL(c.D.U);
    return  c.D.I;
}

INLINE(Vwbu,VWBU_CSZL) (Vwbu x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.B.U = DBU_CSZL(c.B.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwbi,VWBI_CSZL) (Vwbi x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.B.U = DBU_CSZL(c.B.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwbc,VWBC_CSZL) (Vwbc x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.B.U = DBU_CSZL(c.B.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwhu,VWHU_CSZL) (Vwhu x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.H.U = DHU_CSZL(c.H.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwhi,VWHI_CSZL) (Vwhi x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.H.U = DHU_CSZL(c.H.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwhi,VWHF_CSZL) (Vwhf x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    Vwhi n;
    c.H.U = DHU_CSZL(c.H.U);
    n.V0 = vget_lane_f32(c.W.F, 0);
    return  n;
}

INLINE(Vwwu,VWWU_CSZL) (Vwwu x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.W.U = DWU_CSZL(c.W.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwwi,VWWI_CSZL) (Vwwi x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    c.W.U = DWU_CSZL(c.W.U);
    x.V0 = vget_lane_f32(c.W.F, 0);
    return  x;
}

INLINE(Vwwi,VWWF_CSZL) (Vwwf x)
{
    DWRD_VTYPE c = {.W.F={x.V0}};
    Vwwi n;
    c.H.U = DWU_CSZL(c.W.U);
    n.V0 = vget_lane_f32(c.W.F, 0);
    return  n;
}

INLINE(Vdbu,VDBU_CSZL) (Vdbu x) {return DBU_CSZL(x);}
INLINE(Vdbi,VDBI_CSZL) (Vdbi x) {return DBI_CSZL(x);}
INLINE(Vdbc,VDBC_CSZL) (Vdbc x) {x.V0 = DBC_CSZL(x.V0); return x;}
INLINE(Vdhu,VDHU_CSZL) (Vdhu x) {return DHU_CSZL(x);}
INLINE(Vdhi,VDHI_CSZL) (Vdhi x) {return DHI_CSZL(x);}
INLINE(Vdhi,VDHF_CSZL) (Vdhf x) {return DHF_CSZL(x);}
INLINE(Vdwu,VDWU_CSZL) (Vdwu x) {return DWU_CSZL(x);}
INLINE(Vdwi,VDWI_CSZL) (Vdwi x) {return DWI_CSZL(x);}
INLINE(Vdwi,VDWF_CSZL) (Vdwf x) {return DWF_CSZL(x);}
INLINE(Vddu,VDDU_CSZL) (Vddu x) {return DDU_CSZL(x);}
INLINE(Vddi,VDDI_CSZL) (Vddi x) {return DDI_CSZL(x);}
INLINE(Vddi,VDDF_CSZL) (Vddf x) {return DDF_CSZL(x);}

INLINE(Vqbu,VQBU_CSZL) (Vqbu x) {return QBU_CSZL(x);}
INLINE(Vqbi,VQBI_CSZL) (Vqbi x) {return QBI_CSZL(x);}
INLINE(Vqbc,VQBC_CSZL) (Vqbc x) {x.V0 = QBC_CSZL(x.V0); return x;}
INLINE(Vqhu,VQHU_CSZL) (Vqhu x) {return QHU_CSZL(x);}
INLINE(Vqhi,VQHI_CSZL) (Vqhi x) {return QHI_CSZL(x);}
INLINE(Vqhi,VQHF_CSZL) (Vqhf x) {return QHF_CSZL(x);}
INLINE(Vqwu,VQWU_CSZL) (Vqwu x) {return QWU_CSZL(x);}
INLINE(Vqwi,VQWI_CSZL) (Vqwi x) {return QWI_CSZL(x);}
INLINE(Vqwi,VQWF_CSZL) (Vqwf x) {return QWF_CSZL(x);}
INLINE(Vqdu,VQDU_CSZL) (Vqdu x) {return QDU_CSZL(x);}
INLINE(Vqdi,VQDI_CSZL) (Vqdi x) {return QDI_CSZL(x);}
INLINE(Vqdi,VQDF_CSZL) (Vqdf x) {return QDF_CSZL(x);}
INLINE(Vqqu,VQQU_CSZL) (Vqqu x) {x.V0 = QQU_CSZL(x.V0); return x;}
INLINE(Vqqi,VQQI_CSZL) (Vqqi x) {x.V0 = QQI_CSZL(x.V0); return x;}
INLINE(Vqqi,VQQF_CSZL) (Vqqf x) {return ((Vqqi){QQF_CSZL(x.V0)});}

#if 0 // _LEAVE_ARM_CSZL
}
#endif

#if 0 // _ENTER_ARM_CSZR
{
#endif

INLINE( uchar, UCHAR_CSZR) (unsigned x) 
{
    x &= UCHAR_MAX;
    return  __builtin_clz(x)-(UINT_WIDTH-UCHAR_WIDTH);
}

INLINE( schar, SCHAR_CSZR)   (signed x) {return UCHAR_CSZR(x);}
INLINE(  char,  CHAR_CSZR)      (int x) {return UCHAR_CSZR(x);}


INLINE(ushort, USHRT_CSZR) (unsigned x) 
{
    x &= USHRT_MAX;
    return  __builtin_clz(x)-(UINT_WIDTH-USHRT_WIDTH);
}

INLINE( short,  SHRT_CSZR)   (signed x) {return USHRT_CSZR(x);}
INLINE(  uint,  UINT_CSZR)     (uint x) {return __builtin_clz(x);}
INLINE(   int,   INT_CSZR)      (int x) {return __builtin_clz(x);}
INLINE( ulong, ULONG_CSZR)    (ulong x) {return __builtin_clzl(x);}
INLINE(  long,  LONG_CSZR)     (long x) {return __builtin_clzl(x);}
INLINE(ullong,ULLONG_CSZR)   (ullong x) {return __builtin_clzll(x);}
INLINE( llong, LLONG_CSZR)    (llong x) {return __builtin_clzll(x);}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cszrqu) (QUAD_UTYPE x)
{
    QUAD_TYPE v={.U=x}, z;
    z.Hi.U = 0;
    if (v.Hi.U)
        z.Lo.U = __builtin_clzll(v.Hi.U);
    else 
        z.Lo.U = __builtin_clzll(v.Lo.U)+64;
    return  z.U;
}

INLINE(QUAD_ITYPE,cszrqi) (QUAD_ITYPE x)
{
    QUAD_TYPE c = {.I=x};
    c.U = cszrqu(c.U);
    return  c.I;
}

#endif

#define     DBU_CSZR vclz_u8
#define     DBI_CSZR vclz_s8
#if CHAR_MIN
#   define  DBC_CSZR vclz_s8
#else
#   define  DBC_CSZR vclz_u8
#endif
#define     DHU_CSZR vclz_u16
#define     DHI_CSZR vclz_s16

INLINE( int16x4_t,DHF_CSZR) (float16x4_t x)
{
    DWRD_VTYPE c = {.H.F=x};
    c.H.U = DHU_CSZR(c.H.U);
    return  c.H.I;
}

#define     DWU_CSZR vclz_u32
#define     DWI_CSZR vclz_s32

INLINE( int32x2_t,DWF_CSZR) (float32x2_t x)
{
    DWRD_VTYPE c = {.W.F=x};
    c.W.U = DWU_CSZR(c.W.U);
    return  c.W.I;
}

INLINE(uint64x1_t,DDU_CSZR)  (uint64x1_t x)
{
    DWRD_VTYPE c = {.D.U=x};
#if QUAD_NLLONG == 2
    c.U = __builtin_clzll(c.U);
#else
    c.U = __builtin_clzl(c.U);
#endif
    return  c.D.U;
}

INLINE( int64x1_t,DDI_CSZR)   (int64x1_t x)
{
    DWRD_VTYPE c = {.D.I=x};
    c.D.U = DDU_CSZR(c.D.U);
    return  c.D.I;
}

INLINE( int64x1_t,DDF_CSZR) (float64x1_t x)
{
    DWRD_VTYPE c = {.D.F=x};
    c.D.U = DDU_CSZR(c.D.U);
    return  c.D.I;
}

#define     QBU_CSZR vclzq_u8
#define     QBI_CSZR vclzq_s8
#if CHAR_MIN
#   define  QBC_CSZR vclzq_s8
#else
#   define  QBC_CSZR vclzq_u8
#endif
#define     QHU_CSZR vclzq_u16
#define     QHI_CSZR vclzq_s16

INLINE(int16x8_t,QHF_CSZR) (float16x8_t x)
{
    QUAD_VTYPE c = {.H.F=x};
    c.H.U = QHU_CSZR(c.H.U);
    return  c.H.I;
}

#define     QWU_CSZR vclzq_u32
#define     QWI_CSZR vclzq_s32

INLINE(int32x4_t,QWF_CSZR) (float32x4_t x)
{
    QUAD_VTYPE c = {.W.F=x};
    c.W.U = QWU_CSZR(c.W.U);
    return  c.W.I;
}

INLINE(uint64x2_t,QDU_CSZR) (uint64x2_t x)
{
    DWRD_VTYPE l = {.D.U=vget_low_u64(x)};
    DWRD_VTYPE r = {.D.U=vget_high_u64(x)};
#if QUAD_NLLONG == 2
    l.U = __builtin_clzll(l.U);
    r.U = __builtin_clzll(r.U);
#else
    l.U = __builtin_clzl(l.U);
    r.U = __builtin_clzl(r.U);
#endif
    return  vcombine_u64(l.D.U, r.D.U);
}

INLINE(int64x2_t,QDI_CSZR)   (int64x2_t x)
{
    QUAD_VTYPE c = {.D.I=x};
    c.D.U = QDU_CSZR(c.D.U);
    return  c.D.I;
}

INLINE(int64x2_t,QDF_CSZR)   (float64x2_t x)
{
    QUAD_VTYPE c = {.D.F=x};
    c.D.U = QDU_CSZR(c.D.U);
    return  c.D.I;
}

INLINE(uint64x2_t,QQU_CSZR)  (uint64x2_t x)
{
    DWRD_VTYPE l, r={.D.U=vget_high_u64(x)};
#if DWRD_NLONG == 2
    r.U = __builtin_clzll(r.U);
#else
    r.U = __builtin_clzl(r.U);
#endif
    if (r.U == 64)
    {
        l.U = vgetq_lane_u64(x, 0);
#if DWRD_NLONG == 2
        r.U += __builtin_clzll(l.U);
#else
        r.U += __builtin_clzl(l.U);
#endif
    }
    return  vcombine_u64(l.D.U, vdup_n_u64(0));
}

INLINE( int64x2_t,QQI_CSZR)   (int64x2_t x)
{
    QUAD_VTYPE c = {.D.I=x};
    c.D.U = QQU_CSZR(c.D.U);
    return  c.D.I;
}

INLINE( int64x2_t,QQF_CSZR)  (QUAD_FTYPE x)
{
    QUAD_VTYPE c = {.F=x};
    c.D.U = QQU_CSZR(c.D.U);
    return  c.D.I;
}

INLINE(     float,WBU_CSZR)       (float x)
{
    DWRD_VTYPE c = {.W.F={x}};
    c.B.U = DBU_CSZR(c.B.U);
    return  vget_lane_f32(c.W.F, 0);
}

INLINE(     float,WHU_CSZR)       (float x)
{
    DWRD_VTYPE c = {.W.F={x}};
    c.H.U = DHU_CSZR(c.H.U);
    return  vget_lane_f32(c.W.F, 0);
}

INLINE(     float,WWU_CSZR) (float x)
{
    DWRD_VTYPE c = {.W.F={x}};
    c.W.U = DWU_CSZR(c.W.U);
    return  vget_lane_f32(c.W.F, 0);
}

INLINE(Vwbu,VWBU_CSZR) (Vwbu x) {return ((x.V0=WBU_CSZR(x.V0)), x);}
INLINE(Vwbi,VWBI_CSZR) (Vwbi x) {return ((x.V0=WBU_CSZR(x.V0)), x);}
INLINE(Vwbc,VWBC_CSZR) (Vwbc x) {return ((x.V0=WBU_CSZR(x.V0)), x);}

INLINE(Vwhu,VWHU_CSZR) (Vwhu x) {return ((x.V0=WHU_CSZR(x.V0)), x);}
INLINE(Vwhi,VWHI_CSZR) (Vwhi x) {return ((x.V0=WHU_CSZR(x.V0)), x);}
INLINE(Vwhi,VWHF_CSZR) (Vwhf x) {return ((Vwhi){WHU_CSZR(x.V0)});}

INLINE(Vwwu,VWWU_CSZR) (Vwwu x) {return ((x.V0=WWU_CSZR(x.V0)), x);}
INLINE(Vwwi,VWWI_CSZR) (Vwwi x) {return ((x.V0=WWU_CSZR(x.V0)), x);}
INLINE(Vwwi,VWWF_CSZR) (Vwwf x) {return ((Vwwi){WWU_CSZR(x.V0)});}

INLINE(Vdbu,VDBU_CSZR) (Vdbu x) {return DBU_CSZR(x);}
INLINE(Vdbi,VDBI_CSZR) (Vdbu x) {return DBI_CSZR(x);}
INLINE(Vdbc,VDBC_CSZR) (Vdbc x) {x.V0 = DBC_CSZR(x.V0); return x;}
INLINE(Vdhu,VDHU_CSZR) (Vdhu x) {return DHU_CSZR(x);}
INLINE(Vdhi,VDHI_CSZR) (Vdhi x) {return DHI_CSZR(x);}
INLINE(Vdhi,VDHF_CSZR) (Vdhf x) {return DHF_CSZR(x);}
INLINE(Vdwu,VDWU_CSZR) (Vdwu x) {return DWU_CSZR(x);}
INLINE(Vdwi,VDWI_CSZR) (Vdwi x) {return DWI_CSZR(x);}
INLINE(Vdwi,VDWF_CSZR) (Vdwf x) {return DWF_CSZR(x);}
INLINE(Vddu,VDDU_CSZR) (Vddu x) {return DDU_CSZR(x);}
INLINE(Vddi,VDDI_CSZR) (Vddi x) {return DDI_CSZR(x);}
INLINE(Vddi,VDDF_CSZR) (Vddf x) {return DDF_CSZR(x);}

INLINE(Vqbu,VQBU_CSZR) (Vqbu x) {return QBU_CSZR(x);}
INLINE(Vqbi,VQBI_CSZR) (Vqbu x) {return QBI_CSZR(x);}
INLINE(Vqbc,VQBC_CSZR) (Vqbc x) {x.V0 = QBC_CSZR(x.V0); return x;}
INLINE(Vqhu,VQHU_CSZR) (Vqhu x) {return QHU_CSZR(x);}
INLINE(Vqhi,VQHI_CSZR) (Vqhi x) {return QHI_CSZR(x);}
INLINE(Vqhi,VQHF_CSZR) (Vqhf x) {return QHF_CSZR(x);}
INLINE(Vqwu,VQWU_CSZR) (Vqwu x) {return QWU_CSZR(x);}
INLINE(Vqwi,VQWI_CSZR) (Vqwi x) {return QWI_CSZR(x);}
INLINE(Vqwi,VQWF_CSZR) (Vqwf x) {return QWF_CSZR(x);}
INLINE(Vqdu,VQDU_CSZR) (Vqdu x) {return QDU_CSZR(x);}
INLINE(Vqdi,VQDI_CSZR) (Vqdi x) {return QDI_CSZR(x);}
INLINE(Vqdi,VQDF_CSZR) (Vqdf x) {return QDF_CSZR(x);}
INLINE(Vqqu,VQQU_CSZR) (Vqqu x) {x.V0 = QQU_CSZR(x.V0); return x;}
INLINE(Vqqi,VQQI_CSZR) (Vqqi x) {x.V0 = QQI_CSZR(x.V0); return x;}
INLINE(Vqqi,VQQF_CSZR) (Vqqf x) {return ((Vqqi){QQF_CSZR(x.V0)});}

#if 0 // _LEAVE_ARM_CSZR
}
#endif

#if 0 // _ENTER_ARM_DIVN
{
#endif

INLINE(Vwyu,VWYU_DIVN) (Vwyu a, Vwyu b)
{
    float p = VWYU_ASTM(a);
    float q = VWYU_ASTM(b);
    float32x2_t l = vdup_n_f32(p);
    float32x2_t r = vdup_n_f32(q);
    uint32x2_t  x = vreinterpret_u32_f32(l);
    uint32x2_t  y = vreinterpret_u32_f32(r);
    x = vand_u32(x, y);
    l = vreinterpret_f32_u32(x);
    p = vget_lane_f32(l, 0);
    return  WYU_ASTV(p);
}


INLINE(Vwbu,VWBU_DIVN) (Vwbu a, Vwbu b)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t l = VWBU_CVHF(a);
    float16x4_t r = VWBU_CVHF(b);
    l = vdiv_f16(l, r);
    return  VDHF_CVBU(l);
#else
    float32x4_t l = VWBU_CVWF(a);
    float32x4_t r = VWBU_CVWF(b);
    l = vdivq_f32(l, r);
    return  VQWF_CVBU(l);
#endif
}

INLINE(Vwbi,VWBI_DIVN) (Vwbi a, Vwbi b)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t l = VWBI_CVHF(a);
    float16x4_t r = VWBI_CVHF(b);
    l = vdiv_f16(l, r);
    return  VDHF_CVBI(l);
#else
    float32x4_t l = VWBI_CVWF(a);
    float32x4_t r = VWBI_CVWF(b);
    l = vdivq_f32(l, r);
    return  VQWF_CVBI(l);
#endif
}

INLINE(Vwbc,VWBC_DIVN) (Vwbc a, Vwbc b)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t l = VWBC_CVHF(a);
    float16x4_t r = VWBC_CVHF(b);
    l = vdiv_f16(l, r);
    return  VDHF_CVBC(l);
#else
    float32x4_t l = VWBC_CVWF(a);
    float32x4_t r = VWBC_CVWF(b);
    l = vdivq_f32(l, r);
    return  VQWF_CVBC(l);
#endif
}


INLINE(Vwhu,VWHU_DIVN) (Vwhu a, Vwhu b)
{
    float32x2_t l = VWHU_CVWF(a);
    float32x2_t r = VWHU_CVWF(b);
    l = vdiv_f32(l, r);
    return  VDWF_CVHU(l);
}

INLINE(Vwhi,VWHI_DIVN) (Vwhi a, Vwhi b)
{
    float32x2_t l = VWHI_CVWF(a);
    float32x2_t r = VWHI_CVWF(b);
    l = vdiv_f32(l, r);
    return  VDWF_CVHI(l);
}

    
INLINE(Vwwu,VWWU_DIVN) (Vwwu a, Vwwu b)
{
    return  UINT_ASTV( (VWWU_ASTV(a)/VWWU_ASTV(b)) );
}

INLINE(Vwwi,VWWI_DIVN) (Vwwi a, Vwwi b)
{
    return  INT_ASTV( (VWWI_ASTV(a)/VWWI_ASTV(b)) );
}


INLINE(Vdyu,VDYU_DIVN) (Vdyu a, Vdyu b)
{
    return  VDDU_ASYU(vand_u64(VDYU_ASDU(a), VDYU_ASDU(b)));
}


INLINE(Vdbu,VDBU_DIVN) (Vdbu a, Vdbu b)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t l = VDBU_CVHF(a);
    float16x8_t r = VDBU_CVHF(b);
    r = vdivq_f16(l, r);
    return  VQHF_CVBU(r);
#else
    uint8x8_t   c = {0};
    c = vset_lane_u8(vget_lane_u8(a, 0)/vget_lane_u8(b, 0), c, 0);
    c = vset_lane_u8(vget_lane_u8(a, 1)/vget_lane_u8(b, 1), c, 1);
    c = vset_lane_u8(vget_lane_u8(a, 2)/vget_lane_u8(b, 2), c, 2);
    c = vset_lane_u8(vget_lane_u8(a, 3)/vget_lane_u8(b, 3), c, 3);
    c = vset_lane_u8(vget_lane_u8(a, 4)/vget_lane_u8(b, 4), c, 4);
    c = vset_lane_u8(vget_lane_u8(a, 5)/vget_lane_u8(b, 5), c, 5);
    c = vset_lane_u8(vget_lane_u8(a, 6)/vget_lane_u8(b, 6), c, 6);
    c = vset_lane_u8(vget_lane_u8(a, 7)/vget_lane_u8(b, 7), c, 7);
    return c;
#endif
}

INLINE(Vdbi,VDBI_DIVN) (Vdbi a, Vdbi b)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t l = VDBI_CVHF(a);
    float16x8_t r = VDBI_CVHF(b);
    r = vdivq_f16(l, r);
    return  VQHF_CVBI(r);
#else
    int8x8_t    c = {0};
    c = vset_lane_s8(vget_lane_s8(a, 0)/vget_lane_s8(b, 0), c, 0);
    c = vset_lane_s8(vget_lane_s8(a, 1)/vget_lane_s8(b, 1), c, 1);
    c = vset_lane_s8(vget_lane_s8(a, 2)/vget_lane_s8(b, 2), c, 2);
    c = vset_lane_s8(vget_lane_s8(a, 3)/vget_lane_s8(b, 3), c, 3);
    c = vset_lane_s8(vget_lane_s8(a, 4)/vget_lane_s8(b, 4), c, 4);
    c = vset_lane_s8(vget_lane_s8(a, 5)/vget_lane_s8(b, 5), c, 5);
    c = vset_lane_s8(vget_lane_s8(a, 6)/vget_lane_s8(b, 6), c, 6);
    c = vset_lane_s8(vget_lane_s8(a, 7)/vget_lane_s8(b, 7), c, 7);
    return c;
#endif
}

INLINE(Vdbc,VDBC_DIVN) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_DIVN(VDBC_ASBI(a), VDBC_ASBI(b)));
#else
    return  VDBU_ASBC(VDBU_DIVN(VDBC_ASBU(a), VDBC_ASBU(b)));
#endif
}


INLINE(Vdhu,VDHU_DIVN) (Vdhu a, Vdhu b)
{
#if 0
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(a));
    float32x4_t r = vcvtq_f32_u32(vmovl_u16(b));
    r = vdivq_f32(l, r);
    return  vmovn_u32(vcvtq_u32_f32(r));
#else
/*  
Will the compiler be able to keep itself from needlessly
adding an and #0xffff to each operand?
*/
    uint16x4_t  c = {0};
    c = vset_lane_u16(vget_lane_u16(a, 0)/vget_lane_u16(b, 0), c, 0);
    c = vset_lane_u16(vget_lane_u16(a, 1)/vget_lane_u16(b, 1), c, 1);
    c = vset_lane_u16(vget_lane_u16(a, 2)/vget_lane_u16(b, 2), c, 2);
    c = vset_lane_u16(vget_lane_u16(a, 3)/vget_lane_u16(b, 3), c, 3);
    return  c;
#endif
}

INLINE(Vdhi,VDHI_DIVN) (Vdhi a, Vdhi b)
{
#if 0
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(a));
    float32x4_t r = vcvtq_f32_s32(vmovl_s16(b));
    r = vdivq_f32(l, r);
    return  vmovn_s32(vcvtq_s32_f32(r));
#else
    int16x4_t  c = {0};
    c = vset_lane_s16(vget_lane_s16(a, 0)/vget_lane_s16(b, 0), c, 0);
    c = vset_lane_s16(vget_lane_s16(a, 1)/vget_lane_s16(b, 1), c, 1);
    c = vset_lane_s16(vget_lane_s16(a, 2)/vget_lane_s16(b, 2), c, 2);
    c = vset_lane_s16(vget_lane_s16(a, 3)/vget_lane_s16(b, 3), c, 3);
    return  c;
#endif
}


INLINE(Vdwu,VDWU_DIVN) (Vdwu a, Vdwu b)
{
    uint32x2_t c = {0};
    c = vset_lane_u32(vget_lane_u32(a, 0)/vget_lane_u32(b, 0), c, 0);
    c = vset_lane_u32(vget_lane_u32(a, 1)/vget_lane_u32(b, 1), c, 1);
    return  c;
}

INLINE(Vdwi,VDWI_DIVN) (Vdwi a, Vdwi b)
{
    int32x2_t c = {0};
    c = vset_lane_s32(vget_lane_s32(a, 0)/vget_lane_s32(b, 0), c, 0);
    c = vset_lane_s32(vget_lane_s32(a, 1)/vget_lane_s32(b, 1), c, 1);
    return  c;
}


INLINE(Vddu,VDDU_DIVN) (Vddu a, Vddu b)
{
    return  vdup_n_u64(vget_lane_u64(a, 0)/vget_lane_u64(b, 0));
}

INLINE(Vddi,VDDI_DIVN) (Vddi a, Vddi b)
{
    return  vdup_n_s64(vget_lane_s64(a, 0)/vget_lane_s64(b, 0));
}


INLINE(Vqyu,VQYU_DIVN) (Vqyu a, Vqyu b)
{
    return  VQDU_ASYU(vandq_u64(VQYU_ASDU(a), VQYU_ASDU(b)));
}


INLINE(Vqbu,VQBU_DIVN) (Vqbu a, Vqbu b)
{
    return  vcombine_u8(
        VDBU_DIVN(vget_low_u8(a),  vget_low_u8(b)),
        VDBU_DIVN(vget_high_u8(a), vget_high_u8(b))
    );
}

INLINE(Vqbi,VQBI_DIVN) (Vqbi a, Vqbi b)
{
    return  vcombine_s8(
        VDBI_DIVN(vget_low_s8(a),  vget_low_s8(b)),
        VDBI_DIVN(vget_high_s8(a), vget_high_s8(b))
    );
}

INLINE(Vqbc,VQBC_DIVN) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_DIVN(VQBC_ASBI(a), VQBC_ASBI(b)));
#else
    return  VQBU_ASBC(VQBU_DIVN(VQBC_ASBU(a), VQBC_ASBU(b)));
#endif
}


INLINE(Vqhu,VQHU_DIVN) (Vqhu a, Vqhu b)
{
    return  vcombine_u16(
        VDHU_DIVN(vget_low_u16(a),  vget_low_u16(b)),
        VDHU_DIVN(vget_high_u16(a), vget_high_u16(b))
    );
}

INLINE(Vqhi,VQHI_DIVN) (Vqhi a, Vqhi b)
{
    return  vcombine_s16(
        VDHI_DIVN(vget_low_s16(a),  vget_low_s16(b)),
        VDHI_DIVN(vget_high_s16(a), vget_high_s16(b))
    );
}


INLINE(Vqwu,VQWU_DIVN) (Vqwu a, Vqwu b)
{
    return  vcombine_u32(
        VDWU_DIVN(vget_low_u32(a),  vget_low_u32(b)),
        VDWU_DIVN(vget_high_u32(a), vget_high_u32(b))
    );
}

INLINE(Vqwi,VQWI_DIVN) (Vqwi a, Vqwi b)
{
    return  vcombine_s32(
        VDWI_DIVN(vget_low_s32(a),  vget_low_s32(b)),
        VDWI_DIVN(vget_high_s32(a), vget_high_s32(b))
    );
}


INLINE(Vqdu,VQDU_DIVN) (Vqdu a, Vqdu b)
{
    return  vcombine_u64(
        vdup_n_u64(vgetq_lane_u64(a, 0)/vgetq_lane_u64(b, 0)),
        vdup_n_u64(vgetq_lane_u64(a, 1)/vgetq_lane_u64(b, 1))
    );
}

INLINE(Vqdi,VQDI_DIVN) (Vqdi a, Vqdi b)
{
    return  vcombine_s64(
        vdup_n_s64(vgetq_lane_s64(a, 0)/vgetq_lane_s64(b, 0)),
        vdup_n_s64(vgetq_lane_s64(a, 1)/vgetq_lane_s64(b, 1))
    );
}

#if 0 // _LEAVE_ARM_DIVN
}
#endif

#if 0 // _ENTER_ARM_DIV2
{
#endif

INLINE(Vwbu,VDHU_DIV2) (Vdhu a, Vwbu b)
{
    return  UINT8_NEWW(
        (vget_lane_u16(a, 0)/VWBU_GET1(b, 0)),
        (vget_lane_u16(a, 1)/VWBU_GET1(b, 1)),
        (vget_lane_u16(a, 2)/VWBU_GET1(b, 2)),
        (vget_lane_u16(a, 3)/VWBU_GET1(b, 3))
    );
}

INLINE(Vwbi,VDHI_DIV2) (Vdhi a, Vwbi b)
{
    return  VWBI_NEWL(
        (vget_lane_s16(a, 0)/VWBI_GET1(b, 0)),
        (vget_lane_s16(a, 1)/VWBI_GET1(b, 1)),
        (vget_lane_s16(a, 2)/VWBI_GET1(b, 2)),
        (vget_lane_s16(a, 3)/VWBI_GET1(b, 3))
    );
}


INLINE(Vwhu,VDWU_DIV2) (Vdwu a, Vwhu b)
{
    return  VWHU_NEWL(
        (vget_lane_u32(a, 0)/VWHU_GET1(b, 0)),
        (vget_lane_u32(a, 1)/VWHU_GET1(b, 1))
    );
}

INLINE(Vwhi,VDWI_DIV2) (Vdwi a, Vwhi b)
{
    return  VWHI_NEWL(
        (vget_lane_s32(a, 0)/VWHI_GET1(b, 0)),
        (vget_lane_s32(a, 1)/VWHI_GET1(b, 1))
    );
}


INLINE(Vwwu,VDDU_DIV2) (Vddu a, Vwwu b)
{
    return  UINT32_ASTV((vget_lane_u64(a, 0)/VWWU_ASTV(b)));
}

INLINE(Vwwi,VDDI_DIV2) (Vddi a, Vwwi b)
{
    return  INT32_ASTV((vget_lane_s64(a, 0)/VWWI_ASTV(b)));
}


INLINE(Vdbu,VQHU_DIV2) (Vqhu a, Vdbu b)
{
    return  VDBU_NEWL(
        (vgetq_lane_u16(a, 0)/vget_lane_u8(b, 0)),
        (vgetq_lane_u16(a, 1)/vget_lane_u8(b, 1)),
        (vgetq_lane_u16(a, 2)/vget_lane_u8(b, 2)),
        (vgetq_lane_u16(a, 3)/vget_lane_u8(b, 3)),
        (vgetq_lane_u16(a, 4)/vget_lane_u8(b, 4)),
        (vgetq_lane_u16(a, 5)/vget_lane_u8(b, 5)),
        (vgetq_lane_u16(a, 6)/vget_lane_u8(b, 6)),
        (vgetq_lane_u16(a, 7)/vget_lane_u8(b, 7))
    );
}

INLINE(Vdbi,VQHI_DIV2) (Vqhi a, Vdbi b)
{
    return  VDBU_NEWL(
        (vgetq_lane_s16(a, 0)/vget_lane_s8(b, 0)),
        (vgetq_lane_s16(a, 1)/vget_lane_s8(b, 1)),
        (vgetq_lane_s16(a, 2)/vget_lane_s8(b, 2)),
        (vgetq_lane_s16(a, 3)/vget_lane_s8(b, 3)),
        (vgetq_lane_s16(a, 4)/vget_lane_s8(b, 4)),
        (vgetq_lane_s16(a, 5)/vget_lane_s8(b, 5)),
        (vgetq_lane_s16(a, 6)/vget_lane_s8(b, 6)),
        (vgetq_lane_s16(a, 7)/vget_lane_s8(b, 7))
    );
}


INLINE(Vdhu,VQWU_DIV2) (Vqwu a, Vdhu b)
{
    return  VDHU_NEWL(
        (vgetq_lane_u32(a, 0)/vget_lane_u16(b, 0)),
        (vgetq_lane_u32(a, 1)/vget_lane_u16(b, 1)),
        (vgetq_lane_u32(a, 2)/vget_lane_u16(b, 2)),
        (vgetq_lane_u32(a, 3)/vget_lane_u16(b, 3))
    );
}

INLINE(Vdhi,VQWI_DIV2) (Vqwi a, Vdhi b)
{
    return  VDHI_NEWL(
        (vgetq_lane_s32(a, 0)/vget_lane_s16(b, 0)),
        (vgetq_lane_s32(a, 1)/vget_lane_s16(b, 1)),
        (vgetq_lane_s32(a, 2)/vget_lane_s16(b, 2)),
        (vgetq_lane_s32(a, 3)/vget_lane_s16(b, 3))
    );
}


INLINE(Vdwu,VQDU_DIV2) (Vqdu a, Vdwu b)
{
    return  VDWU_NEWL(
        (vgetq_lane_u64(a, 0)/vget_lane_u32(b, 0)),
        (vgetq_lane_u64(a, 1)/vget_lane_u32(b, 1))
    );
}

INLINE(Vdwi,VQDI_DIV2) (Vqdi a, Vdwi b)
{
    return  VDWI_NEWL(
        (vgetq_lane_s64(a, 0)/vget_lane_s32(b, 0)),
        (vgetq_lane_s64(a, 1)/vget_lane_s32(b, 1))
    );
}

#if 0 // _LEAVE_ARM_DIV2
}
#endif

#if 0 // _ENTER_ARM_DIVH
{
#endif
/*
There's a vdivh_f16 but...
*/
INLINE(flt16_t,  BOOL_DIVH)   (_Bool a, flt16_t b) {return a/b;}
INLINE(flt16_t, UCHAR_DIVH)   (uchar a, flt16_t b) {return a/b;}
INLINE(flt16_t, SCHAR_DIVH)   (schar a, flt16_t b) {return a/b;}
INLINE(flt16_t,  CHAR_DIVH)    (char a, flt16_t b) {return a/b;}
INLINE(flt16_t, USHRT_DIVH)  (ushort a, flt16_t b) {return a/b;}
INLINE(flt16_t,  SHRT_DIVH)   (short a, flt16_t b) {return a/b;}
INLINE(flt16_t,  UINT_DIVH)    (uint a, flt16_t b) {return a/b;}
INLINE(flt16_t,   INT_DIVH)     (int a, flt16_t b) {return a/b;}
INLINE(flt16_t, ULONG_DIVH)   (ulong a, flt16_t b) {return a/b;}
INLINE(flt16_t,  LONG_DIVH)    (long a, flt16_t b) {return a/b;}
INLINE(flt16_t,ULLONG_DIVH)  (ullong a, flt16_t b) {return a/b;}
INLINE(flt16_t, LLONG_DIVH)   (llong a, flt16_t b) {return a/b;}

INLINE(flt16_t, FLT16_DIVH) (flt16_t a, flt16_t b) {return a/b;}
INLINE(float,     FLT_DIVH)   (float a, flt16_t b) {return a/b;}
INLINE(double,    DBL_DIVH)  (double a, flt16_t b) {return a/b;}

#if QUAD_NLLONG == 2
INLINE(flt16_t,divhqu) (QUAD_UTYPE a, flt16_t b) {return a/b;}
INLINE(flt16_t,divhqi) (QUAD_ITYPE a, flt16_t b) {return a/b;}
INLINE(QUAD_FTYPE,divhqf) (QUAD_FTYPE a, flt16_t b) {return a/b;}
#endif


INLINE(Vdhf,VWBU_DIVH) (Vwbu a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdiv_f16(VWBU_CVHF(a), b);
#else
    float32x4_t p = VWBU_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vdivq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBI_DIVH) (Vwbi a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdiv_f16(VWBI_CVHF(a), b);
#else
    float32x4_t p = VWBI_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vdivq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBC_DIVH) (Vwbc a, Vdhf b)
{
#if CHAR_MIN
    return VWBI_DIVH(VWBC_ASBI(a), b);
#else
    return VWBU_DIVH(VWBC_ASBU(a), b);
#endif
}


INLINE(float,WHF_DIVH) (float am, float bm)
{
#if defined(SPC_ARM_FP16_SIMD)
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    ah = vdiv_f16(ah, bh);
#else
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x4_t av = vcvt_f32_f16(ah);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    float32x4_t bv = vcvt_f32_f16(bh);
    av = vdivq_f32(av, bv);
    ah = vcvt_f16_f32(av);
#endif
    aw = vreinterpret_f32_f16(ah);
    return  vget_lane_f32(aw, 0);
    
}


INLINE(Vwhf,VWHU_DIVH) (Vwhu a, Vwhf b)
{
    Vwhf    v = VWHU_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_DIVH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHI_DIVH) (Vwhi a, Vwhf b)
{
    Vwhf    v = VWHI_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_DIVH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_DIVH) (Vwhf a, Vwhf b)
{
    float   m = WHF_DIVH(VWHF_ASTM(a), VWHF_ASTM(b));
    return WHF_ASTV(m);
}


INLINE(Vqhf,VDBU_DIVH) (Vdbu a, Vqhf b) 
{
    uint16x8_t  c = vmovl_u8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_u16(c);
    return  vdivq_f16(f, b);
#else

    uint32x4_t  zl = vmovl_u16(vget_low_u16(c));
    float32x4_t fl = vcvtq_f32_u32(zl);
    uint32x4_t  zr = vmovl_u16(vget_high_u16(c));
    float32x4_t fr = vcvtq_f32_u32(zr);
    fl = vdivq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vdivq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBI_DIVH) (Vdbi a, Vqhf b) 
{
    int16x8_t   c = vmovl_s8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_s16(c);
    return  vdivq_f16(f, b);
#else
    int32x4_t   zl = vmovl_s16(vget_low_s16(c));
    float32x4_t fl = vcvtq_f32_s32(zl);
    int32x4_t   zr = vmovl_s16(vget_high_s16(c));
    float32x4_t fr = vcvtq_f32_s32(zr);
    fl = vdivq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vdivq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBC_DIVH) (Vdbc a, Vqhf b)
{
#if CHAR_MIN
    return  VDBI_DIVH(VDBC_ASBI(a), b);
#else
    return  VDBU_DIVH(VDBC_ASBU(a), b);
#endif
}


INLINE(Vdhf,VDHU_DIVH) (Vdhu a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdiv_f16(vcvt_f16_u16(a), b);
#else
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vdivq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHI_DIVH) (Vdhi a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdiv_f16(vcvt_f16_s16(a), b);
#else
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vdivq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHF_DIVH) (Vdhf a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdiv_f16(a, b);
#else
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vdivq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}


INLINE(Vwhf,VDWU_DIVH) (Vdwu a, Vwhf b)
{
    return  VWHF_DIVH(VDWU_CVHF(a), b);
}

INLINE(Vwhf,VDWI_DIVH) (Vdwi a, Vwhf b)
{
    return  VWHF_DIVH(VDWI_CVHF(a), b);
}

INLINE(Vdwf,VDWF_DIVH) (Vdwf a, Vwhf b)
{
    return vdiv_f32(a, VWHF_CVWF(b));
}


INLINE(Vqhf,VQHU_DIVH) (Vqhu a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdivq_f16(vcvtq_f16_u16(a), b);
#else
    return vcombine_f16(
        VDHU_DIVH(vget_low_u16(a),  vget_low_f16(b)),
        VDHU_DIVH(vget_high_u16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHI_DIVH) (Vqhi a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdivq_f16(vcvtq_f16_s16(a), b);
#else
    return vcombine_f16(
        VDHI_DIVH(vget_low_s16(a),  vget_low_f16(b)),
        VDHI_DIVH(vget_high_s16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHF_DIVH) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdivq_f16(a, b);
#else
    return vcombine_f16(
        VDHF_DIVH(vget_low_f16(a),  vget_low_f16(b)),
        VDHF_DIVH(vget_high_f16(a), vget_high_f16(b))
    );
#endif

}


INLINE(Vdhf,VQWU_DIVH) (Vqwu a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_u32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vdivq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vdhf,VQWI_DIVH) (Vqwi a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_s32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vdivq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vqwf,VQWF_DIVH) (Vqwf a, Vdhf b)
{
    return  vdivq_f32(a, vcvt_f32_f16(b));
}

INLINE(Vwhf,VQDU_DIVH) (Vqdu a, Vwhf b)
{
    return  VWHF_DIVH(VQDU_CVHF(a), b);
}

INLINE(Vwhf,VQDI_DIVH) (Vqdi a, Vwhf b)
{
    return  VWHF_DIVH(VQDI_CVHF(a), b);
}

INLINE(Vqdf,VQDF_DIVH) (Vqdf a, Vwhf b)
{
    return  vdivq_f64(a, VWHF_CVDF(b));
}

#if 0 // _LEAVE_ARM_DIVH
}
#endif

#if 0 // _ENTER_ARM_DIVW
{
#endif

INLINE(float,  BOOL_DIVW)   (_Bool a, float b) {return a/b;}

INLINE(float, UCHAR_DIVW)   (uchar a, float b) {return a/b;}
INLINE(float, SCHAR_DIVW)   (schar a, float b) {return a/b;}
INLINE(float,  CHAR_DIVW)    (char a, float b) {return a/b;}
INLINE(float, USHRT_DIVW)  (ushort a, float b) {return a/b;}
INLINE(float,  SHRT_DIVW)   (short a, float b) {return a/b;}
INLINE(float,  UINT_DIVW)    (uint a, float b) {return a/b;}
INLINE(float,   INT_DIVW)     (int a, float b) {return a/b;}
INLINE(float, ULONG_DIVW)   (ulong a, float b) {return a/b;}
INLINE(float,  LONG_DIVW)    (long a, float b) {return a/b;}
INLINE(float,ULLONG_DIVW)  (ullong a, float b) {return a/b;}
INLINE(float, LLONG_DIVW)   (llong a, float b) {return a/b;}

INLINE(float, FLT16_DIVW) (flt16_t a, float b) {return a/b;}
INLINE(float,   FLT_DIVW)   (float a, float b) {return a/b;}
INLINE(double,  DBL_DIVW)  (double a, float b) {return a/b;}

#if QUAD_NLLONG == 2
INLINE(float,divwqu)   (QUAD_UTYPE a, float b) {return a/b;}
INLINE(float,divwqi)   (QUAD_ITYPE a, float b) {return a/b;}
INLINE(QUAD_FTYPE,divwqf) (QUAD_FTYPE a, float b) {return a/b;}
#endif


INLINE(Vqwf,VWBU_DIVW) (Vwbu a, Vqwf b)
{
    return  vdivq_f32(VWBU_CVWF(a), b);
}

INLINE(Vqwf,VWBI_DIVW) (Vwbi a, Vqwf b)
{
    return  vdivq_f32(VWBI_CVWF(a), b);
}

INLINE(Vqwf,VWBC_DIVW) (Vwbc a, Vqwf b)
{
#if CHAR_MIN
    return  VWBI_DIVW(VWBC_ASBI(a), b);
#else
    return  VWBU_DIVW(VWBC_ASBU(a), b);
#endif
}



INLINE(Vdwf,VWHU_DIVW) (Vwhu a, Vdwf b)
{
    return  vdiv_f32(VWHU_CVWF(a), b);
}

INLINE(Vdwf,VWHI_DIVW) (Vwhi a, Vdwf b)
{
    return  vdiv_f32(VWHI_CVWF(a), b);
}

INLINE(Vdwf,VWHF_DIVW) (Vwhf a, Vdwf b)
{
    return  vdiv_f32(VWHF_CVWF(a), b);
}


INLINE(Vwwf,VWWU_DIVW) (Vwwu a, Vwwf b)
{
    return WWF_ASTV((VWWU_ASTV(a)/VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWI_DIVW) (Vwwi a, Vwwf b)
{
    return WWF_ASTV((VWWI_ASTV(a)/VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWF_DIVW) (Vwwf a, Vwwf b)
{
    return WWF_ASTV((VWWF_ASTM(a)/VWWF_ASTM(b)));
}


INLINE(Vqwf,VDHU_DIVW) (Vdhu a, Vqwf b) 
{
    return  vdivq_f32(VDHU_CVWF(a), b);
}

INLINE(Vqwf,VDHI_DIVW) (Vdhi a, Vqwf b) 
{
    return  vdivq_f32(VDHI_CVWF(a), b);
}

INLINE(Vqwf,VDHF_DIVW) (Vdhf a, Vqwf b) 
{
    return  vdivq_f32(VDHF_CVWF(a), b);
}


INLINE(Vdwf,VDWU_DIVW) (Vdwu a, Vdwf b)
{
    return vdiv_f32(vcvt_f32_u32(a), b);
}

INLINE(Vdwf,VDWI_DIVW) (Vdwi a, Vdwf b)
{
    return vdiv_f32(vcvt_f32_s32(a), b);
}

INLINE(Vdwf,VDWF_DIVW) (Vdwf a, Vdwf b)
{
    return vdiv_f32(a, b);
}


INLINE(Vwwf,VDDU_DIVW) (Vddu a, Vwwf b)
{
    return WWF_ASTV((VDDU_ASTV(a)/VWWF_ASTM(b)));
}

INLINE(Vwwf,VDDI_DIVW) (Vddi a, Vwwf b)
{
    return WWF_ASTV((VDDI_ASTV(a)/VWWF_ASTM(b)));
}

INLINE(Vddf,VDDF_DIVW) (Vddf a, Vwwf b)
{
    return vdiv_f64(a, vdup_n_f64(VWWF_ASTM(b)));
}


INLINE(Vqwf,VQWU_DIVW) (Vqwu a, Vqwf b)
{
    return  vdivq_f32(vcvtq_f32_u32(a), b);
}

INLINE(Vqwf,VQWI_DIVW) (Vqwi a, Vqwf b)
{
    return  vdivq_f32(vcvtq_f32_s32(a), b);
}

INLINE(Vqwf,VQWF_DIVW) (Vqwf a, Vqwf b)
{
    return  vdivq_f32(a, b);
}


INLINE(Vdwf,VQDU_DIVW) (Vqdu a, Vdwf b)
{
    return  vdiv_f32(vcvt_f32_f64(vcvtq_f64_u64(a)), b);
}

INLINE(Vdwf,VQDI_DIVW) (Vqdi a, Vdwf b)
{
    return  vdiv_f32(vcvt_f32_f64(vcvtq_f64_s64(a)), b);
}

INLINE(Vqdf,VQDF_DIVW) (Vqdf a, Vdwf b)
{
    return  vdivq_f64(a, vcvt_f64_f32(b));
}

#if 0 // _LEAVE_ARM_DIVW
}
#endif

#if 0 // _ENTER_ARM_DIVD
{
#endif

INLINE(double,  BOOL_DIVD)   (_Bool a, double b) {return a/b;}

INLINE(double, UCHAR_DIVD)   (uchar a, double b) {return a/b;}
INLINE(double, SCHAR_DIVD)   (schar a, double b) {return a/b;}
INLINE(double,  CHAR_DIVD)    (char a, double b) {return a/b;}
INLINE(double, USHRT_DIVD)  (ushort a, double b) {return a/b;}
INLINE(double,  SHRT_DIVD)   (short a, double b) {return a/b;}
INLINE(double,  UINT_DIVD)    (uint a, double b) {return a/b;}
INLINE(double,   INT_DIVD)     (int a, double b) {return a/b;}
INLINE(double, ULONG_DIVD)   (ulong a, double b) {return a/b;}
INLINE(double,  LONG_DIVD)    (long a, double b) {return a/b;}
INLINE(double,ULLONG_DIVD)  (ullong a, double b) {return a/b;}
INLINE(double, LLONG_DIVD)   (llong a, double b) {return a/b;}

INLINE(double, FLT16_DIVD) (flt16_t a, double b) {return a/b;}
INLINE(double,   FLT_DIVD)   (float a, double b) {return a/b;}
INLINE(double,   DBL_DIVD)  (double a, double b) {return a/b;}

#if QUAD_NLLONG == 2
INLINE(double,divdqu)   (QUAD_UTYPE a, double b) {return a/b;}
INLINE(double,divdqi)   (QUAD_ITYPE a, double b) {return a/b;}
INLINE(QUAD_FTYPE,divdqf) (QUAD_FTYPE a, double b) {return a/b;}
#endif


INLINE(Vqdf,VWHU_DIVD) (Vwhu a, Vqdf b)
{
    return  vdivq_f64(VWHU_CVDF(a), b);
}

INLINE(Vqdf,VWHI_DIVD) (Vwhi a, Vqdf b)
{
    return  vdivq_f64(VWHI_CVDF(a), b);
}

INLINE(Vqdf,VWHF_DIVD) (Vwhf a, Vqdf b)
{
    return  vdivq_f64(VWHF_CVDF(a), b);
}


INLINE(Vddf,VWWU_DIVD) (Vwwu a, Vddf b)
{
    return  vdiv_f64(VWWU_CVDF(a), b);
}

INLINE(Vddf,VWWI_DIVD) (Vwwi a, Vddf b)
{
    return  vdiv_f64(VWWI_CVDF(a), b);
}

INLINE(Vddf,VWWF_DIVD) (Vwwf a, Vddf b)
{
    return  vdiv_f64(VWWF_CVDF(a), b);
}


INLINE(Vqdf,VDWU_DIVD) (Vdwu a, Vqdf b)
{
    return  vdivq_f64(vcvtq_f64_u64(vmovl_u32(a)), b);
}

INLINE(Vqdf,VDWI_DIVD) (Vdwi a, Vqdf b)
{
    return  vdivq_f64(vcvtq_f64_s64(vmovl_s32(a)), b);
}

INLINE(Vqdf,VDWF_DIVD) (Vdwf a, Vqdf b)
{
    return  vdivq_f64(vcvt_f64_f32(a), b);
}


INLINE(Vddf,VDDU_DIVD) (Vddu a, Vddf b)
{
    return  vdiv_f64(vcvt_f64_u64(a), b);
}

INLINE(Vddf,VDDI_DIVD) (Vddi a, Vddf b)
{
    return  vdiv_f64(vcvt_f64_s64(a), b);
}

INLINE(Vddf,VDDF_DIVD) (Vddf a, Vddf b)
{
    return  vdiv_f64(a, b);
}


INLINE(Vqdf,VQDU_DIVD) (Vqdu a, Vqdf b)
{
    return  vdivq_f64(vcvtq_f64_u64(a), b);
}

INLINE(Vqdf,VQDI_DIVD) (Vqdi a, Vqdf b)
{
    return  vdivq_f64(vcvtq_f64_s64(a), b);
}

INLINE(Vqdf,VQDF_DIVD) (Vqdf a, Vqdf b)
{
    return  vdivq_f64(a, b);
}

#if 0 // _LEAVE_ARM_DIVD
}
#endif

#if 0 // _ENTER_ARM_DIFU
{
#endif

INLINE(uint16_t,FLT16_DIFU) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabdh_f16(a, b);
#else
    return  vabds_f32(a, b);
#endif
}

INLINE(uint32_t,FLT_DIFU)  (float a,  float b) {return  vabds_f32(a, b);}
INLINE(uint64_t,DBL_DIFU) (double a, double b) {return  vabdd_f64(a, b);}
INLINE(QUAD_UTYPE,difuqf) (QUAD_FTYPE a, QUAD_FTYPE b) 
{
#if 0
    return  (a <= b) ? (b-a) : (a-b);
#else
    return  ((QUAD_UTYPE){0});
#endif
}

INLINE(Vwyu,VWYU_DIFU) (Vwyu a, Vwyu b)
{
    float l = VWYU_ASTM(a);
    float r = VWYU_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    uint32x2_t  x = vreinterpret_u32_f32(p);
    uint32x2_t  y = vreinterpret_u32_f32(q);
    x = veor_u32(x, y);
    p = vreinterpret_f32_u32(x);
    l = vget_lane_f32(p, 0);
    return  WYU_ASTV(l);
}


INLINE(Vwbu,VWBU_DIFU) (Vwbu a, Vwbu b)
{
    float l = VWBU_ASTM(a);
    float r = VWBU_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    uint8x8_t  x = vreinterpret_u8_f32(p);
    uint8x8_t  y = vreinterpret_u8_f32(q);
    x = vabd_u8(x, y);
    p = vreinterpret_f32_u8(x);
    l = vget_lane_f32(p, 0);
    return  WBU_ASTV(l);
}

INLINE(Vwbu,VWBI_DIFU) (Vwbi a, Vwbi b)
{
    float l = VWBI_ASTM(a);
    float r = VWBI_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    int8x8_t    x = vreinterpret_s8_f32(p);
    int8x8_t    y = vreinterpret_s8_f32(q);
    x = vabd_s8(x, y);
    p = vreinterpret_f32_s8(x);
    l = vget_lane_f32(p, 0);
    return  WBU_ASTV(l);
}

INLINE(Vwbu,VWBC_DIFU) (Vwbc a, Vwbc b)
{
#if CHAR_MIN
    return  VWBI_DIFU(VWBC_ASBI(a), VWBC_ASBI(b));
#else
    return  VWBU_DIFU(VWBC_ASBU(a), VWBC_ASBU(b));
#endif
}


INLINE(Vwhu,VWHU_DIFU) (Vwhu a, Vwhu b)
{
    float l = VWHU_ASTM(a);
    float r = VWHU_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    uint16x4_t  x = vreinterpret_u16_f32(p);
    uint16x4_t  y = vreinterpret_u16_f32(q);
    x = vabd_u16(x, y);
    p = vreinterpret_f32_u16(x);
    l = vget_lane_f32(p, 0);
    return  WHU_ASTV(l);
}

INLINE(Vwhu,VWHI_DIFU) (Vwhi a, Vwhi b)
{
    float l = VWHI_ASTM(a);
    float r = VWHI_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    int16x4_t   x = vreinterpret_s16_f32(p);
    int16x4_t   y = vreinterpret_s16_f32(q);
    x = vabd_s16(x, y);
    p = vreinterpret_f32_s16(x);
    l = vget_lane_f32(p, 0);
    return  WHU_ASTV(l);
}

INLINE(Vwhu,VWHF_DIFU) (Vwhf a, Vwhf b)
{
    float       l = VWHF_ASTM(a);
    float       r = VWHF_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    float16x4_t x = vreinterpret_f16_f32(p);
    float16x4_t y = vreinterpret_f16_f32(q);

#if defined(SPC_ARM_FP16_SIMD)
    x = vabd_f16(x, y);
    uint16x4_t  u = vcvt_u16_f16(x);
#else
    float32x4_t z = vabdq_f32(
        vcvt_f32_f16(x), 
        vcvt_f32_f16(y)
    );
    uint32x4_t  w = vcvtq_u32_f32(z);
    uint16x4_t  u = vqmovn_u32(w);
#endif
    p = vreinterpret_f32_u16(u);
    l = vget_lane_f32(p, 0);
    return  WHU_ASTV(l);
}


INLINE(Vwwu,VWWU_DIFU) (Vwwu a, Vwwu b)
{
    float l = VWWU_ASTM(a);
    float r = VWWU_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    uint32x2_t  x = vreinterpret_u32_f32(p);
    uint32x2_t  y = vreinterpret_u32_f32(q);
    x = vabd_u32(x, y);
    p = vreinterpret_f32_u32(x);
    l = vget_lane_f32(p, 0);
    return  WWU_ASTV(l);
}

INLINE(Vwwu,VWWI_DIFU) (Vwwi a, Vwwi b)
{
    float l = VWWI_ASTM(a);
    float r = VWWI_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    int32x2_t   x = vreinterpret_s32_f32(p);
    int32x2_t   y = vreinterpret_s32_f32(q);
    x = vabd_s32(x, y);
    p = vreinterpret_f32_s32(x);
    l = vget_lane_f32(p, 0);
    return  WWU_ASTV(l);
}

INLINE(Vwwu,VWWF_DIFU) (Vwwf a, Vwwf b)
{
    float l = VWWF_ASTM(a);
    float r = VWWF_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    p = vabd_f32(p, q);
    uint32x2_t  u = vcvt_u32_f32(p);
    p = vreinterpret_f32_u32(u);
    l = vget_lane_f32(p, 0);
    return  WWU_ASTV(l);
}


INLINE(Vdyu,VDYU_DIFU) (Vdyu a, Vdyu b)
{
#define     VDYU_DIFU(A, B) DYU_ASTV(veor_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_DIFU(a, b);
}


INLINE(Vdbu,VDBU_DIFU) (Vdbu a, Vdbu b) {return vabd_u8(a, b);}

INLINE(Vdbu,VDBI_DIFU) (Vdbi a, Vdbi b) 
{
    a = vabd_s8(a, b);
    return  vreinterpret_u8_s8(a);
}

INLINE(Vdbu,VDBC_DIFU) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
    return  VDBI_DIFU(VDBC_ASBI(a), VDBC_ASBI(b));
#else
    return  VDBU_DIFU(VDBC_ASBU(a), VDBC_ASBU(b));
#endif
}


INLINE(Vdhu,VDHU_DIFU) (Vdhu a, Vdhu b) {return vabd_u16(a, b);}

INLINE(Vdhi,VDHI_DIFU) (Vdhi a, Vdhi b) 
{
    a = vabd_s16(a, b);
    return  vreinterpret_u16_s16(a);
}

INLINE(Vdwu,VDWU_DIFU) (Vdwu a, Vdwu b) {return vabd_u32(a, b);}

INLINE(Vdwi,VDWI_DIFU) (Vdwi a, Vdwi b)
{
    a = vabd_s32(a, b);
    return  vreinterpret_u32_s32(a);
}

INLINE(Vddu,VDDU_DIFU) (Vddu a, Vddu b) 
{
    uint64x1_t l = VDDU_CGTR(a, b);
    uint64x1_t r = VDDU_CLTR(a, b);
    return  vsub_u64(l, r);
}

INLINE(Vddu,VDDI_DIFU) (Vddi a, Vddi b)
{
    int64x1_t p = VDDI_CGTR(a, b);
    int64x1_t q = VDDI_CLTR(a, b);
    return VDDU_DIFU(
        vreinterpret_u64_s64(p),
        vreinterpret_u64_s64(q)
    );
}


INLINE(Vqyu,VQYU_DIFU) (Vqyu a, Vqyu b)
{
#define     VQYU_DIFU(A, B) QYU_ASTV(veorq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_DIFU(a, b);
}


INLINE(Vqbu,VQBU_DIFU) (Vqbu a, Vqbu b) {return vabdq_u8(a, b);}

INLINE(Vqbu,VQBI_DIFU) (Vqbi a, Vqbi b) 
{
    a = vabdq_s8(a, b);
    return  vreinterpretq_u8_s8(a);
}

INLINE(Vqbu,VQBC_DIFU) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
    return  VQBI_DIFU(VQBC_ASBI(a), VQBC_ASBI(b));
#else
    return  VQBU_DIFU(VQBC_ASBU(a), VQBC_ASBU(b));
#endif
}


INLINE(Vqhu,VQHU_DIFU) (Vqhu a, Vqhu b) {return vabdq_u16(a, b);}

INLINE(Vqhi,VQHI_DIFU) (Vqhi a, Vqhi b) 
{
    a = vabdq_s16(a, b);
    return  vreinterpretq_u16_s16(a);
}


INLINE(Vqwu,VQWU_DIFU) (Vqwu a, Vqwu b) {return vabdq_u32(a, b);}

INLINE(Vqwi,VQWI_DIFU) (Vqwi a, Vqwi b)
{
    a = vabdq_s32(a, b);
    return  vreinterpretq_u32_s32(a);
}


INLINE(Vqdu,VQDU_DIFU) (Vqdu a, Vqdu b) 
{
    uint64x2_t l = VQDU_CGTR(a, b);
    uint64x2_t r = VQDU_CLTR(a, b);
    return  vsubq_u64(l, r);
}

INLINE(Vqdu,VQDI_DIFU) (Vqdi a, Vqdi b)
{
    int64x2_t p = VQDI_CGTR(a, b);
    int64x2_t q = VQDI_CLTR(a, b);
    return  VQDU_DIFU(
        vreinterpretq_u64_s64(p),
        vreinterpretq_u64_s64(q)
    );
}

#if 0 // _LEAVE_ARM_DIFU
}
#endif

#if 0 // _ENTER_ARM_MVWL
{
#endif

INLINE(Vwyu,  BOOL_MVWL)     (_Bool x) 
{
#define     BOOL_MVWL(X)   ((Vwyu){ ((WORD_TYPE){.Y0=X}).F })
    return  BOOL_MVWL(x);
}


INLINE(Vwbu, UCHAR_MVWL)  (unsigned x) 
{
#define     UCHAR_MVWL(X)   ((Vwbu){((WORD_TYPE){.B0.U=X}).F})
    return  UCHAR_MVWL(x);
}

INLINE(Vwbi, SCHAR_MVWL)    (signed x) 
{
#define     SCHAR_MVWL(X)   ((Vwbi){((WORD_TYPE){.B0.I=X}).F})
    return  SCHAR_MVWL(x);
}

INLINE(Vwbc,  CHAR_MVWL)       (int x) 
{
#define     CHAR_MVWL(X)    ((Vwbc){((WORD_TYPE){.C0=X}).F})
    return  CHAR_MVWL(x);
}


INLINE(Vwhu, USHRT_MVWL)  (unsigned x) 
{
#define     USHRT_MVWL(X)   ((Vwhu){((WORD_TYPE){.H0.U=X}).F})
    return  USHRT_MVWL(x);
}

INLINE(Vwhi,  SHRT_MVWL)    (signed x) 
{
#define     SHRT_MVWL(X)    ((Vwhi){((WORD_TYPE){.H0.I=X}).F})
    return  SHRT_MVWL(x);
}


INLINE(Vwwu,  UINT_MVWL)      (uint x) 
{
#define     UINT_MVWL(X)    ((Vwwu){((WORD_TYPE){.U=X}).F})
    return  UINT_MVWL(x);
}

INLINE(Vwwi,   INT_MVWL)       (int x) 
{
#define     INT_MVWL(X)     ((Vwwi){ ((WORD_TYPE){.I=X}).F})
    return  INT_MVWL(x);
}

#if DWRD_NLONG == 2

INLINE(Vwwu, ULONG_MVWL)     (ulong x) 
{
#define     ULONG_MVWL(X)   ((Vwwu){((WORD_TYPE){.U=X}).F})
    return  ULONG_MVWL(x);
}

INLINE(Vwwi,  LONG_MVWL)      (long x) 
{
#define     LONG_MVWL(X)    ((Vwwi){((WORD_TYPE){.I=X}).F})
    return  LONG_MVWL(x);
}

#endif

INLINE(Vwhf,FLT16_MVWL)    (flt16_t x) 
{
#define     FLT16_MVWL(X)   ((Vwhf){((WORD_TYPE){.H0.F=X}).F})
    return  FLT16_MVWL(x);
}

INLINE(Vwwf,  FLT_MVWL)      (float x) 
{
#define     FLT_MVWL(X)     ((Vwwf){X})
    return  FLT_MVWL(x);
}

INLINE(   _Bool,VWYU_MVWL) (Vwyu x) {return ((WORD_TYPE){.F=x.V0}).Y0;}
INLINE( uint8_t,VWBU_MVWL) (Vwbu x) {return ((WORD_TYPE){.F=x.V0}).B0.U;}
INLINE(  int8_t,VWBI_MVWL) (Vwbi x) {return ((WORD_TYPE){.F=x.V0}).B0.I;}
INLINE(    char,VWBC_MVWL) (Vwbc x) {return ((WORD_TYPE){.F=x.V0}).C0;}
INLINE(uint16_t,VWHU_MVWL) (Vwhu x) {return ((WORD_TYPE){.F=x.V0}).H0.U;}
INLINE( int16_t,VWHI_MVWL) (Vwhi x) {return ((WORD_TYPE){.F=x.V0}).H0.I;}
INLINE( flt16_t,VWHF_MVWL) (Vwhf x) {return ((WORD_TYPE){.F=x.V0}).H0.F;}
INLINE(uint32_t,VWWU_MVWL) (Vwwu x) {return ((WORD_TYPE){.F=x.V0})   .U;}
INLINE( int32_t,VWWI_MVWL) (Vwwi x) {return ((WORD_TYPE){.F=x.V0})   .I;}
INLINE(   float,VWWF_MVWL) (Vwwf x) {return x.V0;}

#if 0 // _LEAVE_ARM_MVWL
}
#endif

#if 0 // _ENTER_ARM_MVDL
{
#endif

INLINE(Vdyu,  BOOL_MVDL)  (_Bool x) 
{
#define     BOOL_MVDL(X)   ((Vdyu){vcreate_u64((X?1:0))})
    return  BOOL_MVDL(x);
}


INLINE(Vdbu, UCHAR_MVDL)  (unsigned x) 
{
#define     UCHAR_MVDL(X)  vset_lane_u8(X,vdup_n_u8(0),0)
    return  UCHAR_MVDL(x);
}

INLINE(Vdbi, SCHAR_MVDL)  (signed x) 
{
#define     SCHAR_MVDL(X)  vset_lane_s8(X,vdup_n_s8(0),0)
    return  SCHAR_MVDL(x);
}

INLINE(Vdbc,  CHAR_MVDL)   (int x) 
{
#if CHAR_MIN
#   define  CHAR_MVDL(X)    ((Vdbc){SCHAR_MVDL(X)})
#else
#   define  CHAR_MVDL(X)    ((Vdbc){UCHAR_MVDL(X)})
#endif
    return  CHAR_MVDL(x);
}


INLINE(Vdhu, USHRT_MVDL) (unsigned x) 
{
#define     USHRT_MVDL(X)   vset_lane_u16(X,vdup_n_u16(0),0)
    return  USHRT_MVDL(x);
}

INLINE(Vdhi,  SHRT_MVDL)  (signed x) 
{
#define     SHRT_MVDL(X)    vset_lane_s16(X,vdup_n_s16(0),0)
    return  SHRT_MVDL(x);
}


INLINE(Vdwu,  UINT_MVDL)   (uint x) 
{
#define     UINT_MVDL(X)    vset_lane_u32(X,vdup_n_u32(0),0)
    return  UINT_MVDL(x);
}

INLINE(Vdwi,   INT_MVDL)    (int x) 
{
#define     INT_MVDL(X)     vset_lane_s32(X,vdup_n_s32(0),0)
    return  INT_MVDL(x);
}

#if DWRD_NLONG == 2

INLINE(Vdwu, ULONG_MVDL)  (ulong x) 
{
#define     ULONG_MVDL(X)   vset_lane_u32(X,vdup_n_u32(0),0)
    return  ULONG_MVDL(x);
}

INLINE(Vdwi,  LONG_MVDL)   (long x) 
{
#define     LONG_MVDL(X)    vset_lane_s32(X,vdup_n_s32(0),0)
    return  LONG_MVDL(x);
}

#else

INLINE(Vddu, ULONG_MVDL)  (ulong x) 
{
#define     ULONG_MVDL(X)   vdup_n_u64(X)
    return  ULONG_MVDL(x);
}

INLINE(Vddi,  LONG_MVDL)   (long x) 
{
#define     LONG_MVDL(X)   vdup_n_s64(X)
    return  LONG_MVDL(x);
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vddu, ULLONG_MVDL)  (ullong x) 
{
#define     ULLONG_MVDL(X)   vdup_n_u64(X)
    return  ULLONG_MVDL(x);
}

INLINE(Vddi,  LLONG_MVDL)   (llong x) 
{
#define     LLONG_MVDL(X)   vdup_n_s64(X)
    return  LLONG_MVDL(x);
}

#endif

INLINE(Vdhf,FLT16_MVDL)  (flt16_t x) 
{
#define     FLT16_MVDL(X)   vset_lane_f16(X,vdup_n_f16(0),0)
    return  FLT16_MVDL(x);
}

INLINE(Vdwf,  FLT_MVDL)  (float x) 
{
#define     FLT_MVDL(X)     vset_lane_f32(X,vdup_n_f32(0),0)
    return  FLT_MVDL(x);
}

INLINE(Vddf,  DBL_MVDL)  (double x) 
{
#define     DBL_MVDL(X)     vdup_n_f64(X)
    return  DBL_MVDL(x);
}

INLINE(   _Bool,VDYU_MVDL) (Vdyu x) {return  1&vget_lane_u64(x.V0, 0);}
INLINE( uint8_t,VDBU_MVDL) (Vdbu x) {return  vget_lane_u8(x, 0);}
INLINE(  int8_t,VDBI_MVDL) (Vdbi x) {return  vget_lane_s8(x, 0);}
INLINE(    char,VDBC_MVDL) (Vdbc x) 
{
#if CHAR_MIN
    return  vget_lane_s8(x.V0, 0);
#else
    return  vget_lane_u8(x.V0, 0);
#endif
}
INLINE(uint16_t,VDHU_MVDL) (Vdhu x) {return  vget_lane_u16(x, 0);}
INLINE( int16_t,VDHI_MVDL) (Vdhi x) {return  vget_lane_s16(x, 0);}
INLINE( flt16_t,VDHF_MVDL) (Vdhf x) {return  vget_lane_f16(x, 0);}

INLINE(uint32_t,VDWU_MVDL) (Vdwu x) {return  vget_lane_u32(x, 0);}
INLINE( int32_t,VDWI_MVDL) (Vdwi x) {return  vget_lane_s32(x, 0);}
INLINE(   float,VDWF_MVDL) (Vdwf x) {return  vget_lane_f32(x, 0);}

INLINE(uint64_t,VDDU_MVDL) (Vddu x) {return  vget_lane_u64(x, 0);}
INLINE( int64_t,VDDI_MVDL) (Vddi x) {return  vget_lane_s64(x, 0);}
INLINE(  double,VDDF_MVDL) (Vddf x) {return  vget_lane_f64(x, 0);}
#if 0 // _LEAVE_ARM_MVDL
}
#endif

#if 0 // _ENTER_ARM_MVQL
{
#endif

INLINE(Vqyu,  BOOL_MVQL)  (_Bool x) 
{
#define     BOOL_MVQL(X)   \
((Vqyu){vcombine_u64(vdup_n_u64((X?1:0)),vdup_n_u64(0))})
    return  BOOL_MVQL(x);
}


INLINE(Vqbu, UCHAR_MVQL)  (uchar x) 
{
#define     UCHAR_MVQL(X)  vsetq_lane_u8(X,vdupq_n_u8(0),0)
    return  UCHAR_MVQL(x);
}

INLINE(Vqbi, SCHAR_MVQL)  (schar x) 
{
#define     SCHAR_MVQL(X)  vsetq_lane_s8(X,vdupq_n_s8(0),0)
    return  SCHAR_MVQL(x);
}

INLINE(Vqbc,  CHAR_MVQL)   (char x) 
{
#if CHAR_MIN
#   define  CHAR_MVQL(X)    ((Vqbc){SCHAR_MVQL(X)})
#else
#   define  CHAR_MVQL(X)    ((Vqbc){UCHAR_MVQL(X)})
#endif
    return  CHAR_MVQL(x);
}


INLINE(Vqhu, USHRT_MVQL) (ushort x) 
{
#define     USHRT_MVQL(X)   vsetq_lane_u16(X,vdupq_n_u16(0),0)
    return  USHRT_MVQL(x);
}

INLINE(Vqhi,  SHRT_MVQL)  (short x) 
{
#define     SHRT_MVQL(X)    vsetq_lane_s16(X,vdupq_n_s16(0),0)
    return  SHRT_MVQL(x);
}


INLINE(Vqwu,  UINT_MVQL)   (uint x) 
{
#define     UINT_MVQL(X)    vsetq_lane_u32(X,vdupq_n_u32(0),0)
    return  UINT_MVQL(x);
}

INLINE(Vqwi,   INT_MVQL)    (int x) 
{
#define     INT_MVQL(X)     vsetq_lane_s32(X,vdupq_n_s32(0),0)
    return  INT_MVQL(x);
}

#if DWRD_NLONG == 2

INLINE(Vqwu, ULONG_MVQL)  (ulong x) 
{
#define     ULONG_MVQL(X)   vsetq_lane_u32(X,vdupq_n_u32(0),0)
    return  ULONG_MVQL(x);
}

INLINE(Vqwi,  LONG_MVQL)   (long x) 
{
#define     LONG_MVQL(X)    vsetq_lane_s32(X,vdupq_n_s32(0),0)
    return  LONG_MVQL(x);
}

#else

INLINE(Vqdu, ULONG_MVQL)  (ulong x) 
{
#define     ULONG_MVQL(X)   vsetq_lane_u64(X,vdupq_n_u64(0),0)
    return  ULONG_MVQL(x);
}

INLINE(Vqdi,  LONG_MVQL)   (long x) 
{
#define     LONG_MVQL(X)    vsetq_lane_s64(X,vdupq_n_s64(0),0)
    return  LONG_MVQL(x);
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vqdu, ULLONG_MVQL)  (ullong x) 
{
#define     ULLONG_MVQL(X)   vsetq_lane_u64(X,vdupq_n_u64(0),0)
    return  ULLONG_MVQL(x);
}

INLINE(Vqdi,  LLONG_MVQL)   (llong x) 
{
#define     LLONG_MVQL(X)    vsetq_lane_s64(X,vdupq_n_s64(0),0)
    return  LLONG_MVQL(x);
}

INLINE(Vqqu,mvqlqu) (QUAD_UTYPE x) {return astvqu(x);}
INLINE(Vqqi,mvqlqi) (QUAD_ITYPE x) {return astvqi(x);}

#else

INLINE(Vqqu, ULLONG_MVQL) (ullong x) 
{
#define     ULLONG_MVQL(X)  vdupq_n_u128(x)
    return  ULLONG_MVQL(x);
}

INLINE(Vqqi, LLONG_MVQL) (llong x) 
{
#define     LLONG_MVQL(X)  vdupq_n_s128(x)
    return  LLONG_MVQL(x);
}

#endif

INLINE(Vqhf,FLT16_MVQL)  (flt16_t x) 
{
#define     FLT16_MVQL(X)   vsetq_lane_f16(X,vdupq_n_f16(0),0)
    return  FLT16_MVQL(x);
}

INLINE(Vqwf,  FLT_MVQL)  (float x) 
{
#define     FLT_MVQL(X)     vsetq_lane_f32(X,vdupq_n_f32(0),0)
    return  FLT_MVQL(x);
}

INLINE(Vqdf,  DBL_MVQL)  (double x) 
{
#define     DBL_MVQL(X)     vsetq_lane_f64(X,vdupq_n_f64(0),0)
    return  DBL_MVQL(x);
}

INLINE(Vqqf,mvqlqf) (QUAD_FTYPE x) {return ((Vqqf){x});}


INLINE(   _Bool,VQYU_MVQL) (Vqyu x) 
{
#define     VQYU_MVQL(X) ((_Bool) (1&vgetq_lane_u64(VQYU_ASDU(X),0)))
    return  VQYU_MVQL(x);
}

INLINE( uint8_t,VQBU_MVQL) (Vqbu x) {return  vgetq_lane_u8(x, 0);}
INLINE(  int8_t,VQBI_MVQL) (Vqbi x) {return  vgetq_lane_s8(x, 0);}
INLINE(    char,VQBC_MVQL) (Vqbc x) 
{
#if CHAR_MIN
    return  vgetq_lane_s8(x.V0, 0);
#else
    return  vgetq_lane_u8(x.V0, 0);
#endif
}

INLINE(uint16_t,VQHU_MVQL) (Vqhu x) {return  vgetq_lane_u16(x, 0);}
INLINE( int16_t,VQHI_MVQL) (Vqhi x) {return  vgetq_lane_s16(x, 0);}
INLINE( flt16_t,VQHF_MVQL) (Vqhf x) {return  vgetq_lane_f16(x, 0);}

INLINE(uint32_t,VQWU_MVQL) (Vqwu x) {return  vgetq_lane_u32(x, 0);}
INLINE( int32_t,VQWI_MVQL) (Vqwi x) {return  vgetq_lane_s32(x, 0);}
INLINE(   float,VQWF_MVQL) (Vqwf x) {return  vgetq_lane_f32(x, 0);}

INLINE(uint64_t,VQDU_MVQL) (Vqdu x) {return  vgetq_lane_u64(x, 0);}
INLINE( int64_t,VQDI_MVQL) (Vqdi x) {return  vgetq_lane_s64(x, 0);}
INLINE(  double,VQDF_MVQL) (Vqdf x) {return  vgetq_lane_f64(x, 0);}

INLINE(QUAD_UTYPE,VQQU_MVQL) (Vqqu x) {return VQQU_ASTV(x);}
INLINE(QUAD_ITYPE,VQQI_MVQL) (Vqqi x) {return VQQI_ASTV(x);}
INLINE(QUAD_FTYPE,VQQF_MVQL) (Vqqf x) {return x.V0;}

#if 0 // _LEAVE_ARM_MVQL
}
#endif


#if 0 // _LEAVE_ARM__
}
#endif

