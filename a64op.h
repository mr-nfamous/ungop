/*°′″  «»  ≤≥  ≠≈  —¦  ÷×  !¡  ©®  £€  $¢  №⋕  λμ  πφ  ∑∏  ¶§  †‡  ±∞  √∆  ∫∳ 

This file defines the armv8 ops. It applies to all systems 
since support for Windows armv8 will not be implemented until
Microsoft implements the ACLE.
*/

#if _ENTER_ARM__
{
#endif

#ifdef SPC_ARM_NEON


INLINE(flt16_t, FLT16_ANDS) (flt16_t a, flt16_t b)
{
    return  (
        (HALF_TYPE)
        {
            .U=((HALF_TYPE){.F=a}).U&((HALF_TYPE){.F=b}).U
        }
    ).F;
}

INLINE(flt16_t, FLT16_ANDN) (flt16_t a, flt16_t b)
{
    return  (
        (HALF_TYPE)
        {
            .U=((HALF_TYPE){.F=a}).U&~((HALF_TYPE){.F=b}).U
        }
    ).F;
}


INLINE(flt16_t, FLT16_ORRS) (flt16_t a, flt16_t b)
{
    return  (
        (HALF_TYPE)
        {
            .U=((HALF_TYPE){.F=a}).U|((HALF_TYPE){.F=b}).U
        }
    ).F;
}

INLINE(flt16_t, FLT16_ORRN) (flt16_t a, flt16_t b)
{
    return  (
        (HALF_TYPE)
        {
            .U=((HALF_TYPE){.F=a}).U|~((HALF_TYPE){.F=b}).U
        }
    ).F;
}


INLINE(flt16_t, FLT16_XORS) (flt16_t a, flt16_t b)
{
    return  (
        (HALF_TYPE)
        {
            .U=((HALF_TYPE){.F=a}).U^((HALF_TYPE){.F=b}).U
        }
    ).F;
}

INLINE(flt16_t, FLT16_XORN) (flt16_t a, flt16_t b)
{
    return  (
        (HALF_TYPE)
        {
            .U=((HALF_TYPE){.F=a}).U^~((HALF_TYPE){.F=b}).U
        }
    ).F;
}


INLINE(float, FLT_ANDS) (float a, float b)
{
    return  (
        (WORD_TYPE)
        {
            .U=((WORD_TYPE){.F=a}).U&((WORD_TYPE){.F=b}).U
        }
    ).F;
}

INLINE(float, FLT_ANDN) (float a, float b)
{
    return  (
        (WORD_TYPE)
        {
            .U=((WORD_TYPE){.F=a}).U&~((WORD_TYPE){.F=b}).U
        }
    ).F;
}


INLINE(float, FLT_ORRS) (float a, float b)
{
    return  (
        (WORD_TYPE)
        {
            .U=((WORD_TYPE){.F=a}).U|((WORD_TYPE){.F=b}).U
        }
    ).F;
}

INLINE(float, FLT_ORRN) (float a, float b)
{
    return  (
        (WORD_TYPE)
        {
            .U=((WORD_TYPE){.F=a}).U|~((WORD_TYPE){.F=b}).U
        }
    ).F;
}


INLINE(float, FLT_XORS) (float a, float b)
{
    return  (
        (WORD_TYPE)
        {
            .U=((WORD_TYPE){.F=a}).U^((WORD_TYPE){.F=b}).U
        }
    ).F;
}

INLINE(float, FLT_XORN) (float a, float b)
{
    return  (
        (WORD_TYPE)
        {
            .U=((WORD_TYPE){.F=a}).U^~((WORD_TYPE){.F=b}).U
        }
    ).F;
}


INLINE(double, DBL_ANDS) (double a, double b)
{
    return  (
        (DWRD_TYPE)
        {
            .U=((DWRD_TYPE){.F=a}).U&((DWRD_TYPE){.F=b}).U
        }
    ).F;
}

INLINE(double, DBL_ANDN) (double a, double b)
{
    return  (
        (DWRD_TYPE)
        {
            .U=((DWRD_TYPE){.F=a}).U&~((DWRD_TYPE){.F=b}).U
        }
    ).F;
}


INLINE(double, DBL_ORRS) (double a, double b)
{
    return  (
        (DWRD_TYPE)
        {
            .U=((DWRD_TYPE){.F=a}).U|((DWRD_TYPE){.F=b}).U
        }
    ).F;
}

INLINE(double, DBL_ORRN) (double a, double b)
{
    return  (
        (DWRD_TYPE)
        {
            .U=((DWRD_TYPE){.F=a}).U|~((DWRD_TYPE){.F=b}).U
        }
    ).F;
}


INLINE(double, DBL_XORS) (double a, double b)
{
    return  (
        (DWRD_TYPE)
        {
            .U=((DWRD_TYPE){.F=a}).U^((DWRD_TYPE){.F=b}).U
        }
    ).F;
}

INLINE(double, DBL_XORN) (double a, double b)
{
    return  (
        (DWRD_TYPE)
        {
            .U=((DWRD_TYPE){.F=a}).U^~((DWRD_TYPE){.F=b}).U
        }
    ).F;
}

#define MY_VSET2(V, F, v, k0, k1) \
F(k1, F(k0, v, V##_K0), V##_K1)

#define MY_VSET4(V, F, v, k0, k1, k2, k3) \
F(k3, F(k2, MY_VSET2(V, F, v, k0, k1), V##_K2), V##_K3)

#define MY_VSET8(V, F, v, k0, k1, k2, k3, k4, k5, k6, k7) \
F(k7, F(k6, F(k5, F(k4, MY_VSET4(V, F, v, k0, k1, k2, k3), \
V##_K4), V##_K5), V##_K6), V##_K7)

/*  Putting these here to avoid 24+ if / else/endif blocks */

#if CHAR_MIN
#   define  DBC_ASYU        vreinterpret_u64_s8
#   define  DBC_ASBU        vreinterpret_u8_s8
#   define  DBC_ASBI(V)     (V)
#   define  DBC_ASHU        vreinterpret_u16_s8
#   define  DBC_ASHI        vreinterpret_s16_s8
#   define  DBC_ASHF        vreinterpret_f16_s8
#   define  DBC_ASWU        vreinterpret_u32_s8
#   define  DBC_ASWI        vreinterpret_s32_s8
#   define  DBC_ASWF        vreinterpret_f32_s8
#   define  DBC_ASDU        vreinterpret_u64_s8
#   define  DBC_ASDI        vreinterpret_s64_s8
#   define  DBC_ASDF        vreinterpret_f64_s8

#   define  QBC_ASYU        vreinterpretq_u64_s8
#   define  QBC_ASBU        vreinterpretq_u8_s8
#   define  QBC_ASBI(V)     (V)
#   define  QBC_ASHU        vreinterpretq_u16_s8
#   define  QBC_ASHI        vreinterpretq_s16_s8
#   define  QBC_ASHF        vreinterpretq_f16_s8
#   define  QBC_ASWU        vreinterpretq_u32_s8
#   define  QBC_ASWI        vreinterpretq_s32_s8
#   define  QBC_ASWF        vreinterpretq_f32_s8
#   define  QBC_ASDU        vreinterpretq_u64_s8
#   define  QBC_ASDI        vreinterpretq_s64_s8
#   define  QBC_ASDF        vreinterpretq_f64_s8

#else

#   define  DBC_ASYU        vreinterpret_u64_u8
#   define  DBC_ASBU(V)     (V)
#   define  DBC_ASBI        vreinterpret_s8_u8
#   define  DBC_ASHU        vreinterpret_u16_u8
#   define  DBC_ASHI        vreinterpret_s16_u8
#   define  DBC_ASHF        vreinterpret_f16_u8
#   define  DBC_ASWU        vreinterpret_u32_u8
#   define  DBC_ASWI        vreinterpret_s32_u8
#   define  DBC_ASWF        vreinterpret_f32_u8
#   define  DBC_ASDU        vreinterpret_u64_u8
#   define  DBC_ASDI        vreinterpret_s64_u8
#   define  DBC_ASDF        vreinterpret_f64_u8

#   define  QBC_ASYU        vreinterpretq_u64_u8
#   define  QBC_ASBU(V)     (V)
#   define  QBC_ASBI        vreinterpretq_s8_u8
#   define  QBC_ASHU        vreinterpretq_u16_u8
#   define  QBC_ASHI        vreinterpretq_s16_u8
#   define  QBC_ASHF        vreinterpretq_f16_u8
#   define  QBC_ASWU        vreinterpretq_u32_u8
#   define  QBC_ASWI        vreinterpretq_s32_u8
#   define  QBC_ASWF        vreinterpretq_f32_u8
#   define  QBC_ASDU        vreinterpretq_u64_u8
#   define  QBC_ASDI        vreinterpretq_s64_u8
#   define  QBC_ASDF        vreinterpretq_f64_u8
#endif

#if _ENTER_ARM_VOID
{
#endif

#define     FLT16_VOID  ((flt16_t) 0)

#define     WYU_VOID    (0.0f)
#define     WBU_VOID    (0.0f)
#define     WBI_VOID    (0.0f)
#define     WBC_VOID    (0.0f)
#define     WHU_VOID    (0.0f)
#define     WHI_VOID    (0.0f)
#define     WHF_VOID    (0.0f)
#define     WWU_VOID    (0.0f)
#define     WWI_VOID    (0.0f)
#define     WWF_VOID    (0.0f)

#define     DYU_VOID    ((uint64x1_t){0})
#define     DBU_VOID    ((uint8x8_t){0})
#define     DBI_VOID     ((int8x8_t){0})
#if CHAR_MIN
#   define  DBC_VOID     ((int8x8_t){0})
#else
#   define  DBC_VOID    ((uint8x8_t){0})
#endif

#define     DHU_VOID     ((uint16x4_t){0})
#define     DHI_VOID      ((int16x4_t){0})
#define     DHF_VOID    ((float16x4_t){0})
#define     DWU_VOID     ((uint32x2_t){0})
#define     DWI_VOID      ((int32x2_t){0})
#define     DWF_VOID    ((float32x2_t){0})
#define     DDU_VOID     ((uint64x1_t){0})
#define     DDI_VOID      ((int64x1_t){0})
#define     DDF_VOID    ((float64x1_t){0})

#define     QYU_VOID    ((uint64x2_t){0})
#define     QBU_VOID    ((uint8x16_t){0})
#define     QBI_VOID     ((int8x16_t){0})
#if CHAR_MIN
#   define  QBC_VOID     ((int8x16_t){0})
#else
#   define  QBC_VOID    ((uint8x16_t){0})
#endif

#define     QHU_VOID     ((uint16x8_t){0})
#define     QHI_VOID      ((int16x8_t){0})
#define     QHF_VOID    ((float16x8_t){0})
#define     QWU_VOID     ((uint32x4_t){0})
#define     QWI_VOID      ((int32x4_t){0})
#define     QWF_VOID    ((float32x4_t){0})
#define     QDU_VOID     ((uint64x2_t){0})
#define     QDI_VOID      ((int64x2_t){0})
#define     QDF_VOID    ((float64x2_t){0})

#define     VWYU_VOID   ((VWYU_TYPE){WYU_VOID})
#define     VWBU_VOID   ((VWBU_TYPE){WBU_VOID})
#define     VWBI_VOID   ((VWBI_TYPE){WBI_VOID})
#define     VWBC_VOID   ((VWBC_TYPE){WBC_VOID})
#define     VWHU_VOID   ((VWHU_TYPE){WHU_VOID})
#define     VWHI_VOID   ((VWHI_TYPE){WHI_VOID})
#define     VWHF_VOID   ((VWHF_TYPE){WHF_VOID})
#define     VWWU_VOID   ((VWWU_TYPE){WWU_VOID})
#define     VWWI_VOID   ((VWWI_TYPE){WWI_VOID})
#define     VWWF_VOID   ((VWWF_TYPE){WWF_VOID})

#define     VDYU_VOID   DYU_ASTV(DYU_VOID)
#define     VDBU_VOID   DBU_VOID
#define     VDBI_VOID   DBI_VOID
#define     VDBC_VOID   DBC_ASTV(DBC_VOID)
#define     VDHU_VOID   DHU_VOID
#define     VDHI_VOID   DHI_VOID
#define     VDHF_VOID   DHF_VOID
#define     VDWU_VOID   DWU_VOID
#define     VDWI_VOID   DWI_VOID
#define     VDWF_VOID   DWF_VOID
#define     VDDU_VOID   DDU_VOID
#define     VDDI_VOID   DDI_VOID
#define     VDDF_VOID   DDF_VOID

#define     VQYU_VOID   QYU_ASTV(QYU_VOID)
#define     VQBU_VOID   QBU_VOID
#define     VQBI_VOID   QBI_VOID
#define     VQBC_VOID   QBC_ASTV(QBC_VOID)
#define     VQHU_VOID   QHU_VOID
#define     VQHI_VOID   QHI_VOID
#define     VQHF_VOID   QHF_VOID
#define     VQWU_VOID   QWU_VOID
#define     VQWI_VOID   QWI_VOID
#define     VQWF_VOID   QWF_VOID
#define     VQDU_VOID   QDU_VOID
#define     VQDI_VOID   QDI_VOID
#define     VQDF_VOID   QDF_VOID

#if _LEAVE_ARM_VOID
}
#endif

#if _ENTER_ARM_ASTM
{
#endif

#define     VWYU_ASTM(V)    _Generic((V),VWYU_TYPE:(V).V0)
#define     VWBU_ASTM(V)    _Generic((V),VWBU_TYPE:(V).V0)
#define     VWBI_ASTM(V)    _Generic((V),VWBI_TYPE:(V).V0)
#define     VWBC_ASTM(V)    _Generic((V),VWBC_TYPE:(V).V0)
#define     VWHU_ASTM(V)    _Generic((V),VWHU_TYPE:(V).V0)
#define     VWHI_ASTM(V)    _Generic((V),VWHI_TYPE:(V).V0)
#define     VWHF_ASTM(V)    _Generic((V),VWHF_TYPE:(V).V0)
#define     VWWU_ASTM(V)    _Generic((V),VWWU_TYPE:(V).V0)
#define     VWWI_ASTM(V)    _Generic((V),VWWI_TYPE:(V).V0)
#define     VWWF_ASTM(V)    _Generic((V),VWWF_TYPE:(V).V0)

#define     VDYU_ASTM(V)    _Generic((V),VDYU_TYPE:(V).V0)
#define     VDBU_ASTM       VDBU_REQS
#define     VDBI_ASTM       VDBI_REQS
#define     VDBC_ASTM(V)    _Generic((V),VDBC_TYPE:(V).V0)
#define     VDHU_ASTM       VDHU_REQS
#define     VDHI_ASTM       VDHI_REQS
#define     VDHF_ASTM       VDHF_REQS
#define     VDWU_ASTM       VDWU_REQS
#define     VDWI_ASTM       VDWI_REQS
#define     VDWF_ASTM       VDWF_REQS
#define     VDDU_ASTM       VDDU_REQS
#define     VDDI_ASTM       VDDI_REQS
#define     VDDF_ASTM       VDDF_REQS

#define     VQYU_ASTM(V)    _Generic((V),VQYU_TYPE:(V).V0)
#define     VQBU_ASTM       VQBU_REQS
#define     VQBI_ASTM       VQBI_REQS
#define     VQBC_ASTM(V)    _Generic((V),VQBC_TYPE:(V).V0)
#define     VQHU_ASTM       VQHU_REQS
#define     VQHI_ASTM       VQHI_REQS
#define     VQHF_ASTM       VQHF_REQS
#define     VQWU_ASTM       VQWU_REQS
#define     VQWI_ASTM       VQWI_REQS
#define     VQWF_ASTM       VQWF_REQS
#define     VQDU_ASTM       VQDU_REQS
#define     VQDI_ASTM       VQDI_REQS
#define     VQDF_ASTM       VQDF_REQS
#if _LEAVE_ARM_ASTM
}
#endif

#if _ENTER_ARM_ASTV
{
#endif

#define     WYU_ASTV(M)     ((VWYU_TYPE){M})
#define     WBU_ASTV(M)     ((VWBU_TYPE){M})
#define     WBI_ASTV(M)     ((VWBI_TYPE){M})
#define     WBC_ASTV(M)     ((VWBC_TYPE){M})
#define     WHU_ASTV(M)     ((VWHU_TYPE){M})
#define     WHI_ASTV(M)     ((VWHI_TYPE){M})
#define     WHF_ASTV(M)     ((VWHF_TYPE){M})
#define     WWU_ASTV(M)     ((VWWU_TYPE){M})
#define     WWI_ASTV(M)     ((VWWI_TYPE){M})
#define     WWF_ASTV(M)     ((VWWF_TYPE){M})

#define     DYU_ASTV(M)     ((VDYU_TYPE){_Generic((M),VDYU_MTYPE:(M))})
#define     DBU_ASTV        VDBU_REQS
#define     DBI_ASTV        VDBI_REQS
#define     DBC_ASTV(M)     ((VDBC_TYPE){_Generic((M),VDBC_MTYPE:(M))})
#define     DHU_ASTV        VDHU_REQS
#define     DHI_ASTV        VDHI_REQS
#define     DHF_ASTV        VDHF_REQS
#define     DWU_ASTV        VDWU_REQS
#define     DWI_ASTV        VDWI_REQS
#define     DWF_ASTV        VDWF_REQS
#define     DDU_ASTV        VDDU_REQS
#define     DDI_ASTV        VDDI_REQS
#define     DDF_ASTV        VDDF_REQS

#define     QYU_ASTV(M)     ((VQYU_TYPE){M})
#define     QBU_ASTV        VQBU_REQS
#define     QBI_ASTV        VQBI_REQS
#define     QBC_ASTV(M)     ((VQBC_TYPE){M})
#define     QHU_ASTV        VQHU_REQS
#define     QHI_ASTV        VQHI_REQS
#define     QHF_ASTV        VQHF_REQS
#define     QWU_ASTV        VQWU_REQS
#define     QWI_ASTV        VQWI_REQS
#define     QWF_ASTV        VQWF_REQS
#define     QDU_ASTV        VQDU_REQS
#define     QDI_ASTV        VQDI_REQS
#define     QDF_ASTV        VQDF_REQS

INLINE(Vwwu,  UINT_ASTV)   (uint m)
{
#define     UINT_ASTV(M)     WWU_ASTV(((WORD_TYPE){.U=M}).F)
    return  UINT_ASTV(m);
}

INLINE(Vwwi,   INT_ASTV)    (int m)
{
#define     INT_ASTV(M)     WWI_ASTV(((WORD_TYPE){.I=M}).F)
    return  INT_ASTV(m);
}

INLINE(Vwwf,   FLT_ASTV)  (float m)
{
#define     FLT_ASTV(M)     WWF_ASTV(M)
    return  FLT_ASTV(m);
}


INLINE(Vddf,   DBL_ASTV) (double m) 
{
#define     DBL_ASTV(M)     vdup_n_f64(M)
    return  DBL_ASTV(m);
}

#if DWRD_NLONG == 2

INLINE(Vwwu, ULONG_ASTV)  (ulong m)
{
#define     ULONG_ASTV(M)   ((VWWU_TYPE){((WORD_TYPE){.U=M}).F})

    return  ULONG_ASTV(m);
}

INLINE(Vwwi,  LONG_ASTV)   (long m)
{
#define     LONG_ASTV(M)   ((VWWI_TYPE){((WORD_TYPE){.I=M}).F})
    return  LONG_ASTV(m);
}

#else

INLINE(Vddu, ULONG_ASTV)  (ulong m)
{
#define     ULONG_ASTV(M)   vdup_n_u64(M)
    return  ULONG_ASTV(m);
}

INLINE(Vddi,  LONG_ASTV)   (long m)
{
#define     LONG_ASTV(M)    vdup_n_s64(M)
    return  LONG_ASTV(m);
}

#endif

INLINE(Vddu,ULLONG_ASTV) (ullong m)
{
#define     ULLONG_ASTV(M)   vdup_n_u64(M)
    return  ULLONG_ASTV(m);
}

INLINE(Vddi, LLONG_ASTV)  (llong m)
{
#define     LLONG_ASTV(M)    vdup_n_s64(M)
    return  LLONG_ASTV(m);
}


INLINE(uint32_t,VWWU_ASTV) (Vwwu v)
{
#   define  VWWU_ASTV(V)    (((WORD_TYPE){.F=VWWU_ASTM(V)}).U)
    return  VWWU_ASTV(v);
}

INLINE( int32_t,VWWI_ASTV) (Vwwi v)
{
#   define  VWWI_ASTV(V)    (((WORD_TYPE){.F=VWWI_ASTM(V)}).I)
    return  VWWI_ASTV(v);
}

INLINE(   float,VWWF_ASTV) (Vwwf v)
{
#   define  VWWF_ASTV(V)    VWWF_ASTM(V)
    return  VWWF_ASTV(v);
}


INLINE(uint64_t,VDDU_ASTV) (Vddu v)
{
#   define  VDDU_ASTV(V)    vget_lane_u64(V,0)
    return  VDDU_ASTV(v);
}

INLINE( int64_t,VDDI_ASTV) (Vddi v)
{
#   define  VDDI_ASTV(V)    vget_lane_s64(V,0)
    return  VDDI_ASTV(v);
}

INLINE(  double,VDDF_ASTV) (Vddf v)
{
#   define  VDDF_ASTV(V)    vget_lane_f64(V,0)
    return  VDDF_ASTV(v);
}

#if _LEAVE_ARM_ASTV
}
#endif

#if _ENTER_ARM_ASTU
{
#endif

#if 0
//efine     DBU_ASTU
#define     DBI_ASTU        DBI_ASBU
#define     DBC_ASTU        DBC_ASBU
//efine     DHU_ASTU
#define     DHI_ASTU        DHI_ASHU
#define     DHF_ASTU        DHF_ASHU
//efine     DWU_ASTU
#define     DWI_ASTU        DWI_ASWU
#define     DWF_ASTU        DWF_ASWU
//efine     DDU_ASTU
#define     DDI_ASTU        DDI_ASDU
#define     DDF_ASTU        DDF_ASDU

//efine     QBU_ASTU
#define     QBI_ASTU        QBI_ASBU
#define     QBC_ASTU        QBC_ASBU
//efine     QHU_ASTU
#define     QHI_ASTU        QHI_ASHU
#define     QHF_ASTU        QHF_ASHU
//efine     QWU_ASTU
#define     QWI_ASTU        QWI_ASWU
#define     QWF_ASTU        QWF_ASWU
//efine     QDU_ASTU
#define     QDI_ASTU        QDI_ASDU
#define     QDF_ASTU        QDF_ASDU
#endif

//efine     VWBU_ASTU
#define     VWBI_ASTU       VWBI_ASBU
#define     VWBC_ASTU       VWBC_ASBU
//efine     VWHU_ASTU
#define     VWHI_ASTU       VWHI_ASHU
#define     VWHF_ASTU       VWHF_ASHU
//efine     VWWU_ASTU
#define     VWWI_ASTU       VWWI_ASWU
#define     VWWF_ASTU       VWWF_ASWU

//efine     VDBU_ASTU
#define     VDBI_ASTU       VDBI_ASBU
#define     VDBC_ASTU       VDBC_ASBU
//efine     VDHU_ASTU
#define     VDHI_ASTU       VDHI_ASHU
#define     VDHF_ASTU       VDHF_ASHU
//efine     VDWU_ASTU
#define     VDWI_ASTU       VDWI_ASWU
#define     VDWF_ASTU       VDWF_ASWU
//efine     VDDU_ASTU
#define     VDDI_ASTU       VDDI_ASDU
#define     VDDF_ASTU       VDDF_ASDU

//efine     VQBU_ASTU
#define     VQBI_ASTU       VQBI_ASBU
#define     VQBC_ASTU       VQBC_ASBU
//efine     VQHU_ASTU
#define     VQHI_ASTU       VQHI_ASHU
#define     VQHF_ASTU       VQHF_ASHU
//efine     VQWU_ASTU
#define     VQWI_ASTU       VQWI_ASWU
#define     VQWF_ASTU       VQWF_ASWU
//efine     VQDU_ASTU
#define     VQDI_ASTU       VQDI_ASDU
#define     VQDF_ASTU       VQDF_ASDU

#if _LEAVE_ARM_ASTU
}
#endif

#if _ENTER_ARM_ASTI
{
#endif

#define     DBU_ASTI        DBU_ASBI
//efine     DBI_ASTI
#define     DBC_ASTI        DBC_ASBI
#define     DHU_ASTI        DHU_ASHI
//efine     DHI_ASTI
#define     DHF_ASTI        DHF_ASHI
#define     DWU_ASTI        DWU_ASWI
//efine     DWI_ASTI
#define     DWF_ASTI        DWF_ASWI
#define     DDU_ASTI        DDU_ASDI
//efine     DFI_ASTI
#define     DDF_ASTI        DDF_ASDI

#define     QBU_ASTI        QBU_ASBI
//efine     QBI_ASTI
#define     QBC_ASTI        QBC_ASBI
#define     QHU_ASTI        QHU_ASHI
//efine     QHI_ASTI
#define     QHF_ASTI        QHF_ASHI
#define     QWU_ASTI        QWU_ASWI
//efine     QWI_ASTI
#define     QWF_ASTI        QWF_ASWI
#define     QDU_ASTI        QDU_ASDI
//efine     QDI_ASTI
#define     QDF_ASTI        QDF_ASDI

#define     VWBU_ASTI       VWBU_ASBI
//efine     VWBI_ASTI
#define     VWBC_ASTI       VWBC_ASBI
#define     VWHU_ASTI       VWHU_ASHI
//efine     VWHI_ASTI
#define     VWHF_ASTI       VWHF_ASHI
#define     VWWU_ASTI       VWWU_ASWI
//efine     VWWI_ASTI
#define     VWWF_ASTI       VWWF_ASWI

#define     VDBU_ASTI       VDBU_ASBI
//efine     VDBI_ASTI
#define     VDBC_ASTI       VDBC_ASBI
#define     VDHU_ASTI       VDHU_ASHI
//efine     VDHI_ASTI
#define     VDHF_ASTI       VDHF_ASHI
#define     VDWU_ASTI       VDWU_ASWI
//efine     VDWI_ASTI
#define     VDWF_ASTI       VDWF_ASWI
#define     VDDU_ASTI       VDDU_ASDI
//efine     VDDI_ASTI
#define     VDDF_ASTI       VDDF_ASDI

#define     VQBU_ASTI       VQBU_ASBI
//efine     VQBI_ASTI
#define     VQBC_ASTI       VQBC_ASBI
#define     VQHU_ASTI       VQHU_ASHI
//efine     VQHI_ASTI
#define     VQHF_ASTI       VQHF_ASHI
#define     VQWU_ASTI       VQWU_ASWI
//efine     VQWI_ASTI
#define     VQWF_ASTI       VQWF_ASWI
#define     VQDU_ASTI       VQDU_ASDI
//efine     VQDI_ASTI
#define     VQDF_ASTI       VQDF_ASDI
#if _LEAVE_ARM_ASTI
}
#endif

#if _ENTER_ARM_ASTF
{
#endif

#define     DHU_ASTF        DHU_ASHF
#define     DHI_ASTF        DHI_ASHF
//efine     DHF_ASTF
#define     DWU_ASTF        DWU_ASWF
#define     DWI_ASTF        DWI_ASWF
//efine     DWF_ASTF
#define     DDU_ASTF        DDU_ASDF
#define     DDI_ASTF        DDI_ASDF
//efine     DDF_ASTF

#define     QHU_ASTF        QHU_ASHF
#define     QHI_ASTF        QHI_ASHF
//efine     QHF_ASTF
#define     QWU_ASTF        QWU_ASWF
#define     QWI_ASTF        QWI_ASWF
//efine     QWF_ASTF
#define     QDU_ASTF        QDU_ASDF
#define     QDI_ASTF        QDI_ASDF
//efine     QDF_ASTF

#define     VWHU_ASTF       VWHU_ASHF
#define     VWHI_ASTF       VWHI_ASHF
//efine     VWHF_ASTF
#define     VWWU_ASTF       VWWU_ASWF
#define     VWWI_ASTF       VWWI_ASWF
//efine     VWWF_ASTF

#define     VDHU_ASTF       VDHU_ASHF
#define     VDHI_ASTF       VDHI_ASHF
//efine     VDHF_ASTF
#define     VDWU_ASTF       VDWU_ASWF
#define     VDWI_ASTF       VDWI_ASWF
//efine     VDWF_ASTF
#define     VDDU_ASTF       VDDU_ASDF
#define     VDDI_ASTF       VDDI_ASDF
//efine     VDDF_ASTF

#define     VQHU_ASTF       VQHU_ASHF
#define     VQHI_ASTF       VQHI_ASHF
//efine     VQHF_ASTF
#define     VQWU_ASTF       VQWU_ASWF
#define     VQWI_ASTF       VQWI_ASWF
//efine     VQWF_ASTF
#define     VQDU_ASTF       VQDU_ASDF
#define     VQDI_ASTF       VQDI_ASDF
//efine     VQDF_ASTF

#if _LEAVE_ARM_ASTF
}
#endif


#if _ENTER_ARM_ASYU
{
#endif

#define     DBU_ASYU        vreinterpret_u64_u8
#define     DBI_ASYU        vreinterpret_u64_s8
#define     DHU_ASYU        vreinterpret_u64_u16
#define     DHI_ASYU        vreinterpret_u64_s16
#define     DHF_ASYU        vreinterpret_u64_f16
#define     DWU_ASYU        vreinterpret_u64_u32
#define     DWI_ASYU        vreinterpret_u64_s32
#define     DWF_ASYU        vreinterpret_u64_f32
#define     DDU_ASYU        VDDU_REQS
#define     DDI_ASYU        vreinterpret_u64_s64
#define     DDF_ASYU        vreinterpret_u64_f64

#define     QBU_ASYU        vreinterpretq_u64_u8
#define     QBI_ASYU        vreinterpretq_u64_s8
#define     QHU_ASYU        vreinterpretq_u64_u16
#define     QHI_ASYU        vreinterpretq_u64_s16
#define     QHF_ASYU        vreinterpretq_u64_f16
#define     QWU_ASYU        vreinterpretq_u64_u32
#define     QWI_ASYU        vreinterpretq_u64_s32
#define     QWF_ASYU        vreinterpretq_u64_f32
#define     QDU_ASYU        VQDU_REQS
#define     QDI_ASYU        vreinterpretq_u64_s64
#define     QDF_ASYU        vreinterpretq_u64_f64

INLINE(Vwyu,VWBU_ASYU) (Vwbu v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWBI_ASYU) (Vwbi v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWBC_ASYU) (Vwbc v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWHU_ASYU) (Vwhu v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWHI_ASYU) (Vwhi v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWHF_ASYU) (Vwhf v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWWU_ASYU) (Vwwu v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWWI_ASYU) (Vwwi v) {return (VWYU_TYPE){v.V0};}
INLINE(Vwyu,VWWF_ASYU) (Vwwf v) {return (VWYU_TYPE){v.V0};}

#define     VWBU_ASYU(V)    WYU_ASTV(VWBU_ASTM(V))
#define     VWBI_ASYU(V)    WYU_ASTV(VWBI_ASTM(V))
#define     VWBC_ASYU(V)    WYU_ASTV(VWBC_ASTM(V))
#define     VWHU_ASYU(V)    WYU_ASTV(VWHU_ASTM(V))
#define     VWHI_ASYU(V)    WYU_ASTV(VWHI_ASTM(V))
#define     VWHF_ASYU(V)    WYU_ASTV(VWHF_ASTM(V))
#define     VWWU_ASYU(V)    WYU_ASTV(VWWU_ASTM(V))
#define     VWWI_ASYU(V)    WYU_ASTV(VWWI_ASTM(V))
#define     VWWF_ASYU(V)    WYU_ASTV(VWWF_ASTM(V))

INLINE(Vdyu,VDBU_ASYU) (Vdbu v)
{
#define     VDBU_ASYU(V)    DYU_ASTV(DBU_ASYU(VDBU_ASTM(V)))
    return  VDBU_ASYU(v);
}

INLINE(Vdyu,VDBI_ASYU) (Vdbi v)
{
#define     VDBI_ASYU(V)    DYU_ASTV(DBI_ASYU(VDBI_ASTM(V)))
    return  VDBI_ASYU(v);
}

INLINE(Vdyu,VDBC_ASYU) (Vdbc v)
{
#define     VDBC_ASYU(V)    DYU_ASTV(DBC_ASYU(VDBC_ASTM(V)))
    return  VDBC_ASYU(v);
}


INLINE(Vdyu,VDHU_ASYU) (Vdhu v)
{
#define     VDHU_ASYU(V)    DYU_ASTV(DHU_ASYU(VDHU_ASTM(V)))
    return  VDHU_ASYU(v);
}

INLINE(Vdyu,VDHI_ASYU) (Vdhi v)
{
#define     VDHI_ASYU(V)    DYU_ASTV(DHI_ASYU(VDHI_ASTM(V)))
    return  VDHI_ASYU(v);
}

INLINE(Vdyu,VDHF_ASYU) (Vdhf v)
{
#define     VDHF_ASYU(V)    DYU_ASTV(DHF_ASYU(VDHF_ASTM(V)))
    return  VDHF_ASYU(v);
}


INLINE(Vdyu,VDWU_ASYU) (Vdwu v)
{
#define     VDWU_ASYU(V)    DYU_ASTV(DWU_ASYU(VDWU_ASTM(V)))
    return  VDWU_ASYU(v);
}

INLINE(Vdyu,VDWI_ASYU) (Vdwi v)
{
#define     VDWI_ASYU(V)    DYU_ASTV(DWI_ASYU(VDWI_ASTM(V)))
    return  VDWI_ASYU(v);
}

INLINE(Vdyu,VDWF_ASYU) (Vdwf v)
{
#define     VDWF_ASYU(V)    DYU_ASTV(DWF_ASYU(VDWF_ASTM(V)))
    return  VDWF_ASYU(v);
}


INLINE(Vdyu,VDDU_ASYU) (Vddu v)
{
#define     VDDU_ASYU(V)    DYU_ASTV(V)
    return  VDDU_ASYU(v);
}

INLINE(Vdyu,VDDI_ASYU) (Vddi v)
{
#define     VDDI_ASYU(V)    DYU_ASTV(DDI_ASYU(V))
    return  VDDI_ASYU(v);
}

INLINE(Vdyu,VDDF_ASYU) (Vddf v)
{
#define     VDDF_ASYU(V)    DYU_ASTV(DDF_ASYU(V))
    return  VDDF_ASYU(v);
}


INLINE(Vqyu,VQBU_ASYU) (Vqbu v)
{
#define     VQBU_ASYU(V)    QYU_ASTV(QBU_ASYU(V))
    return  VQBU_ASYU(v);
}

INLINE(Vqyu,VQBI_ASYU) (Vqbi v)
{
#define     VQBI_ASYU(V)    QYU_ASTV(QBI_ASYU(V))
    return  VQBI_ASYU(v);
}

INLINE(Vqyu,VQBC_ASYU) (Vqbc v)
{
#define     VQBC_ASYU(V)    QYU_ASTV(QBC_ASYU(VQBC_ASTM(V)))
    return  VQBC_ASYU(v);
}


INLINE(Vqyu,VQHU_ASYU) (Vqhu v)
{
#define     VQHU_ASYU(V)    QYU_ASTV(QHU_ASYU(V))
    return  VQHU_ASYU(v);
}

INLINE(Vqyu,VQHI_ASYU) (Vqhi v)
{
#define     VQHI_ASYU(V)    QYU_ASTV(QHI_ASYU(V))
    return  VQHI_ASYU(v);
}

INLINE(Vqyu,VQHF_ASYU) (Vqhf v)
{
#define     VQHF_ASYU(V)    QYU_ASTV(QHF_ASYU(V))
    return  VQHF_ASYU(v);
}


INLINE(Vqyu,VQWU_ASYU) (Vqwu v)
{
#define     VQWU_ASYU(V)    QYU_ASTV(QWU_ASYU(V))
    return  VQWU_ASYU(v);
}

INLINE(Vqyu,VQWI_ASYU) (Vqwi v)
{
#define     VQWI_ASYU(V)    QYU_ASTV(QWI_ASYU(V))
    return  VQWI_ASYU(v);
}

INLINE(Vqyu,VQWF_ASYU) (Vqwf v)
{
#define     VQWF_ASYU(V)    QYU_ASTV(QWF_ASTV(V))
    return  VQWF_ASYU(v);
}


INLINE(Vqyu,VQDU_ASYU) (Vqdu v)
{
#define     VQDU_ASYU(V)    QYU_ASTV(V)
    return  VQDU_ASYU(v);
}

INLINE(Vqyu,VQDI_ASYU) (Vqdi v)
{
#define     VQDI_ASYU(V)    QYU_ASTV(QDI_ASYU(V))
    return  VQDI_ASYU(v);
}

INLINE(Vqyu,VQDF_ASYU) (Vqdf v)
{
#define     VQDF_ASYU(V)    QYU_ASTV(QDF_ASYU(V))
    return  VQDF_ASYU(v);
}

#if _LEAVE_ARM_ASYU
}
#endif

#if _ENTER_ARM_ASBU
{
#endif

#define     DYU_ASBU        vreinterpret_u8_u64
//efine     DBU_ASBU
#define     DBI_ASBU        vreinterpret_u8_s8
//efine     DBC_ASBU
#define     DHU_ASBU        vreinterpret_u8_u16
#define     DHI_ASBU        vreinterpret_u8_s16
#define     DHF_ASBU        vreinterpret_u8_f16
#define     DWU_ASBU        vreinterpret_u8_u32
#define     DWI_ASBU        vreinterpret_u8_s32
#define     DWF_ASBU        vreinterpret_u8_f32
#define     DDU_ASBU        vreinterpret_u8_u64
#define     DDI_ASBU        vreinterpret_u8_s64
#define     DDF_ASBU        vreinterpret_u8_f64

#define     QYU_ASBU        vreinterpretq_u8_u64
//efine     QBU_ASBU
#define     QBI_ASBU        vreinterpretq_u8_s8
//efine     QBC_ASBU
#define     QHU_ASBU        vreinterpretq_u8_u16
#define     QHU_ASBU        vreinterpretq_u8_u16
#define     QHI_ASBU        vreinterpretq_u8_s16
#define     QHF_ASBU        vreinterpretq_u8_f16
#define     QWU_ASBU        vreinterpretq_u8_u32
#define     QWI_ASBU        vreinterpretq_u8_s32
#define     QWF_ASBU        vreinterpretq_u8_f32
#define     QDU_ASBU        vreinterpretq_u8_u64
#define     QDI_ASBU        vreinterpretq_u8_s64
#define     QDF_ASBU        vreinterpretq_u8_f64

INLINE(Vwbu,VWYU_ASBU) (Vwyu v)
{
#define     VWYU_ASBU(V)    WBU_ASTV(VWYU_ASTM(V))
    return  (Vwbu){VWYU_ASTM(v)};
}

INLINE(Vwbu,VWBI_ASBU) (Vwbi v)
{
#   define  VWBI_ASBU(V)    WBU_ASTV(VWBI_ASTM(V))
    return  VWBI_ASBU(v);
}

INLINE(Vwbu,VWBC_ASBU) (Vwbc v)
{
#   define  VWBC_ASBU(V)    WBU_ASTV(VWBC_ASTM(V))
    return  VWBC_ASBU(v);
}


INLINE(Vwbu,VWHU_ASBU) (Vwhu v)
{
#define     VWHU_ASBU(V)    WBU_ASTV(VWHU_ASTM(V))
    return  VWHU_ASBU(v);
}

INLINE(Vwbu,VWHI_ASBU) (Vwhi v)
{
#define     VWHI_ASBU(V)    WBU_ASTV(VWHI_ASTM(V))
    return  VWHI_ASBU(v);
}

INLINE(Vwbu,VWHF_ASBU) (Vwhf v)
{
#define     VWHF_ASBU(V)    WBU_ASTV(VWHF_ASTM(V))
    return  VWHF_ASBU(v);
}


INLINE(Vwbu,VWWU_ASBU) (Vwwu v)
{
#define     VWWU_ASBU(V)    WBU_ASTV(VWWU_ASTM(V))
    return  VWWU_ASBU(v);
}

INLINE(Vwbu,VWWI_ASBU) (Vwwi v)
{
#define     VWWI_ASBU(V)    WBU_ASTV(VWWI_ASTM(V))
    return  VWWI_ASBU(v);
}

INLINE(Vwbu,VWWF_ASBU) (Vwwf v)
{
#define     VWWF_ASBU(V)    WBU_ASTV(VWWF_ASTM(V))
    return  VWWF_ASBU(v);
}


INLINE(Vdbu,VDYU_ASBU) (Vdyu v)
{
#define     VDYU_ASBU(V)    DYU_ASBU(VDYU_ASTM(V))
    return  VDYU_ASBU(v);
}

INLINE(Vdbu,VDBI_ASBU) (Vdbi v)
{
    return  vreinterpret_u8_s8(v);
}

INLINE(Vdbu,VDBC_ASBU) (Vdbc v)
{
#   define  VDBC_ASBU(V)    DBC_ASBU(VDBC_ASTM(V))
    return  VDBC_ASBU(v);
}

INLINE(Vdbu,VDHU_ASBU) (Vdhu v) {return vreinterpret_u8_u16(v);}
INLINE(Vdbu,VDHI_ASBU) (Vdhi v) {return vreinterpret_u8_s16(v);}
INLINE(Vdbu,VDHF_ASBU) (Vdhf v) {return vreinterpret_u8_f16(v);}
INLINE(Vdbu,VDWU_ASBU) (Vdwu v) {return vreinterpret_u8_u32(v);}
INLINE(Vdbu,VDWI_ASBU) (Vdwi v) {return vreinterpret_u8_s32(v);}
INLINE(Vdbu,VDWF_ASBU) (Vdwf v) {return vreinterpret_u8_f32(v);}
INLINE(Vdbu,VDDU_ASBU) (Vddu v) {return vreinterpret_u8_u64(v);}
INLINE(Vdbu,VDDI_ASBU) (Vddi v) {return vreinterpret_u8_s64(v);}
INLINE(Vdbu,VDDF_ASBU) (Vddf v) {return vreinterpret_u8_f64(v);}


INLINE(Vqbu,VQYU_ASBU) (Vqyu v)
{
#define     VQYU_ASBU(V)    QYU_ASBU(VQYU_ASTM(V))
    return  VQYU_ASBU(v);
}

INLINE(Vqbu,VQBI_ASBU) (Vqbi v) {return vreinterpretq_u8_s8(v);}
INLINE(Vqbu,VQBC_ASBU) (Vqbc v)
{
#   define  VQBC_ASBU(V)    QBC_ASBU(VQBC_ASTM(V))
    return  VQBC_ASBU(v);
}

INLINE(Vqbu,VQHU_ASBU) (Vqhu v) {return vreinterpretq_u8_u16(v);}
INLINE(Vqbu,VQHI_ASBU) (Vqhi v) {return vreinterpretq_u8_s16(v);}
INLINE(Vqbu,VQHF_ASBU) (Vqhf v) {return vreinterpretq_u8_f16(v);}
INLINE(Vqbu,VQWU_ASBU) (Vqwu v) {return vreinterpretq_u8_u32(v);}
INLINE(Vqbu,VQWI_ASBU) (Vqwi v) {return vreinterpretq_u8_s32(v);}
INLINE(Vqbu,VQWF_ASBU) (Vqwf v) {return vreinterpretq_u8_f32(v);}
INLINE(Vqbu,VQDU_ASBU) (Vqdu v) {return vreinterpretq_u8_u64(v);}
INLINE(Vqbu,VQDI_ASBU) (Vqdi v) {return vreinterpretq_u8_s64(v);}
INLINE(Vqbu,VQDF_ASBU) (Vqdf v) {return vreinterpretq_u8_f64(v);}

#if _LEAVE_ARM_ASBU
}
#endif

#if _ENTER_ARM_ASBI
{
#endif

#define     DYU_ASBI        vreinterpret_s8_u64
#define     DBU_ASBI        vreinterpret_s8_u8
//          DBC_ASBI
#define     DHU_ASBI        vreinterpret_s8_u16
#define     DHI_ASBI        vreinterpret_s8_s16
#define     DHF_ASBI        vreinterpret_s8_f16
#define     DWU_ASBI        vreinterpret_s8_u32
#define     DWI_ASBI        vreinterpret_s8_s32
#define     DWF_ASBI        vreinterpret_s8_f32
#define     DDU_ASBI        vreinterpret_s8_u64
#define     DDI_ASBI        vreinterpret_s8_s64
#define     DDF_ASBI        vreinterpret_s8_f64

#define     QYU_ASBI        vreinterpretq_s8_u64
#define     QBU_ASBI        vreinterpretq_s8_u8
#define     QHU_ASBI        vreinterpretq_s8_u16
#define     QHI_ASBI        vreinterpretq_s8_s16
#define     QHF_ASBI        vreinterpretq_s8_f16
#define     QWU_ASBI        vreinterpretq_s8_u32
#define     QWI_ASBI        vreinterpretq_s8_s32
#define     QWF_ASBI        vreinterpretq_s8_f32
#define     QDU_ASBI        vreinterpretq_s8_u64
#define     QDI_ASBI        vreinterpretq_s8_s64
#define     QDF_ASBI        vreinterpretq_s8_f64

INLINE(Vwbi,VWYU_ASBI) (Vwyu v)
{
#define     VWYU_ASBI(V)    WBI_ASTV(VWYU_ASTM(V))
    return  VWYU_ASBI(v);
}

INLINE(Vwbi,VWBU_ASBI) (Vwbu v)
{
#   define  VWBU_ASBI(V)    WBI_ASTV(VWBU_ASTM(V))
    return  VWBU_ASBI(v);
}

INLINE(Vwbi,VWBC_ASBI) (Vwbc v)
{
#   define  VWBC_ASBI(V)    WBI_ASTV(VWBC_ASTM(V))
    return  VWBC_ASBI(v);
}


INLINE(Vwbi,VWHU_ASBI) (Vwhu v)
{
#define     VWHU_ASBI(V)    WBI_ASTV(VWHU_ASTM(V))
    return  VWHU_ASBI(v);
}

INLINE(Vwbi,VWHI_ASBI) (Vwhi v)
{
#define     VWHI_ASBI(V)    WBI_ASTV(VWHI_ASTM(V))
    return  VWHI_ASBI(v);
}

INLINE(Vwbi,VWHF_ASBI) (Vwhf v)
{
#define     VWHF_ASBI(V)    WBI_ASTV(VWHF_ASTM(V))
    return  VWHF_ASBI(v);
}


INLINE(Vwbi,VWWU_ASBI) (Vwwu v)
{
#define     VWWU_ASBI(V)    WBI_ASTV(VWWU_ASTM(V))
    return  VWWU_ASBI(v);
}

INLINE(Vwbi,VWWI_ASBI) (Vwwi v)
{
#define     VWWI_ASBI(V)    WBI_ASTV(VWWI_ASTM(V))
    return  VWWI_ASBI(v);
}

INLINE(Vwbi,VWWF_ASBI) (Vwwf v)
{
#define     VWWF_ASBI(V)    WBI_ASTV(VWWF_ASTM(V))
    return  VWWF_ASBI(v);
}


INLINE(Vdbi,VDYU_ASBI) (Vdyu v)
{
#define     VDYU_ASBI(V)    DYU_ASBI(VDYU_ASTM(V))
    return  VDYU_ASBI(v);
}

INLINE(Vdbi,VDBU_ASBI) (Vdbu v) {return vreinterpret_s8_u8(v);}

INLINE(Vdbi,VDBC_ASBI) (Vdbc v)
{
#define     VDBC_ASBI(V)    DBC_ASBI(VDBC_ASTM(V))
    return  VDBC_ASBI(v);
}

INLINE(Vdbi,VDHU_ASBI) (Vdhu v) {return vreinterpret_s8_u16(v);}
INLINE(Vdbi,VDHI_ASBI) (Vdhi v) {return vreinterpret_s8_s16(v);}
INLINE(Vdbi,VDHF_ASBI) (Vdhf v) {return vreinterpret_s8_f16(v);}
INLINE(Vdbi,VDWU_ASBI) (Vdwu v) {return vreinterpret_s8_u32(v);}
INLINE(Vdbi,VDWI_ASBI) (Vdwi v) {return vreinterpret_s8_s32(v);}
INLINE(Vdbi,VDWF_ASBI) (Vdwf v) {return vreinterpret_s8_f32(v);}
INLINE(Vdbi,VDDU_ASBI) (Vddu v) {return vreinterpret_s8_u64(v);}
INLINE(Vdbi,VDDI_ASBI) (Vddi v) {return vreinterpret_s8_s64(v);}
INLINE(Vdbi,VDDF_ASBI) (Vddf v) {return vreinterpret_s8_f64(v);}

INLINE(Vqbi,VQYU_ASBI) (Vqyu v)
{
#define     VQYU_ASBI(V)    QYU_ASBI(VQYU_ASTM(V))
    return  VQYU_ASBI(v);
}

INLINE(Vqbi,VQBU_ASBI) (Vqbu v) {return vreinterpretq_s8_u8(v);}

INLINE(Vqbi,VQBC_ASBI) (Vqbc v)
{
#   define  VQBC_ASBI(V)    QBC_ASBI(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqbi,VQHU_ASBI) (Vqhu v) {return vreinterpretq_s8_u16(v);}
INLINE(Vqbi,VQHI_ASBI) (Vqhi v) {return vreinterpretq_s8_s16(v);}
INLINE(Vqbi,VQHF_ASBI) (Vqhf v) {return vreinterpretq_s8_f16(v);}
INLINE(Vqbi,VQWU_ASBI) (Vqwu v) {return vreinterpretq_s8_u32(v);}
INLINE(Vqbi,VQWI_ASBI) (Vqwi v) {return vreinterpretq_s8_s32(v);}
INLINE(Vqbi,VQWF_ASBI) (Vqwf v) {return vreinterpretq_s8_f32(v);}
INLINE(Vqbi,VQDU_ASBI) (Vqdu v) {return vreinterpretq_s8_u64(v);}
INLINE(Vqbi,VQDI_ASBI) (Vqdi v) {return vreinterpretq_s8_s64(v);}
INLINE(Vqbi,VQDF_ASBI) (Vqdf v) {return vreinterpretq_s8_f64(v);}

#if _LEAVE_ARM_ASBI
}
#endif

#if _ENTER_ARM_ASBC
{
#endif

#if CHAR_MIN

#   define  DYU_ASBC        vreinterpret_s8_u64
#   define  DBU_ASBC        vreinterpret_s8_u8
#   define  DBI_ASBC(M)     (M)
//  define  DBC_ASBC
#   define  DHU_ASBC        vreinterpret_s8_u16
#   define  DHI_ASBC        vreinterpret_s8_s16
#   define  DHF_ASBC        vreinterpret_s8_f16
#   define  DWU_ASBC        vreinterpret_s8_u32
#   define  DWI_ASBC        vreinterpret_s8_s32
#   define  DWF_ASBC        vreinterpret_s8_f32
#   define  DDU_ASBC        vreinterpret_s8_u64
#   define  DDI_ASBC        vreinterpret_s8_s64
#   define  DDF_ASBC        vreinterpret_s8_f64

#   define  QYU_ASBC        vreinterpretq_s8_u64
#   define  QBU_ASBC        vreinterpretq_s8_u8
#   define  QBI_ASBC(M)     (M)
//  define  QBC_ASBC
#   define  QHU_ASBC        vreinterpretq_s8_u16
#   define  QHI_ASBC        vreinterpretq_s8_s16
#   define  QHF_ASBC        vreinterpretq_s8_f16
#   define  QWU_ASBC        vreinterpretq_s8_u32
#   define  QWI_ASBC        vreinterpretq_s8_s32
#   define  QWF_ASBC        vreinterpretq_s8_f32
#   define  QDU_ASBC        vreinterpretq_s8_u64
#   define  QDI_ASBC        vreinterpretq_s8_s64
#   define  QDF_ASBC        vreinterpretq_s8_f64

#else

#   define  DYU_ASBC        vreinterpret_u8_u64
#   define  DBU_ASBC(M)     (M)
#   define  DBI_ASBC        vreinterpret_u8_s8
//  define  DBC_ASBC
#   define  DHU_ASBC        vreinterpret_u8_u16
#   define  DHI_ASBC        vreinterpret_u8_s16
#   define  DHF_ASBC        vreinterpret_u8_f16
#   define  DWU_ASBC        vreinterpret_u8_u32
#   define  DWI_ASBC        vreinterpret_u8_s32
#   define  DWF_ASBC        vreinterpret_u8_f32
#   define  DDU_ASBC        vreinterpret_u8_u64
#   define  DDI_ASBC        vreinterpret_u8_s64
#   define  DDF_ASBC        vreinterpret_u8_f64

#   define  QYU_ASBC        vreinterpretq_u8_u64
#   define  QBU_ASBC(M)     (M)
#   define  QBI_ASBC        vreinterpretq_u8_s8
//  define  QBC_ASBC
#   define  QHU_ASBC        vreinterpretq_u8_u16
#   define  QHI_ASBC        vreinterpretq_u8_s16
#   define  QHF_ASBC        vreinterpretq_u8_f16
#   define  QWU_ASBC        vreinterpretq_u8_u32
#   define  QWI_ASBC        vreinterpretq_u8_s32
#   define  QWF_ASBC        vreinterpretq_u8_f32
#   define  QDU_ASBC        vreinterpretq_u8_u64
#   define  QDI_ASBC        vreinterpretq_u8_s64
#   define  QDF_ASBC        vreinterpretq_u8_f64

#endif

INLINE(Vwbc,VWYU_ASBC) (Vwyu v)
{
#define     VWYU_ASBC(V)    WBC_ASTV(VWYU_ASTM(V))
    return  VWYU_ASBC(v);
}

INLINE(Vwbc,VWBU_ASBC) (Vwbu v)
{
#define     VWBU_ASBC(V)    WBC_ASTV(VWBU_ASTM(V))
    return  VWBU_ASBC(v);
}

INLINE(Vwbc,VWBI_ASBC) (Vwbi v)
{
#define     VWBI_ASBC(V)    WBC_ASTV(VWBI_ASTM(V))
    return                  WBC_ASTV(VWBI_ASTM(v));
}


INLINE(Vwbc,VWHU_ASBC) (Vwhu v)
{
#define     VWHU_ASBC(V)    WBC_ASTV(VWHU_ASTM(V))
    return                  WBC_ASTV(VWHU_ASTM(v));
}

INLINE(Vwbc,VWHI_ASBC) (Vwhi v)
{
#define     VWHI_ASBC(V)    WBC_ASTV(VWHI_ASTM(V))
    return                  WBC_ASTV(VWHI_ASTM(v));
}

INLINE(Vwbc,VWHF_ASBC) (Vwhf v)
{
#define     VWHF_ASBC(V)    WBC_ASTV(VWHF_ASTM(V))
    return                  WBC_ASTV(VWHF_ASTM(v));
}


INLINE(Vwbc,VWWU_ASBC) (Vwwu v)
{
#define     VWWU_ASBC(V)    WBC_ASTV(VWWU_ASTM(V))
    return                  WBC_ASTV(VWWU_ASTM(v));
}

INLINE(Vwbc,VWWI_ASBC) (Vwwi v)
{
#define     VWWI_ASBC(V)    WBC_ASTV(VWWI_ASTM(V))
    return                  WBC_ASTV(VWWI_ASTM(v));
}

INLINE(Vwbc,VWWF_ASBC) (Vwwf v)
{
#define     VWWF_ASBC(V)    WBC_ASTV(VWWF_ASTM(V))
    return                  WBC_ASTV(VWWF_ASTM(v));
}


INLINE(Vdbc,VDYU_ASBC) (Vdyu v)
{
#define     VDYU_ASBC(V)    DBC_ASTV(DYU_ASBC(VDYU_ASTM(V)))
    return  VDYU_ASBC(v);
}

INLINE(Vdbc,VDBU_ASBC) (Vdbu v)
{
#define     VDBU_ASBC(V)    DBC_ASTV(DBU_ASBC(V))
    return  VDBU_ASBC(v);
}

INLINE(Vdbc,VDBI_ASBC) (Vdbi v)
{
#define     VDBI_ASBC(V)    DBC_ASTV(DBI_ASBC(V))
    return  VDBI_ASBC(v);
}


INLINE(Vdbc,VDHU_ASBC) (Vdhu v)
{
#define     VDHU_ASBC(V)    DBC_ASTV(DHU_ASBC(V))
    return  VDHU_ASBC(v);
}

INLINE(Vdbc,VDHI_ASBC) (Vdhi v)
{
#define     VDHI_ASBC(V)    DBC_ASTV(DHI_ASBC(V))
    return  VDHI_ASBC(v);
}

INLINE(Vdbc,VDHF_ASBC) (Vdhf v)
{
#define     VDHF_ASBC(V)    DBC_ASTV(DHF_ASBC(V))
    return  VDHF_ASBC(v);
}


INLINE(Vdbc,VDWU_ASBC) (Vdwu v)
{
#define     VDWU_ASBC(V)    DBC_ASTV(DWU_ASBC(V))
    return  VDWU_ASBC(v);
}

INLINE(Vdbc,VDWI_ASBC) (Vdwi v)
{
#define     VDWI_ASBC(V)    DBC_ASTV(DWI_ASBC(V))
    return  VDWI_ASBC(v);
}

INLINE(Vdbc,VDWF_ASBC) (Vdwf v)
{
#define     VDWF_ASBC(V)    DBC_ASTV(DWF_ASBC(V))
    return  VDWF_ASBC(v);
}


INLINE(Vdbc,VDDU_ASBC) (Vddu v)
{
#define     VDDU_ASBC(V)    DBC_ASTV(DDU_ASBC(V))
    return  VDDU_ASBC(v);
}

INLINE(Vdbc,VDDI_ASBC) (Vddi v)
{
#define     VDDI_ASBC(V)    DBC_ASTV(DDI_ASBC(V))
    return  VDDI_ASBC(v);
}

INLINE(Vdbc,VDDF_ASBC) (Vddf v)
{
#define     VDDF_ASBC(V)    DBC_ASTV(DDF_ASBC(VDDF_ASTM(V)))
    return  VDDF_ASBC(v);
}


INLINE(Vqbc,VQYU_ASBC) (Vqyu v)
{
#define     VQYU_ASBC(V)    QBC_ASTV(QYU_ASBC(VQYU_ASTM(V)))
    return  VQYU_ASBC(v);
}

INLINE(Vqbc,VQBU_ASBC) (Vqbu v)
{
#define     VQBU_ASBC(V)    QBC_ASTV(QBU_ASBC(V))
    return  VQBU_ASBC(v);
}

INLINE(Vqbc,VQBI_ASBC) (Vqbi v)
{
#define     VQBI_ASBC(V)    QBC_ASTV(QBI_ASBC(V))
    return  VQBI_ASBC(v);
}


INLINE(Vqbc,VQHU_ASBC) (Vqhu v)
{
#define     VQHU_ASBC(V)    QBC_ASTV(QHU_ASBC(V))
    return  VQHU_ASBC(v);
}

INLINE(Vqbc,VQHI_ASBC) (Vqhi v)
{
#define     VQHI_ASBC(V)    QBC_ASTV(QHI_ASBC(V))
    return  VQHI_ASBC(v);
}

INLINE(Vqbc,VQHF_ASBC) (Vqhf v)
{
#define     VQHF_ASBC(V)    QBC_ASTV(QHF_ASBC(V))
    return  VQHF_ASBC(v);
}


INLINE(Vqbc,VQWU_ASBC) (Vqwu v)
{
#define     VQWU_ASBC(V)    QBC_ASTV(QWU_ASBC(V))
    return  VQWU_ASBC(v);
}

INLINE(Vqbc,VQWI_ASBC) (Vqwi v)
{
#define     VQWI_ASBC(V)    QBC_ASTV(QWI_ASBC(V))
    return  VQWI_ASBC(v);
}

INLINE(Vqbc,VQWF_ASBC) (Vqwf v)
{
#define     VQWF_ASBC(V)    QBC_ASTV(QWF_ASBC(V))
    return  VQWF_ASBC(v);
}


INLINE(Vqbc,VQDU_ASBC) (Vqdu v)
{
#define     VQDU_ASBC(V)    QBC_ASTV(QDU_ASBC(V))
    return  VQDU_ASBC(v);
}

INLINE(Vqbc,VQDI_ASBC) (Vqdi v)
{
#define     VQDI_ASBC(V)    QBC_ASTV(QDI_ASBC(V))
    return  VQDI_ASBC(v);
}

INLINE(Vqbc,VQDF_ASBC) (Vqdf v)
{
#define     VQDF_ASBC(V)    QBC_ASTV(QDF_ASBC(V))
    return  VQDF_ASBC(v);
}

#if _LEAVE_ARM_ASBC
}
#endif


#if _ENTER_ARM_ASHU
{
#endif

#define     DYU_ASHU        vreinterpret_u16_u64
#define     DBU_ASHU        vreinterpret_u16_u8
#define     DBI_ASHU        vreinterpret_u16_s8
//efine     DBC_ASHU
//efine     DHU_ASHU
#define     DHI_ASHU        vreinterpret_u16_s16
#define     DHF_ASHU        vreinterpret_u16_f16
#define     DWU_ASHU        vreinterpret_u16_u32
#define     DWI_ASHU        vreinterpret_u16_s32
#define     DWF_ASHU        vreinterpret_u16_f32
#define     DDU_ASHU        vreinterpret_u16_u64
#define     DDI_ASHU        vreinterpret_u16_s64
#define     DDF_ASHU        vreinterpret_u16_f64

#define     QYU_ASHU        vreinterpretq_u16_u64
#define     QBU_ASHU        vreinterpretq_u16_u8
#define     QBI_ASHU        vreinterpretq_u16_s8
//efine     QBC_ASHU
//efine     QHU_ASHU
#define     QHI_ASHU        vreinterpretq_u16_s16
#define     QHF_ASHU        vreinterpretq_u16_f16
#define     QWU_ASHU        vreinterpretq_u16_u32
#define     QWI_ASHU        vreinterpretq_u16_s32
#define     QWF_ASHU        vreinterpretq_u16_f32
#define     QDU_ASHU        vreinterpretq_u16_u64
#define     QDI_ASHU        vreinterpretq_u16_s64
#define     QDF_ASHU        vreinterpretq_u16_f64


INLINE(Vwhu,VWYU_ASHU) (Vwyu v)
{
#define     VWYU_ASHU(V)    WHU_ASTV(VWYU_ASTM(V))
    return  VWYU_ASHU(v);
}

INLINE(Vwhu,VWBU_ASHU) (Vwbu v)
{
#   define  VWBU_ASHU(V)    WHU_ASTV(VWBU_ASTM(V))
    return  VWBU_ASHU(v);
}

INLINE(Vwhu,VWBI_ASHU) (Vwbi v)
{
#define     VWBI_ASHU(V)    WHU_ASTV(VWBI_ASTM(V))
    return  VWBI_ASHU(v);
}

INLINE(Vwhu,VWBC_ASHU) (Vwbc v)
{
#   define  VWBC_ASHU(V)    WHU_ASTV(VWBC_ASTM(V))
    return  VWBC_ASHU(v);
}


INLINE(Vwhu,VWHI_ASHU) (Vwhi v)
{
#define     VWHI_ASHU(V)    WHU_ASTV(VWHI_ASTM(V))
    return  VWHI_ASHU(v);
}

INLINE(Vwhu,VWHF_ASHU) (Vwhf v)
{
#define     VWHF_ASHU(V)    WHU_ASTV(VWHF_ASTM(V))
    return  VWHF_ASHU(v);
}


INLINE(Vwhu,VWWU_ASHU) (Vwwu v)
{
#define     VWWU_ASHU(V)    WHU_ASTV(VWWU_ASTM(V))
    return  VWWU_ASHU(v);
}

INLINE(Vwhu,VWWI_ASHU) (Vwwi v)
{
#define     VWWI_ASHU(V)    WHU_ASTV(VWWI_ASTM(V))
    return  VWWI_ASHU(v);
}

INLINE(Vwhu,VWWF_ASHU) (Vwwf v)
{
#define     VWWF_ASHU(V)    WHU_ASTV(VWWF_ASTM(V))
    return  VWWF_ASHU(v);
}


INLINE(Vdhu,VDYU_ASHU) (Vdyu v)
{
#define     VDYU_ASHU(V)    DYU_ASHU(VDYU_ASTM(V))
    return                  DYU_ASHU(VDYU_ASTM(v));
}

INLINE(Vdhu,VDBU_ASHU) (Vdbu v) {return vreinterpret_u16_u8(v);}
INLINE(Vdhu,VDBI_ASHU) (Vdbi v) {return vreinterpret_u16_s8(v);}
INLINE(Vdhu,VDBC_ASHU) (Vdbc v)
{
#define     VDBC_ASHU(V)    DBC_ASHU(VDBC_ASTM(V))
    return  VDBC_ASHU(v);
}

INLINE(Vdhu,VDHI_ASHU) (Vdhi v) {return vreinterpret_u16_s16(v);}
INLINE(Vdhu,VDHF_ASHU) (Vdhf v) {return vreinterpret_u16_f16(v);}
INLINE(Vdhu,VDWU_ASHU) (Vdwu v) {return vreinterpret_u16_u32(v);}
INLINE(Vdhu,VDWI_ASHU) (Vdwi v) {return vreinterpret_u16_s32(v);}
INLINE(Vdhu,VDWF_ASHU) (Vdwf v) {return vreinterpret_u16_f32(v);}
INLINE(Vdhu,VDDU_ASHU) (Vddu v) {return vreinterpret_u16_u64(v);}
INLINE(Vdhu,VDDI_ASHU) (Vddi v) {return vreinterpret_u16_s64(v);}
INLINE(Vdhu,VDDF_ASHU) (Vddf v) {return vreinterpret_u16_f64(v);}


INLINE(Vqhu,VQYU_ASHU) (Vqyu v)
{
#define     VQYU_ASHU(V)    QYU_ASHU(VQYU_ASTM(V))
    return  VQYU_ASHU(v);
}

INLINE(Vqhu,VQBU_ASHU) (Vqbu v) {return vreinterpretq_u16_u8(v);}
INLINE(Vqhu,VQBI_ASHU) (Vqbi v) {return vreinterpretq_u16_s8(v);}
INLINE(Vqhu,VQBC_ASHU) (Vqbc v)
{
#   define  VQBC_ASHU(V)    QBC_ASHU(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqhu,VQHI_ASHU) (Vqhi v) {return vreinterpretq_u16_s16(v);}
INLINE(Vqhu,VQHF_ASHU) (Vqhf v) {return vreinterpretq_u16_f16(v);}
INLINE(Vqhu,VQWU_ASHU) (Vqwu v) {return vreinterpretq_u16_u32(v);}
INLINE(Vqhu,VQWI_ASHU) (Vqwi v) {return vreinterpretq_u16_s32(v);}
INLINE(Vqhu,VQWF_ASHU) (Vqwf v) {return vreinterpretq_u16_f32(v);}
INLINE(Vqhu,VQDU_ASHU) (Vqdu v) {return vreinterpretq_u16_u64(v);}
INLINE(Vqhu,VQDI_ASHU) (Vqdi v) {return vreinterpretq_u16_s64(v);}
INLINE(Vqhu,VQDF_ASHU) (Vqdf v) {return vreinterpretq_u16_f64(v);}

#if _LEAVE_ARM_ASHU
}
#endif

#if _ENTER_ARM_ASHI
{
#endif

#define     DYU_ASHI        vreinterpret_s16_u64
#define     DBU_ASHI        vreinterpret_s16_u8
#define     DBI_ASHI        vreinterpret_s16_s8
#define     DHU_ASHI        vreinterpret_s16_u16
//efine     DHI_ASHI
#define     DHF_ASHI        vreinterpret_s16_f16
#define     DWU_ASHI        vreinterpret_s16_u32
#define     DWI_ASHI        vreinterpret_s16_s32
#define     DWF_ASHI        vreinterpret_s16_f32
#define     DDU_ASHI        vreinterpret_s16_u64
#define     DDI_ASHI        vreinterpret_s16_s64
#define     DDF_ASHI        vreinterpret_s16_f64


#define     QYU_ASHI        vreinterpretq_s16_u64
#define     QBU_ASHI        vreinterpretq_s16_u8
#define     QBI_ASHI        vreinterpretq_s16_s8
#define     QHU_ASHI        vreinterpretq_s16_u16
//efine     QHI_ASHI
#define     QHF_ASHI        vreinterpretq_s16_f16
#define     QWU_ASHI        vreinterpretq_s16_u32
#define     QWI_ASHI        vreinterpretq_s16_s32
#define     QWF_ASHI        vreinterpretq_s16_f32
#define     QDU_ASHI        vreinterpretq_s16_u64
#define     QDI_ASHI        vreinterpretq_s16_s64
#define     QDF_ASHI        vreinterpretq_s16_f64

INLINE(Vwhi,VWYU_ASHI) (Vwyu v)
{
#define     VWYU_ASHI(V)    WHI_ASTV(VWYU_ASTM(V))
    return  VWYU_ASHI(v);
}

INLINE(Vwhi,VWBU_ASHI) (Vwbu v)
{
#   define  VWBU_ASHI(V)    WHI_ASTV(VWBU_ASTM(V))
    return  VWBU_ASHI(v);
}

INLINE(Vwhi,VWBI_ASHI) (Vwbi v)
{
#define     VWBI_ASHI(V)    WHI_ASTV(VWBI_ASTM(V))
    return  VWBI_ASHI(v);
}

INLINE(Vwhi,VWBC_ASHI) (Vwbc v)
{
#   define  VWBC_ASHI(V)    WHI_ASTV(VWBC_ASTM(V))
    return  VWBC_ASHI(v);
}


INLINE(Vwhi,VWHU_ASHI) (Vwhu v)
{
#define     VWHU_ASHI(V)    WHI_ASTV(VWHU_ASTM(V))
    return  VWHU_ASHI(v);
}

INLINE(Vwhi,VWHF_ASHI) (Vwhf v)
{
#define     VWHF_ASHI(V)    WHI_ASTV(VWHF_ASTM(V))
    return  VWHF_ASHI(v);
}


INLINE(Vwhi,VWWU_ASHI) (Vwwu v)
{
#define     VWWU_ASHI(V)    WHI_ASTV(VWWU_ASTM(V))
    return  VWWU_ASHI(v);
}

INLINE(Vwhi,VWWI_ASHI) (Vwwi v)
{
#define     VWWI_ASHI(V)    WHI_ASTV(VWWI_ASTM(V))
    return  VWWI_ASHI(v);
}

INLINE(Vwhi,VWWF_ASHI) (Vwwf v)
{
#define     VWWF_ASHI(V)    WHI_ASTV(VWWF_ASTM(V))
    return  VWWF_ASHI(v);
}


INLINE(Vdhi,VDYU_ASHI) (Vdyu v)
{
#define     VDYU_ASHI(V)    DYU_ASHI(VDYU_ASTM(V))
    return  VDYU_ASHI(v);
}

INLINE(Vdhi,VDBU_ASHI) (Vdbu v) {return vreinterpret_s16_u8(v);}
INLINE(Vdhi,VDBI_ASHI) (Vdbi v) {return vreinterpret_s16_s8(v);}
INLINE(Vdhi,VDBC_ASHI) (Vdbc v)
{
#define     VDBC_ASHI(V)    DBC_ASHI(VDBC_ASTM(V))
    return  VDBC_ASHI(v);
}

INLINE(Vdhi,VDHU_ASHI) (Vdhi v) {return vreinterpret_s16_u16(v);}
INLINE(Vdhi,VDHF_ASHI) (Vdhf v) {return vreinterpret_s16_f16(v);}
INLINE(Vdhi,VDWU_ASHI) (Vdwu v) {return vreinterpret_s16_u32(v);}
INLINE(Vdhi,VDWI_ASHI) (Vdwi v) {return vreinterpret_s16_s32(v);}
INLINE(Vdhi,VDWF_ASHI) (Vdwf v) {return vreinterpret_s16_f32(v);}
INLINE(Vdhi,VDDU_ASHI) (Vddu v) {return vreinterpret_s16_u64(v);}
INLINE(Vdhi,VDDI_ASHI) (Vddi v) {return vreinterpret_s16_s64(v);}
INLINE(Vdhi,VDDF_ASHI) (Vddf v) {return vreinterpret_s16_f64(v);}


INLINE(Vqhi,VQYU_ASHI) (Vqyu v)
{
#define     VQYU_ASHI(V)    QYU_ASHI(VQYU_ASTM(V))
    return  VQYU_ASHI(v);
}

INLINE(Vqhi,VQBU_ASHI) (Vqbu v) {return vreinterpretq_s16_u8(v);}
INLINE(Vqhi,VQBI_ASHI) (Vqbi v) {return vreinterpretq_s16_s8(v);}
INLINE(Vqhi,VQBC_ASHI) (Vqbc v)
{
#   define  VQBC_ASHI(V)    QBC_ASHI(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqhi,VQHU_ASHI) (Vqhu v) {return vreinterpretq_s16_u16(v);}
INLINE(Vqhi,VQHF_ASHI) (Vqhf v) {return vreinterpretq_s16_f16(v);}
INLINE(Vqhi,VQWU_ASHI) (Vqwu v) {return vreinterpretq_s16_u32(v);}
INLINE(Vqhi,VQWI_ASHI) (Vqwi v) {return vreinterpretq_s16_s32(v);}
INLINE(Vqhi,VQWF_ASHI) (Vqwf v) {return vreinterpretq_s16_f32(v);}
INLINE(Vqhi,VQDU_ASHI) (Vqdu v) {return vreinterpretq_s16_u64(v);}
INLINE(Vqhi,VQDI_ASHI) (Vqdi v) {return vreinterpretq_s16_s64(v);}
INLINE(Vqhi,VQDF_ASHI) (Vqdf v) {return vreinterpretq_s16_f64(v);}

#if _LEAVE_ARM_ASHI
}
#endif

#if _ENTER_ARM_ASHF
{
#endif

#define     DYU_ASHF        vreinterpret_f16_u64
#define     DBU_ASHF        vreinterpret_f16_u8
#define     DBI_ASHF        vreinterpret_f16_s8
#define     DHU_ASHF        vreinterpret_f16_u16
#define     DHI_ASHF        vreinterpret_f16_s16
//efine     DHF_ASHF
#define     DWU_ASHF        vreinterpret_f16_u32
#define     DWI_ASHF        vreinterpret_f16_s32
#define     DWF_ASHF        vreinterpret_f16_f32
#define     DDU_ASHF        vreinterpret_f16_u64
#define     DDI_ASHF        vreinterpret_f16_s64
#define     DDF_ASHF        vreinterpret_f16_f64


#define     QYU_ASHF        vreinterpretq_f16_u64
#define     QBU_ASHF        vreinterpretq_f16_u8
#define     QBI_ASHF        vreinterpretq_f16_s8
#define     QHU_ASHF        vreinterpretq_f16_u16
#define     QHI_ASHF        vreinterpretq_f16_s16
//efine     QHF_ASHF
#define     QWU_ASHF        vreinterpretq_f16_u32
#define     QWI_ASHF        vreinterpretq_f16_s32
#define     QWF_ASHF        vreinterpretq_f16_f32
#define     QDU_ASHF        vreinterpretq_f16_u64
#define     QDI_ASHF        vreinterpretq_f16_s64
#define     QDF_ASHF        vreinterpretq_f16_f64


INLINE(Vwhf,VWYU_ASHF) (Vwyu v)
{
#define     VWYU_ASHF(V)    WHF_ASTV(VWYU_ASTM(V))
    return  VWYU_ASHF(v);
}

INLINE(Vwhf,VWBU_ASHF) (Vwbu v)
{
#   define  VWBU_ASHF(V)    WHF_ASTV(VWBU_ASTM(V))
    return  VWBU_ASHF(v);
}

INLINE(Vwhf,VWBI_ASHF) (Vwbi v)
{
#define     VWBI_ASHF(V)    WHF_ASTV(VWBI_ASTM(V))
    return  VWBI_ASHF(v);
}

INLINE(Vwhf,VWBC_ASHF) (Vwbc v)
{
#   define  VWBC_ASHF(V)    WHF_ASTV(VWBC_ASTM(V))
    return  VWBC_ASHF(v);
}


INLINE(Vwhf,VWHU_ASHF) (Vwhu v)
{
#define     VWHU_ASHF(V)    WHF_ASTV(VWHU_ASTM(V))
    return  VWHU_ASHF(v);
}

INLINE(Vwhf,VWHI_ASHF) (Vwhi v)
{
#define     VWHI_ASHF(V)    WHF_ASTV(VWHI_ASTM(V))
    return  VWHI_ASHF(v);
}


INLINE(Vwhf,VWWU_ASHF) (Vwwu v)
{
#define     VWWU_ASHF(V)    WHF_ASTV(VWWU_ASTM(V))
    return  VWWU_ASHF(v);
}

INLINE(Vwhf,VWWI_ASHF) (Vwwi v)
{
#define     VWWI_ASHF(V)    WHF_ASTV(VWWI_ASTM(V))
    return  VWWI_ASHF(v);
}

INLINE(Vwhf,VWWF_ASHF) (Vwwf v)
{
#define     VWWF_ASHF(V)    WHF_ASTV(VWWF_ASTM(V))
    return  VWWF_ASHF(v);
}


INLINE(Vdhf,VDYU_ASHF) (Vdyu v)
{
#define     VDYU_ASHF(V)    DYU_ASHF(VDYU_ASTM(V))
    return  VDYU_ASHF(v);
}

INLINE(Vdhf,VDBU_ASHF) (Vdbu v) {return vreinterpret_f16_u8(v);}
INLINE(Vdhf,VDBI_ASHF) (Vdbi v) {return vreinterpret_f16_s8(v);}
INLINE(Vdhf,VDBC_ASHF) (Vdbc v)
{
#define     VDBC_ASHF(V)    DBC_ASHF(VDBC_ASTM(V))
    return  VDBC_ASHF(v);
}

INLINE(Vdhf,VDHU_ASHF) (Vdhu v) {return vreinterpret_f16_u16(v);}
INLINE(Vdhf,VDHI_ASHF) (Vdhi v) {return vreinterpret_f16_s16(v);}
INLINE(Vdhf,VDWU_ASHF) (Vdwu v) {return vreinterpret_f16_u32(v);}
INLINE(Vdhf,VDWI_ASHF) (Vdwi v) {return vreinterpret_f16_s32(v);}
INLINE(Vdhf,VDWF_ASHF) (Vdwf v) {return vreinterpret_f16_f32(v);}
INLINE(Vdhf,VDDU_ASHF) (Vddu v) {return vreinterpret_f16_u64(v);}
INLINE(Vdhf,VDDI_ASHF) (Vddi v) {return vreinterpret_f16_s64(v);}
INLINE(Vdhf,VDDF_ASHF) (Vddf v) {return vreinterpret_f16_f64(v);}


INLINE(Vqhf,VQYU_ASHF) (Vqyu v)
{
#define     VQYU_ASHF(V)    QYU_ASHF(VQYU_ASTM(V))
    return  VQYU_ASHF(v);
}

INLINE(Vqhf,VQBU_ASHF) (Vqbu v) {return vreinterpretq_f16_u8(v);}
INLINE(Vqhf,VQBI_ASHF) (Vqbi v) {return vreinterpretq_f16_s8(v);}
INLINE(Vqhf,VQBC_ASHF) (Vqbc v)
{
#   define  VQBC_ASHF(V)    QBC_ASHF(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqhf,VQHU_ASHF) (Vqhu v) {return vreinterpretq_f16_u16(v);}
INLINE(Vqhf,VQHI_ASHF) (Vqhi v) {return vreinterpretq_f16_s16(v);}
INLINE(Vqhf,VQWU_ASHF) (Vqwu v) {return vreinterpretq_f16_u32(v);}
INLINE(Vqhf,VQWI_ASHF) (Vqwi v) {return vreinterpretq_f16_s32(v);}
INLINE(Vqhf,VQWF_ASHF) (Vqwf v) {return vreinterpretq_f16_f32(v);}
INLINE(Vqhf,VQDU_ASHF) (Vqdu v) {return vreinterpretq_f16_u64(v);}
INLINE(Vqhf,VQDI_ASHF) (Vqdi v) {return vreinterpretq_f16_s64(v);}
INLINE(Vqhf,VQDF_ASHF) (Vqdf v) {return vreinterpretq_f16_f64(v);}
#if _LEAVE_ARM_ASHF
}
#endif

#if _ENTER_ARM_ASWU
{
#endif

#define     DYU_ASWU        vreinterpret_u32_u64
#define     DBU_ASWU        vreinterpret_u32_u8
#define     DBI_ASWU        vreinterpret_u32_s8
//efine     DBC_ASWU
#define     DHU_ASWU        vreinterpret_u32_u16
#define     DHI_ASWU        vreinterpret_u32_s16
#define     DHF_ASWU        vreinterpret_u32_f16
//efine     DWU_ASWU
#define     DWI_ASWU        vreinterpret_u32_s32
#define     DWF_ASWU        vreinterpret_u32_f32
#define     DDU_ASWU        vreinterpret_u32_u64
#define     DDI_ASWU        vreinterpret_u32_s64
#define     DDF_ASWU        vreinterpret_u32_f64


#define     QYU_ASWU        vreinterpretq_u32_u64
#define     QBU_ASWU        vreinterpretq_u32_u8
#define     QBI_ASWU        vreinterpretq_u32_s8
//efine     QBC_ASWU
#define     QHU_ASWU        vreinterpretq_u32_u16
#define     QHI_ASWU        vreinterpretq_u32_s16
#define     QHF_ASWU        vreinterpretq_u32_f16
//efine     QWU_ASWU
#define     QWI_ASWU        vreinterpretq_u32_s32
#define     QWF_ASWU        vreinterpretq_u32_f32
#define     QDU_ASWU        vreinterpretq_u32_u64
#define     QDI_ASWU        vreinterpretq_u32_s64
#define     QDF_ASWU        vreinterpretq_u32_f64


INLINE(Vwwu,VWYU_ASWU) (Vwyu v)
{
#define     VWYU_ASWU(V)    WWU_ASTV(VWYU_ASTM(V))
    return  VWYU_ASWU(v);
}

INLINE(Vwwu,VWBU_ASWU) (Vwbu v)
{
#   define  VWBU_ASWU(V)    WWU_ASTV(VWBU_ASTM(V))
    return  VWBU_ASWU(v);
}

INLINE(Vwwu,VWBI_ASWU) (Vwbi v)
{
#define     VWBI_ASWU(V)    WWU_ASTV(VWBI_ASTM(V))
    return  VWBI_ASWU(v);
}

INLINE(Vwwu,VWBC_ASWU) (Vwbc v)
{
#   define  VWBC_ASWU(V)    WWU_ASTV(VWBC_ASTM(V))
    return  VWBC_ASWU(v);
}


INLINE(Vwwu,VWHU_ASWU) (Vwhu v)
{
#define     VWHU_ASWU(V)    WWU_ASTV(VWHU_ASTM(V))
    return  VWHU_ASWU(v);
}

INLINE(Vwwu,VWHI_ASWU) (Vwhi v)
{
#define     VWHI_ASWU(V)    WWU_ASTV(VWHI_ASTM(V))
    return  VWHI_ASWU(v);
}

INLINE(Vwwu,VWHF_ASWU) (Vwhf v)
{
#define     VWHF_ASWU(V)    WWU_ASTV(VWHF_ASTM(V))
    return  VWHF_ASWU(v);
}


INLINE(Vwwu,VWWI_ASWU) (Vwwi v)
{
#define     VWWI_ASWU(V)    WWU_ASTV(VWWI_ASTM(V))
    return  VWWI_ASWU(v);
}

INLINE(Vwwu,VWWF_ASWU) (Vwwf v)
{
#define     VWWF_ASWU(V)    WWU_ASTV(VWWF_ASTM(V))
    return  VWWF_ASWU(v);
}


INLINE(Vdwu,VDYU_ASWU) (Vdyu v)
{
#define     VDYU_ASWU(V)    DYU_ASWU(VDYU_ASTM(V))
    return  VDYU_ASWU(v);
}

INLINE(Vdwu,VDBU_ASWU) (Vdbu v) {return vreinterpret_u32_u8(v);}
INLINE(Vdwu,VDBI_ASWU) (Vdbi v) {return vreinterpret_u32_s8(v);}
INLINE(Vdwu,VDBC_ASWU) (Vdbc v)
{
#define     VDBC_ASWU(V)    DBC_ASWU(VDBC_ASTM(V))
    return  VDBC_ASWU(v);
}

INLINE(Vdwu,VDHU_ASWU) (Vdhu v) {return vreinterpret_u32_u16(v);}
INLINE(Vdwu,VDHI_ASWU) (Vdhi v) {return vreinterpret_u32_s16(v);}
INLINE(Vdwu,VDHF_ASWU) (Vdhf v) {return vreinterpret_u32_f16(v);}
INLINE(Vdwu,VDWI_ASWU) (Vdwi v) {return vreinterpret_u32_s32(v);}
INLINE(Vdwu,VDWF_ASWU) (Vdwf v) {return vreinterpret_u32_f32(v);}
INLINE(Vdwu,VDDU_ASWU) (Vddu v) {return vreinterpret_u32_u64(v);}
INLINE(Vdwu,VDDI_ASWU) (Vddi v) {return vreinterpret_u32_s64(v);}
INLINE(Vdwu,VDDF_ASWU) (Vddf v) {return vreinterpret_u32_f64(v);}


INLINE(Vqwu,VQYU_ASWU) (Vqyu v)
{
#define     VQYU_ASWU(V)    QYU_ASWU(VQYU_ASTM(V))
    return  VQYU_ASWU(v);
}

INLINE(Vqwu,VQBU_ASWU) (Vqbu v) {return vreinterpretq_u32_u8(v);}
INLINE(Vqwu,VQBI_ASWU) (Vqbi v) {return vreinterpretq_u32_s8(v);}
INLINE(Vqwu,VQBC_ASWU) (Vqbc v)
{
#   define  VQBC_ASWU(V)    QBC_ASWU(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqwu,VQHU_ASWU) (Vqhu v) {return vreinterpretq_u32_u16(v);}
INLINE(Vqwu,VQHI_ASWU) (Vqhi v) {return vreinterpretq_u32_s16(v);}
INLINE(Vqwu,VQHF_ASWU) (Vqhf v) {return vreinterpretq_u32_f16(v);}
INLINE(Vqwu,VQWI_ASWU) (Vqwi v) {return vreinterpretq_u32_s32(v);}
INLINE(Vqwu,VQWF_ASWU) (Vqwf v) {return vreinterpretq_u32_f32(v);}
INLINE(Vqwu,VQDU_ASWU) (Vqdu v) {return vreinterpretq_u32_u64(v);}
INLINE(Vqwu,VQDI_ASWU) (Vqdi v) {return vreinterpretq_u32_s64(v);}
INLINE(Vqwu,VQDF_ASWU) (Vqdf v) {return vreinterpretq_u32_f64(v);}

#if _LEAVE_ARM_ASWU
}
#endif

#if _ENTER_ARM_ASWI
{
#endif

#define     DYU_ASWI        vreinterpret_s32_u64
#define     DBU_ASWI        vreinterpret_s32_u8
#define     DBI_ASWI        vreinterpret_s32_s8
//efine     DBC_ASWI
#define     DHU_ASWI        vreinterpret_s32_u16
#define     DHI_ASWI        vreinterpret_s32_s16
#define     DHF_ASWI        vreinterpret_s32_f16
#define     DWU_ASWI        vreinterpret_s32_u32
//efine     DWI_ASWI
#define     DWF_ASWI        vreinterpret_s32_f32
#define     DDU_ASWI        vreinterpret_s32_u64
#define     DDI_ASWI        vreinterpret_s32_s64
#define     DDF_ASWI        vreinterpret_s32_f64


#define     QYU_ASWI        vreinterpretq_s32_u64
#define     QBU_ASWI        vreinterpretq_s32_u8
#define     QBI_ASWI        vreinterpretq_s32_s8
//efine     QBC_ASWI
#define     QHU_ASWI        vreinterpretq_s32_u16
#define     QHI_ASWI        vreinterpretq_s32_s16
#define     QHF_ASWI        vreinterpretq_s32_f16
#define     QWU_ASWI        vreinterpretq_s32_u32
//efine     QWI_ASWI
#define     QWF_ASWI        vreinterpretq_s32_f32
#define     QDU_ASWI        vreinterpretq_s32_u64
#define     QDI_ASWI        vreinterpretq_s32_s64
#define     QDF_ASWI        vreinterpretq_s32_f64


INLINE(Vwwi,VWYU_ASWI) (Vwyu v)
{
#define     VWYU_ASWI(V)    WWI_ASTV(VWYU_ASTM(V))
    return  VWYU_ASWI(v);
}

INLINE(Vwwi,VWBU_ASWI) (Vwbu v)
{
#   define  VWBU_ASWI(V)    WWI_ASTV(VWBU_ASTM(V))
    return  VWBU_ASWI(v);
}

INLINE(Vwwi,VWBI_ASWI) (Vwbi v)
{
#define     VWBI_ASWI(V)    WWI_ASTV(VWBI_ASTM(V))
    return  VWBI_ASWI(v);
}

INLINE(Vwwi,VWBC_ASWI) (Vwbc v)
{
#   define  VWBC_ASWI(V)    WWI_ASTV(VWBC_ASTM(V))
    return  VWBC_ASWI(v);
}


INLINE(Vwwi,VWHU_ASWI) (Vwhu v)
{
#define     VWHU_ASWI(V)    WWI_ASTV(VWHU_ASTM(V))
    return  VWHU_ASWI(v);
}

INLINE(Vwwi,VWHI_ASWI) (Vwhi v)
{
#define     VWHI_ASWI(V)    WWI_ASTV(VWHI_ASTM(V))
    return  VWHI_ASWI(v);
}

INLINE(Vwwi,VWHF_ASWI) (Vwhf v)
{
#define     VWHF_ASWI(V)    WWI_ASTV(VWHF_ASTM(V))
    return  VWHF_ASWI(v);
}


INLINE(Vwwi,VWWU_ASWI) (Vwwu v)
{
#define     VWWU_ASWI(V)    WWI_ASTV(VWWU_ASTM(V))
    return  VWWU_ASWI(v);
}

INLINE(Vwwi,VWWF_ASWI) (Vwwf v)
{
#define     VWWF_ASWI(V)    WWI_ASTV(VWWF_ASTM(V))
    return  VWWF_ASWI(v);
}


INLINE(Vdwi,VDYU_ASWI) (Vdyu v)
{
#define     VDYU_ASWI(V)    DYU_ASWI(VDYU_ASTM(V))
    return  VDYU_ASWI(v);
}

INLINE(Vdwi,VDBU_ASWI) (Vdbu v) {return vreinterpret_s32_u8(v);}
INLINE(Vdwi,VDBI_ASWI) (Vdbi v) {return vreinterpret_s32_s8(v);}
INLINE(Vdwi,VDBC_ASWI) (Vdbc v)
{
#define     VDBC_ASWI(V)    DBC_ASWI(VDBC_ASTM(V))
    return  VDBC_ASWI(v);
}

INLINE(Vdwi,VDHU_ASWI) (Vdhu v) {return vreinterpret_s32_u16(v);}
INLINE(Vdwi,VDHI_ASWI) (Vdhi v) {return vreinterpret_s32_s16(v);}
INLINE(Vdwi,VDHF_ASWI) (Vdhf v) {return vreinterpret_s32_f16(v);}
INLINE(Vdwi,VDWU_ASWI) (Vdwu v) {return vreinterpret_s32_u32(v);}
INLINE(Vdwi,VDWF_ASWI) (Vdwf v) {return vreinterpret_s32_f32(v);}
INLINE(Vdwi,VDDU_ASWI) (Vddu v) {return vreinterpret_s32_u64(v);}
INLINE(Vdwi,VDDI_ASWI) (Vddi v) {return vreinterpret_s32_s64(v);}
INLINE(Vdwi,VDDF_ASWI) (Vddf v) {return vreinterpret_s32_f64(v);}

INLINE(Vqwi,VQYU_ASWI) (Vqyu v)
{
#define     VQYU_ASWI(V)    QYU_ASWI(VQYU_ASTM(V))
    return  VQYU_ASWI(v);
}

INLINE(Vqwi,VQBU_ASWI) (Vqbu v) {return vreinterpretq_s32_u8(v);}
INLINE(Vqwi,VQBI_ASWI) (Vqbi v) {return vreinterpretq_s32_s8(v);}
INLINE(Vqwi,VQBC_ASWI) (Vqbc v)
{
#   define  VQBC_ASWI(V)    QBC_ASWI(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqwi,VQHU_ASWI) (Vqhu v) {return vreinterpretq_s32_u16(v);}
INLINE(Vqwi,VQHI_ASWI) (Vqhi v) {return vreinterpretq_s32_s16(v);}
INLINE(Vqwi,VQHF_ASWI) (Vqhf v) {return vreinterpretq_s32_f16(v);}
INLINE(Vqwi,VQWU_ASWI) (Vqwu v) {return vreinterpretq_s32_u32(v);}
INLINE(Vqwi,VQWF_ASWI) (Vqwf v) {return vreinterpretq_s32_f32(v);}
INLINE(Vqwi,VQDU_ASWI) (Vqdu v) {return vreinterpretq_s32_u64(v);}
INLINE(Vqwi,VQDI_ASWI) (Vqdi v) {return vreinterpretq_s32_s64(v);}
INLINE(Vqwi,VQDF_ASWI) (Vqdf v) {return vreinterpretq_s32_f64(v);}

#if _LEAVE_ARM_ASWI
}
#endif

#if _ENTER_ARM_ASWF
{
#endif

#define     DYU_ASWF        vreinterpret_f32_u64
#define     DBU_ASWF        vreinterpret_f32_u8
#define     DBI_ASWF        vreinterpret_f32_s8
//efine     DBC_ASWF
#define     DHU_ASWF        vreinterpret_f32_u16
#define     DHI_ASWF        vreinterpret_f32_s16
#define     DHF_ASWF        vreinterpret_f32_f16
#define     DWU_ASWF        vreinterpret_f32_u32
#define     DWI_ASWF        vreinterpret_f32_s32
//efine     DWF_ASWF
#define     DDU_ASWF        vreinterpret_f32_u64
#define     DDI_ASWF        vreinterpret_f32_s64
#define     DDF_ASWF        vreinterpret_f32_f64

#define     QYU_ASWF        vreinterpretq_f32_u64
#define     QBU_ASWF        vreinterpretq_f32_u8
#define     QBI_ASWF        vreinterpretq_f32_s8
//efine     QBC_ASWF
#define     QHU_ASWF        vreinterpretq_f32_u16
#define     QHI_ASWF        vreinterpretq_f32_s16
#define     QHF_ASWF        vreinterpretq_f32_f16
#define     QWU_ASWF        vreinterpretq_f32_u32
#define     QWI_ASWF        vreinterpretq_f32_s32
//efine     QWF_ASWF
#define     QDU_ASWF        vreinterpretq_f32_u64
#define     QDI_ASWF        vreinterpretq_f32_s64
#define     QDF_ASWF        vreinterpretq_f32_f64


INLINE(Vwwf,VWYU_ASWF) (Vwyu v)
{
#define     VWYU_ASWF(V)    WWF_ASTV(VWYU_ASTM(V))
    return  VWYU_ASWF(v);
}

INLINE(Vwwf,VWBU_ASWF) (Vwbu v)
{
#   define  VWBU_ASWF(V)    WWF_ASTV(VWBU_ASTM(V))
    return  VWBU_ASWF(v);
}

INLINE(Vwwf,VWBI_ASWF) (Vwbi v)
{
#define     VWBI_ASWF(V)    WWF_ASTV(VWBI_ASTM(V))
    return  VWBI_ASWF(v);
}

INLINE(Vwwf,VWBC_ASWF) (Vwbc v)
{
#   define  VWBC_ASWF(V)    WWF_ASTV(VWBC_ASTM(V))
    return  VWBC_ASWF(v);
}


INLINE(Vwwf,VWHU_ASWF) (Vwhu v)
{
#define     VWHU_ASWF(V)    WWF_ASTV(VWHU_ASTM(V))
    return  VWHU_ASWF(v);
}

INLINE(Vwwf,VWHI_ASWF) (Vwhi v)
{
#define     VWHI_ASWF(V)    WWF_ASTV(VWHI_ASTM(V))
    return  VWHI_ASWF(v);
}

INLINE(Vwwf,VWHF_ASWF) (Vwhf v)
{
#define     VWHF_ASWF(V)    WWF_ASTV(VWHF_ASTM(V))
    return  VWHF_ASWF(v);
}


INLINE(Vwwf,VWWU_ASWF) (Vwwu v)
{
#define     VWWU_ASWF(V)    WWF_ASTV(VWWU_ASTM(V))
    return  VWWU_ASWF(v);
}

INLINE(Vwwf,VWWI_ASWF) (Vwwi v)
{
#define     VWWI_ASWF(V)    WWF_ASTV(VWWI_ASTM(V))
    return  VWWI_ASWF(v);
}


INLINE(Vdwf,VDYU_ASWF) (Vdyu v)
{
#define     VDYU_ASWF(V)    DYU_ASWF(VDYU_ASTM(V))
    return  VDYU_ASWF(v);
}

INLINE(Vdwf,VDBU_ASWF) (Vdbu v) {return vreinterpret_f32_u8(v);}
INLINE(Vdwf,VDBI_ASWF) (Vdbi v) {return vreinterpret_f32_s8(v);}
INLINE(Vdwf,VDBC_ASWF) (Vdbc v)
{
#define     VDBC_ASWF(V)    DBC_ASWF(VDBC_ASTM(V))
    return  VDBC_ASWF(v);
}

INLINE(Vdwf,VDHU_ASWF) (Vdhu v) {return vreinterpret_f32_u16(v);}
INLINE(Vdwf,VDHI_ASWF) (Vdhi v) {return vreinterpret_f32_s16(v);}
INLINE(Vdwf,VDHF_ASWF) (Vdhf v) {return vreinterpret_f32_f16(v);}
INLINE(Vdwf,VDWU_ASWF) (Vdwu v) {return vreinterpret_f32_u32(v);}
INLINE(Vdwf,VDWI_ASWF) (Vdwi v) {return vreinterpret_f32_s32(v);}
INLINE(Vdwf,VDDU_ASWF) (Vddu v) {return vreinterpret_f32_u64(v);}
INLINE(Vdwf,VDDI_ASWF) (Vddi v) {return vreinterpret_f32_s64(v);}
INLINE(Vdwf,VDDF_ASWF) (Vddf v) {return vreinterpret_f32_f64(v);}


INLINE(Vqwf,VQYU_ASWF) (Vqyu v)
{
#define     VQYU_ASWF(V)    QYU_ASWF(VQYU_ASTM(V))
    return  VQYU_ASWF(v);
}

INLINE(Vqwf,VQBU_ASWF) (Vqbu v) {return vreinterpretq_f32_u8(v);}
INLINE(Vqwf,VQBI_ASWF) (Vqbi v) {return vreinterpretq_f32_s8(v);}
INLINE(Vqwf,VQBC_ASWF) (Vqbc v)
{
#   define  VQBC_ASWF(V)    QBC_ASWF(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqwf,VQHU_ASWF) (Vqhu v) {return vreinterpretq_f32_u16(v);}
INLINE(Vqwf,VQHI_ASWF) (Vqhi v) {return vreinterpretq_f32_s16(v);}
INLINE(Vqwf,VQHF_ASWF) (Vqhf v) {return vreinterpretq_f32_f16(v);}
INLINE(Vqwf,VQWU_ASWF) (Vqwu v) {return vreinterpretq_f32_u32(v);}
INLINE(Vqwf,VQWI_ASWF) (Vqwi v) {return vreinterpretq_f32_s32(v);}
INLINE(Vqwf,VQDU_ASWF) (Vqdu v) {return vreinterpretq_f32_u64(v);}
INLINE(Vqwf,VQDI_ASWF) (Vqdi v) {return vreinterpretq_f32_s64(v);}
INLINE(Vqwf,VQDF_ASWF) (Vqdf v) {return vreinterpretq_f32_f64(v);}

#if _LEAVE_ARM_ASWF
}
#endif

#if _ENTER_ARM_ASDU
{
#endif

#define     DYU_ASDU        VDDU_REQS
#define     DBU_ASDU        vreinterpret_u64_u8
#define     DBI_ASDU        vreinterpret_u64_s8
//efine     DBC_ASDU
#define     DHU_ASDU        vreinterpret_u64_u16
#define     DHI_ASDU        vreinterpret_u64_s16
#define     DHF_ASDU        vreinterpret_u64_f16
#define     DWU_ASDU        vreinterpret_u64_u32
#define     DWI_ASDU        vreinterpret_u64_s32
#define     DWF_ASDU        vreinterpret_u64_f32
//efine     DDU_ASDU
#define     DDI_ASDU        vreinterpret_u64_s64
#define     DDF_ASDU        vreinterpret_u64_f64

#define     QYU_ASDU        VQDU_REQS
#define     QBU_ASDU        vreinterpretq_u64_u8
#define     QBI_ASDU        vreinterpretq_u64_s8
#define     QHU_ASDU        vreinterpretq_u64_u16
#define     QHI_ASDU        vreinterpretq_u64_s16
#define     QHF_ASDU        vreinterpretq_u64_f16
#define     QWU_ASDU        vreinterpretq_u64_u32
#define     QWI_ASDU        vreinterpretq_u64_s32
#define     QWF_ASDU        vreinterpretq_u64_f32
#define     QDI_ASDU        vreinterpretq_u64_s64
#define     QDF_ASDU        vreinterpretq_u64_f64

INLINE(Vddu,VDYU_ASDU) (Vdyu v)
{
#define     VDYU_ASDU(V)    DYU_ASDU(VDYU_ASTM(V))
    return  VDYU_ASDU(v);
}

INLINE(Vddu,VDBU_ASDU) (Vdbu v) {return vreinterpret_u64_u8(v);}
INLINE(Vddu,VDBI_ASDU) (Vdbi v) {return vreinterpret_u64_s8(v);}
INLINE(Vddu,VDBC_ASDU) (Vdbc v)
{
#define     VDBC_ASDU(V)    DBC_ASDU(VDBC_ASTM(V))
    return  VDBC_ASDU(v);
}

INLINE(Vddu,VDHU_ASDU) (Vdhu v) {return vreinterpret_u64_u16(v);}
INLINE(Vddu,VDHI_ASDU) (Vdhi v) {return vreinterpret_u64_s16(v);}
INLINE(Vddu,VDHF_ASDU) (Vdhf v) {return vreinterpret_u64_f16(v);}
INLINE(Vddu,VDWU_ASDU) (Vdwu v) {return vreinterpret_u64_u32(v);}
INLINE(Vddu,VDWI_ASDU) (Vdwi v) {return vreinterpret_u64_s32(v);}
INLINE(Vddu,VDWF_ASDU) (Vdwf v) {return vreinterpret_u64_f32(v);}
INLINE(Vddu,VDDI_ASDU) (Vddi v) {return vreinterpret_u64_s64(v);}
INLINE(Vddu,VDDF_ASDU) (Vddf v) {return vreinterpret_u64_f64(v);}

INLINE(Vqdu,VQYU_ASDU) (Vqyu v)
{
#define     VQYU_ASDU(V)    QYU_ASDU(VQYU_ASTM(V))
    return  VQYU_ASDU(v);
}

INLINE(Vqdu,VQBU_ASDU) (Vqbu v) {return vreinterpretq_u64_u8(v);}
INLINE(Vqdu,VQBI_ASDU) (Vqbi v) {return vreinterpretq_u64_s8(v);}
INLINE(Vqdu,VQBC_ASDU) (Vqbc v)
{
#   define  VQBC_ASDU(V)    QBC_ASDU(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqdu,VQHU_ASDU) (Vqhu v) {return vreinterpretq_u64_u16(v);}
INLINE(Vqdu,VQHI_ASDU) (Vqhi v) {return vreinterpretq_u64_s16(v);}
INLINE(Vqdu,VQHF_ASDU) (Vqhf v) {return vreinterpretq_u64_f16(v);}
INLINE(Vqdu,VQWU_ASDU) (Vqwu v) {return vreinterpretq_u64_u32(v);}
INLINE(Vqdu,VQWI_ASDU) (Vqwi v) {return vreinterpretq_u64_s32(v);}
INLINE(Vqdu,VQWF_ASDU) (Vqwf v) {return vreinterpretq_u64_f32(v);}
INLINE(Vqdu,VQDI_ASDU) (Vqdi v) {return vreinterpretq_u64_s64(v);}
INLINE(Vqdu,VQDF_ASDU) (Vqdf v) {return vreinterpretq_u64_f64(v);}

#if _LEAVE_ARM_ASDU
}
#endif

#if _ENTER_ARM_ASDI
{
#endif

#define     DYU_ASDI        vreinterpret_s64_u64
#define     DBU_ASDI        vreinterpret_s64_u8
#define     DBI_ASDI        vreinterpret_s64_s8
//          DBC_ASDI
#define     DHU_ASDI        vreinterpret_s64_u16
#define     DHI_ASDI        vreinterpret_s64_s16
#define     DHF_ASDI        vreinterpret_s64_f16
#define     DWU_ASDI        vreinterpret_s64_u32
#define     DWI_ASDI        vreinterpret_s64_s32
#define     DWF_ASDI        vreinterpret_s64_f32
#define     DDU_ASDI        vreinterpret_s64_u64
#define     DDF_ASDI        vreinterpret_s64_f64

#define     QYU_ASDI        vreinterpretq_s64_u64
#define     QBU_ASDI        vreinterpretq_s64_u8
#define     QBI_ASDI        vreinterpretq_s64_s8
//efine     QBC_ASDI
#define     QHU_ASDI        vreinterpretq_s64_u16
#define     QHI_ASDI        vreinterpretq_s64_s16
#define     QHF_ASDI        vreinterpretq_s64_f16
#define     QWU_ASDI        vreinterpretq_s64_u32
#define     QWI_ASDI        vreinterpretq_s64_s32
#define     QWF_ASDI        vreinterpretq_s64_f32
#define     QDU_ASDI        vreinterpretq_s64_u64
#define     QDF_ASDI        vreinterpretq_s64_f64

INLINE(Vddi,VDYU_ASDI) (Vdyu v)
{
#define     VDYU_ASDI(V)    DYU_ASDI(VDYU_ASTM(V))
    return  VDYU_ASDI(v);
}

INLINE(Vddi,VDBU_ASDI) (Vdbu v) {return vreinterpret_s64_u8(v);}
INLINE(Vddi,VDBI_ASDI) (Vdbi v) {return vreinterpret_s64_s8(v);}
INLINE(Vddi,VDBC_ASDI) (Vdbc v)
{
#define     VDBC_ASDI(V)    DBC_ASDI(VDBC_ASTM(V))
    return  VDBC_ASDI(v);
}

INLINE(Vddi,VDHU_ASDI) (Vdhu v) {return vreinterpret_s64_u16(v);}
INLINE(Vddi,VDHI_ASDI) (Vdhi v) {return vreinterpret_s64_s16(v);}
INLINE(Vddi,VDHF_ASDI) (Vdhf v) {return vreinterpret_s64_f16(v);}
INLINE(Vddi,VDWU_ASDI) (Vdwu v) {return vreinterpret_s64_u32(v);}
INLINE(Vddi,VDWI_ASDI) (Vdwi v) {return vreinterpret_s64_s32(v);}
INLINE(Vddi,VDWF_ASDI) (Vdwf v) {return vreinterpret_s64_f32(v);}
INLINE(Vddi,VDDU_ASDI) (Vddi v) {return vreinterpret_s64_u64(v);}
INLINE(Vddi,VDDF_ASDI) (Vddf v) {return vreinterpret_s64_f64(v);}

INLINE(Vqdi,VQYU_ASDI) (Vqyu v)
{
#define     VQYU_ASDI(V)    QYU_ASDI(VQYU_ASTM(V))
    return  VQYU_ASDI(v);
}

INLINE(Vqdi,VQBU_ASDI) (Vqbu v) {return vreinterpretq_s64_u8(v);}
INLINE(Vqdi,VQBI_ASDI) (Vqbi v) {return vreinterpretq_s64_s8(v);}
INLINE(Vqdi,VQBC_ASDI) (Vqbc v)
{
#   define  VQBC_ASDI(V)    QBC_ASDI(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqdi,VQHU_ASDI) (Vqhu v) {return vreinterpretq_s64_u16(v);}
INLINE(Vqdi,VQHI_ASDI) (Vqhi v) {return vreinterpretq_s64_s16(v);}
INLINE(Vqdi,VQHF_ASDI) (Vqhf v) {return vreinterpretq_s64_f16(v);}
INLINE(Vqdi,VQWU_ASDI) (Vqwu v) {return vreinterpretq_s64_u32(v);}
INLINE(Vqdi,VQWI_ASDI) (Vqwi v) {return vreinterpretq_s64_s32(v);}
INLINE(Vqdi,VQWF_ASDI) (Vqwf v) {return vreinterpretq_s64_f32(v);}
INLINE(Vqdi,VQDU_ASDI) (Vqdi v) {return vreinterpretq_s64_u64(v);}
INLINE(Vqdi,VQDF_ASDI) (Vqdf v) {return vreinterpretq_s64_f64(v);}

#if _LEAVE_ARM_ASDI
}
#endif

#if _ENTER_ARM_ASDF
{
#endif

#define     DYU_ASDF        vreinterpret_f64_u64
#define     DBU_ASDF        vreinterpret_f64_u8
#define     DBI_ASDF        vreinterpret_f64_s8
#define     DHU_ASDF        vreinterpret_f64_u16
#define     DHI_ASDF        vreinterpret_f64_s16
#define     DHF_ASDF        vreinterpret_f64_f16
#define     DWU_ASDF        vreinterpret_f64_u32
#define     DWI_ASDF        vreinterpret_f64_s32
#define     DWF_ASDF        vreinterpret_f64_f32
#define     DDU_ASDF        vreinterpret_f64_u64
#define     DDI_ASDF        vreinterpret_f64_u64
//efine     DDF_ASDF

#define     QYU_ASDF        vreinterpretq_f64_u64
#define     QBU_ASDF        vreinterpretq_f64_u8
#define     QBI_ASDF        vreinterpretq_f64_s8
#define     QHU_ASDF        vreinterpretq_f64_u16
#define     QHI_ASDF        vreinterpretq_f64_s16
#define     QHF_ASDF        vreinterpretq_f64_f16
#define     QWU_ASDF        vreinterpretq_f64_u32
#define     QWI_ASDF        vreinterpretq_f64_s32
#define     QWF_ASDF        vreinterpretq_f64_f32
#define     QDU_ASDF        vreinterpretq_f64_u64
#define     QDI_ASDF        vreinterpretq_f64_s64
//efine     QDF_ASDF

INLINE(Vddf,VDYU_ASDF) (Vdyu v)
{
#define     VDYU_ASDF(V)    DYU_ASDF(VDYU_ASTM(V))
    return  VDYU_ASDF(v);
}

INLINE(Vddf,VDBU_ASDF) (Vdbu v) {return vreinterpret_f64_u8(v);}
INLINE(Vddf,VDBI_ASDF) (Vdbi v) {return vreinterpret_f64_s8(v);}
INLINE(Vddf,VDBC_ASDF) (Vdbc v)
{
#define     VDBC_ASDF(V)    DBC_ASDF(VDBC_ASTM(V))
    return  VDBC_ASDF(v);
}

INLINE(Vddf,VDHU_ASDF) (Vdhu v) {return vreinterpret_f64_u16(v);}
INLINE(Vddf,VDHI_ASDF) (Vdhi v) {return vreinterpret_f64_s16(v);}
INLINE(Vddf,VDHF_ASDF) (Vdhf v) {return vreinterpret_f64_f16(v);}
INLINE(Vddf,VDWU_ASDF) (Vdwu v) {return vreinterpret_f64_u32(v);}
INLINE(Vddf,VDWI_ASDF) (Vdwi v) {return vreinterpret_f64_s32(v);}
INLINE(Vddf,VDWF_ASDF) (Vdwf v) {return vreinterpret_f64_f32(v);}
INLINE(Vddf,VDDU_ASDF) (Vddu v) {return vreinterpret_f64_u64(v);}
INLINE(Vddf,VDDI_ASDF) (Vddi v) {return vreinterpret_f64_s64(v);}

INLINE(Vqdf,VQYU_ASDF) (Vqyu v)
{
#define     VQYU_ASDF(V)    QYU_ASDF(VQYU_ASTM(V))
    return  VQYU_ASDF(v);
}

INLINE(Vqdf,VQBU_ASDF) (Vqbu v) {return vreinterpretq_f64_u8(v);}
INLINE(Vqdf,VQBI_ASDF) (Vqbi v) {return vreinterpretq_f64_s8(v);}
INLINE(Vqdf,VQBC_ASDF) (Vqbc v)
{
#   define  VQBC_ASDF(V)    QBC_ASDF(VQBC_ASTM(V))
    return  VQBC_ASTM(v);
}

INLINE(Vqdf,VQHU_ASDF) (Vqhu v) {return vreinterpretq_f64_u16(v);}
INLINE(Vqdf,VQHI_ASDF) (Vqhi v) {return vreinterpretq_f64_s16(v);}
INLINE(Vqdf,VQHF_ASDF) (Vqhf v) {return vreinterpretq_f64_f16(v);}
INLINE(Vqdf,VQWU_ASDF) (Vqwu v) {return vreinterpretq_f64_u32(v);}
INLINE(Vqdf,VQWI_ASDF) (Vqwi v) {return vreinterpretq_f64_s32(v);}
INLINE(Vqdf,VQWF_ASDF) (Vqwf v) {return vreinterpretq_f64_f32(v);}
INLINE(Vqdf,VQDU_ASDF) (Vqdu v) {return vreinterpretq_f64_u64(v);}
INLINE(Vqdf,VQDI_ASDF) (Vqdf v) {return vreinterpretq_f64_s64(v);}

#if _LEAVE_ARM_ASDF
}
#endif


#if _ENTER_ARM_UNOS
{
#endif

INLINE(void *, ADDR_UNOS) (Rc(0, ADDR_WIDTH) n)
{
    return (void *) (UINTPTR_MAX>>(ADDR_WIDTH-n));
}

INLINE( _Bool,  BOOL_UNOS) (Rc(0, 1) n)
{
    return n;
}


INLINE( uchar, UCHAR_UNOS) (Rc(0,  UCHAR_WIDTH) n)
{
#define     UCHAR_UNOS(N)   ((uchar)(UCHAR_MAX>>(CHAR_WIDTH-N)))
    return  UCHAR_UNOS(n);
}

INLINE( schar, SCHAR_UNOS) (Rc(0,  SCHAR_WIDTH) n)
{
#define     SCHAR_UNOS(N)   ((schar)(UCHAR_MAX>>(CHAR_WIDTH-N)))
    return  SCHAR_UNOS(n);
}

INLINE(  char,  CHAR_UNOS) (Rc(0,   CHAR_WIDTH) n)
{
#define     CHAR_UNOS(N)    ((char)(UCHAR_MAX>>(CHAR_WIDTH-N)))
    return  CHAR_UNOS(n);
}


INLINE(ushort, USHRT_UNOS) (Rc(0,  USHRT_WIDTH) n)
{
#define     USHRT_UNOS(N) ((ushort)(USHRT_MAX>>(SHRT_WIDTH-N)))
    return  USHRT_UNOS(n);
}

INLINE( short,  SHRT_UNOS) (Rc(0,   SHRT_WIDTH) n)
{
#define     SHRT_UNOS(N) ((short)(USHRT_MAX>>(SHRT_WIDTH-N)))
    return  SHRT_UNOS(n);
}


INLINE(  uint,  UINT_UNOS) (Rc(0,   UINT_WIDTH) n)
{
#define     UINT_UNOS(N) ((uint)(UINT_MAX>>(INT_WIDTH-N)))
    return  UINT_UNOS(n);
}

INLINE(   int,   INT_UNOS) (Rc(0,    INT_WIDTH) n)
{
#define     INT_UNOS(N) ((int)(UINT_MAX>>(INT_WIDTH-N)))
    return  INT_UNOS(n);
}


INLINE( ulong, ULONG_UNOS) (Rc(0,  ULONG_WIDTH) n)
{
#define     ULONG_UNOS(N)   ((ulong)(ULONG_MAX>>(LONG_WIDTH-N)))
    return  ULONG_UNOS(n);
}

INLINE(  long,  LONG_UNOS) (Rc(0,   LONG_WIDTH) n)
{
#define     LONG_UNOS(N)    ((long)(ULONG_MAX>>(LONG_WIDTH-N)))
    return  LONG_UNOS(n);
}


INLINE(ullong,ULLONG_UNOS) (Rc(0, ULLONG_WIDTH) n)
{
#define     ULLONG_UNOS(N) ((ullong)(ULLONG_MAX>>(LLONG_WIDTH-N)))
    return  ULLONG_UNOS(n);
}

INLINE( llong, LLONG_UNOS) (Rc(0,  LLONG_WIDTH) n)
{
#define     LLONG_UNOS(N)   ((llong)(ULLONG_MAX>>(LLONG_WIDTH-N)))
    return  LLONG_UNOS(n);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,unosqu) (Rc(0, 128) n) 
{
#   define  unosqu(N) ((~((QUAD_UTYPE) 0))>>N)

    return (~((QUAD_UTYPE){0}))>>(128-n);
}

INLINE(QUAD_ITYPE,unosqi) (Rc(0, 128) n)
{
#   define  unosqi(N) ((QUAD_ITYPE)((~((QUAD_UTYPE) 0))>>N))
    return (unosqi)(n);
}

#endif

INLINE(Vwyu,VWYU_UNOS) (Rc(0, 1) n)
{
#define     VWYU_UNOS(N) VWWU_ASYU(UINT_ASTV((N?UINT_MAX:0)))
    return  VWYU_UNOS(n);
}


INLINE(Vwbu,VWBU_UNOS) (Rc(0, 8) n)
{
#define     WBU_UNOS(N)                     \
(                                           \
    (8 == N)                                \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u8(            \
                vdup_n_u8(UINT8_MAX)        \
            ),                              \
            0                               \
        )                                   \
    :   vget_lane_f32(                      \
            vreinterpret_f32_u8(            \
                vshr_n_u8(                  \
                    vdup_n_u8(UINT8_MAX),   \
                    (8-(N&7))               \
                )                           \
            ),                              \
            0                               \
        )                                   \
)

#define     VWBU_UNOS(N)    WBU_ASTV(WBU_UNOS(N))
    uint8x8_t   d = vdup_n_u8((UINT8_MAX>>(8-n)));
    float32x2_t m = vreinterpret_f32_u8(d);
    float       f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_UNOS) (Rc(0, 8) n)
{
#define     VWBI_UNOS(N)    WBI_ASTV(WBU_UNOS(N))
    uint8x8_t   d = vdup_n_u8((UINT8_MAX>>(8-n)));
    float32x2_t m = vreinterpret_f32_u8(d);
    float       f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_UNOS) (Rc(0, 8) n)
{
#define     VWBC_UNOS(N)    WBC_ASTV(WBU_UNOS(N))
    uint8x8_t   d = vdup_n_u8((UINT8_MAX>>(8-n)));
    float32x2_t m = vreinterpret_f32_u8(d);
    float       f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
}


INLINE(Vwhu,VWHU_UNOS) (Rc(0, 16) n)
{
#define     WHU_UNOS(N)                     \
(                                           \
    (16 == N)                               \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u16(           \
                vdup_n_u16(UINT16_MAX)      \
            ),                              \
            0                               \
        )                                   \
    :   vget_lane_f32(                      \
            vreinterpret_f32_u16(           \
                vshr_n_u16(                 \
                    vdup_n_u16(UINT16_MAX), \
                    (16-(N&15))             \
                )                           \
            ),                              \
            0                               \
        )                                   \
)

#define     VWHU_UNOS(N)    WHU_ASTV(WHU_UNOS(N))
    uint16x4_t  d = vdup_n_u16((UINT16_MAX>>(16-n)));
    float32x2_t m = vreinterpret_f32_u16(d);
    float       f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_UNOS) (Rc(0, 16) n)
{
#define     VWHI_UNOS(N)    WHI_ASTV(WHU_UNOS(N))
    uint16x4_t  d = vdup_n_u16((UINT16_MAX>>(16-n)));
    float32x2_t m = vreinterpret_f32_u16(d);
    float       f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_UNOS) (Rc(0, 32) n)
{
#define     WWU_UNOS(N)                     \
(                                           \
    (32 == N)                               \
    ?   vget_lane_f32(                      \
            vreinterpret_f32_u32(           \
                vdup_n_u32(UINT32_MAX)      \
            ),                              \
            0                               \
        )                                   \
    :   vget_lane_f32(                      \
            vreinterpret_f32_u32(           \
                vshr_n_u32(                 \
                    vdup_n_u32(UINT32_MAX), \
                    (32-(N&31))             \
                )                           \
            ),                              \
            0                               \
        )                                   \
)

#define     VWWU_UNOS(N)    WWU_ASTV(WWU_UNOS(N))
    uint32x2_t  d = vdup_n_u32((UINT32_MAX>>(32-n)));
    float32x2_t m = vreinterpret_f32_u32(d);
    float       f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_UNOS) (Rc(0, 32) n)
{
#define     VWWI_UNOS(N)    WWI_ASTV(WWU_UNOS(N))
    uint32x2_t  d = vdup_n_u32((UINT32_MAX>>(32-n)));
    float32x2_t m = vreinterpret_f32_u32(d);
    float       f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}


INLINE(Vdyu,VDYU_UNOS) (Rc(0, 1) n)
{
#define     VDYU_UNOS(N)                    \
(                                           \
    (N == 1)                                \
    ?   VDDU_ASYU(vdup_n_u64(UINT64_MAX))   \
    :   ((Vdyu){0})                         \
)

    uint64x1_t  l = vdup_n_u64(n);
    uint64x1_t  r = vdup_n_u64(1);
    l = vceq_u64(l, r);
    return  VDDU_ASYU(l);
}


INLINE(Vdbu,VDBU_UNOS) (Rc(0, 8) n)
{
#define     VDBU_UNOS(N)   vdup_n_u8((UINT8_MAX>>(8-N)))
    int8x8_t r = vdup_n_s8(8-n);
    return  vshl_u8(vdup_n_u8(UINT8_MAX), vneg_s8(r));
}

INLINE(Vdbi,VDBI_UNOS) (Rc(0, 8) n)
{
#define     VDBI_UNOS(N)    vdup_n_s8((UINT8_MAX>>(8-N)))
    return  vreinterpret_s8_u8( (VDBU_UNOS)(n) );
}

INLINE(Vdbc,VDBC_UNOS) (Rc(0, 8) n)
{
#define     VDBC_UNOS(N)    VDBU_ASBC(vdup_n_u8((UINT8_MAX>>(8-N))))
    return  VDBC_UNOS(n);
}


INLINE(Vdhu,VDHU_UNOS) (Rc(0, 16) n)
{
#define     VDHU_UNOS(N)    vdup_n_u16((UINT16_MAX>>(16-N)))
    return  VDHU_UNOS(n);
}

INLINE(Vdhi,VDHI_UNOS) (Rc(0, 16) n)
{
#define     VDHI_UNOS(N)    VDHU_ASHI(VDHU_UNOS(N))
    return  VDHI_UNOS(n);
}


INLINE(Vdwu,VDWU_UNOS) (Rc(0, 32) n)
{
#define     VDWU_UNOS(N)    vdup_n_u32((UINT32_MAX>>(32-N)))
    return  VDWU_UNOS(n);
}

INLINE(Vdwi,VDWI_UNOS) (Rc(0, 32) n)
{
#define     VDWI_UNOS(N)    vreinterpret_s32_u32(VDWU_UNOS(N))
    return  VDWI_UNOS(n);
}


INLINE(Vddu,VDDU_UNOS) (Rc(0, 64) n)
{
#define     VDDU_UNOS(N)    vdup_n_u64((UINT64_MAX>>(64-N)))
    return  VDDU_UNOS(n);
}

INLINE(Vddi,VDDI_UNOS) (Rc(0, 64) n)
{
#define     VDDI_UNOS(N)    vreinterpret_s64_u64(VDDU_UNOS(N))
    return  VDDU_UNOS(n);
}


INLINE(Vqyu,VQYU_UNOS) (Rc(0, 1) n)
{
#define     VQYU_UNOS(N)                    \
(                                           \
    (N == 1)                                \
    ?   VQDU_ASYU(vdupq_n_u64(UINT64_MAX))  \
    :   ((Vqyu){0})                         \
)

    uint64x2_t  l = vdupq_n_u64(n);
    uint64x2_t  r = vdupq_n_u64(1);
    l = vceqq_u64(l, r);
    return  VQDU_ASYU(l);
}


INLINE(Vqbu,VQBU_UNOS) (Rc(0, 8) n)
{
#define     VQBU_UNOS(N)   vdupq_n_u8((UINT8_MAX>>(8-N)))
    return  VQBU_UNOS(n);
}

INLINE(Vqbi,VQBI_UNOS) (Rc(0, 8) n)
{
#define     VQBI_UNOS(N)    vdupq_n_s8((UINT8_MAX>>(8-N)))
    return  vreinterpretq_s8_u8( VQBU_UNOS(n) );
}

INLINE(Vqbc,VQBC_UNOS) (Rc(0, 8) n)
{
#define     VQBC_UNOS(N)    VQBU_ASBC(vdupq_n_u8((UINT8_MAX>>(8-N))))
    return  VQBC_UNOS(n);
}


INLINE(Vqhu,VQHU_UNOS) (Rc(0, 16) n)
{
#define     VQHU_UNOS(N)   vdupq_n_u16((UINT16_MAX>>(16-N)))
    return  VQHU_UNOS(n);
}

INLINE(Vqhi,VQHI_UNOS) (Rc(0, 16) n)
{
#define     VQHI_UNOS(N) \
vreinterpretq_s16_u16(vdupq_n_u16( (UINT16_MAX>>(16-N)) ))

    return  VQHI_UNOS(n);
}


INLINE(Vqwu,VQWU_UNOS) (Rc(0, 32) n)
{
#define     VQWU_UNOS(N)    vdupq_n_u32((UINT32_MAX>>(32-N)))
    return  VQWU_UNOS(n);
}

INLINE(Vqwi,VQWI_UNOS) (Rc(0, 32) n)
{
#define     VQWI_UNOS(N)    vreinterpretq_s32_u32(VQWU_UNOS(N))
    return  VQWI_UNOS(n);
}


INLINE(Vqdu,VQDU_UNOS) (Rc(0, 64) n)
{
#define     VQDU_UNOS(N)    vdupq_n_u64((UINT64_MAX>>(64-N)))
    return  VQDU_UNOS(n);
}

INLINE(Vqdi,VQDI_UNOS) (Rc(0, 64) n)
{
#define     VQDI_UNOS(N)    vreinterpretq_s64_u64(VQDU_UNOS(N))
    return  VQDU_UNOS(n);
}

#if _LEAVE_ARM_UNOS
}
#endif


#if _ENTER_ARM_PERM
{
#endif
/*
    TBX <Vd>.<Ta>, { <Vn>.16B }, <Vm>.<Ta>
    TBX <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B }, <Vm>.<Ta>
    TBX <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B }, <Vm>.<Ta>
    TBX <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B, <Vn+3>.16B }, <Vm>.<Ta>

uint8x16t vqtbx1q_u8(
    uint8x16_t      a (  <Vd.16B>  ),
    uint8x16_t      t ( {<Vn.16B>} ),
    uint8x16_t      i (  <Vm.16B>  )
) => TBX <Vd.16B>, {<Vn.16B>}, <Vm.16B>

uint8x16t vqtbx2q_u8(
    uint8x16_t      a (  <Vd.16B>  ),
    uint8x16_t      t ( {<Vn.16B>, <Vn+1.16B} ),
    uint8x16_t      i (  <Vm.16B>  )
) => TBX <Vd.16B>, {<Vn.16B>}, <Vm.16B>

*/

#define     V2_PERM(F,A,B,K0,K1)        F((A),(B),(K0),(K1))
#define     V4_PERM(F,A,B,K0,K1,K2,K3)  F((A),(B),(K0),(K1),(K2),(K3))
#define     V8_PERM(F,A,B,                  \
    K0,  K1,  K2,  K3,  K4,  K5,  K6,  K7)  \
F(                                          \
    (A),(B),                                \
    (K0),(K1),(K2),(K3),(K4),(K5),(K6),(K7) \
)

#define     V16_PERM(                               \
    F,A,B,                                          \
    K0,  K1,  K2,  K3,  K4,  K5,  K6,  K7,          \
    K8,  K9,  K10, K11, K12, K13, K14, K15  )       \
F(                                                  \
    (A),(B),                                         \
    (K0 ),(K1 ),(K2 ),(K3 ),(K4 ),(K5 ),(K6 ),(K7 ),\
    (K8 ),(K9 ),(K10),(K11),(K12),(K13),(K14),(K15) \
)

#define     V32_PERM(                   \
    F, A, B,                            \
    K0, K1, K2, K3, K4, K5, K6, K7,     \
    K8, K9, K10,K11,K12,K13,K14,K15,    \
    K16,K17,K18,K19,K20,K21,K22,K23,    \
    K24,K25,K26,K27,K28,K29,K30,K31)    \
F(                                      \
    (A),(B),                            \
    (K0 ),(K1 ),(K2 ),(K3 ),            \
    (K4 ),(K5 ),(K6 ),(K7 ),            \
    (K8 ),(K9 ),(K10),(K11),            \
    (K12),(K13),(K14),(K15),            \
    (K16),(K17),(K18),(K19),            \
    (K20),(K21),(K22),(K23),            \
    (K24),(K25),(K26),(K27),            \
    (K28),(K29),(K30),(K31)             \
)

#define     V64_PERM(                   \
    F, A, B,                            \
    K0, K1, K2, K3, K4, K5, K6, K7,     \
    K8, K9, K10,K11,K12,K13,K14,K15,    \
    K16,K17,K18,K19,K20,K21,K22,K23,    \
    K24,K25,K26,K27,K28,K29,K30,K31,    \
    K32,K33,K34,K35,K36,K37,K38,K39,    \
    K40,K41,K42,K43,K44,K45,K46,K47,    \
    K48,K49,K50,K51,K52,K53,K54,K55,    \
    K56,K57,K58,K59,K60,K61,K62,K63)    \
F(                                      \
    (A),(B),                            \
    (K0 ),(K1 ),(K2 ),(K3 ),(K4 ),(K5 ),(K6 ),(K7 ),\
    (K8 ),(K9 ),(K10),(K11),(K12),(K13),(K14),(K15),\
    (K16),(K17),(K18),(K19),(K20),(K21),(K22),(K23),\
    (K24),(K25),(K26),(K27),(K28),(K29),(K30),(K31),\
    (K32),(K33),(K34),(K35),(K36),(K37),(K38),(K39),\
    (K40),(K41),(K42),(K43),(K44),(K45),(K46),(K47),\
    (K48),(K49),(K50),(K51),(K52),(K53),(K54),(K55),\
    (K56),(K57),(K58),(K59),(K60),(K61),(K62),(K63) \
)

#define     WB_PERM(A,B, K0,K1,K2,K3)           \
vget_lane_f32(                                  \
    vreinterpret_f32_u8(                        \
        DB_PERM(                                \
            vreinterpret_u8_f32(vdup_n_f32(A)), \
            vreinterpret_u8_f32(vdup_n_f32(B)), \
            K0, K1, K2, K3,                     \
            -1, -1, -1, -1                      \
        )                                       \
    ),                                          \
    V2_K0                                       \
)

#define     WH_PERM(A, B, K0, K1)               \
vget_lane_f32(                                  \
    vreinterpret_f32_u8(                        \
        DH_PERM(                                \
            vreinterpret_u8_f32(vdup_n_f32(A)), \
            vreinterpret_u8_f32(vdup_n_f32(B)), \
            K0, K1, -1, -1                      \
        )                                       \
    ),                                          \
    V2_K0                                       \
)


#define     DB_PERM(                \
    A, B,                           \
    K0, K1, K2, K3, K4, K5, K6, K7) \
vtbx1_u8(                           \
    A, B,                           \
    vreinterpret_u8_u64(            \
        vdup_n_u64(                 \
            ((K0&0xffull)<<000)     \
        |   ((K1&0xffull)<<010)     \
        |   ((K2&0xffull)<<020)     \
        |   ((K3&0xffull)<<030)     \
        |   ((K4&0xffull)<<040)     \
        |   ((K5&0xffull)<<050)     \
        |   ((K6&0xffull)<<060)     \
        |   ((K7&0xffull)<<070)     \
        )                           \
    )                               \
)

#define     DH_PERM(A, B, K0, K1, K2, K3)           \
vtbx1_u8(                                           \
    A, B,                                           \
    vreinterpret_u8_u64(                            \
        vdup_n_u64(                                 \
            (                                       \
                (K0&0xfc) ? 0xffffull :             \
                (                                   \
                    ((HALF_B0+2ull*K0)<<000)        \
                |   ((HALF_B1+2ull*K0)<<010)        \
                )                                   \
            )                                       \
        |   (                                       \
                (K1&0xfc) ? (0xffffull<<020) :      \
                (                                   \
                    ((HALF_B0+2ull*K1)<<020)        \
                |   ((HALF_B1+2ull*K1)<<030)        \
                )                                   \
            )                                       \
        |   (                                       \
                (K2&0xfc) ? (0xffffull<<040) :      \
                (                                   \
                    ((HALF_B0+2ull*K2)<<040)        \
                |   ((HALF_B1+2ull*K2)<<050)        \
                )                                   \
            )                                       \
        |   (                                       \
                (K3&0xfc) ? (0xffffull<<060) :      \
                (                                   \
                    ((HALF_B0+2ull*K3)<<060)        \
                |   ((HALF_B1+2ull*K3)<<070)        \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     DW_PERM(A, B, K0, K1)                   \
vtbx1_u8(                                           \
    A, B,                                           \
    vreinterpret_u8_u64(                            \
        vdup_n_u64(                                 \
            (                                       \
                (K0&0xfe) ? (0xffffffffull<<000) :  \
                (                                   \
                    ((WORD_B0+K0*4ull)<<000)        \
                |   ((WORD_B1+K0*4ull)<<010)        \
                |   ((WORD_B2+K0*4ull)<<020)        \
                |   ((WORD_B3+K0*4ull)<<030)        \
                )                                   \
            )                                       \
        |   (                                       \
                (K1&0xfe) ? (0xffffffffull<<040) :  \
                (                                   \
                    ((WORD_B0+K1*4ull)<<040)        \
                |   ((WORD_B1+K1*4ull)<<050)        \
                |   ((WORD_B2+K1*4ull)<<060)        \
                |   ((WORD_B3+K1*4ull)<<070)        \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     QB_PERM(                \
    A, B,                           \
    K0,  K1,  K2,  K3,              \
    K4,  K5,  K6,  K7,              \
    K8,  K9,  K10, K11,             \
    K12, K13, K14, K15)             \
vqtbx1q_u8(                         \
    A,B,                            \
    vreinterpretq_u8_u64(           \
        vcombine_u64(               \
            vdup_n_u64(             \
                ((K0 &255ull)<<000) \
            |   ((K1 &255ull)<<010) \
            |   ((K2 &255ull)<<020) \
            |   ((K3 &255ull)<<030) \
            |   ((K4 &255ull)<<040) \
            |   ((K5 &255ull)<<050) \
            |   ((K6 &255ull)<<060) \
            |   ((K7 &255ull)<<070) \
            ),                      \
            vdup_n_u64(             \
                ((K8 &255ull)<<000) \
            |   ((K9 &255ull)<<010) \
            |   ((K10&255ull)<<020) \
            |   ((K11&255ull)<<030) \
            |   ((K12&255ull)<<040) \
            |   ((K13&255ull)<<050) \
            |   ((K14&255ull)<<060) \
            |   ((K15&255ull)<<070) \
            )                       \
        )                           \
    )                               \
)

#define     QH_PERM(                \
    A, B,                           \
    K0, K1, K2, K3, K4, K5, K6, K7) \
vqtbx1q_u8(                                         \
    A,B,                                            \
    vreinterpretq_u8_u64(                           \
        vcombine_u64(                               \
            vdup_n_u64(                             \
                (                                   \
                    (K0&0xf8) ? 0xffffull :         \
                    (                               \
                        ((HALF_B0+2ull*K0)<<000)    \
                    |   ((HALF_B1+2ull*K0)<<010)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K1&0xf8) ? (0xffffull<<020) :  \
                    (                               \
                        ((HALF_B0+2ull*K1)<<020)    \
                    |   ((HALF_B1+2ull*K1)<<030)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K2&0xf8) ? (0xffffull<<040) :  \
                    (                               \
                        ((HALF_B0+2ull*K2)<<040)    \
                    |   ((HALF_B1+2ull*K2)<<050)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K3&0xf8) ? (0xffffull<<060) :  \
                    (                               \
                        ((HALF_B0+2ull*K3)<<060)    \
                    |   ((HALF_B1+2ull*K3)<<070)    \
                    )                               \
                )                                   \
            ),                                      \
            vdup_n_u64(                             \
                (                                   \
                    (K4&0xf8) ? 0xffffull :         \
                    (                               \
                        ((2ull*K4+HALF_B0)<<000)    \
                    |   ((2ull*K4+HALF_B1)<<010)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K5&0xf8) ? (0xffffull<<020) :  \
                    (                               \
                        ((HALF_B0+2ull*K5)<<020)    \
                    |   ((HALF_B1+2ull*K5)<<030)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K6&0xf8) ? (0xffffull<<040) :  \
                    (                               \
                        ((HALF_B0+2ull*K6)<<040)    \
                    |   ((HALF_B1+2ull*K6)<<050)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (K7&0xf8) ? (0xffffull<<060) :  \
                    (                               \
                        ((HALF_B0+2ull*K7)<<060)    \
                    |   ((HALF_B1+2ull*K7)<<070)    \
                    )                               \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     QW_PERM(A, B, K0, K1, K2, K3)               \
vqtbx1q_u8(                                             \
    A, B,                                               \
    vreinterpretq_u8_u64(                               \
        vcombine_u64(                                   \
            vdup_n_u64(                                 \
                (                                       \
                    (K0&0x80) ? (0xffffffffull<<000) :  \
                    (                                   \
                        ((WORD_B0+K0*4ull)<<000)        \
                    |   ((WORD_B1+K0*4ull)<<010)        \
                    |   ((WORD_B2+K0*4ull)<<020)        \
                    |   ((WORD_B3+K0*4ull)<<030)        \
                    )                                   \
                )                                       \
                |                                       \
                (                                       \
                    (K1&0x80) ? (0xffffffffull<<040) :  \
                    (                                   \
                        ((WORD_B0+K1*4ull)<<040)        \
                    |   ((WORD_B1+K1*4ull)<<050)        \
                    |   ((WORD_B2+K1*4ull)<<060)        \
                    |   ((WORD_B3+K1*4ull)<<070)        \
                    )                                   \
                )                                       \
            ),                                          \
            vdup_n_u64(                                 \
                (                                       \
                    (K2&0x80) ? (0xffffffffull<<000) :  \
                    (                                   \
                        ((WORD_B0+K2*4ull)<<000)        \
                    |   ((WORD_B1+K2*4ull)<<010)        \
                    |   ((WORD_B2+K2*4ull)<<020)        \
                    |   ((WORD_B3+K2*4ull)<<030)        \
                    )                                   \
                )                                       \
                |                                       \
                (                                       \
                    (K3&0x80) ? (0xffffffffull<<040) :  \
                    (                                   \
                        ((WORD_B0+K3*4ull)<<040)        \
                    |   ((WORD_B1+K3*4ull)<<050)        \
                    |   ((WORD_B2+K3*4ull)<<060)        \
                    |   ((WORD_B3+K3*4ull)<<070)        \
                    )                                   \
                )                                       \
            )                                           \
        )                                               \
    )                                                   \
)

#define     QD_PERM(A, B, K0, K1)   \
vqtbx1q_u8(                         \
    A, B,                           \
    vcombine_u8(                    \
        vdup_n_u8(254&K0 ? -1 : K0),\
        vdup_n_u8(254&K1 ? -1 : K1) \
    )                               \
)

#define     WBU_PERM(A, B, ...)     V4_PERM(WB_PERM,(A),(B),__VA_ARGS__)
#define     WBI_PERM(A, B, ...)     V4_PERM(WB_PERM,(A),(B),__VA_ARGS__)
#define     WBC_PERM(A, B, ...)     V4_PERM(WB_PERM,(A),(B),__VA_ARGS__)

#define     WHU_PERM(A, B, ...)     V2_PERM(WH_PERM,(A),(B),__VA_ARGS__)
#define     WHI_PERM(A, B, ...)     V2_PERM(WH_PERM,(A),(B),__VA_ARGS__)
#define     WHF_PERM(A, B, ...)     V2_PERM(WH_PERM,(A),(B),__VA_ARGS__)

#define     DBU_PERM(A, B, ...) \
         V8_PERM(DB_PERM,        (A),         (B), __VA_ARGS__)

#define     DBI_PERM(A, B, ...) \
DBU_ASBI(V8_PERM(DB_PERM,DBI_ASBU(A), DBI_ASBU(B), __VA_ARGS__))

#define     DBC_PERM(A, B, ...) \
DBU_ASBC(V8_PERM(DB_PERM,DBC_ASBU(A), DBC_ASBU(B), __VA_ARGS__))


#define     DHU_PERM(A, B, ...) \
DBU_ASHU(V4_PERM(DH_PERM,DHU_ASBU(A), DHU_ASBU(B), __VA_ARGS__))

#define     DHI_PERM(A, B, ...) \
DBU_ASHI(V4_PERM(DH_PERM,DHI_ASBU(A), DHI_ASBU(B), __VA_ARGS__))

#define     DHF_PERM(A, B, ...) \
DBU_ASHF(V4_PERM(DH_PERM,DHF_ASBU(A), DHF_ASBU(B), __VA_ARGS__))


#define     DWU_PERM(A, B, ...) \
DBU_ASWU(V2_PERM(DW_PERM,DWU_ASBU(A), DWU_ASBU(B), __VA_ARGS__))

#define     DWI_PERM(A, B, ...) \
DBU_ASWI(V2_PERM(DW_PERM,DWI_ASBU(A), DWI_ASBU(B), __VA_ARGS__))

#define     DWF_PERM(A, B, ...) \
DBU_ASWF(V2_PERM(DW_PERM,DWF_ASBU(A), DWF_ASBU(B), __VA_ARGS__))


#define     QBU_PERM(A, B, ...) \
         V16_PERM(QB_PERM,         (A),         (B), __VA_ARGS__)

#define     QBI_PERM(A, B, ...) \
QBU_ASBI(V16_PERM(QB_PERM, QBI_ASBU(A), QBI_ASBU(B), __VA_ARGS__))

#define     QBC_PERM(A, B, ...) \
QBU_ASBC(V16_PERM(QB_PERM, QBC_ASBU(A), QBC_ASBU(B), __VA_ARGS__))


#define     QHU_PERM(A, B, ...) \
QBU_ASHU( V8_PERM(QH_PERM, QHU_ASBU(A), QHU_ASBU(B), __VA_ARGS__))

#define     QHI_PERM(A, B, ...) \
QBU_ASHI( V8_PERM(QH_PERM, QHI_ASBU(A), QHI_ASBU(B), __VA_ARGS__))

#define     QHF_PERM(A, B, ...) \
QBU_ASHF( V8_PERM(QH_PERM, QHF_ASBU(A), QHU_ASBU(B), __VA_ARGS__))


#define     QWU_PERM(A, B, ...) \
QBU_ASWU( V4_PERM(QW_PERM, QWU_ASBU(A), QWU_ASBU(B), __VA_ARGS__))

#define     QWI_PERM(A, B, ...) \
QBU_ASWI( V4_PERM(QW_PERM, QWI_ASBU(A), QWI_ASBU(B), __VA_ARGS__))

#define     QWF_PERM(A, B, ...) \
QBU_ASWF( V4_PERM(QW_PERM, QWF_ASBU(A), QWF_ASBU(B), __VA_ARGS__))


#define     QDU_PERM(A, B, ...) \
QBU_ASDU( V2_PERM(QD_PERM, QDU_ASBU(A), QDU_ASBU(B), __VA_ARGS__))

#define     QDI_PERM(A, B, ...) \
QBU_ASDI( V2_PERM(QD_PERM, QDI_ASBU(A), QDI_ASBU(B), __VA_ARGS__))

#define     QDF_PERM(A, B, ...) \
QBU_ASDF( V2_PERM(QD_PERM, QDF_ASBU(A), QDF_ASBU(B), __VA_ARGS__))

INLINE(Vwbu,VWBU_PERM)
(
    Vwbu a, Vwbu b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VWBU_PERM(A, B, ...)    \
WBU_ASTV(                           \
    WBU_PERM(                       \
        VWBU_ASTM(A),               \
        VWBU_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWBU_PERM(a, b, k0,k1,k2,k3);
}

INLINE(Vwbi,VWBI_PERM)
(
    Vwbi a, Vwbi b,
    Rc(-1,3) k0, Rc(-1,3) k1, Rc(-1,3) k2, Rc(-1,3) k3
)
{
#define     VWBI_PERM(A, B, ...)    \
WBI_ASTV(                           \
    WBI_PERM(                       \
        VWBI_ASTM(A),               \
        VWBI_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWBI_PERM(a, b, k0,k1,k2,k3);
}

INLINE(Vwbc,VWBC_PERM)
(
    Vwbc a, Vwbc b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VWBC_PERM(A, B, ...)    \
WBC_ASTV(                           \
    WBC_PERM(                       \
        VWBC_ASTM(A),               \
        VWBC_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWBC_PERM(a, b, k0,k1,k2,k3);
}


INLINE(Vwhu,VWHU_PERM)
(
    Vwhu a, Vwhu b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VWHU_PERM(A, B, ...)    \
WHU_ASTV(                           \
    WHU_PERM(                       \
        VWHU_ASTM(A),               \
        VWHU_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWHU_PERM(a, b, k0,k1);
}


INLINE(Vwhi,VWHI_PERM)
(
    Vwhi a, Vwhi b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VWHI_PERM(A, B, ...)    \
WHI_ASTV(                           \
    WHI_PERM(                       \
        VWHI_ASTM(A),               \
        VWHI_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWHI_PERM(a, b, k0,k1);
}

INLINE(Vwhf,VWHF_PERM)
(
    Vwhf a, Vwhf b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VWHF_PERM(A, B, ...)    \
WHF_ASTV(                           \
    WHF_PERM(                       \
        VWHF_ASTM(A),               \
        VWHF_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VWHF_PERM(a, b, k0,k1);
}


INLINE(Vdbu,VDBU_PERM)
(
    Vdbu a, Vdbu b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VDBU_PERM(...)  DBU_PERM(__VA_ARGS__)
    return  DBU_PERM(a,b, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vdbi,VDBI_PERM)
(
    Vdbi a, Vdbi b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VDBI_PERM(...)  DBI_PERM(__VA_ARGS__)
    return  DBI_PERM(a,b, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vdbc,VDBC_PERM)
(
    Vdbc a, Vdbc b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VDBC_PERM(A, B, ...)    \
DBC_ASTV(                           \
    DBC_PERM(                       \
        VDBC_ASTM(A),               \
        VDBC_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VDBC_PERM(a,b, k0,k1,k2,k3,k4,k5,k6,k7);
}


INLINE(Vdhu,VDHU_PERM)
(
    Vdhu a, Vdhu b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VDHU_PERM(...)   DHU_PERM(__VA_ARGS__)
    return  DHU_PERM(a,b, k0,k1,k2,k3);
}

INLINE(Vdhi,VDHI_PERM)
(
    Vdhi a, Vdhi b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VDHI_PERM(...)   DHI_PERM(__VA_ARGS__)
    return  DHI_PERM(a,b, k0,k1,k2,k3);
}

INLINE(Vdhf,VDHF_PERM)
(
    Vdhf a, Vdhf b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VDHF_PERM(...)   DHF_PERM(__VA_ARGS__)
    return  DHF_PERM(a,b, k0,k1,k2,k3);
}


INLINE(Vdwu,VDWU_PERM)
(
    Vdwu a, Vdwu b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VDWU_PERM(...)  DWU_PERM(__VA_ARGS__)
    return  DWU_PERM(a,b, k0,k1);
}

INLINE(Vdwi,VDWI_PERM)
(
    Vdwi a, Vdwi b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VDWI_PERM(...)  DWI_PERM(__VA_ARGS__)
    return  DWI_PERM(a,b, k0,k1);
}

INLINE(Vdwf,VDWF_PERM)
(
    Vdwf a, Vdwf b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VDWF_PERM(...)  DWF_PERM(__VA_ARGS__)
    return  DWF_PERM(a,b, k0,k1);
}


INLINE(Vqbu,VQBU_PERM)
(
    Vqbu a, Vqbu b,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBU_PERM(...)  QBU_PERM(__VA_ARGS__)
    return  QBU_PERM(
        a, b,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqbi,VQBI_PERM)
(
    Vqbi a, Vqbi b,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBI_PERM(...)  QBI_PERM(__VA_ARGS__)
    return  QBI_PERM(
        a, b,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqbc,VQBC_PERM)
(
    Vqbc a, Vqbc b,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBC_PERM(A, B, ...)    \
QBC_ASTV(                           \
    QBC_PERM(                       \
        VQBC_ASTM(A),               \
        VQBC_ASTM(B),               \
        __VA_ARGS__                 \
    )                               \
)
    return  VQBC_PERM(
        a,b,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqhu,VQHU_PERM)
(
    Vqhu a, Vqhu b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VQHU_PERM(...)  QHU_PERM(__VA_ARGS__)
    return  QHU_PERM(a,b, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vqhi,VQHI_PERM)
(
    Vqhi a, Vqhi b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VQHI_PERM(...)  QHI_PERM(__VA_ARGS__)
    return  QHI_PERM(a,b, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vqhf,VQHF_PERM)
(
    Vqhf a, Vqhf b,
    Rc(-1,+7) k0, Rc(-1,+7) k1, Rc(-1,+7) k2, Rc(-1,+7) k3,
    Rc(-1,+7) k4, Rc(-1,+7) k5, Rc(-1,+7) k6, Rc(-1,+7) k7
)
{
#define     VQHF_PERM(...)  QHF_PERM(__VA_ARGS__)
    return  QHF_PERM(a,b, k0,k1,k2,k3, k4,k5,k6,k7);
}


INLINE(Vqwu,VQWU_PERM)
(
    Vqwu a, Vqwu b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VQWU_PERM(...)  QWU_PERM(__VA_ARGS__)
    return  QWU_PERM(a,b, k0,k1,k2,k3);
}

INLINE(Vqwi,VQWI_PERM)
(
    Vqwi a, Vqwi b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VQWI_PERM(...)  QWI_PERM(__VA_ARGS__)
    return  QWI_PERM(a,b, k0,k1,k2,k3);
}

INLINE(Vqwf,VQWF_PERM)
(
    Vqwf a, Vqwf b,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VQWF_PERM(...)  QWF_PERM(__VA_ARGS__)
    return  QWF_PERM(a,b, k0,k1,k2,k3);
}

INLINE(Vqdu,VQDU_PERM)
(
    Vqdu a, Vqdu b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VQDU_PERM(...)  QDU_PERM(__VA_ARGS__)
    return  QDU_PERM(a,b, k0,k1);
}

INLINE(Vqdi,VQDI_PERM)
(
    Vqdi a, Vqdi b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VQDI_PERM(...)  QDI_PERM(__VA_ARGS__)
    return  QDI_PERM(a,b, k0,k1);
}

INLINE(Vqdf,VQDF_PERM)
(
    Vqdf a, Vqdf b,
    Rc(-1,+1) k0, Rc(-1,+1) k1
)
{
#define     VQDF_PERM(...)  QDF_PERM(__VA_ARGS__)
    return  QDF_PERM(a,b, k0,k1);
}

#if _LEAVE_ARM_PERM_
}
#endif

#if _ENTER_ARM_PERS
{
#endif

#define     V2_PERS(f, v, k0, k1)           f((v),(k0),(k1))
#define     V4_PERS(f, v, k0, k1, k2, k3)   f((v),(k0),(k1),(k2),(k3))
#define     V8_PERS(f, v,                   \
    k0,  k1,  k2,  k3,  k4,  k5,  k6,  k7)  \
f(                                          \
    (v),                                    \
    (k0),(k1),(k2),(k3),(k4),(k5),(k6),(k7) \
)

#define     V16_PERS(                               \
    f, v,                                           \
    k0,  k1,  k2,  k3,  k4,  k5,  k6,  k7,          \
    k8,  k9,  k10, k11, k12, k13, k14, k15  )       \
f(                                                  \
    (v),                                            \
    (k0 ),(k1 ),(k2 ),(k3 ),(k4 ),(k5 ),(k6 ),(k7 ),\
    (k8 ),(k9 ),(k10),(k11),(k12),(k13),(k14),(k15) \
)

#define     V32_PERS(                   \
    f, v,                               \
    k0, k1, k2, k3, k4, k5, k6, k7,     \
    k8, k9, k10,k11,k12,k13,k14,k15,    \
    k16,k17,k18,k19,k20,k21,k22,k23,    \
    k24,k25,k26,k27,k28,k29,k30,k31)    \
f(                                      \
    (v),                                \
    (k0 ),(k1 ),(k2 ),(k3 ),(k4 ),(k5 ),(k6 ),(k7 ),\
    (k8 ),(k9 ),(k10),(k11),(k12),(k13),(k14),(k15),\
    (k16),(k17),(k18),(k19),(k20),(k21),(k22),(k23),\
    (k24),(k25),(k26),(k27),(k28),(k29),(k30),(k31) \
)

#define     V64_PERS(                   \
    f, v,                               \
    k0, k1, k2, k3, k4, k5, k6, k7,     \
    k8, k9, k10,k11,k12,k13,k14,k15,    \
    k16,k17,k18,k19,k20,k21,k22,k23,    \
    k24,k25,k26,k27,k28,k29,k30,k31,    \
    k32,k33,k34,k35,k36,k37,k38,k39,    \
    k40,k41,k42,k43,k44,k45,k46,k47,    \
    k48,k49,k50,k51,k52,k53,k54,k55,    \
    k56,k57,k58,k59,k60,k61,k62,k63)    \
f(                                      \
    v,                                  \
    (k0 ),(k1 ),(k2 ),(k3 ),(k4 ),(k5 ),(k6 ),(k7 ),\
    (k8 ),(k9 ),(k10),(k11),(k12),(k13),(k14),(k15),\
    (k16),(k17),(k18),(k19),(k20),(k21),(k22),(k23),\
    (k24),(k25),(k26),(k27),(k28),(k29),(k30),(k31),\
    (k32),(k33),(k34),(k35),(k36),(k37),(k38),(k39),\
    (k40),(k41),(k42),(k43),(k44),(k45),(k46),(k47),\
    (k48),(k49),(k50),(k51),(k52),(k53),(k54),(k55),\
    (k56),(k57),(k58),(k59),(k60),(k61),(k62),(k63) \
)

#define     WB_PERS(v, B0, B1, B2, B3)          \
vget_lane_f32(                                  \
    vreinterpret_f32_u8(                        \
        DB_PERS(                                \
            vreinterpret_u8_f32(vdup_n_f32(v)), \
            B0, B1, B2, B3,                     \
            -1, -1, -1, -1                      \
        )                                       \
    ),                                          \
    V2_K0                                       \
)

#define     WH_PERS(v, H0, H1)                  \
vget_lane_f32(                                  \
    vreinterpret_f32_u8(                        \
        DH_PERS(                                \
            vreinterpret_u8_f32(vdup_n_f32(v)), \
            H0, H1, -1, -1                      \
        )                                       \
    ),                                          \
    V2_K0                                       \
)


#define     DB_PERS(v, B0,B1,B2,B3,B4,B5,B6,B7) \
vtbl1_u8(                       \
    v,                          \
    vreinterpret_u8_u64(        \
        vdup_n_u64(             \
            ((0xffull&B0)<<000) \
        |   ((0xffull&B1)<<010) \
        |   ((0xffull&B2)<<020) \
        |   ((0xffull&B3)<<030) \
        |   ((0xffull&B4)<<040) \
        |   ((0xffull&B5)<<050) \
        |   ((0xffull&B6)<<060) \
        |   ((0xffull&B7)<<070) \
        )                       \
    )                           \
)

#define     DH_PERS(v, H0, H1, H2, H3)              \
vtbl1_u8(                                           \
    v,                                              \
    vreinterpret_u8_u64(                            \
        vdup_n_u64(                                 \
            (                                       \
                (H0&0x80) ? 0xffffull :             \
                (                                   \
                    ((HALF_B0+2ull*H0)<<000)        \
                |   ((HALF_B1+2ull*H0)<<010)        \
                )                                   \
            )                                       \
        |   (                                       \
                (H1&0x80) ? (0xffffull<<020) :      \
                (                                   \
                    ((HALF_B0+2ull*H1)<<020)        \
                |   ((HALF_B1+2ull*H1)<<030)        \
                )                                   \
            )                                       \
        |   (                                       \
                (H2&0x80) ? (0xffffull<<040) :      \
                (                                   \
                    ((HALF_B0+2ull*H2)<<040)        \
                |   ((HALF_B1+2ull*H2)<<050)        \
                )                                   \
            )                                       \
        |   (                                       \
                (H3&0x80) ? (0xffffull<<060) :      \
                (                                   \
                    ((HALF_B0+2ull*H3)<<060)        \
                |   ((HALF_B1+2ull*H3)<<070)        \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     DW_PERS(v, W0, W1)                      \
vtbl1_u8(                                           \
    v,                                              \
    vreinterpret_u8_u64(                            \
        vdup_n_u64(                                 \
            (                                       \
                (W0&0x80) ? (0xffffffffull<<000) :  \
                (                                   \
                    ((WORD_B0+W0*4ull)<<000)        \
                |   ((WORD_B1+W0*4ull)<<010)        \
                |   ((WORD_B2+W0*4ull)<<020)        \
                |   ((WORD_B3+W0*4ull)<<030)        \
                )                                   \
            )                                       \
        |   (                                       \
                (W1&0x80) ? (0xffffffffull<<040) :  \
                (                                   \
                    ((WORD_B0+W1*4ull)<<040)        \
                |   ((WORD_B1+W1*4ull)<<050)        \
                |   ((WORD_B2+W1*4ull)<<060)        \
                |   ((WORD_B3+W1*4ull)<<070)        \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     QB_PERS(                \
    v,                              \
    B0,  B1,  B2,  B3,              \
    B4,  B5,  B6,  B7,              \
    B8,  B9,  B10, B11,             \
    B12, B13, B14, B15              \
)                                   \
vqtbl1q_u8(                         \
    v,                              \
    vreinterpretq_u8_u64(           \
        vcombine_u64(               \
            vdup_n_u64(             \
                ((B0 &255ull)<<000) \
            |   ((B1 &255ull)<<010) \
            |   ((B2 &255ull)<<020) \
            |   ((B3 &255ull)<<030) \
            |   ((B4 &255ull)<<040) \
            |   ((B5 &255ull)<<050) \
            |   ((B6 &255ull)<<060) \
            |   ((B7 &255ull)<<070) \
            ),                      \
            vdup_n_u64(             \
                ((B8 &255ull)<<000) \
            |   ((B9 &255ull)<<010) \
            |   ((B10&255ull)<<020) \
            |   ((B11&255ull)<<030) \
            |   ((B12&255ull)<<040) \
            |   ((B13&255ull)<<050) \
            |   ((B14&255ull)<<060) \
            |   ((B15&255ull)<<070) \
            )                       \
        )                           \
    )                               \
)

#define     QH_PERS(v, H0,H1,H2,H3,H4,H5,H6,H7)     \
vqtbl1q_u8(                                         \
    v,                                              \
    vreinterpretq_u8_u64(                           \
        vcombine_u64(                               \
            vdup_n_u64(                             \
                (                                   \
                    (H0&0x80) ? 0xffffull :         \
                    (                               \
                        ((HALF_B0+2ull*H0)<<000)    \
                    |   ((HALF_B1+2ull*H0)<<010)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H1&0x80) ? (0xffffull<<020) :  \
                    (                               \
                        ((HALF_B0+2ull*H1)<<020)    \
                    |   ((HALF_B1+2ull*H1)<<030)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H2&0x80) ? (0xffffull<<040) :  \
                    (                               \
                        ((HALF_B0+2ull*H2)<<040)    \
                    |   ((HALF_B1+2ull*H2)<<050)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H3&0x80) ? (0xffffull<<060) :  \
                    (                               \
                        ((HALF_B0+2ull*H3)<<060)    \
                    |   ((HALF_B1+2ull*H3)<<070)    \
                    )                               \
                )                                   \
            ),                                      \
            vdup_n_u64(                             \
                (                                   \
                    (H4&0x80) ? 0xffffull :         \
                    (                               \
                        ((HALF_B0+2ull*H4)<<000)    \
                    |   ((HALF_B1+2ull*H4)<<010)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H5&0x80) ? (0xffffull<<020) :  \
                    (                               \
                        ((HALF_B0+2ull*H5)<<020)    \
                    |   ((HALF_B1+2ull*H5)<<030)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H6&0x80) ? (0xffffull<<040) :  \
                    (                               \
                        ((HALF_B0+2ull*H6)<<040)    \
                    |   ((HALF_B1+2ull*H6)<<050)    \
                    )                               \
                )                                   \
                |                                   \
                (                                   \
                    (H7&0x80) ? (0xffffull<<060) :  \
                    (                               \
                        ((HALF_B0+2ull*H7)<<060)    \
                    |   ((HALF_B1+2ull*H7)<<070)    \
                    )                               \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

#define     QW_PERS(v, W0, W1, W2, W3)                  \
vqtbl1q_u8(                                             \
    v,                                                  \
    vreinterpretq_u8_u64(                               \
        vcombine_u64(                                   \
            vdup_n_u64(                                 \
                (                                       \
                    (W0&0x80) ? (0xffffffffull<<000) :  \
                    (                                   \
                        ((WORD_B0+W0*4ull)<<000)        \
                    |   ((WORD_B1+W0*4ull)<<010)        \
                    |   ((WORD_B2+W0*4ull)<<020)        \
                    |   ((WORD_B3+W0*4ull)<<030)        \
                    )                                   \
                )                                       \
                |                                       \
                (                                       \
                    (W1&0x80) ? (0xffffffffull<<040) :  \
                    (                                   \
                        ((WORD_B0+W1*4ull)<<040)        \
                    |   ((WORD_B1+W1*4ull)<<050)        \
                    |   ((WORD_B2+W1*4ull)<<060)        \
                    |   ((WORD_B3+W1*4ull)<<070)        \
                    )                                   \
                )                                       \
            ),                                          \
            vdup_n_u64(                                 \
                (                                       \
                    (W2&0x80) ? (0xffffffffull<<000) :  \
                    (                                   \
                        ((WORD_B0+W2*4ull)<<000)        \
                    |   ((WORD_B1+W2*4ull)<<010)        \
                    |   ((WORD_B2+W2*4ull)<<020)        \
                    |   ((WORD_B3+W2*4ull)<<030)        \
                    )                                   \
                )                                       \
                |                                       \
                (                                       \
                    (W3&0x80) ? (0xffffffffull<<040) :  \
                    (                                   \
                        ((WORD_B0+W3*4ull)<<040)        \
                    |   ((WORD_B1+W3*4ull)<<050)        \
                    |   ((WORD_B2+W3*4ull)<<060)        \
                    |   ((WORD_B3+W3*4ull)<<070)        \
                    )                                   \
                )                                       \
            )                                           \
        )                                               \
    )                                                   \
)

#define     QD_PERS(v, D0, D1)      \
vqtbl1q_u8(                         \
    v,                              \
    vcombine_u8(                    \
        vdup_n_u8(128&D0 ? -1 : D0),\
        vdup_n_u8(128&D1 ? -1 : D1) \
    )                               \
)

#define     WBU_PERS(M, ...)    V4_PERS(WB_PERS,(M),__VA_ARGS__)
#define     WBI_PERS(M, ...)    V4_PERS(WB_PERS,(M),__VA_ARGS__)
#define     WBC_PERS(M, ...)    V4_PERS(WB_PERS,(M),__VA_ARGS__)

#define     WHU_PERS(M, ...)    V2_PERS(WH_PERS,(M),__VA_ARGS__)
#define     WHI_PERS(M, ...)    V2_PERS(WH_PERS,(M),__VA_ARGS__)
#define     WHF_PERS(M, ...)    V2_PERS(WH_PERS,(M),__VA_ARGS__)


#define     DBU_PERS(M, ...)    \
        V8_PERS(DB_PERS,          (M), __VA_ARGS__)

#define     DBI_PERS(M, ...)    \
DBU_ASBI(V8_PERS(DB_PERS, DBI_ASBU(M), __VA_ARGS__))

#define     DBC_PERS(M, ...)    \
DBU_ASBC(V8_PERS(DB_PERS, DBC_ASBU(M), __VA_ARGS__))


#define     DHU_PERS(M, ...)    \
DBU_ASHU(V4_PERS(DH_PERS, DHU_ASBU(M), __VA_ARGS__))

#define     DHI_PERS(M, ...)    \
DBU_ASHI(V4_PERS(DH_PERS, DHI_ASBU(M), __VA_ARGS__))

#define     DHF_PERS(M, ...)    \
DBU_ASHF(V4_PERS(DH_PERS, DHF_ASBU(M), __VA_ARGS__))


#define     DWU_PERS(M, ...)    \
DBU_ASWU(V2_PERS(DW_PERS, DWU_ASBU(M), __VA_ARGS__))

#define     DWI_PERS(M, ...)    \
DBU_ASWI(V2_PERS(DW_PERS, DWI_ASBU(M), __VA_ARGS__))

#define     DWF_PERS(M,...)     \
DBU_ASWF(V2_PERS(DW_PERS, DWF_ASBU(M), __VA_ARGS__))


#define     QBU_PERS(M, ...)    \
         V16_PERS(QB_PERS,         (M), __VA_ARGS__)

#define     QBI_PERS(M, ...)    \
QBU_ASBI(V16_PERS(QB_PERS, QBI_ASBU(M), __VA_ARGS__))

#define     QBC_PERS(M, ...)    \
QBU_ASBC(V16_PERS(QB_PERS, QBC_ASBU(M), __VA_ARGS__))


#define     QHU_PERS(M, ...)    \
QBU_ASHU(V8_PERS(QH_PERS, QHU_ASBU(M), __VA_ARGS__))

#define     QHI_PERS(M, ...)    \
QBU_ASHI(V8_PERS(QH_PERS, QHI_ASBU(M), __VA_ARGS__))

#define     QHF_PERS(M, ...)    \
QBU_ASHF(V8_PERS(QH_PERS, QHF_ASBU(M), __VA_ARGS__))


#define     QWU_PERS(M, ...)    \
QBU_ASWU(V4_PERS(QW_PERS, QWU_ASBU(M), __VA_ARGS__))

#define     QWI_PERS(M, ...)    \
QBU_ASWI(V4_PERS(QW_PERS, QWI_ASBU(M), __VA_ARGS__))

#define     QWF_PERS(M, ...)    \
QBU_ASWF(V4_PERS(QW_PERS, QWF_ASBU(M), __VA_ARGS__))


#define     QDU_PERS(M, ...)    \
QBU_ASDU(V2_PERS(QD_PERS, QDU_ASBU(M), __VA_ARGS__))

#define     QDI_PERS(M, ...)    \
QBU_ASDI(V2_PERS(QD_PERS, QDI_ASBU(M), __VA_ARGS__))

#define     QDF_PERS(M, ...)    \
QBU_ASDF(V2_PERS(QD_PERS, QDF_ASBU(M), __VA_ARGS__))

INLINE(uint32_t,UINT32_PERB)
(
    uint32_t v,
    Rc(0,+31) k0,  Rc(0,+31) k1,  Rc(0,+31) k2,  Rc(0,+31) k3,
    Rc(0,+31) k4,  Rc(0,+31) k5,  Rc(0,+31) k6,  Rc(0,+31) k7
)
{
    return (
        (((v>>k0)&1))
    |   (((v>>k1)&1)<<1)
    |   (((v>>k2)&1)<<2)
    |   (((v>>k3)&1)<<3)
    |   (((v>>k4)&1)<<4)
    |   (((v>>k5)&1)<<5)
    |   (((v>>k6)&1)<<6)
    |   (((v>>k7)&1)<<7)
    );
}

INLINE(uint32_t,UINT32_PERH)
(
    uint32_t v,
    Rc(0,+31) k0,  Rc(0,+31) k1,  Rc(0,+31) k2,  Rc(0,+31) k3,
    Rc(0,+31) k4,  Rc(0,+31) k5,  Rc(0,+31) k6,  Rc(0,+31) k7,
    Rc(0,+31) k8,  Rc(0,+31) k9,  Rc(0,+31) k10, Rc(0,+31) k11,
    Rc(0,+31) k12, Rc(0,+31) k13, Rc(0,+31) k14, Rc(0,+31) k15
)
{
    return (
        (((v>>k0)&1))
    |   (((v>>k1)&1)<<1)
    |   (((v>>k2)&1)<<2)
    |   (((v>>k3)&1)<<3)
    |   (((v>>k4)&1)<<4)
    |   (((v>>k5)&1)<<5)
    |   (((v>>k6)&1)<<6)
    |   (((v>>k7)&1)<<7)
    |   (((v>>k8)&1)<<8)
    |   (((v>>k9)&1)<<9)
    |   (((v>>k10)&1)<<10)
    |   (((v>>k11)&1)<<11)
    |   (((v>>k12)&1)<<12)
    |   (((v>>k13)&1)<<13)
    |   (((v>>k14)&1)<<14)
    |   (((v>>k15)&1)<<15)
    );
}


INLINE(Vwbu,VWBU_PERS)
(
    Vwbu v,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VWBU_PERS(V, ...) WBU_ASTV(WBU_PERS(VWBU_ASTM(V),__VA_ARGS__))
    return  VWBU_PERS(v, k0,k1,k2,k3);
}

INLINE(Vwbi,VWBI_PERS)
(
    Vwbi v,
    Rc(-1,3) k0, Rc(-1,3) k1, Rc(-1,3) k2, Rc(-1,3) k3
)
{
#define     VWBI_PERS(V, ...) WBI_ASTV(WBI_PERS(VWBI_ASTM(V),__VA_ARGS__))

    return  VWBI_PERS(v, k0,k1,k2,k3);
}

INLINE(Vwbc,VWBC_PERS)
(
    Vwbc v,
    Rc(-1,+3) k0, Rc(-1,+3) k1, Rc(-1,+3) k2, Rc(-1,+3) k3
)
{
#define     VWBC_PERS(V, ...) WBC_ASTV(WBC_PERS(VWBC_ASTM(V),__VA_ARGS__))
    return  VWBC_PERS(v, k0,k1,k2,k3);
}

INLINE(Vwhu,VWHU_PERS) (Vwhu v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VWHU_PERS(V, ...) WHU_ASTV(WHU_PERS(VWHU_ASTM(V),__VA_ARGS__))
    return  VWHU_PERS(v, k0,k1);
}

INLINE(Vwhi,VWHI_PERS) (Vwhi v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VWHI_PERS(V, ...) WHI_ASTV(WHI_PERS(VWHI_ASTM(V),__VA_ARGS__))
    return  VWHI_PERS(v, k0,k1);
}

INLINE(Vwhf,VWHF_PERS) (Vwhf v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VWHF_PERS(V, ...) WHF_ASTV(WHF_PERS(VWHF_ASTM(V),__VA_ARGS__))
    return  VWHF_PERS(v, k0,k1);
}


INLINE(Vdbu,VDBU_PERS)
(
    Vdbu v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VDBU_PERS(...)  DBU_PERS(__VA_ARGS__)
    return  VDBU_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vdbi,VDBI_PERS)
(
    Vdbi v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VDBI_PERS(...)  DBI_PERS(__VA_ARGS__)
    return  VDBI_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vdbc,VDBC_PERS)
(
    Vdbc v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VDBC_PERS(V, ...)  \
DBC_ASTV(DBC_PERS(VDBC_ASTM(V),__VA_ARGS__))
    return  VDBC_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}


INLINE(Vdhu,VDHU_PERS)
(
    Vdhu v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VDHU_PERS(...)  DHU_PERS(__VA_ARGS__)
    return  VDHU_PERS(v, k0,k1,k2,k3);
}

INLINE(Vdhi,VDHI_PERS)
(
    Vdhi v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VDHI_PERS(...)  DHI_PERS(__VA_ARGS__)
    return  VDHI_PERS(v, k0,k1,k2,k3);
}

INLINE(Vdhf,VDHF_PERS)
(
    Vdhf v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VDHF_PERS(...)  DHF_PERS(__VA_ARGS__)
    return  VDHF_PERS(v, k0,k1,k2,k3);
}


INLINE(Vdwu,VDWU_PERS) (Vdwu v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VDWU_PERS(...)  DWU_PERS(__VA_ARGS__)
    return  VDWU_PERS(v, k0,k1);
}

INLINE(Vdwi,VDWI_PERS) (Vdwi v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VDWI_PERS(...)  DWI_PERS(__VA_ARGS__)
    return  VDWI_PERS(v, k0,k1);
}

INLINE(Vdwf,VDWF_PERS) (Vdwf v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VDWF_PERS(...)  DWF_PERS(__VA_ARGS__)
    return  VDWF_PERS(v, k0,k1);
}


INLINE(Vqbu,VQBU_PERS)
(
    Vqbu v,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBU_PERS(...)  QBU_PERS(__VA_ARGS__)
    return  VQBU_PERS(
        v,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqbi,VQBI_PERS)
(
    Vqbi v,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBI_PERS(...)   QBI_PERS(__VA_ARGS__)
    return  VQBI_PERS(
        v,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqbc,VQBC_PERS)
(
    Vqbc v,
    Rc(-1,+15)  k0, Rc(-1,+15)  k1, Rc(-1,+15)  k2, Rc(-1,+15)  k3,
    Rc(-1,+15)  k4, Rc(-1,+15)  k5, Rc(-1,+15)  k6, Rc(-1,+15)  k7,
    Rc(-1,+15)  k8, Rc(-1,+15)  k9, Rc(-1,+15) k10, Rc(-1,+15) k11,
    Rc(-1,+15) k12, Rc(-1,+15) k13, Rc(-1,+15) k14, Rc(-1,+15) k15
)
{
#define     VQBC_PERS(V, ...)  \
    QBC_ASTV(QBC_PERS(VQBC_ASTM(V),__VA_ARGS__))
    return  VQBC_PERS(
        v,
        k0, k1, k2, k3,  k4, k5, k6, k7,
        k8, k9,k10,k11,  k12,k14,k14,k15
    );
}

INLINE(Vqhu,VQHU_PERS)
(
    Vqhu v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VQHU_PERS(...)  QHU_PERS(__VA_ARGS__)
    return  VQHU_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vqhi,VQHI_PERS)
(
    Vqhi v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VQHI_PERS(...)  QHI_PERS(__VA_ARGS__)
    return  VQHI_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}

INLINE(Vqhf,VQHF_PERS)
(
    Vqhf v,
    Rc(-1, +7) k0, Rc(-1, +7) k1, Rc(-1, +7) k2, Rc(-1, +7) k3,
    Rc(-1, +7) k4, Rc(-1, +7) k5, Rc(-1, +7) k6, Rc(-1, +7) k7
)
{
#define     VQHF_PERS(...)  QHF_PERS(__VA_ARGS__)
    return  VQHF_PERS(v, k0,k1,k2,k3,k4,k5,k6,k7);
}


INLINE(Vqwu,VQWU_PERS)
(
    Vqwu v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VQWU_PERS(...)   QWU_PERS(__VA_ARGS__)
    return  VQWU_PERS(v, k0,k1,k2,k3);
}

INLINE(Vqwi,VQWI_PERS)
(
    Vqwi v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VQWI_PERS(...)   QWI_PERS(__VA_ARGS__)
    return  VQWI_PERS(v, k0,k1,k2,k3);
}

INLINE(Vqwf,VQWF_PERS)
(
    Vqwf v,
    Rc(-1, +3) k0, Rc(-1, +3) k1, Rc(-1, +3) k2, Rc(-1, +3) k3
)
{
#define     VQWF_PERS(...)   QWF_PERS(__VA_ARGS__)
    return  VQWF_PERS(v, k0,k1,k2,k3);
}

INLINE(Vqdu,VQDU_PERS) (Vqdu v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VQDU_PERS(...)   QDU_PERS(__VA_ARGS__)
    return  VQDU_PERS(v, k0,k1);
}

INLINE(Vqdi,VQDI_PERS) (Vqdi v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VQDI_PERS(...)   QDI_PERS(__VA_ARGS__)
    return  VQDI_PERS(v, k0,k1);
}

INLINE(Vqdf,VQDF_PERS) (Vqdf v, Rc(-1, +1) k0, Rc(-1, +1) k1)
{
#define     VQDF_PERS(...)   QDF_PERS(__VA_ARGS__)
    return  VQDF_PERS(v, k0,k1);
}

#if 0 // ice cream cone
#endif // ice cream cone

#if _LEAVE_ARM_PERS_
}
#endif


#if _ENTER_ARM_BLNM
{
#endif

INLINE(Vwbu,VWBU_BLNM)
(
    Vwbu a, Vwbu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     WBU_BLNM(A, B, K0, K1, K2, K3)          \
vget_lane_f32(                                      \
    vreinterpret_f32_u8(                            \
        vtbl1_u8(                                   \
            vreinterpret_u8_f32(                    \
                vset_lane_f32(                      \
                    B,                              \
                    vdup_n_f32(A),                  \
                    1                               \
                )                                   \
            ),                                      \
            vcreate_u8(                             \
                0ull                                \
                |   (K0?0x00000004u:0x00000000u)    \
                |   (K1?0x00000500u:0x00000100u)    \
                |   (K2?0x00060000u:0x00020000u)    \
                |   (K3?0x07000000u:0x03000000u)    \
            )                                       \
        )                                           \
    ),                                              \
    0                                               \
)

#define     VWBU_BLNM(A, B, ...) \
WBU_ASTV(WBU_BLNM(VWBU_ASTM(A), VWBU_ASTM(B), __VA_ARGS__))

    return  VWBU_BLNM(a, b, k0, k1, k2, k3);
}

INLINE(Vwbi,VWBI_BLNM)
(
    Vwbi a, Vwbi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VWBI_BLNM(A, B, ...) \
WBI_ASTV(WBU_BLNM(VWBI_ASTM(A),VWBI_ASTM(B),__VA_ARGS__))

    return  VWBI_BLNM(a, b, k0, k1, k2, k3);
}

INLINE(Vwbc,VWBC_BLNM)
(
    Vwbc a, Vwbc b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VWBC_BLNM(A, B, ...) \
WBC_ASTV(WBU_BLNM(VWBC_ASTM(A),VWBC_ASTM(B),__VA_ARGS__))

    return  VWBC_BLNM(a, b, k0, k1, k2, k3);
}


INLINE(Vwhu,VWHU_BLNM)
(
    Vwhu a, Vwhu b,
    Rc(0, 1) k0, Rc(0, 1) k1
)
{
#define     WHU_BLNM(A, B, K0, K1)                  \
vget_lane_f32(                                      \
    vreinterpret_f32_u8(                            \
        vtbl1_u8(                                   \
            vreinterpret_u8_f32(                    \
                vset_lane_f32(                      \
                    B,                              \
                    vdup_n_f32(A),                  \
                    1                               \
                )                                   \
            ),                                      \
            vcreate_u8(                             \
                0ull                                \
                |   (K0?0x00000504u:0x00000100u)    \
                |   (K1?0x07060000u:0x03020000u)    \
            )                                       \
        )                                           \
    ),                                              \
    0                                               \
)

#define     VWHU_BLNM(A, B, ...) \
WHU_ASTV(WHU_BLNM(VWHU_ASTM(A),VWHU_ASTM(B),__VA_ARGS__))

    return  VWHU_BLNM(a, b, k0, k1);
}

INLINE(Vwhi,VWHI_BLNM)
(
    Vwhi a, Vwhi b,
    Rc(0, 1) k0, Rc(0, 1) k1
)
{
#define     VWHI_BLNM(A, B, ...) \
WHI_ASTV(WHU_BLNM(VWHI_ASTM(A),VWHI_ASTM(B),__VA_ARGS__))

    return  VWHI_BLNM(a, b, k0, k1);
}

INLINE(Vwhf,VWHF_BLNM)
(
    Vwhf a, Vwhf b,
    Rc(0, 1) k0, Rc(0, 1) k1
)
{
#define     VWHF_BLNM(A, B, ...) \
WHF_ASTV(WHU_BLNM(VWHF_ASTM(A),VWHF_ASTM(B),__VA_ARGS__))

    return  VWHF_BLNM(a, b, k0, k1);
}


INLINE(Vdbu,VDBU_BLNM)
(
    Vdbu a, Vdbu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VDBU_BLND(                  \
    A, B,                               \
    K0, K1, K2, K3,  K4, K5, K6, K7     \
)                                       \
vqtbl1_u8(                              \
    vcombine_u8(A, B),                  \
    vreinterpret_u8_u64(                \
        vdup_n_u64(                     \
            ((K0?0x08ull:0x00ull)<<0000)\
        |   ((K1?0x09ull:0x01ull)<<0010)\
        |   ((K2?0x0aull:0x02ull)<<0020)\
        |   ((K3?0x0bull:0x03ull)<<0030)\
        |   ((K4?0x0cull:0x04ull)<<0040)\
        |   ((K5?0x0dull:0x05ull)<<0050)\
        |   ((K6?0x0eull:0x06ull)<<0060)\
        |   ((K7?0x0full:0x07ull)<<0070)\
        )                               \
    )                                   \
)
    return  VDBU_BLND(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7
    );
}

INLINE(Vdbi,VDBI_BLNM)
(
    Vdbi a, Vdbi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VDBI_BLND(                  \
    A, B,                               \
    K0, K1, K2, K3, K4, K5, K6, K7      \
)                                       \
vqtbl1_s8(                              \
    vcombine_s8(A, B),                  \
    vreinterpret_u8_u64(                \
        vdup_n_u64(                     \
            ((K0?0x08ull:0x00ull)<<0000)\
        |   ((K1?0x09ull:0x01ull)<<0010)\
        |   ((K2?0x0aull:0x02ull)<<0020)\
        |   ((K3?0x0bull:0x03ull)<<0030)\
        |   ((K4?0x0cull:0x04ull)<<0040)\
        |   ((K5?0x0dull:0x05ull)<<0050)\
        |   ((K6?0x0eull:0x06ull)<<0060)\
        |   ((K7?0x0full:0x07ull)<<0070)\
        )                               \
    )                                   \
)
    return  VDBI_BLND(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7
    );
}

INLINE(Vdbc,VDBC_BLNM)
(
    Vdbc a, Vdbc b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VDBC_BLND(A, B, ...) \
VDBU_ASBC(VDBU_BLNM(VDBC_ASBU(A),VDBC_ASBU(B),__VA_ARGS__))

    return  VDBC_BLND(a, b, k0, k1, k2, k3, k4, k5, k6, k7);
}


INLINE(Vdhu,VDHU_BLNM)
(
    Vdhu a, Vdhu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VDHU_BLND(A, B,  K0, K1, K2, K3)    \
vreinterpret_u16_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_u16(                   \
            vcombine_u16(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0908ull:0x0100ull)<<0000)\
            |   ((K1?0x0b0aull:0x0302ull)<<0020)\
            |   ((K2?0x0d0cull:0x0504ull)<<0040)\
            |   ((K3?0x0f0eull:0x0706ull)<<0060)\
            )                                   \
        )                                       \
    )                                           \
)

    return  VDHU_BLND(a, b, k0, k1, k2, k3);
}

INLINE(Vdhi,VDHI_BLNM)
(
    Vdhi a, Vdhi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VDHI_BLND(A, B,  K0, K1, K2, K3)    \
vreinterpret_s16_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_s16(                   \
            vcombine_s16(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0908ull:0x0100ull)<<0000)\
            |   ((K1?0x0b0aull:0x0302ull)<<0020)\
            |   ((K2?0x0d0cull:0x0504ull)<<0040)\
            |   ((K3?0x0f0eull:0x0706ull)<<0060)\
            )                                   \
        )                                       \
    )                                           \
)

    return  VDHI_BLND(a, b, k0, k1, k2, k3);
}

INLINE(Vdhf,VDHF_BLNM)
(
    Vdhf a, Vdhf b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VDHF_BLND(A, B,  K0, K1, K2, K3)    \
vreinterpret_f16_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_f16(                   \
            vcombine_f16(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0908ull:0x0100ull)<<0000)\
            |   ((K1?0x0b0aull:0x0302ull)<<0020)\
            |   ((K2?0x0d0cull:0x0504ull)<<0040)\
            |   ((K3?0x0f0eull:0x0706ull)<<0060)\
            )                                   \
        )                                       \
    )                                           \
)
    return  VDHF_BLND(a, b, k0, k1, k2, k3);
}



INLINE(Vdwu,VDWU_BLNM) (Vdwu a, Vdwu b, Rc(0,1) k0, Rc(0,1) k1)
{
#define     VDWU_BLND(A, B, K0, K1)             \
vreinterpret_u32_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_u32(                   \
            vcombine_u32(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0b0a0908ull:0x03020100ull)<<0000)\
            |   ((K1?0x0f0e0d0cull:0x07060504ull)<<0040)\
            )                                   \
        )                                       \
    )                                           \
)

    return  VDWU_BLND(a, b, k0, k1);
}

INLINE(Vdwi,VDWI_BLNM) (Vdwi a, Vdwi b, Rc(0,1) k0, Rc(0,1) k1)
{
#define     VDWI_BLND(A, B, K0, K1)             \
vreinterpret_s32_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_s32(                   \
            vcombine_s32(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0b0a0908ull:0x03020100ull)<<0000)\
            |   ((K1?0x0f0e0d0cull:0x07060504ull)<<0040)\
            )                                   \
        )                                       \
    )                                           \
)

    return  VDWI_BLND(a, b, k0, k1);
}

INLINE(Vdwf,VDWF_BLNM) (Vdwf a, Vdwf b, Rc(0,1) k0, Rc(0,1) k1)
{
#define     VDWF_BLND(A, B, K0, K1)             \
vreinterpret_f32_u8(                            \
    vqtbl1_u8(                                  \
        vreinterpretq_u8_f32(                   \
            vcombine_f32(A, B)                  \
        ),                                      \
        vreinterpret_u8_u64(                    \
            vdup_n_u64(                         \
                ((K0?0x0b0a0908ull:0x03020100ull)<<0000)\
            |   ((K1?0x0f0e0d0cull:0x07060504ull)<<0040)\
            )                                   \
        )                                       \
    )                                           \
)

    return  VDWI_BLNM(a, b, k0, k1);
}


INLINE(Vqbu,VQBU_BLNM)
(
    Vqbu a, Vqbu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7,
    Rc(0, 1) k8, Rc(0, 1) k9, Rc(0, 1) k10,Rc(0, 1) k11,
    Rc(0, 1) k12,Rc(0, 1) k13,Rc(0, 1) k14,Rc(0, 1) k15
)
{
#define     VQBU_BLNQ(                          \
    A, B,                                       \
    K0, K1, K2, K3, K4, K5, K6, K7,             \
    K8, K9, K10,K11,K12,K13,K14,K15             \
) \
(                                               \
    vqtbx1q_u8(                                 \
        A,                                      \
        B,                                      \
        vreinterpretq_u8_u64(                   \
            vcombine_u64(                       \
                vdup_n_u64(                     \
                    ((K0 ?0x00ull:0xffull)<<000)\
                |   ((K1 ?0x01ull:0xffull)<<010)\
                |   ((K2 ?0x02ull:0xffull)<<020)\
                |   ((K3 ?0x03ull:0xffull)<<030)\
                |   ((K4 ?0x04ull:0xffull)<<040)\
                |   ((K5 ?0x05ull:0xffull)<<050)\
                |   ((K6 ?0x06ull:0xffull)<<060)\
                |   ((K7 ?0x07ull:0xffull)<<070)\
                ),                              \
                vdup_n_u64(                     \
                    ((K8 ?0x08ull:0xffull)<<000)\
                |   ((K9 ?0x09ull:0xffull)<<010)\
                |   ((K10?0x0aull:0xffull)<<020)\
                |   ((K11?0x0bull:0xffull)<<030)\
                |   ((K12?0x0cull:0xffull)<<040)\
                |   ((K13?0x0dull:0xffull)<<050)\
                |   ((K14?0x0eull:0xffull)<<060)\
                |   ((K15?0x0full:0xffull)<<070)\
                )                               \
            )                                   \
        )                                       \
    )                                           \
)

    return  VQBU_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7,
        k8, k9, k10,k11,k12,k13,k14,k15
    );
}

INLINE(Vqbi,VQBI_BLNM)
(
    Vqbi a, Vqbi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7,
    Rc(0, 1) k8, Rc(0, 1) k9, Rc(0, 1) k10,Rc(0, 1) k11,
    Rc(0, 1) k12,Rc(0, 1) k13,Rc(0, 1) k14,Rc(0, 1) k15
)
{
#define     VQBI_BLNQ(                          \
    A, B,                                       \
    K0, K1, K2, K3, K4, K5, K6, K7,             \
    K8, K9, K10,K11,K12,K13,K14,K15             \
)                                               \
(                                               \
    vqtbx1q_u8(                                 \
        A,                                      \
        B,                                      \
        vreinterpretq_u8_u64(                   \
            vcombine_u64(                       \
                vdup_n_u64(                     \
                    ((K0 ?0x00ull:0xffull)<<000)\
                |   ((K1 ?0x01ull:0xffull)<<010)\
                |   ((K2 ?0x02ull:0xffull)<<020)\
                |   ((K3 ?0x03ull:0xffull)<<030)\
                |   ((K4 ?0x04ull:0xffull)<<040)\
                |   ((K5 ?0x05ull:0xffull)<<050)\
                |   ((K6 ?0x06ull:0xffull)<<060)\
                |   ((K7 ?0x07ull:0xffull)<<070)\
                ),                              \
                vdup_n_u64(                     \
                    ((K8 ?0x08ull:0xffull)<<000)\
                |   ((K9 ?0x09ull:0xffull)<<010)\
                |   ((K10?0x0aull:0xffull)<<020)\
                |   ((K11?0x0bull:0xffull)<<030)\
                |   ((K12?0x0cull:0xffull)<<040)\
                |   ((K13?0x0dull:0xffull)<<050)\
                |   ((K14?0x0eull:0xffull)<<060)\
                |   ((K15?0x0full:0xffull)<<070)\
                )                               \
            )                                   \
        )                                       \
    )                                           \
)

    return  VQBI_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7,
        k8, k9, k10,k11,k12,k13,k14,k15
    );
}

INLINE(Vqbc,VQBC_BLNM)
(
    Vqbc a, Vqbc b,
    Rc(0,1) k0, Rc(0,1) k1, Rc(0,1) k2, Rc(0,1) k3,
    Rc(0,1) k4, Rc(0,1) k5, Rc(0,1) k6, Rc(0,1) k7,
    Rc(0,1) k8, Rc(0,1) k9, Rc(0,1) k10,Rc(0,1) k11,
    Rc(0,1) k12,Rc(0,1) k13,Rc(0,1) k14,Rc(0,1) k15
)
{
#define     VQBC_BLNQ(A, B, ...) \
VQBU_ASBC(VQBU_BLNM(VQBC_ASBU(A),VQBC_ASBU(B),__VA_ARGS__))

    return  VQBC_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7,
        k8, k9, k10,k11,k12,k13,k14,k15
    );
}


INLINE(Vqhu,VQHU_BLNM)
(
    Vqhu a, Vqhu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VQHU_BLNQ(                              \
    A, B,                                           \
    K0, K1, K2, K3, K4, K5, K6, K7                  \
)                                                   \
vreinterpretq_u16_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_u16(A),                    \
        vreinterpretq_u8_u16(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x0100ull:0xffffull)<<000) \
                |   ((K1?0x0302ull:0xffffull)<<020) \
                |   ((K2?0x0504ull:0xffffull)<<040) \
                |   ((K3?0x0706ull:0xffffull)<<060) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K4?0x0908ull:0xffffull)<<000) \
                |   ((K5?0x0b0aull:0xffffull)<<020) \
                |   ((K6?0x0d0cull:0xffffull)<<040) \
                |   ((K7?0x0f0eull:0xffffull)<<060) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQHU_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7
    );
}

INLINE(Vqhi,VQHI_BLNM)
(
    Vqhi a, Vqhi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VQHI_BLNQ(                              \
    A, B,                                           \
    K0, K1, K2, K3, K4, K5, K6, K7                  \
)                                                   \
vreinterpretq_s16_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_s16(A),                    \
        vreinterpretq_u8_s16(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x0100ull:0xffffull)<<000) \
                |   ((K1?0x0302ull:0xffffull)<<020) \
                |   ((K2?0x0504ull:0xffffull)<<040) \
                |   ((K3?0x0706ull:0xffffull)<<060) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K4?0x0908ull:0xffffull)<<000) \
                |   ((K5?0x0b0aull:0xffffull)<<020) \
                |   ((K6?0x0d0cull:0xffffull)<<040) \
                |   ((K7?0x0f0eull:0xffffull)<<060) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQHI_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7
    );
}

INLINE(Vqhf,VQHF_BLNM)
(
    Vqhf a, Vqhf b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3,
    Rc(0, 1) k4, Rc(0, 1) k5, Rc(0, 1) k6, Rc(0, 1) k7
)
{
#define     VQHF_BLNQ(                              \
    A, B,                                           \
    K0, K1, K2, K3, K4, K5, K6, K7                  \
)                                                   \
vreinterpretq_f16_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_f16(A),                    \
        vreinterpretq_u8_f16(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x0100ull:0xffffull)<<000) \
                |   ((K1?0x0302ull:0xffffull)<<020) \
                |   ((K2?0x0504ull:0xffffull)<<040) \
                |   ((K3?0x0706ull:0xffffull)<<060) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K4?0x0908ull:0xffffull)<<000) \
                |   ((K5?0x0b0aull:0xffffull)<<020) \
                |   ((K6?0x0d0cull:0xffffull)<<040) \
                |   ((K7?0x0f0eull:0xffffull)<<060) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQHF_BLNQ(
        a, b,
        k0, k1, k2, k3, k4, k5, k6, k7
    );
}


INLINE(Vqwu,VQWU_BLNM)
(
    Vqwu a, Vqwu b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VQWU_BLNQ(A, B, K0, K1, K2, K3)         \
vreinterpretq_u32_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_u32(A),                    \
        vreinterpretq_u8_u32(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x03020100ull:0xffffffffull)<<000) \
                |   ((K1?0x07060504ull:0xffffffffull)<<040) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K2?0x0b0a0908ull:0xffffffffull)<<000) \
                |   ((K3?0x0f0e0d0cull:0xffffffffull)<<040) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQWU_BLNQ(a, b, k0, k1, k2, k3);
}

INLINE(Vqwi,VQWI_BLNM)
(
    Vqwi a, Vqwi b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VQWI_BLNQ(A, B, K0, K1, K2, K3)         \
vreinterpretq_s32_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_s32(A),                    \
        vreinterpretq_u8_s32(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x03020100ull:0xffffffffull)<<000) \
                |   ((K1?0x07060504ull:0xffffffffull)<<040) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K2?0x0b0a0908ull:0xffffffffull)<<000) \
                |   ((K3?0x0f0e0d0cull:0xffffffffull)<<040) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQWI_BLNQ(a, b, k0, k1, k2, k3);
}

INLINE(Vqwf,VQWF_BLNM)
(
    Vqwf a, Vqwf b,
    Rc(0, 1) k0, Rc(0, 1) k1, Rc(0, 1) k2, Rc(0, 1) k3
)
{
#define     VQWF_BLNQ(A, B, K0, K1, K2, K3)         \
vreinterpretq_f32_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_f32(A),                    \
        vreinterpretq_u8_f32(B),                    \
        vreinterpretq_u8_f64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    ((K0?0x03020100ull:0xffffffffull)<<000) \
                |   ((K1?0x07060504ull:0xffffffffull)<<040) \
                ),                                  \
                vdup_n_u64(                         \
                    ((K2?0x0b0a0908ull:0xffffffffull)<<000) \
                |   ((K3?0x0f0e0d0cull:0xffffffffull)<<040) \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQWF_BLNQ(a, b, k0, k1, k2, k3);
}


INLINE(Vqdu,VQDU_BLNM) (Vqdu a, Vqdu b, Rc(0, 1) k0, Rc(0, 1) k1)
{
#define     VQDU_BLNQ(A, B, K0, K1)                 \
vreinterpretq_u64_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_u64(A),                    \
        vreinterpretq_u8_u64(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    K0                              \
                    ?   0x0706050403020100ull       \
                    :   0xffffffffffffffffull       \
                ),                                  \
                vdup_n_u64(                         \
                    K1                              \
                    ?   0x0f0e0d0c0b0a0908ull       \
                    :   0xffffffffffffffffull       \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQDU_BLNQ(a, b, k0, k1);
}

INLINE(Vqdi,VQDI_BLNM) (Vqdi a, Vqdi b, Rc(0, 1) k0, Rc(0, 1) k1)
{
#define     VQDI_BLNQ(A, B, K0, K1)                 \
vreinterpretq_s64_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_s64(A),                    \
        vreinterpretq_u8_s64(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    K0                              \
                    ?   0x0706050403020100ull       \
                    :   0xffffffffffffffffull       \
                ),                                  \
                vdup_n_u64(                         \
                    K1                              \
                    ?   0x0f0e0d0c0b0a0908ull       \
                    :   0xffffffffffffffffull       \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQDI_BLNQ(a, b, k0, k1);
}

INLINE(Vqdf,VQDF_BLNM) (Vqdf a, Vqdf b, Rc(0, 1) k0, Rc(0, 1) k1)
{
#define     VQDF_BLNQ(A, B, K0, K1)                 \
vreinterpretq_f64_u8(                               \
    vqtbx1q_u8(                                     \
        vreinterpretq_u8_f64(A),                    \
        vreinterpretq_u8_f64(B),                    \
        vreinterpretq_u8_u64(                       \
            vcombine_u64(                           \
                vdup_n_u64(                         \
                    K0                              \
                    ?   0x0706050403020100ull       \
                    :   0xffffffffffffffffull       \
                ),                                  \
                vdup_n_u64(                         \
                    K1                              \
                    ?   0x0f0e0d0c0b0a0908ull       \
                    :   0xffffffffffffffffull       \
                )                                   \
            )                                       \
        )                                           \
    )                                               \
)

    return  VQDF_BLNQ(a, b, k0, k1);
}

#if _LEAVE_ARM_BLNM
}
#endif

#if _ENTER_ARM_MANT
{
#endif

INLINE(uint16_t,manthf) (flt16_t x)
{
    union {
        flt16_t F;
        struct {uint16_t Mant:10, Expo:5, Sign:1, :0;};
    } v = {x};
    return  v.Mant;
}

INLINE(uint32_t,mantwf) (float x)
{
    union {
        float F;
        struct {uint32_t Mant:23, Expo:8, Sign:1, :0;};
    } v = {x};
    return  v.Mant;
}

INLINE(uint64_t,mantdf) (double x)
{
    union {
        double F;
        struct {uint64_t Mant:52, Expo:11, Sign:1, :0;};
    } v = {x};
    return  v.Mant;
}

INLINE(Vdhu,mantdhf) (Vdhf x)
{
    return  vand_u16(
        vreinterpret_u16_f16(x),
        vdup_n_u16(0x3ff)
    );
}

INLINE(Vdwu,mantdwf) (Vdwf x)
{
    return  vand_u32(
        vreinterpret_u32_f32(x),
        vdup_n_u32(0x7fffff)
    );
}

INLINE(Vddu,mantddf) (Vddf x)
{
    return  vand_u64(
        vreinterpret_u64_f64(x),
        vdup_n_u64(0xfffffffffffffULL)
    );
}

INLINE(Vqhu,mantqhf) (Vqhf x)
{
    return  vandq_u16(
        vreinterpretq_u16_f16(x),
        vdupq_n_u16(0x3ff)
    );
}

INLINE(Vqwu,mantqwf) (Vqwf x)
{
    return  vandq_u32(
        vreinterpretq_u32_f32(x),
        vdupq_n_u32(0x7fffff)
    );
}

INLINE(Vqdu,mantqdf) (Vqdf x)
{
    return  vandq_u64(
        vreinterpretq_u64_f64(x),
        vdupq_n_u64(0xfffffffffffffULL)
    );
}

#if _LEAVE_ARM_MANT
}
#endif

#if _ENTER_ARM_EXPO
{
#endif

INLINE(uint16_t,expohf) (flt16_t x)
{
    union {
        flt16_t F;
        struct {uint16_t Mant:10, Expo:5, Sign:1, :0;};
    } v = {x};
    return  v.Expo;
}

INLINE(uint32_t,expowf) (float x)
{
    union {
        float F;
        struct {uint32_t Mant:23, Expo:8, Sign:1, :0;};
    } v = {x};
    return  v.Expo;
}

INLINE(uint64_t,expodf) (double x)
{
    union {
        double F;
        struct {uint64_t Mant:52, Expo:11, Sign:1, :0;};
    } v = {x};
    return  v.Expo;
}

INLINE(Vdhu,expodhf) (Vdhf x)
{
#define     expodhf(X)          \
vand_u16(                       \
    vdup_n_u16(0x1f),           \
    vshr_n_u16(                 \
        vreinterpret_u16_f16(X),\
        10                      \
    )                           \
)

    return  expodhf(x);
}

INLINE(Vdwu,expodwf) (Vdwf x)
{
#define     expodwf(X)          \
vand_u32(                       \
    vdup_n_u32(0xff),           \
    vshr_n_u32(                 \
        vreinterpret_u32_f32(X),\
        23                      \
    )                           \
)

    return  expodwf(x);
}

INLINE(Vddu,expoddf) (Vddf x)
{
#define     expoddf(X)          \
vand_u64(                       \
    vdup_n_u64(0x7ff),          \
    vshr_n_u64(                 \
        vreinterpret_u64_f64(X),\
        52                      \
    )                           \
)

    return  expoddf(x);
}


INLINE(Vqhu,expoqhf) (Vqhf x)
{
#define     expoqhf(X)              \
vandq_u16(                          \
    vdupq_n_u16(0x1f),              \
    vshrq_n_u16(                    \
        vreinterpretq_u16_f16(X),   \
        10                          \
    )                               \
)

    return  expoqhf(x);
}

INLINE(Vqwu,expoqwf) (Vqwf x)
{
#define     expoqwf(X)              \
vandq_u32(                          \
    vdupq_n_u32(0xff),              \
    vshrq_n_u32(                    \
        vreinterpretq_u32_f32(X),   \
        23                          \
    )                               \
)

    return  expoqwf(x);
}

INLINE(Vqdu,expoqdf) (Vqdf x)
{
#define     expoqdf(X)              \
vandq_u64(                          \
    vdupq_n_u64(0x7ff),             \
    vshrq_n_u64(                    \
        vreinterpretq_u64_f64(X),   \
        52                          \
    )                               \
)

    return  expoqdf(x);
}


#if _LEAVE_ARM_EXPO
}
#endif


#if _ENTER_ARM_CVBU
{
#endif

INLINE(uint8_t,   BOOL_CVBU)     (_Bool x) {return x;}
INLINE(uint8_t,  UCHAR_CVBU)     (uchar x) {return x;}
INLINE(uint8_t,  SCHAR_CVBU)     (schar x) {return x;}
INLINE(uint8_t,   CHAR_CVBU)      (char x) {return x;}
INLINE(uint8_t,  USHRT_CVBU)    (ushort x) {return x;}
INLINE(uint8_t,   SHRT_CVBU)     (short x) {return x;}
INLINE(uint8_t,   UINT_CVBU)      (uint x) {return x;}
INLINE(uint8_t,    INT_CVBU)       (int x) {return x;}
INLINE(uint8_t,  ULONG_CVBU)     (ulong x) {return x;}
INLINE(uint8_t,   LONG_CVBU)      (long x) {return x;}
INLINE(uint8_t, ULLONG_CVBU)    (ullong x) {return x;}
INLINE(uint8_t,  LLONG_CVBU)     (llong x) {return x;}
INLINE(uint8_t,  FLT16_CVBU)   (flt16_t x) {return x;}
INLINE(uint8_t,    FLT_CVBU)     (float x) {return x;}
INLINE(uint8_t,    DBL_CVBU)    (double x) {return x;}
INLINE(uint8_t,cvbuac) (void volatile const *x)
{
    return  (uintptr_t) x;
}
#if QUAD_NLLONG == 2
INLINE(uint8_t,cvbuqu) (QUAD_UTYPE x) {return x;}
INLINE(uint8_t,cvbuqi) (QUAD_ITYPE x) {return x;}
INLINE(uint8_t,cvbuqf) (QUAD_FTYPE x) {return x;}
#endif

INLINE(Vwbu,VWBU_CVBU) (Vwbu x) {return x;}
INLINE(Vwbu,VWBI_CVBU) (Vwbi x) {return VWBI_ASBU(x);}
INLINE(Vwbu,VWBC_CVBU) (Vwbc x) {return VWBC_ASBU(x);}

INLINE(Vdbu,VDBU_CVBU) (Vdbu x) {return x;}
INLINE(Vdbu,VDBI_CVBU) (Vdbi x) {return VDBI_ASBU(x);}
INLINE(Vdbu,VDBC_CVBU) (Vdbc x) {return VDBC_ASBU(x);}

INLINE(Vwbu,VDHU_CVBU) (Vdhu x)
{
    uint8x8_t   b = vreinterpret_u8_u16(x);
    b = vuzp1_u8(b, b);
    float32x2_t m = vreinterpret_f32_u8(b);
    return  WBU_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vwbu,VDHI_CVBU) (Vdhi x)
{
    uint8x8_t   b = vreinterpret_u8_s16(x);
    b = vuzp1_u8(b, b);
    float32x2_t m = vreinterpret_f32_u8(b);
    return  WBU_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vwbu,VDHF_CVBU) (Vdhf x)
{
    uint16x4_t u = vreinterpret_u16_f16(x);
    uint16x4_t m = vand_u16(u, vdup_n_u16(0x03ff));
    uint16x4_t e = vand_u16(u, vdup_n_u16(0x7c00));
    e = vshr_n_u16(e, 10);
    uint16x4_t s = vtst_u16(u, vdup_n_u16(0x8000));
    uint16x4_t l = vcge_u16(e, vdup_n_u16(15));
    uint16x4_t c = vclt_u16(e, vdup_n_u16(23));
    c = vand_u16(l, c);
    e = vsub_u16(
        e,
        vand_u16(c, vdup_n_u16(15))
    );
    e = vshl_u16(vdup_n_u16(1),  e);
    l = vmovn_u32(vshrq_n_u32(vmull_u16(m, e), 10));
    e = vadd_u16(e, l);
    uint8x8_t   b = vreinterpret_u8_u16(e);
    b = vtbl1_u8(b, vcreate_u8(0xffffffff06040200));
    float32x2_t v = vreinterpret_f32_u8(b);
    return  WBU_ASTV(vget_lane_f32(v, 0));
}


INLINE(Vqbu,VQBU_CVBU) (Vqbu x) {return x;}
INLINE(Vqbu,VQBI_CVBU) (Vqbi x) {return VQBI_ASBU(x);}
INLINE(Vqbu,VQBC_CVBU) (Vqbc x) {return VQBC_ASBU(x);}

INLINE(Vdbu,VQHU_CVBU) (Vqhu x) {return vmovn_u16(x);}
INLINE(Vdbu,VQHI_CVBU) (Vqhi x) {return VDBI_ASBU(vmovn_s16(x));}
INLINE(Vdbu,VQHF_CVBU) (Vqhf x)
{
    uint16x8_t u = vreinterpretq_u16_f16(x);
    uint16x8_t m = vandq_u16(u, vdupq_n_u16(0x03ff));
    uint16x8_t e = vandq_u16(u, vdupq_n_u16(0x7c00));
    e = vshrq_n_u16(e, 10);
    uint16x8_t s = vtstq_u16(u, vdupq_n_u16(0x8000));
    uint16x8_t l = vcgeq_u16(e, vdupq_n_u16(15));
    uint16x8_t c = vcltq_u16(e, vdupq_n_u16(23));
    c = vandq_u16(l, c);
    e = vsubq_u16(
        e,
        vandq_u16(c, vdupq_n_u16(15))
    );
    e = vshlq_u16(vdupq_n_u16(1),  e);
    l = vcombine_u16(
        vmovn_u32(
            vshrq_n_u32(
                vmull_u16(
                    vget_low_u16(m),
                    vget_low_u16(e)
                ),
                10
            )
        ),
        vmovn_u32(
            vshrq_n_u32(
                vmull_u16(
                    vget_high_u16(m),
                    vget_high_u16(e)
                ),
                10
            )
        )
    );
    e = vaddq_u16(e, l);
    return  vmovn_u16(e);
}

INLINE(Vwbu,VQWU_CVBU) (Vqwu x)
{
    uint8x16_t  b = vreinterpretq_u8_u32(x);
    uint64x1_t  m = vdup_n_u64(0xffffffff0c080400ull);
    uint8x8_t   t = vreinterpret_u8_u64(m);
    t = vqtbl1_u8(b, t);
    return  WBU_ASTV(vget_lane_f32(vreinterpret_f32_u8(t), V2_K0));
}

INLINE(Vwbu,VQWI_CVBU) (Vqwi x)
{
    return  VQWU_CVBU(VQWI_ASTU(x));
}

INLINE(Vwbu,VQWF_CVBU) (Vqwf x)
{
    return  VQWU_CVBU(vcvtq_u32_f32(x));
}

#if _LEAVE_ARM_CVBU
}
#endif

#if _ENTER_ARM_CVBI
{
#endif

INLINE(int8_t,   ADDR_CVBI) (void const *x) {return (intptr_t) x;}
INLINE(int8_t,   BOOL_CVBI)     (_Bool x) {return x;}
INLINE(int8_t,  UCHAR_CVBI)     (uchar x) {return x;}
INLINE(int8_t,  SCHAR_CVBI)     (schar x) {return x;}
INLINE(int8_t,   CHAR_CVBI)      (char x) {return x;}
INLINE(int8_t,  USHRT_CVBI)    (ushort x) {return x;}
INLINE(int8_t,   SHRT_CVBI)     (short x) {return x;}
INLINE(int8_t,   UINT_CVBI)      (uint x) {return x;}
INLINE(int8_t,    INT_CVBI)       (int x) {return x;}
INLINE(int8_t,  ULONG_CVBI)     (ulong x) {return x;}
INLINE(int8_t,   LONG_CVBI)      (long x) {return x;}
INLINE(int8_t, ULLONG_CVBI)    (ullong x) {return x;}
INLINE(int8_t,  LLONG_CVBI)     (llong x) {return x;}
INLINE(int8_t,  FLT16_CVBI)   (flt16_t x) {return x;}
INLINE(int8_t,    FLT_CVBI)     (float x) {return x;}
INLINE(int8_t,    DBL_CVBI)    (double x) {return x;}
#if QUAD_NLLONG == 2
INLINE(int8_t,cvbiqu) (QUAD_UTYPE x) {return x;}
INLINE(int8_t,cvbiqi) (QUAD_ITYPE x) {return x;}
INLINE(int8_t,cvbiqf) (QUAD_FTYPE x) {return x;}
#endif

INLINE(int8_t,cvbiac) (void volatile const *x)
{
    return  (intptr_t) x;
}

INLINE(Vwbi,VWBU_CVBI) (Vwbu x) {return VWBU_ASBI(x);}
INLINE(Vwbi,VWBI_CVBI) (Vwbi x) {return x;}
INLINE(Vwbi,VWBC_CVBI) (Vwbc x) {return VWBC_ASBI(x);}

INLINE(Vdbi,VDBU_CVBI) (Vdbu x) {return VDBU_ASBI(x);}
INLINE(Vdbi,VDBI_CVBI) (Vdbi x) {return x;}
INLINE(Vdbi,VDBC_CVBI) (Vdbc x) {return VDBC_ASBI(x);}

INLINE(Vwbi,VDHU_CVBI) (Vdhu x) {return VWBU_ASBI(VDHU_CVBU(x));}
INLINE(Vwbi,VDHI_CVBI) (Vdhi x) {return VWBU_ASBI(VDHI_CVBU(x));}
INLINE(Vwbi,VDHF_CVBI) (Vdhf x)
{
    return  VWBI_VOID;
/*  TODO: actually implement arm's V*HF_CV* ops
    int16x4_t   h = vcvt_s16_f16(x);
    uint8x8_t   b = vreinterpret_u8_s16(h);
    b = vzup1_u8(b, b);
    float32x2_t m = vreinterpret_f32_u8(b);
    return  WBI_ASTV(vget_lane_f32(m, V2_K0));
*/
}

INLINE(Vqbi,VQBU_CVBI) (Vqbu x) {return VQBU_ASBI(x);}
INLINE(Vqbi,VQBI_CVBI) (Vqbi x) {return x;}
INLINE(Vqbi,VQBC_CVBI) (Vqbc x) {return VQBC_ASBI(x);}

INLINE(Vdbi,VQHU_CVBI) (Vqhu x) {return vmovn_s16(VQHU_ASHI(x));}
INLINE(Vdbi,VQHI_CVBI) (Vqhi x) {return vmovn_s16(x);}
INLINE(Vdbi,VQHF_CVBI) (Vqhf x) {return VDBU_VOID;}

INLINE(Vwbi,VQWU_CVBI) (Vqwu x) {return VWBU_ASBI(VQWU_CVBU(x));}
INLINE(Vwbi,VQWI_CVBI) (Vqwi x)
{
    uint8x16_t  b = vreinterpretq_u8_s32(x);
    uint64x1_t  m = vdup_n_u64(0xffffffff0c080400ull);
    //                                    3 2 1 0
    uint8x8_t   t = vreinterpret_u8_u64(m);
    t = vqtbl1_u8(b, t);
    return WBI_ASTV(vget_lane_f32(vreinterpret_f32_u8(t), V2_K0));
}

INLINE(Vwbi,VQWF_CVBI) (Vqwf x)
{
    return  VQWI_CVBI(vcvtq_s32_f32(x));
}

#if _LEAVE_ARM_CVBI
}
#endif

#if _ENTER_ARM_CVBC
{
#endif

INLINE(char,   BOOL_CVBC)     (_Bool x) {return x;}
INLINE(char,  UCHAR_CVBC)     (uchar x) {return x;}
INLINE(char,  SCHAR_CVBC)     (schar x) {return x;}
INLINE(char,   CHAR_CVBC)      (char x) {return x;}
INLINE(char,  USHRT_CVBC)    (ushort x) {return x;}
INLINE(char,   SHRT_CVBC)     (short x) {return x;}
INLINE(char,   UINT_CVBC)      (uint x) {return x;}
INLINE(char,    INT_CVBC)       (int x) {return x;}
INLINE(char,  ULONG_CVBC)     (ulong x) {return x;}
INLINE(char,   LONG_CVBC)      (long x) {return x;}
INLINE(char, ULLONG_CVBC)    (ullong x) {return x;}
INLINE(char,  LLONG_CVBC)     (llong x) {return x;}
INLINE(char,  FLT16_CVBC)   (flt16_t x) {return x;}
INLINE(char,    FLT_CVBC)     (float x) {return x;}
INLINE(char,    DBL_CVBC)    (double x) {return x;}
INLINE(char,cvbcac) (void volatile const *x)
{
#if CHAR_MIN
    return  (intptr_t) x;
#else
    return  (uintptr_t) x;
#endif
}
#if QUAD_NLLONG == 2
INLINE(char,cvbcqu) (QUAD_UTYPE x) {return x;}
INLINE(char,cvbcqi) (QUAD_ITYPE x) {return x;}
INLINE(char,cvbcqf) (QUAD_FTYPE x) {return x;}
#endif

INLINE(Vwbc,VWBU_CVBC) (Vwbu x) {return VWBU_ASBC(x);}
INLINE(Vwbc,VWBI_CVBC) (Vwbi x) {return VWBI_ASBC(x);}
INLINE(Vwbc,VWBC_CVBC) (Vwbc x) {return x;}

INLINE(Vdbc,VDBU_CVBC) (Vdbu x) {return VDBU_ASBC(x);}
INLINE(Vdbc,VDBI_CVBC) (Vdbi x) {return VDBI_ASBC(x);}
INLINE(Vdbc,VDBC_CVBC) (Vdbc x) {return x;}

INLINE(Vwbc,VDHU_CVBC) (Vdhu x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VDHU_CVBI(x));
#else
    return  VWBU_ASBC(VDHU_CVBU(x));
#endif
}

INLINE(Vwbc,VDHI_CVBC) (Vdhi x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VDHI_CVBI(x));
#else
    return  VWBU_ASBC(VDHI_CVBU(x));
#endif

}

INLINE(Vwbc,VDHF_CVBC) (Vdhf x)
{
// TODO
    return  VWBC_VOID;
}


INLINE(Vqbc,VQBU_CVBC) (Vqbu x) {return VQBU_ASBC(x);}
INLINE(Vqbc,VQBI_CVBC) (Vqbi x) {return VQBI_ASBC(x);}
INLINE(Vqbc,VQBC_CVBC) (Vqbc x) {return x;}

INLINE(Vdbc,VQHU_CVBC) (Vqhu x)
{
#if CHAR_MIN
    return  VDBI_ASBC(VQHU_CVBI(x));
#else
    return  VDBU_ASBC(VQHU_CVBU(x));
#endif
}

INLINE(Vdbc,VQHI_CVBC) (Vqhi x)
{
#if CHAR_MIN
    return  VDBI_ASBC(VQHI_CVBI(x));
#else
    return  VDBU_ASBC(VQHI_CVBU(x));
#endif
}

INLINE(Vdbc,VQHF_CVBC) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  VQHI_CVBC(vcvtq_s16_f16(x));
#else
#   if CHAR_MIN
    return  VDBI_ASBC(VQHF_CVBI(x));
#   else
    return  VDBU_ASBC(VQHF_CVBU(x));
#   endif
#endif

}

INLINE(Vwbc,VQWU_CVBC) (Vqwu x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VQWU_CVBI(x));
#else
    return  VWBU_ASBC(VQWU_CVBU(x));
#endif
}


INLINE(Vwbc,VQWI_CVBC) (Vqwi x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VQWU_CVBI(x));
#else
    return  VWBU_ASBC(VQWU_CVBU(x));
#endif
}


INLINE(Vwbc,VQWF_CVBC) (Vqwf x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VQWF_CVBI(x));
#else
    return  VWBU_ASBC(VQWF_CVBU(x));
#endif
}

#if _LEAVE_ARM_CVBC
}
#endif

#if _ENTER_ARM_CVBZ
{
#endif

INLINE(uint8_t,  BOOL_CVBZ)  (_Bool x) {return x;}
INLINE(uint8_t, UCHAR_CVBZ)  (uchar x) {return x;}
INLINE(uint8_t, SCHAR_CVBZ)  (schar x) {return vqmovunh_s16(x);}
INLINE(uint8_t,  CHAR_CVBZ)   (char x) {return vqmovunh_s16(x);}
INLINE(uint8_t, USHRT_CVBZ) (ushort x) {return  vqmovnh_u16(x);}
INLINE(uint8_t,  SHRT_CVBZ)  (short x) {return vqmovunh_s16(x);}
INLINE(uint8_t,  UINT_CVBZ)   (uint x)
{
    return  x|vtstd_u64(x, UINT64_MAX-UINT8_MAX);
}

INLINE(uint8_t,   INT_CVBZ)    (int x)
{
    return  vqmovunh_s16(vqmovns_s32(x));
}

INLINE(uint8_t, ULONG_CVBZ)  (ulong x)
{
    return  x|vtstd_u64(x, UINT64_MAX-UINT8_MAX);
}

INLINE(uint8_t,  LONG_CVBZ)   (long x)
{
#if DWRD_NLONG == 2
    return  vqmovunh_s16(vqmovns_s32(x));
#else
    uint32_t m = vqmovund_s64(x);
    return  m|vtstd_u64(m, UINT64_MAX-UINT8_MAX);
#endif
}

INLINE(uint8_t,ULLONG_CVBZ) (ullong x)
{
    return  x|vtstd_u64(x, UINT64_MAX-UINT8_MAX);
}

INLINE(uint8_t, LLONG_CVBZ)  (llong x)
{
    uint32_t m = vqmovund_s64(x);
    return  m|vtstd_u64(m, UINT64_MAX-UINT8_MAX);
}

INLINE(uint8_t, FLT16_CVBZ) (flt16_t x)
{
    uint32_t m = vcvts_u32_f32(x);
    return   m|vtstd_u64(m, UINT64_MAX-UINT8_MAX);
}

INLINE(uint8_t,   FLT_CVBZ)   (float x)
{
#define     FLT_CVBZ    FLT_CVBZ
    uint32_t m = vcvts_u32_f32(x);
    return m|vtstd_u64(m, UINT64_MAX-UINT8_MAX);
}

INLINE(uint8_t,   DBL_CVBZ) (double x)
{
#define     DBL_CVBZ    DBL_CVBZ
    uint64_t m = vcvtd_u64_f64(x);
    return m|vtstd_u64(m, UINT64_MAX-UINT8_MAX);
}

INLINE(uint8_t,cvbzqu) (QUAD_UTYPE x)
{
    return (x >= UINT8_MAX) ? UINT8_MAX : x;
}

INLINE(uint8_t,cvbzqi) (QUAD_ITYPE x)
{
    if (x >= UINT8_MAX) return UINT8_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE(uint8_t,cvbzqf) (QUAD_FTYPE x)
{
    if (x >= UINT8_MAX) return UINT8_MAX;
    if (x <= 0) return 0;
    return x;
}


INLINE(float,WBI_CVBZ) (float m)
{
    float32x2_t d = vdup_n_f32(m);
    int16x8_t   q = vmovl_s8(vreinterpret_s8_f32(d));
    d = vreinterpret_f32_u8(vqmovun_s16(q));
    return  vget_lane_f32(d, V2_K0);
}

INLINE(Vwbu,VWBU_CVBZ) (Vwbu x) {return x;}
INLINE(Vwbu,VWBI_CVBZ) (Vwbi x)
{
    return  WBU_ASTV(WBI_CVBZ(VWBI_ASTM(x)));
}

INLINE(Vwbu,VWBC_CVBZ) (Vwbc v)
{
#if CHAR_MIN
    return  WBU_ASTV(WBI_CVBZ(VWBC_ASTM(x)));
#else
    return  VWBC_ASBU(v);
#endif
}


INLINE(Vdbu,VDBU_CVBZ) (Vdbu x) {return x;}
INLINE(Vdbu,VDBI_CVBZ) (Vdbi x) {return vqmovun_s16(vmovl_s8(x));}
INLINE(Vdbu,VDBC_CVBZ) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_CVBZ(VDBC_ASBI(x));
#else
    return  VDBC_ASBU(x);
#endif
}

INLINE(Vwbu,VDHU_CVBZ) (Vdhu x)
{
#define     DHU_CVBZ(V)     \
vget_lane_f32(              \
    vreinterpret_f32_u8(    \
        vqmovn_u16(         \
            vcombine_u16(   \
                V,          \
                VDHU_VOID   \
            )               \
        )                   \
    ),                      \
    V2_K0                   \
)

    return WBU_ASTV(DHU_CVBZ(x));
}

INLINE(Vwbu,VDHI_CVBZ) (Vdhi x)
{
#define     DHI_CVBZ(V)     \
vget_lane_f32(              \
    vreinterpret_f32_s8(    \
        vqmovun_s16(        \
            vcombine_s16(   \
                V,          \
                VDHI_VOID   \
            )               \
        )                   \
    ),                      \
    V2_K0                   \
)

    return  WBU_ASTV(DHI_CVBZ(x));
}

INLINE(Vwbu,VDHF_CVBZ) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CVBZ(M)             \
vget_lane_f32(                      \
    vreinterpret_f32_u8(            \
        vqmovn_u16(                 \
            vcombine_u16(           \
                vcvt_u16_f16(M),    \
                vdup_n_u16(0)       \
            )                       \
        )                           \
    ),                              \
    V2_K0                           \
)
#   define  VDHF_CVBZ(V)        WBU_ASTV(DHF_CVBZ(VDHF_ASTM(V)))
    return  VDHF_CVBZ(x);
#else
    float32x4_t qwf = vcvt_f32_f16(x);
    uint32x4_t  qwu = vcvtq_u32_f32(qwf);
    uint16x4_t  dhu = vqmovn_u32(qwu);
    uint16x8_t  qhu = vcombine_u16(dhu, VDHU_VOID);
    uint8x8_t   dbu = vqmovn_u16(qhu);
    float32x2_t dwf = vreinterpret_f32_u8(dbu);
    return  WBU_ASTV(vget_lane_f32(dwf, V2_K0));
#endif
}

INLINE(Vqbu,VQBU_CVBZ) (Vqbu x) {return x;}
INLINE(Vqbu,VQBI_CVBZ) (Vqbi x)
{
    return vcombine_u8(
        vqmovun_s16(vmovl_s8(vget_low_s8(x))),
        vqmovun_s16(vmovl_s8(vget_high_s8(x)))
    );
}
INLINE(Vqbu,VQBC_CVBZ) (Vqbc x)
{
#if CHAR_MIN
    return  VQBU_CVBZ(VQBC_ASBI(x));
#else
    return  VQBC_ASBU(x);
#endif
}

INLINE(Vdbu,VQHU_CVBZ) (Vqhu x) {return  vqmovn_u16(x);}
INLINE(Vdbu,VQHI_CVBZ) (Vqhi x) {return vqmovun_s16(x);}
INLINE(Vdbu,VQHF_CVBZ) (Vqhf x) {return VDBU_VOID;}

INLINE(Vwbu,VQWU_CVBZ) (Vqwu v)
{
#define     QWU_CVBZ(M)     DHU_CVBZ(vqmovn_u32(M))
#define     VQWU_CVBZ(V)    WBU_ASTV(QWU_CVBZ(V))
    return  VQWU_CVBZ(v);
}

INLINE(Vwbu,VQWI_CVBZ) (Vqwi v)
{
#define     QWI_CVBZ(M)     DHU_CVBZ(vqmovun_s32(M))
#define     VQWI_CVBZ(V)    WBU_ASTV(QWI_CVBZ(V))
    return  VQWI_CVBZ(v);
}

INLINE(Vwbu,VQWF_CVBZ) (Vqwf v)
{
#define     QWF_CVBZ(M)             \
vget_lane_f32(                      \
    vreinterpret_f32_u8(            \
        vqmovn_u16(                 \
            vcombine_u16(           \
                vqmovn_u32(         \
                    vcvtq_u32_f32(M)\
                ),                  \
                VDHU_VOID           \
            )                       \
        )                           \
    ),                              \
    V2_K0                           \
)

#define     VQWF_CVBZ(V)            WBU_ASTV(QWF_CVBZ(VQWF_ASTM(V)))
    return  VQWI_CVBZ(v);
}

#if _LEAVE_ARM_CVBZ
}
#endif

#if _ENTER_ARM_CVBS
{
#endif

INLINE(int8_t,  BOOL_CVBS)   (_Bool x) {return x;}
INLINE(int8_t, UCHAR_CVBS)   (uchar x) {return vqmovnh_s16(x);}
INLINE(int8_t, SCHAR_CVBS)   (schar x) {return x;}
INLINE(int8_t,  CHAR_CVBS)    (char x) {return vqmovnh_s16(x);}
INLINE(int8_t, USHRT_CVBS)  (ushort x) {return vqmovnh_s16(vqmovns_s32(x));}
INLINE(int8_t,  SHRT_CVBS)   (short x) {return vqmovnh_s16(x);}

INLINE(int8_t,  UINT_CVBS)    (uint x)
{
#define     UINT_CVBS(X)                \
(                                       \
    (int8_t)                            \
    (                                   \
        (                               \
            vtstd_u64(                  \
                (UINT64_MAX-INT8_MAX),  \
                X                       \
            )>>57                       \
        )                               \
    |   (X)                             \
    )                                   \
)

    return  UINT_CVBS(x);
}

INLINE(int8_t,   INT_CVBS)     (int x) {return vqmovnh_s16(vqmovns_s32(x));}
INLINE(int8_t, ULONG_CVBS)   (ulong x) {return UINT_CVBS(x);}
INLINE(int8_t,  LONG_CVBS)    (long x)
{
#if DWRD_NLONG == 2
    return  INT_CVBS(x);
#else
    return  vqmovnh_s16(vqmovns_s32(vqmovnd_s64(x)));
#endif
}

INLINE(int8_t,ULLONG_CVBS)  (ullong x)
{
#if QUAD_NLONG == 2
    return UINT_CVBS(x);
#else
    return x <= INT8_MAX ? x : INT8_MAX;
#endif
}

INLINE(int8_t, LLONG_CVBS)   (llong x)
{
#if QUAD_NLONG == 2
    return  vqmovnh_s16(vqmovns_s32(vqmovnd_s64(x)));
#else
    if (x < INT8_MIN) return INT8_MIN;
    if (x > INT8_MAX) return INT8_MAX;
    return x;
#endif
}


INLINE(int8_t, FLT16_CVBS) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vqmovnh_s16(vcvth_s16_f16(x));
#else
    return  vqmovnh_s16(vqmovns_s32(vcvts_s32_f32(x)));
#endif
}

INLINE(int8_t,   FLT_CVBS) (float x)
{
    return  vqmovnh_s16(vqmovns_s32(vcvts_s32_f32(x)));
}

INLINE(int8_t,   DBL_CVBS) (double x)
{
    return  vqmovnh_s16(vqmovns_s32(vqmovnd_s64(vcvtd_s64_f64(x))));
}

#if QUAD_NLLONG == 2

INLINE(int8_t,cvbsqu) (QUAD_UTYPE x)
{
    return (x >= INT8_MAX) ? INT8_MAX : x;
}

INLINE(int8_t,cvbsqi) (QUAD_ITYPE x)
{
    if (x >= INT8_MAX) return INT8_MAX;
    if (x <= INT8_MIN) return INT8_MIN;
    return x;
}

INLINE(int8_t,cvbsqf) (QUAD_FTYPE x)
{
    if (x >= INT8_MAX) return INT8_MAX;
    if (x <= INT8_MIN) return INT8_MIN;
    return x;
}
#endif

#define     DBU_CVBS(X) vqmovn_s16(vreinterpretq_s16_u16(vmovl_u8(X)))
#define     DBI_CVBS(X) X

INLINE(float,WBU_CVBS) (float x)
{
    float32x2_t m = vdup_n_f32(x);
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  u = vmovl_u16(d);
    int16x8_t   i = vreinterpretq_s16_u16(u);
    int8x8_t    t = vqmovn_s16(i);
    m = vreinterpret_f32_s8(t);
    return  vget_lane_f32(m, V2_K0);
}

INLINE(float,DHU_CVBS) (uint16x4_t x)
{
    uint16x4_t y = vdup_n_u16(UINT16_MAX-INT8_MAX);
    y = vtst_u16(x, y);
    x = vorr_u16(x, y);
    y = vdup_n_u16(INT8_MAX);
    x = vand_u16(x, y);
    uint16x8_t  q = vcombine_u16(x, x);
    uint8x8_t   d = vmovn_u16(q);
    float32x2_t m = vreinterpret_f32_u8(d);
    return vget_lane_f32(m, V2_K0);
}

INLINE(float,DHI_CVBS) (int16x4_t x)
{
    int16x8_t   q = vcombine_s16(x, x);
    int8x8_t    d = vqmovn_s16(q);
    float32x2_t m = vreinterpret_f32_s8(d);
    return  vget_lane_f32(m, V2_K0);
}

#define     QBU_CVBS(X)         \
vreinterpretq_s8_u8(            \
    vandq_u8(                   \
        vdupq_n_u8(0x7f),       \
        vorrq_u8(               \
            vtstq_u8(           \
                vdupq_n_u8(128),\
                X               \
            ),                  \
            X                   \
        )                       \
    )                           \
)
#define     QBI_CVBS(X) X

#define     QHU_CVBS(X)                 \
vqmovn_s16(                             \
    vreinterpretq_s16_u16(              \
        vandq_u16(                      \
            vdupq_n_u16(0x7fff),        \
            vorrq_u16(                  \
                vtstq_u16(              \
                    vdupq_n_u16(0x8000),\
                    X                   \
                ),                      \
                X                       \
            )                           \
        )                               \
    )                                   \
)

#define     QHI_CVBS    vqmovn_s16

INLINE(float,QWU_CVBS) (uint32x4_t x)
{
    uint32x4_t y = vdupq_n_u32(UINT32_MAX-INT8_MAX);
    y = vtstq_u32(x, y);
    x = vorrq_u32(x, y);
    y = vdupq_n_u32(INT8_MAX);
    x = vandq_u32(x, y);
    uint64x1_t  m = vdup_n_u64(UINT64_C(0xffffffff0c080400));
    uint8x8_t   t = vqtbl1_u8(
        vreinterpretq_u8_u32(x),
        vreinterpret_u8_u64(m)
    );
    return  vget_lane_f32(vreinterpret_f32_u8(t), V2_K0);
}

INLINE(float,QWI_CVBS) (int32x4_t x)
{
    int16x4_t   l = vqmovn_s32(x);
    int16x8_t   h = vcombine_s16(l, l);
    int8x8_t    b = vqmovn_s16(h);
    float32x2_t m = vreinterpret_f32_s8(b);
    return  vget_lane_f32(m, V2_K0);
}

INLINE(float,QWF_CVBS) (float32x4_t x)
{
    return  QWI_CVBS(vcvtq_s32_f32(x));
}



INLINE(Vwbi,VWBU_CVBS) (Vwbu x)
{
    return  WBI_ASTV(WBU_CVBS(VWBU_ASTM(x)));
}

INLINE(Vwbi,VWBI_CVBS) (Vwbi x) {return x;}

INLINE(Vwbi,VWBC_CVBS) (Vwbc x)
{
#if CHAR_MIN
    return  VWBC_ASBI(x);
#else
    return  WBI_ASTV(WBU_CVBS(VWBC_ASTM(x)));
#endif
}


INLINE(Vdbi,VDBU_CVBS) (Vdbu x) {return  DBU_CVBS(x);}
INLINE(Vdbi,VDBI_CVBS) (Vdbi x) {return  DBI_CVBS(x);}
INLINE(Vdbi,VDBC_CVBS) (Vdbc x)
{
#if CHAR_MIN
    return  VDBC_ASBI(x);
#else
    return  DBU_CVBS(VDBC_ASBU(x));
#endif
}


INLINE(Vwbi,VDHU_CVBS) (Vdhu x)
{
    return  WBI_ASTV(DHU_CVBS(x));
}

INLINE(Vwbi,VDHI_CVBS) (Vdhi x)
{
    return  WBI_ASTV(DHI_CVBS(x));
}

INLINE(Vwbi,VDHF_CVBS) (Vdhf x)
{
    return  VWBI_VOID;
}


INLINE(Vqbi,VQBU_CVBS) (Vqbu x) {return QBU_CVBS(x);}
INLINE(Vqbi,VQBI_CVBS) (Vqbi x) {return QBI_CVBS(x);}
INLINE(Vqbi,VQBC_CVBS) (Vqbc x)
{
#if CHAR_MIN
    return  VQBC_ASBI(x);
#else
    return  VQBU_CVBS(VQBC_ASBU(x));
#endif
}

INLINE(Vdbi,VQHU_CVBS) (Vqhu x) {return QHU_CVBS(x);}
INLINE(Vdbi,VQHI_CVBS) (Vqhi x) {return QHI_CVBS(x);}
INLINE(Vdbi,VQHF_CVBS) (Vqhf x) {return VDBU_VOID;}

INLINE(Vwbi,VQWU_CVBS) (Vqwu x) {return WBI_ASTV(QWU_CVBS(x));}
INLINE(Vwbi,VQWI_CVBS) (Vqwi x) {return WBI_ASTV(QWI_CVBS(x));}
INLINE(Vwbi,VQWF_CVBS) (Vqwf x) {return WBI_ASTV(QWF_CVBS(x));}


#if _LEAVE_ARM_CVBS
}
#endif

#if _ENTER_ARM_CVHU
{
#endif

INLINE(uint16_t,   BOOL_CVHU)     (_Bool x) {return x;}
INLINE(uint16_t,  UCHAR_CVHU)     (uchar x) {return x;}
INLINE(uint16_t,   CHAR_CVHU)      (char x) {return x;}
INLINE(uint16_t,  SCHAR_CVHU)     (schar x) {return x;}
INLINE(uint16_t,  USHRT_CVHU)    (ushort x) {return x;}
INLINE(uint16_t,   SHRT_CVHU)     (short x) {return x;}
INLINE(uint16_t,    INT_CVHU)       (int x) {return x;}
INLINE(uint16_t,   UINT_CVHU)      (uint x) {return x;}
INLINE(uint16_t,  ULONG_CVHU)     (ulong x) {return x;}
INLINE(uint16_t,   LONG_CVHU)      (long x) {return x;}
INLINE(uint16_t, ULLONG_CVHU)    (ullong x) {return x;}
INLINE(uint16_t,  LLONG_CVHU)     (llong x) {return x;}
INLINE(uint16_t,  FLT16_CVHU)   (flt16_t x) {return x;}
INLINE(uint16_t,    FLT_CVHU)     (float x) {return x;}
INLINE(uint16_t,    DBL_CVHU)    (double x) {return x;}
INLINE(uint16_t,cvhuac) (void volatile const *x)
{
    return  (uintptr_t) x;
}

#if QUAD_NLLONG == 2
INLINE(uint16_t,cvhuqu) (QUAD_UTYPE x) {return x;}
INLINE(uint16_t,cvhuqi) (QUAD_ITYPE x) {return x;}
INLINE(uint16_t,cvhuqf) (QUAD_FTYPE x) {return x;}
#endif

#define     WBU_CVHU(X)         \
vreinterpret_u16_u8(            \
    vzip1_u8(                   \
        vreinterpret_u8_f32(    \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        ),                      \
        vdup_n_u8(0)            \
    )                           \
)


#define     WBI_CVHU(X)         \
vreinterpret_u16_s16(           \
    vget_low_s16(               \
        vmovl_s8(               \
            vreinterpret_s8_f32(\
                vdup_n_f32(     \
                    _Generic(   \
                        X,      \
                        float:X \
                    )           \
                )               \
            )                   \
        )                       \
    )                           \
)

#if CHAR_MIN
#   define  WBC_CVHU        WBI_CVHU
#else
#   define  WBC_CVHU        WBU_CVHU
#endif

#define     WHU_CVHU(X)     (X)
#define     WHI_CVHU(X)     (X)
#define     WHF_CVHU(X)     (0.0f)

#define     DBU_CVHU(X)     vmovl_u8(X)
#define     DBI_CVHU(X)     vreinterpretq_u16_s16(vmovl_s8(X))
#if CHAR_MIN
#   define  DBC_CVHU        DBI_CVHU
#else
#   define  DBC_CVHU        DBU_CVHU
#endif

#define     DHU_CVHU(X)     (X)
#define     DHI_CVHU(X)     vreinterpret_u16_s16(X)
#define     DHF_CVHU(X)     vdup_n_u16(0)

//  {.H0={W0.B0, W0.B1}, .H1={W1.B0, W1.B1}}

#define     DWU_CVHU(X)                     \
vget_lane_f32(                              \
    vreinterpret_f32_u8(                    \
        vtbl1_u8(                           \
            vreinterpret_u8_u32(X),         \
            vreinterpret_u8_u64(            \
                vdup_n_u64(                 \
                    0x0504010005040100ull   \
                )                           \
            )                               \
        )                                   \
    ),                                      \
    V2_K0                                   \
)

#define     DWI_CVHU(X)                     \
vget_lane_f32(                              \
    vreinterpret_f32_u8(                    \
        vtbl1_u8(                           \
            vreinterpret_u8_s32(X),         \
            vreinterpret_u8_u64(            \
                vdup_n_u64(                 \
                    0x0504010005040100ull   \
                )                           \
            )                               \
        )                                   \
    ),                                      \
    V2_K0                                   \
)

#define     DWF_CVHU(X)     DWI_CVHU(vcvt_s32_f32(X))

#define     QHU_CVHU(X)     (X)
#define     QHI_CVHU(X)     vreinterpretq_u16_s16(X)
#define     QHF_CVHU(X)     vdupq_n_u16(0)

#define     QWU_CVHU(X)     vmovn_u32(X)
#define     QWI_CVHU(X)     vreinterpret_u16_s16(vmovn_s32(X))
#define     QWF_CVHU(X)     QWI_CVHU(vcvtq_s32_f32(X))

// u64×2=>u32×2; u32×2##u32×2=>u32×4; u32×4=>
#define     QDU_CVHU(X)                     \
vget_lane_f32(                              \
    vreinterpret_f32_u8(                    \
        vqtbl1_u8(                          \
            vreinterpretq_u8_u64(X),        \
            vreinterpret_u8_u64(            \
                vdup_n_u64(                 \
                    0x0908010009080100ull   \
                )                           \
            )                               \
        )                                   \
    ),                                      \
    V2_K0                                   \
)

#define     QDI_CVHU(X)                     \
vget_lane_f32(                              \
    vreinterpret_f32_u8(                    \
        vqtbl1_u8(                          \
            vreinterpretq_u8_s64(X),        \
            vreinterpret_u8_u64(            \
                vdup_n_u64(                 \
                    0x0908010009080100ull   \
                )                           \
            )                               \
        )                                   \
    ),                                      \
    V2_K0                                   \
)

#define     QDF_CVHU(X) QDI_CVHU(vcvtq_s64_f64(X))

INLINE(Vdhu,VWBU_CVHU) (Vwbu x) {return  WBU_CVHU(VWBU_ASTM(x));}
INLINE(Vdhu,VWBI_CVHU) (Vwbi x) {return  WBI_CVHU(VWBI_ASTM(x));}
INLINE(Vdhu,VWBC_CVHU) (Vwbc x) {return  WBC_CVHU(VWBC_ASTM(x));}

INLINE(Vwhu,VWHU_CVHU) (Vwhu x) {return  VWHU_ASHU(x);}
INLINE(Vwhu,VWHI_CVHU) (Vwhi x) {return  VWHI_ASHU(x);}
INLINE(Vwhu,VWHF_CVHU) (Vwhf x) {return  VWHU_VOID;}

INLINE(Vqhu,VDBU_CVHU) (Vdbu x) {return  DBU_CVHU(x);}
INLINE(Vqhu,VDBI_CVHU) (Vdbi x) {return  DBI_CVHU(x);}
INLINE(Vqhu,VDBC_CVHU) (Vdbc x) {return  DBC_CVHU(VDBC_ASTM(x));}

INLINE(Vdhu,VDHU_CVHU) (Vdhu x) {return  DHU_CVHU(x);}
INLINE(Vdhu,VDHI_CVHU) (Vdhi x) {return  DHI_CVHU(x);}
INLINE(Vdhu,VDHF_CVHU) (Vdhf x) {return  DHF_CVHU(x);}

INLINE(Vwhu,VDWU_CVHU) (Vdwu x) {return  WHU_ASTV(DWU_CVHU(x));}
INLINE(Vwhu,VDWI_CVHU) (Vdwi x) {return  WHU_ASTV(DWI_CVHU(x));}
INLINE(Vwhu,VDWF_CVHU) (Vdwf x) {return  WHU_ASTV(DWF_CVHU(x));}

INLINE(Vqhu,VQHU_CVHU) (Vqhu x) {return  QHU_CVHU(x);}
INLINE(Vqhu,VQHI_CVHU) (Vqhi x) {return  QHI_CVHU(x);}
INLINE(Vqhu,VQHF_CVHU) (Vqhf x) {return  QHF_CVHU(x);}

INLINE(Vdhu,VQWU_CVHU) (Vqwu x) {return  QWU_CVHU(x);}
INLINE(Vdhu,VQWI_CVHU) (Vqwi x) {return  QWI_CVHU(x);}
INLINE(Vdhu,VQWF_CVHU) (Vqwf x) {return  QWF_CVHU(x);}

INLINE(Vwhu,VQDU_CVHU) (Vqdu x) {return  WHU_ASTV(QDU_CVHU(x));}
INLINE(Vwhu,VQDI_CVHU) (Vqdi x) {return  WHU_ASTV(QDI_CVHU(x));}
INLINE(Vwhu,VQDF_CVHU) (Vqdf x) {return  WHU_ASTV(QDF_CVHU(x));}

#if _LEAVE_ARM_CVHU
}
#endif

#if _ENTER_ARM_CVHI
{
#endif

#define     WBU_CVHI(X)         \
vreinterpret_s16_u8(            \
    vzip1_u8(                   \
        vreinterpret_u8_f32(    \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        ),                      \
        vdup_n_u8(0)            \
    )                           \
)

#define     WBI_CVHI(X)     \
vget_low_s16(               \
    vmovl_s8(               \
        vreinterpret_s8_f32(\
            vdup_n_f32(     \
                _Generic(   \
                    X,      \
                    float:X \
                )           \
            )               \
        )                   \
    )                       \
)

INLINE(int16_t,   BOOL_CVHI)   (_Bool x) {return x;}
INLINE(int16_t,  UCHAR_CVHI)   (uchar x) {return x;}
INLINE(int16_t,  SCHAR_CVHI)   (schar x) {return x;}
INLINE(int16_t,   CHAR_CVHI)    (char x) {return x;}
INLINE(int16_t,  USHRT_CVHI)  (ushort x) {return x;}
INLINE(int16_t,   SHRT_CVHI)   (short x) {return x;}
INLINE(int16_t,    INT_CVHI)     (int x) {return x;}
INLINE(int16_t,   UINT_CVHI)    (uint x) {return x;}
INLINE(int16_t,  ULONG_CVHI)   (ulong x) {return x;}
INLINE(int16_t,   LONG_CVHI)    (long x) {return x;}
INLINE(int16_t, ULLONG_CVHI)  (ullong x) {return x;}
INLINE(int16_t,  LLONG_CVHI)   (llong x) {return x;}
INLINE(int16_t,  FLT16_CVHI) (flt16_t x) {return x;}
INLINE(int16_t,    FLT_CVHI)   (float x) {return x;}
INLINE(int16_t,    DBL_CVHI)  (double x) {return x;}
INLINE(int16_t,cvhiac) (void volatile const *x)
{
    return  (intptr_t) x;
}
#if QUAD_NLLONG == 2
INLINE(int16_t, cvhiqu)   (QUAD_UTYPE x) {return x;}
INLINE(int16_t, cvhiqi)   (QUAD_ITYPE x) {return x;}
INLINE(int16_t, cvhiqf)   (QUAD_FTYPE x) {return x;}
#endif


INLINE(Vdhi,VWBU_CVHI) (Vwbu x)
{
    float32x2_t m = vdup_n_f32(VWBU_ASTM(x));
    uint16x8_t  u = vmovl_u8(vreinterpret_u8_f32(m));
    int16x8_t   i = vreinterpretq_s16_u16(u);
    return  vget_low_s16(i);
}

INLINE(Vdhi,VWBI_CVHI) (Vwbi x)
{
    float32x2_t m = vdup_n_f32(VWBI_ASTM(x));
    int16x8_t   i = vmovl_s8(vreinterpret_s8_f32(m));
    return  vget_low_s16(i);
}

INLINE(Vdhi,VWBC_CVHI) (Vwbc x)
{
    float32x2_t m = vdup_n_f32(VWBC_ASTM(x));
    int16x8_t   i;
#if CHAR_MIN
    i = vmovl_s8(vreinterpret_s8_f32(m));
#else
    uint16x8_t  u = vmovl_u8(vreinterpret_u8_f32(m));
    i = vreinterpretq_s16_u16(u);
#endif
    return  vget_low_s16(i);
}

INLINE(Vwhi,VWHU_CVHI) (Vwhu x) {return VWHU_ASHI(x);}
INLINE(Vwhi,VWHI_CVHI) (Vwhi x) {return x;}
INLINE(Vwhi,VWHF_CVHI) (Vwhf x)
{
    float32x2_t v = vdup_n_f32(VWHF_ASTM(x));
    float16x4_t h = vreinterpret_f16_f32(v);
    int16x4_t   i;
#if defined(SPC_ARM_FP16_SIMD)
    i = vcvt_s16_f16(h);
#else
    i = vmovn_s32(vcvtq_s32_f32(vcvt_f32_f16(h)));
#endif
    v = vreinterpret_f32_s16(i);
    return  WHI_ASTV(vget_lane_f32(v, 0));
}

INLINE(Vqhi,VDBU_CVHI) (Vdbu x) {return  VQHU_ASHI(vmovl_u8(x));}
INLINE(Vqhi,VDBI_CVHI) (Vdbi x) {return  vmovl_s8(x);}
INLINE(Vqhi,VDBC_CVHI) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_CVHI(VDBC_ASBI(x));
#else
    return  VDBU_CVHI(VDBC_ASBU(x));
#endif
}

INLINE(Vdhi,VDHU_CVHI) (Vdhu x) {return VDHU_ASHI(x);}
INLINE(Vdhi,VDHI_CVHI) (Vdhi x) {return x;}
INLINE(Vdhi,VDHF_CVHI) (Vdhf x)
{
    return VDHI_VOID;
}

INLINE(Vwhi,VDWU_CVHI) (Vdwu x)
{
#define     VDWU_CVHI(X) WHI_ASTV(DWU_CVHU(X))
    return  VDWU_CVHI(x);
}

INLINE(Vwhi,VDWI_CVHI) (Vdwi x)
{
#define     VDWI_CVHI(X) WHI_ASTV(DWU_CVHU(vreinterpret_u32_s32(X)))
    return  VDWI_CVHI(x);
}

INLINE(Vwhi,VDWF_CVHI) (Vdwf x)
{
    int32x2_t i = vcvt_s32_f32(x);
    uint8x8_t r = vreinterpret_u8_s32(i);
    r = vtbl1_u8(
        r,
        vcreate_u8(0xffffffff05040100ull)
    );
    float32x2_t m = vreinterpret_f32_u8(r);
    return  WHI_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vqhi,VQHU_CVHI) (Vqhu x) {return vreinterpretq_s16_u16(x);}
INLINE(Vqhi,VQHI_CVHI) (Vqhi x) {return x;}
INLINE(Vqhi,VQHF_CVHI) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_s16_f16(x);
#else
    float16x4_t l = vget_low_f16(x);
    float16x4_t r = vget_high_f16(x);
    return  vcombine_s16(
        vmovn_s32(vcvtq_s32_f32(vcvt_f32_f16(l))),
        vmovn_s32(vcvtq_s32_f32(vcvt_f32_f16(r)))
    );

#endif
}

INLINE(Vdhi,VQWU_CVHI) (Vqwu x) {return VDHU_ASHI(vmovn_u32(x));}
INLINE(Vdhi,VQWI_CVHI) (Vqwi x) {return vmovn_s32(x);}
INLINE(Vdhi,VQWF_CVHI) (Vqwf x) {return vmovn_s32(vcvtq_s32_f32(x));}

INLINE(Vwhi,VQDU_CVHI) (Vqdu x)
{

#define     VQDU_CVHI(X) WHI_ASTV(QDU_CVHU(X))
    return  VQDU_CVHI(x);
}

INLINE(Vwhi,VQDI_CVHI) (Vqdi x)
{
#define     VQDI_CVHI(X) WHI_ASTV(QDI_CVHU(X))
    return  VQDI_CVHI(x);
}

INLINE(Vwhi,VQDF_CVHI) (Vqdf x)
{
#define     VQDF_CVHI(X) WHI_ASTV(QDI_CVHU(vcvtq_s64_f64(X)))
    return  VQDF_CVHI(x);
}

#if _LEAVE_ARM_CVHI
}
#endif

#if _ENTER_ARM_CVHZ
{
#endif

INLINE(uint16_t,   BOOL_CVHZ)  (_Bool x) {return x;}
INLINE(uint16_t,  UCHAR_CVHZ)  (uchar x) {return x;}
INLINE(uint16_t,  SCHAR_CVHZ)  (schar x) {return vqmovunh_s16(x);}
INLINE(uint16_t,   CHAR_CVHZ)   (char x) 
{
#if CHAR_MIN
    return  SCHAR_CVHZ(x);
#else
    return  UCHAR_CVHZ(x);
#endif
}

INLINE(uint16_t,  USHRT_CVHZ) (ushort x) {return x;}
INLINE(uint16_t,   SHRT_CVHZ)  (short x) {return vqmovuns_s32(x);}
INLINE(uint16_t,   UINT_CVHZ)   (uint x) {return  vqmovns_u32(x);}
INLINE(uint16_t,    INT_CVHZ)    (int x) {return vqmovuns_s32(x);}
INLINE(uint16_t,  ULONG_CVHZ)  (ulong x)
{
#if DWRD_NLONG == 2
    return  vqmovns_u32(x);
#else
    return  vqmovns_u32(vqmovnd_u64(x));
#endif
}

INLINE(uint16_t,   LONG_CVHZ)   (long x)
{
#if DWRD_NLONG == 2
    return  vqmovuns_s32(x);
#else
    return  vqmovns_u32(vqmovund_s64(x));
#endif
}

INLINE(uint16_t, ULLONG_CVHZ) (ullong x)
{
#if QUAD_NLLONG == 2
    return  vqmovns_u32(vqmovnd_u64(x));
#else

#endif
}

INLINE(uint16_t,  LLONG_CVHZ)  (llong x)
{
#if QUAD_NLLONG == 2
    return  vqmovns_u32(vqmovund_s64(x));
#else

#endif
}

INLINE(uint16_t, FLT16_CVHZ) (flt16_t x)
{
    return  0;
}

INLINE(uint16_t, FLT_CVHZ) (float x)
{
    return  vqmovns_u32(vcvts_u32_f32(x));
}

INLINE(uint16_t, DBL_CVHZ) (double x)
{
    return  vqmovns_u32(vqmovnd_u64(vcvtd_u64_f64(x)));
}

INLINE(uint16_t,cvhzqu) (QUAD_UTYPE x)
{
    return (x >= UINT16_MAX) ? UINT16_MAX : x;
}

INLINE(uint16_t,cvhzqi) (QUAD_ITYPE x)
{
    if (x >= UINT16_MAX) return UINT16_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE(uint16_t,cvhzqf) (QUAD_FTYPE x)
{
    if (x >= UINT16_MAX) return UINT16_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE(Vdhu,VWBU_CVHZ) (Vwbu x)
{
    float32x2_t m = vdup_n_f32(VWBU_ASTM(x));
    uint8x8_t   l = vreinterpret_u8_f32(m);
    uint8x8_t   r = vdup_n_u8(0);
    r = vzip1_u8(l, r);
    return vreinterpret_u16_u8(r);
}

INLINE(Vdhu,VWBI_CVHZ) (Vwbi x)
{
    float32x2_t m = vdup_n_f32(VWBI_ASTM(x));
    int16x8_t   q = vmovl_s8(vreinterpret_s8_f32(m));
    int16x4_t   l = vget_low_s16(q);
    int32x4_t   c = vmovl_s16(l);
    return  vqmovun_s32(c);
}

INLINE(Vdhu,VWBC_CVHZ) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVHZ(VWBC_ASBI(x));
#else
    return  VWBU_CVHZ(VWBC_ASBU(x));
#endif
}


INLINE(Vwhu,VWHU_CVHZ) (Vwhu x) {return x;}
INLINE(Vwhu,VWHI_CVHZ) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    int32x4_t   q = vmovl_s16(vreinterpret_s16_f32(m));
    uint16x4_t  v = vqmovun_s32(q);
    m = vreinterpret_f32_u16(v);
    return  WHU_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vwhu,VWHF_CVHZ) (Vwhf x) {return VWHU_VOID;}

INLINE(Vqhu,VDBU_CVHZ) (Vdbu x) {return vmovl_u8(x);}
INLINE(Vqhu,VDBI_CVHZ) (Vdbi x)
{
    int16x8_t m = vmovl_s8(x);
    return vcombine_u16(
        vqmovun_s32(vmovl_s16(vget_low_s16(m))),
        vqmovun_s32(vmovl_s16(vget_high_s16(m)))
    );
}

INLINE(Vqhu,VDBC_CVHZ) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_CVHZ(VDBC_ASBI(x));
#else
    return  VDBU_CVHZ(VDBC_ASBU(x));
#endif
}

INLINE(Vdhu,VDHU_CVHZ) (Vdhu x) {return x;}
INLINE(Vdhu,VDHI_CVHZ) (Vdhi x) {return vqmovun_s32(vmovl_s16(x));}
INLINE(Vdhu,VDHF_CVHZ) (Vdhf x) {return VDHU_VOID;}

INLINE(Vwhu,VDWU_CVHZ) (Vdwu x)
{
#define     DWU_CVHZ(V)     \
vget_lane_f32(              \
    vreinterpret_f32_u16(   \
        vqmovn_u32(         \
            vcombine_u32(   \
                V,          \
                VDWU_VOID   \
            )               \
        )                   \
    ),                      \
    V2_K0                   \
)
#define     VDWU_CVHZ(X)    WHU_ASTV(DWU_CVHZ(X))
    return  VDWU_CVHZ(x);
}

INLINE(Vwhu,VDWI_CVHZ) (Vdwi x)
{
#define     DWI_CVHZ(V)     \
vget_lane_f32(              \
    vreinterpret_f32_s16(   \
        vqmovun_s32(        \
            vcombine_s32(   \
                V,          \
                VDWI_VOID   \
            )               \
        )                   \
    ),                      \
    V2_K0                   \
)
#define     VDWI_CVHZ(X)    WHU_ASTV(DWI_CVHZ(X))
    return  VDWI_CVHZ(x);
}

INLINE(Vwhu,VDWF_CVHZ) (Vdwf x)
{
    return  VDWI_CVHZ(vcvt_s32_f32(x));
}

INLINE(Vqhu,VQHU_CVHZ) (Vqhu x) {return x;}
INLINE(Vqhu,VQHI_CVHZ) (Vqhi x)
{
    return vcombine_u16(
        vqmovun_s32(vmovl_s16(vget_low_s16( x))),
        vqmovun_s32(vmovl_s16(vget_high_s16(x)))
    );

}

INLINE(Vqhu,VQHF_CVHZ) (Vqhf x) {return VQHU_VOID;}

INLINE(Vdhu,VQWU_CVHZ) (Vqwu x) {return vqmovn_u32(x);}
INLINE(Vdhu,VQWI_CVHZ) (Vqwi x) {return vqmovun_s32(x);}
INLINE(Vdhu,VQWF_CVHZ) (Vqwf x) {return vqmovun_s32(vcvtq_s32_f32(x));}

INLINE(Vwhu,VQDU_CVHZ) (Vqdu x)
{
    x = vorrq_u64(
        vtstq_u64(
            vdupq_n_u64(UINT64_MAX-UINT16_MAX),
            x
        ),
        x
    );
    uint8x8_t t = vqtbl1_u8(
        vreinterpretq_u8_u64(x),
        vreinterpret_u8_u64(vdup_n_u64(UINT64_C(0xffff0908ffff0100)))
    );
    float32x2_t m = vreinterpret_f32_u8(t);
    return  WHU_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vwhu,VQDI_CVHZ) (Vqdi x)
{
// TODO: implement arm's 'cvhzqdi' properly
    uint16x4_t v = VDHU_VOID;
    v = vset_lane_u16(
        vqmovns_u32(vqmovund_s64(vgetq_lane_s64(x, V2_K0))),
        v,
        V4_K0
    );
    v = vset_lane_u16(
        vqmovns_u32(vqmovund_s64(vgetq_lane_s64(x, V2_K0))),
        v,
        V4_K0
    );
    float32x2_t m = vreinterpret_f32_u16(v);
    return  WHU_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vwhu,VQDF_CVHZ) (Vqdf x) {return VQDI_CVHZ(vcvtq_s64_f64(x));}


#if _LEAVE_ARM_CVHZ
}
#endif

#if _ENTER_ARM_CVHS
{
#endif

INLINE(int16_t,  BOOL_CVHS)  (_Bool x) {return x;}
INLINE(int16_t, UCHAR_CVHS)  (uchar x) {return x;}
INLINE(int16_t, SCHAR_CVHS)  (schar x) {return x;}
INLINE(int16_t,  CHAR_CVHS)   (char x) {return x;}
INLINE(int16_t, USHRT_CVHS) (ushort x) {return vqmovns_s32(x);}
INLINE(int16_t,  SHRT_CVHS)  (short x) {return vqmovns_s32(x);}
INLINE(int16_t,  UINT_CVHS)   (uint x)
{
#define     UINT_CVHS(X)                \
(                                       \
    (int16_t)                           \
    (                                   \
        (                               \
            vtstd_u64(                  \
                (UINT64_MAX-INT16_MAX), \
                (X)                     \
            )>>49                       \
        )                               \
    |   (X)                             \
    )                                   \
)
    return  UINT_CVHS(x);
}

INLINE(int16_t,   INT_CVHS)    (int x) {return vqmovns_s32(x);}
INLINE(int16_t, ULONG_CVHS)  (ulong x) {return   UINT_CVHS(x);}

INLINE(int16_t,  LONG_CVHS)   (long x)
{
#if DWRD_NLONG == 2
    return  vqmovns_s32(x);
#else
    return  vqmovns_s32(vqmovnd_s64(x));
#endif
}

INLINE(int16_t,ULLONG_CVHS) (ullong x) {return  UINT_CVBS(x);}

INLINE(int16_t, LLONG_CVHS)  (llong x)
{
#if QUAD_NLONG == 2
    return  vqmovnh_s16(vqmovns_s32(vqmovnd_s64(x)));
#else
#   error "is there a 'vqmovnq_s128' yet?"
#endif
}

INLINE(int16_t, FLT16_CVHS) (flt16_t x) {return 0;}
INLINE(int16_t,   FLT_CVHS)  (float x)
{
    return  vqmovns_s32(vcvts_s32_f32(x));
}

INLINE(int16_t,   DBL_CVHS) (double x)
{
    return  vqmovns_s32(vqmovnd_s64(vcvtd_s64_f64(x)));
}

INLINE(int16_t,cvhsqu) (QUAD_UTYPE x)
{
    return (x >= INT16_MAX) ? INT16_MAX : x;
}

INLINE(int16_t,cvhsqi) (QUAD_ITYPE x)
{
    if (x >= INT16_MAX) return INT16_MAX;
    if (x <= INT16_MIN) return INT16_MIN;
    return x;
}

INLINE(int16_t,cvhsqf) (QUAD_FTYPE x)
{
    if (x >= INT16_MAX) return INT16_MAX;
    if (x <= INT16_MIN) return INT16_MIN;
    return x;
}


INLINE(int16x4_t, WBU_CVHS) (float x)
{
    float32x2_t m = vdup_n_f32(x);
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  u = vmovl_u16(d);
    return  vget_low_s16(vreinterpretq_s16_u16(u));
}

INLINE(int16x4_t, WBI_CVHS) (float x)
{
    float32x2_t m = vdup_n_f32(x);
    int8x8_t    d = vreinterpret_s8_f32(m);
    int16x8_t   i = vmovl_s16(d);
    return  vget_low_s16(i);
}

INLINE(float,     WHU_CVHS) (float x)
{
    float32x2_t m = vdup_n_f32(x);
    uint16x4_t  u = vreinterpret_u16_f32(m);
    u = vand_u16(
        vdup_n_u16(0x7fff),
        vorr_u16(
            vtst_u16(vdup_n_u16(0x8000), u),
            u
        )
    );
    m = vreinterpret_f32_u16(u);
    return vget_lane_f32(m, V2_K0);
}

INLINE(float,     WHI_CVHS)      (float x) {return x;}

INLINE(float,     WHF_CVHS)      (float x) {return 0.0f;}


INLINE(int16x8_t, DBU_CVHS)   (uint8x8_t x)
{
    return vreinterpretq_s16_u16(vmovl_u8(x));
}

INLINE(int16x8_t, DBI_CVHS)    (int8x8_t x) {return vmovl_u8(x);}


INLINE(int16x4_t, DHU_CVHS)  (uint16x4_t x)
{
    return vreinterpret_s16_u16(
        vand_u16(
            vdup_n_u16(0x7fff),
            vorr_u16(
                vtst_u16(vdup_n_u16(0x8000), x),
                x
            )
        )
    );
}

INLINE(int16x4_t, DHI_CVHS)   (int16x4_t x) {return x;}
INLINE(int16x4_t, DHF_CVHS) (float16x4_t x) {return VDHI_VOID;}

INLINE(float,     DWU_CVHS)  (uint32x2_t x)
{
    x = vand_u32(
        vdup_n_u32(0x7fffffffu),
        vorr_u32(
            vtst_u32(vdup_n_u32(0x80000000u), x),
            x
        )
    );
    int32x2_t   i = vreinterpret_s32_u32(x);
    int32x4_t   s = vcombine_s32(i, i);
    int16x4_t   v = vqmovn_s32(s);
    float32x2_t m = vreinterpret_f32_s16(v);
    return vget_lane_f32(m, V2_K0);
}

INLINE(float,     DWI_CVHS)   (int32x2_t x)
{
    int32x4_t   s = vcombine_s32(x, x);
    int16x4_t   v = vqmovn_s32(s);
    float32x2_t m = vreinterpret_f32_s16(v);
    return vget_lane_f32(m, V2_K0);
}

INLINE(float,     DWF_CVHS) (float32x2_t x)
{
    return  DWI_CVHS(vcvt_s32_f32(x));
}

INLINE(int16x8_t, QHU_CVHS)  (uint16x8_t x)
{
    return vreinterpretq_s16_u16(
        vandq_u16(
            vdupq_n_u16(0x7fff),
            vorrq_u16(
                vtstq_u16(vdupq_n_u16(0x8000), x),
                x
            )
        )
    );
}

INLINE(int16x8_t, QHI_CVHS)   (int16x8_t x) {return x;}
INLINE(int16x8_t, QHF_CVHS) (float16x8_t x) {return VQHI_VOID;}


INLINE(int16x4_t, QWU_CVHS)  (uint32x4_t x)
{
    x = vandq_u32(
        vdupq_n_u32(0x7fffffffu),
        vorrq_u32(
            vtstq_u32(vdupq_n_u32(0x80000000u), x),
            x
        )
    );
    return vqmovn_s32(vreinterpretq_s32_u32(x));
}

INLINE(int16x4_t, QWI_CVHS)   (int32x4_t x) {return vqmovn_s32(x);}
INLINE(int16x4_t, QWF_CVHS) (float32x4_t x)
{
    return vqmovn_s32(vcvtq_s32_f32(x));
}

INLINE(float,     QDU_CVHS)  (uint64x2_t x)
{
    return  DWU_CVHS(vqmovn_u64(x));
}

INLINE(float,     QDI_CVHS)   (int64x2_t x)
{
    return  DWI_CVHS(vqmovn_s64(x));
}

INLINE(float,     QDF_CVHS) (float64x2_t x)
{
    return  QDI_CVHS(vcvtq_s64_f64(x));
}


INLINE(Vdhi,VWBU_CVHS) (Vwbu x)
{
    return  WBU_CVHS(VWBU_ASTM(x));
}

INLINE(Vdhi,VWBI_CVHS) (Vwbi x)
{
    return  WBI_CVHS(VWBI_ASTM(x));
}

INLINE(Vdhi,VWBC_CVHS) (Vwbc x)
{
#if CHAR_MIN
    return  WBI_CVHS(VWBC_ASTM(x));
#else
    return  WBU_CVHS(VWBC_ASTM(x));
#endif
}


INLINE(Vwhi,VWHU_CVHS) (Vwhu x)
{
    return  WHI_ASTV(WHU_CVHS(VWHU_ASTM(x)));
}

INLINE(Vwhi,VWHI_CVHS) (Vwhi x) {return x;}
INLINE(Vwhi,VWHF_CVHS) (Vwhf x) {return VWHI_VOID;}

INLINE(Vqhi,VDBU_CVHS) (Vdbu x) {return DBU_CVHS(x);}
INLINE(Vqhi,VDBI_CVHS) (Vdbi x) {return DBI_CVHS(x);}
INLINE(Vqhi,VDBC_CVHS) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_CVHS(VDBC_ASBI(x));
#else
    return  VDBU_CVHS(VDBC_ASBU(x));
#endif
}

INLINE(Vdhi,VDHU_CVHS) (Vdhu x) {return DHU_CVHS(x);}
INLINE(Vdhi,VDHI_CVHS) (Vdhi x) {return DHI_CVHS(x);}
INLINE(Vdhi,VDHF_CVHS) (Vdhf x) {return DHF_CVHS(x);}

INLINE(Vwhi,VDWU_CVHS) (Vdwu x) {return WHI_ASTV(DWU_CVHS(x));}
INLINE(Vwhi,VDWI_CVHS) (Vdwi x) {return WHI_ASTV(DWI_CVHS(x));}
INLINE(Vwhi,VDWF_CVHS) (Vdwf x) {return WHI_ASTV(DWF_CVHS(x));}

//     Vohi,VQBU_CVHS
INLINE(Vqhi,VQHU_CVHS) (Vqhu x) {return QHU_CVHS(x);}
INLINE(Vqhi,VQHI_CVHS) (Vqhi x) {return QHI_CVHS(x);}
INLINE(Vqhi,VQHF_CVHS) (Vqhf x) {return QHF_CVHS(x);}

INLINE(Vdhi,VQWU_CVHS) (Vqwu x) {return QWU_CVHS(x);}
INLINE(Vdhi,VQWI_CVHS) (Vqwi x) {return QWI_CVHS(x);}
INLINE(Vdhi,VQWF_CVHS) (Vqwf x) {return QWF_CVHS(x);}

INLINE(Vwhi,VQDU_CVHS) (Vqdu x) {return WHI_ASTV(QDU_CVHS(x));}
INLINE(Vwhi,VQDI_CVHS) (Vqdi x) {return WHI_ASTV(QDI_CVHS(x));}
INLINE(Vwhi,VQDF_CVHS) (Vqdf x) {return WHI_ASTV(QDF_CVHS(x));}

#if _LEAVE_ARM_CVHS
}
#endif

#if _ENTER_ARM_CVHF
{
#endif
/*
INLINE(float,FLT16_CVWF) (flt16_t m)
{
    HALF_TYPE src={.F=m};
    WORD_TYPE dst;
    dst.Mant = src.Mant<<13;
    dst.Expo = src.Expo+112;
    dst.Sign = src.Sign;
    return  dst.F;
}

INLINE(flt16_t,FLT_CVHF) (float m)
{
    WORD_TYPE src = {.F=m};
    HALF_TYPE dst = {.Sign=src.Sign};
    if (src.Expo < 112)
        return  dst.F;
    if (src.Expo > 142)
        dst.Expo = 31;
    else
        dst.Expo = src.Expo-112;
    dst.Mant = src.Mant>>13;
    return  dst.F;
}

1.0f16:
        E       M
    H   15      0
    W   127     0
    D   1023    0
    Q   8191    0

FLT16_MIN:
    H   1       0
    W   113     0
    D   1009    0

FLT16_MAX:
    H   30      (1023<<0)
    W   142     (1023<<13) == 8380416
    D   1038    (1023<<42) == 4499201580859392



INLINE(flt16_t,DBL_CVHF) (double m)
{
    DWRD_TYPE src = {.F=m};
    HALF_TYPE dst = {.Sign=src.Sign};
    if (src.Expo < 1009)
        return  dst.F;
    if (src.Expo > 1038)
        dst.Expo = 31;
    else
        dst.Expo = src.Expo-1008;
    dst.Mant = src.Mant>>42;
    return  dst.F;
}
*/

INLINE(flt16_t,  BOOL_CVHF)   (_Bool x) {return x;}
INLINE(flt16_t, UCHAR_CVHF)   (uchar x) {return x;}
INLINE(flt16_t, SCHAR_CVHF)   (schar x) {return x;}
INLINE(flt16_t, USHRT_CVHF)  (ushort x) {return x;}
INLINE(flt16_t,  SHRT_CVHF)   (short x) {return x;}
INLINE(flt16_t,   INT_CVHF)     (int x) {return x;}
INLINE(flt16_t,  UINT_CVHF)    (uint x) {return x;}
INLINE(flt16_t, ULONG_CVHF)   (ulong x) {return x;}
INLINE(flt16_t,  LONG_CVHF)    (long x) {return x;}
INLINE(flt16_t,ULLONG_CVHF)  (ullong x) {return x;}
INLINE(flt16_t, LLONG_CVHF)   (llong x) {return x;}
INLINE(flt16_t, FLT16_CVHF) (flt16_t x) {return x;}
INLINE(flt16_t,   FLT_CVHF)   (float x) {return x;}
INLINE(flt16_t,   DBL_CVHF)  (double x) {return x;}
#if QUAD_NLLONG == 2
INLINE(flt16_t, cvhfqu)  (QUAD_UTYPE x) {return x;}
INLINE(flt16_t, cvhfqi)  (QUAD_ITYPE x) {return x;}
INLINE(flt16_t, cvhfqf)  (QUAD_FTYPE x) {return x;}
#endif

INLINE(Vdhf,VWBU_CVHF) (Vwbu x)
{
    float32x2_t w = vdup_n_f32(VWBU_ASTM(x));
    uint8x8_t  bv = vreinterpret_u8_f32(w);
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvt_f16_u16(vget_low_u16(vmovl_u8(bv)));
#else
    uint8x8_t  bz = vclz_u8(bv);
    uint16x4_t  v = vget_low_u16(vmovl_u8(bv));
    uint16x4_t  z = vget_low_u16(vmovl_u8(bz));
    uint16x4_t  e = vsub_u16(vdup_n_u16(22), z);
    int16x4_t   n = vreinterpret_s16_u16(z);
    uint16x4_t  d = vshl_u16(vdup_n_u16(128), vneg_s16(n));
    uint16x4_t  m = vsub_u16(v, d);
    n = vadd_s16(vdup_n_s16(3), n);
    m = vshl_u16(m, n);
    uint16x4_t  t = vcgt_u16(v, vdup_n_u16(0));
    m = vand_u16(m, t);
    e = vand_u16(e, t);
    e = vshl_n_u16(e, 10);
    e = vorr_u16(m, e);
    return  vreinterpret_f16_u16(e);
#endif
}

INLINE(Vdhf,VWBI_CVHF) (Vwbi x)
{
    float32x2_t m = vdup_n_f32(VWBI_ASTM(x));
    int8x8_t    i = vreinterpret_s8_f32(m);
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvt_f16_s16(vget_low_s16(vmovl_s8(i)));
#else
    int16x4_t   h = vget_low_s16(vmovl_s8(i));
    int32x4_t   w = vmovl_s16(h);
    return  vcvt_f16_f32(vcvtq_f32_s32(w));
#endif
}

INLINE(Vdhf,VWBC_CVHF) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVHF(VWBC_ASBI(x));
#else
    return  VWBU_CVHF(VWBC_ASBU(x));
#endif
}


INLINE(Vwhf,VWHU_CVHF) (Vwhu x)
{
    float32x2_t w = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  h = vreinterpret_u16_f32(w);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t f = vcvt_f16_u16(h);
    w = vreinterpret_f32_f16(f);
    return  WHF_ASTV(vget_lane_f32(w, 0));
#else
    uint16x4_t  z = vclz_u16(h);
    uint16x4_t  e = vsub_u16(vdup_n_u16(22), z);
    int16x4_t   n = vreinterpret_s16_u16(z);
    uint16x4_t  d = vshl_u16(vdup_n_u16(128), vneg_s16(n));
    uint16x4_t  m = vsub_u16(h, d);
    n = vadd_s16(vdup_n_s16(3), n);
    m = vshl_u16(m, n);
    uint16x4_t  t = vcgt_u16(h, vdup_n_u16(0));
    m = vand_u16(m, t);
    e = vand_u16(e, t);
    e = vshl_n_u16(e, 10);
    h = vorr_u16(m, e);
    w = vreinterpret_f32_u16(h);
    return  WHF_ASTV(vget_lane_f32(w, 0));
#endif
}

INLINE(Vwhf,VWHI_CVHF) (Vwhi x)
{
    float32x2_t w = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   h = vreinterpret_s16_f32(w);
    float16x4_t f;
#if defined(SPC_ARM_FP16_SIMD)
    f = vcvt_f16_s16(h);
#else
    f = vcvt_f16_f32(vcvtq_f32_s32(vmovl_s16(h)));
#endif
    w = vreinterpret_f32_f16(f);
    return  WHF_ASTV(vget_lane_f32(w, 0));
}

INLINE(Vwhf,VWHF_CVHF) (Vwhf x) {return x;}


INLINE(Vqhf,VDBU_CVHF) (Vdbu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_f16_u16(vmovl_u8(x));
#else
    uint16x8_t  zh = vmovl_u8(x);
    uint32x4_t  zl = vmovl_u16(vget_low_u16(zh));
    uint32x4_t  zr = vmovl_u16(vget_high_u16(zh));
    float32x4_t fl = vcvtq_f32_u32(zl);
    float32x4_t fr = vcvtq_f32_u32(zr);
    return  vcombine_f16(vcvt_f16_f32(fl), vcvt_f16_f32(fr));
#endif
}

INLINE(Vqhf,VDBI_CVHF) (Vdbi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_f16_s16(vmovl_s8(x));
#else
    int16x8_t   z = vmovl_s8(x);
    int32x4_t   p = vmovl_s16(vget_low_s16(z));
    int32x4_t   q = vmovl_s16(vget_high_s16(z));
    float32x4_t l = vcvtq_f32_s32(p);
    float32x4_t r = vcvtq_f32_s32(q);
    return  vreinterpretq_f16_u16(
        vcombine_u16(
            vreinterpret_u16_f16(vcvt_f16_f32(l)),
            vreinterpret_u16_f16(vcvt_f16_f32(r))
        )
    );
#endif
}

INLINE(Vqhf,VDBC_CVHF) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_CVHF(VDBC_ASBI(x));
#else
    return  VDBU_CVHF(VDBC_ASBU(x));
#endif
}


INLINE(Vdhf,VDHU_CVHF) (Vdhu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvt_f16_u16(x);
#else
    return  vcvt_f16_f32(vcvtq_f32_u32(vmovl_u16(x)));
#endif
}

INLINE(Vdhf,VDHI_CVHF) (Vdhi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvt_f16_s16(x);
#else
    return  vcvt_f16_f32(vcvtq_f32_s32(vmovl_s16(x)));
#endif
}

INLINE(Vdhf,VDHF_CVHF) (Vdhf x) {return x;}


INLINE(Vwhf,VDWU_CVHF) (Vdwu x)
{
    uint8x8_t   b = vreinterpret_u8_u32(x);
    b = vtbl1_u8(
        b,
        vcreate_u8(0xffffffff05040100ULL)
    );
    uint16x4_t  h = vreinterpret_u16_u8(b);
    float16x4_t f;
#if defined(SPC_ARM_FP16_SIMD)
    f = vcvt_f16_u16(h);
#else
    f = vcvt_f16_f32(vcvtq_f32_u32(vmovl_u16(h)));
#endif
    float32x2_t m = vreinterpret_f32_f16(f);
    return  WHF_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwhf,VDWI_CVHF) (Vdwi x)
{
    uint8x8_t   b = vreinterpret_u8_s32(x);
    b = vtbl1_u8(
        b,
        vcreate_u8(0xffffffff05040100ULL)
    );
    int16x4_t  h = vreinterpret_s16_u8(b);
    float16x4_t f;
#if defined(SPC_ARM_FP16_SIMD)
    f = vcvt_f16_s16(h);
#else
    f = vcvt_f16_f32(vcvtq_f32_s32(vmovl_s16(h)));
#endif
    float32x2_t m = vreinterpret_f32_f16(f);
    return  WHF_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwhf,VDWF_CVHF) (Vdwf x)
{
    float32x4_t m = vcombine_f32(x, x);
    float16x4_t f = vcvt_f16_f32(m);
    x = vreinterpret_f32_f16(f);
    return  WHF_ASTV(vget_lane_f32(x, 0));
}

INLINE(Vqhf,VQHU_CVHF) (Vqhu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_f16_u16(x);
#else
    uint32x4_t  zl = vmovl_u16(vget_low_u16(x));
    uint32x4_t  zr = vmovl_u16(vget_high_u16(x));
    float32x4_t ml = vcvtq_f32_u32(zl);
    float32x4_t mr = vcvtq_f32_u32(zr);
    float16x4_t fl = vcvt_f16_f32(ml);
    float16x4_t fr = vcvt_f16_f32(mr);
    return  vcombine_f16(fl, fr);
#endif
}

INLINE(Vqhf,VQHI_CVHF) (Vqhi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_f16_s16(x);
#else
    int32x4_t   zl = vmovl_s16(vget_low_s16(x));
    int32x4_t   zr = vmovl_s16(vget_high_s16(x));
    float32x4_t ml = vcvtq_f32_s32(zl);
    float32x4_t mr = vcvtq_f32_s32(zr);
    float16x4_t fl = vcvt_f16_f32(ml);
    float16x4_t fr = vcvt_f16_f32(mr);
    return  vcombine_f16(fl, fr);
#endif
}


INLINE(Vqhf,VQHF_CVHF) (Vqhf x) {return x;}

INLINE(Vdhf,VQWU_CVHF) (Vqwu x) {return vcvt_f16_f32(vcvtq_f32_u32(x));}
INLINE(Vdhf,VQWI_CVHF) (Vqwi x) {return vcvt_f16_f32(vcvtq_f32_s32(x));}
INLINE(Vdhf,VQWF_CVHF) (Vqwf x) {return vcvt_f16_f32(x);}

INLINE(Vwhf,VQDU_CVHF) (Vqdu x) {return VDWU_CVHF(vmovn_u64(x));}
INLINE(Vwhf,VQDI_CVHF) (Vqdi x) {return VDWI_CVHF(vmovn_s64(x));}
INLINE(Vwhf,VQDF_CVHF) (Vqdf x)
{
//  TODO: make sure this actually works...
    uint64x2_t  du = vreinterpretq_u64_f64(x);
    uint64x2_t  dm = vandq_u64(vdupq_n_u64(0x000fffffffffffffULL), du);
    uint64x2_t  de = vandq_u64(vdupq_n_u64(0x7ff0000000000000ULL), du);
    uint64x2_t  ds = vtstq_u64(vdupq_n_u64(0x8000000000000000ULL), du);

    uint32x2_t  e = vmovn_u64(vshrq_n_u64(de, 52));
    uint32x2_t  m = vdup_n_u32(1008);
    uint32x2_t  c = vcgt_u32(e, m);

    // subtract 1008 from e
    e = vsub_u32(e, m);

    // set e=0 if !(e > 1008)
    e = vand_u32(e, c);

    // set e=31 if (e > 31)
    e = vmin_u32(e, vdup_n_u32(31));

    uint8x8_t   b = vqtbl1_u8(
        vreinterpretq_u8_u64(ds),
        vcreate_u8(0xffffffff09080100ULL)
    );
    uint16x4_t  f = vreinterpret_u16_u8(b);

    // set dst.Sign
    f = vand_u16(vdup_n_u16(0x8000), f);

    b = vtbl1_u8(
        vreinterpret_u8_u16(e),
        vcreate_u8(0xffffffff05040100ULL)
    );

    // set dst.Expo
    f = vorr_u16(f, vshl_n_u16(vreinterpret_u16_u8(b), 10));

    b = vqtbl1_u8(
        vreinterpretq_u8_u64(vshrq_n_u64(dm, 42)),
        vcreate_u8(0xffffffff09080100ULL)
    );
    // set dst.Mant
    f = vorr_u16(f, vreinterpret_u16_u8(b));

    float32x2_t v = vreinterpret_f32_u16(f);
    return  WHF_ASTV(vget_lane_f32(v, 0));
}

#if _LEAVE_ARM_CVHF
}
#endif

#if _ENTER_ARM_CVWU
{
#endif

#define     WHU_CVWU(X)         \
vreinterpret_u32_u16(           \
    vzip1_u16(                  \
        vreinterpret_u16_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        ),                      \
        vdup_n_u16(0)           \
    )                           \
)

INLINE(uint32_t,   BOOL_CVWU)     (_Bool x) {return x;}
INLINE(uint32_t,  UCHAR_CVWU)     (uchar x) {return x;}
INLINE(uint32_t,  SCHAR_CVWU)     (schar x) {return x;}
INLINE(uint32_t,   CHAR_CVWU)      (char x) {return x;}
INLINE(uint32_t,  USHRT_CVWU)    (ushort x) {return x;}
INLINE(uint32_t,   SHRT_CVWU)     (short x) {return x;}
INLINE(uint32_t,   UINT_CVWU)      (uint x) {return x;}
INLINE(uint32_t,    INT_CVWU)       (int x) {return x;}
INLINE(uint32_t,  ULONG_CVWU)     (ulong x) {return x;}
INLINE(uint32_t,   LONG_CVWU)      (long x) {return x;}
INLINE(uint32_t, ULLONG_CVWU)    (ullong x) {return x;}
INLINE(uint32_t,  LLONG_CVWU)     (llong x) {return x;}
INLINE(uint32_t,  FLT16_CVWU)   (flt16_t x) {return x;}
INLINE(uint32_t,    FLT_CVWU)     (float x) {return x;}
INLINE(uint32_t,    DBL_CVWU)    (double x) {return x;}
INLINE(uint32_t,cvwuac) (void volatile const *x)
{
    return  (uintptr_t) x;
}
#if QUAD_NLLONG == 2
INLINE(uint32_t,cvwuwu) (QUAD_UTYPE x) {return x;}
INLINE(uint32_t,cvwuwi) (QUAD_ITYPE x) {return x;}
INLINE(uint32_t,cvwuwf) (QUAD_FTYPE x) {return x;}
#endif


INLINE(Vqwu,VWBU_CVWU) (Vwbu x)
{
    float32x2_t m = vdup_n_f32(VWBU_ASTM(x));
    uint8x8_t   t = vreinterpret_u8_f32(m);
    return  vcombine_u8(
        vtbl1_u8(
            t,
            vcreate_u8(
                // {0,-1,-1,-1,  4,-1,-1,-1}
                UINT64_C(0xffffff04ffffff00)
            )
        ),
        vtbl1_u8(
            t,
            vcreate_u8(
                // {8,-1,-1,-1,  12,-1,-1,-1}
                UINT64_C(0xffffff0cffffff08)
            )
        )
    );
}

INLINE(Vqwu,VWBI_CVWU) (Vwbi x)
{
    float32x2_t dwf = vdup_n_f32(VWBI_ASTM(x));
    int8x8_t    dbi = vreinterpret_s8_f32(dwf);
    int16x8_t   qhi = vmovl_s8(dbi);
    int32x4_t   qwi = vmovl_s16(vget_low_s16(qhi));
    return  vreinterpretq_u32_s32(qwi);
}

INLINE(Vqwu,VWBC_CVWU) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVWU(VWBC_ASBI(x));
#else
    return  VWBU_CVWU(VWBC_ASBU(x));
#endif
}


INLINE(Vdwu,VWHU_CVWU) (Vwhu x)
{
    float32x2_t mdwf = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  mdhu = vreinterpret_u16_f32(mdwf);
    uint32x4_t  mqwu = vmovl_u16(mdhu);
    return  vget_low_u32(mqwu);
}

INLINE(Vdwu,VWHI_CVWU) (Vwhi x)
{
    float32x2_t dwf = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   dhi = vreinterpret_s16_f32(dwf);
    int32x4_t   qwi = vmovl_s16(dhi);
    uint32x4_t  qwu = vreinterpretq_u32_s32(qwi);
    return  vget_low_u32(qwu);
}

INLINE(Vdwu,VWHF_CVWU) (Vwhf x)
{
    float32x2_t dwf = vdup_n_f32(VWHF_ASTM(x));
    float16x4_t dhf = vreinterpret_f16_f32(dwf);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_low_u32(vmovl_u16(vcvt_u16_f16(dhf)));
#else
    return  vget_low_u32(vcvtq_u32_f32(vcvt_f32_f16(dhf)));
#endif
}


INLINE(Vwwu,VWWU_CVWU) (Vwwu x) {return x;}
INLINE(Vwwu,VWWI_CVWU) (Vwwi x) {return VWWI_ASTU(x);}
INLINE(Vwwu,VWWF_CVWU) (Vwwf x)
{
    return UINT_ASTV(vcvts_u32_f32(VWWF_ASTM(x)));
}

INLINE(Vqwu,VDHU_CVWU) (Vdhu x) {return vmovl_u16(x);}
INLINE(Vqwu,VDHI_CVWU) (Vdhi x) {return VQWI_ASTU(vmovl_s16(x));}
INLINE(Vqwu,VDHF_CVWU) (Vdhf x)
{

#if defined(SPC_ARM_FP16_SIMD)
    return  vreinterpretq_u32_s32(vmovl_s16(vcvt_s16_f16(x)));
#else
    return  vreinterpretq_u32_s32(vcvtq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vdwu,VDWU_CVWU) (Vdwu x) {return x;}
INLINE(Vdwu,VDWI_CVWU) (Vdwi x) {return vreinterpret_u32_s32(x);}
INLINE(Vdwu,VDWF_CVWU) (Vdwf x)
{
    return  vreinterpret_u32_s32(vcvt_s32_f32(x));
}

INLINE(Vwwu,VDDU_CVWU) (Vddu x)
{
    return  UINT_ASTV(vget_lane_u64(x, 0));
}

INLINE(Vwwu,VDDI_CVWU) (Vddi x)
{
    return  UINT_ASTV(vget_lane_s64(x, 0));
}

INLINE(Vwwu,VDDF_CVWU) (Vddf x)
{
    return  UINT_ASTV(vget_lane_f64(x, 0));
}

INLINE(Vqwu,VQWU_CVWU) (Vqwu x) {return x;}
INLINE(Vqwu,VQWI_CVWU) (Vqwi x) {return vreinterpretq_u32_s32(x);}
INLINE(Vqwu,VQWF_CVWU) (Vqwf x) {return vcvtq_u32_f32(x);}

INLINE(Vdwu,VQDU_CVWU) (Vqdu x) {return vmovn_u64(x);}
INLINE(Vdwu,VQDI_CVWU) (Vqdi x) {return vreinterpret_u32_s32(vmovn_s64(x));}
INLINE(Vdwu,VQDF_CVWU) (Vqdu x) {return vmovn_u64(vcvtq_u64_f64(x));}


#if _LEAVE_ARM_CVWU
}
#endif

#if _ENTER_ARM_CVWZ
{
#endif

INLINE(uint32_t,  BOOL_CVWZ)   (_Bool x) {return x;}
INLINE(uint32_t, UCHAR_CVWZ)   (uchar x) {return x;}
INLINE(uint32_t,  CHAR_CVWZ)    (char x)
{
#if CHAR_MIN
    return  vqmovunh_s16(x);
#else
    return  x;
#endif
}
INLINE(uint32_t, SCHAR_CVWZ)   (schar x) {return vqmovunh_s16(x);}
INLINE(uint32_t, USHRT_CVWZ)  (ushort x) {return x;}
INLINE(uint32_t,  SHRT_CVWZ)   (short x) {return vqmovuns_s32(x);}
INLINE(uint32_t,  UINT_CVWZ)    (uint x) {return x;}
INLINE(uint32_t,   INT_CVWZ)     (int x) {return vqmovund_s64(x);}

INLINE(uint32_t, ULONG_CVWZ)   (ulong x)
{
#if DWRD_NLONG == 2
    return  x;
#else
    return  vqmovnd_u64(x);
#endif
}

INLINE(uint32_t,  LONG_CVWZ)    (long x) {return vqmovund_s64(x);}
INLINE(uint32_t,ULLONG_CVWZ)  (ullong x)
{
#if QUAD_NLLONG == 2
    return  vqmovnd_u64(x);
#else
#endif
}

INLINE(uint32_t, LLONG_CVWZ)   (llong x)
{
#if QUAD_NLLONG == 2
    return  vqmovund_s64(x);
#else
#endif
}

INLINE(uint32_t, FLT16_CVWZ) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvth_u16_f16(x);
#else
    return  vcvts_u32_f32(x);
#endif
}

INLINE(uint32_t,   FLT_CVWZ)   (float x) {return vcvts_u32_f32(x);}
INLINE(uint32_t,   DBL_CVWZ)  (double x)
{
    return  vqmovnd_u64(vcvtd_u64_f64(x));
}

INLINE(uint32_t,cvwzqu) (QUAD_UTYPE x)
{
    return (x >= UINT32_MAX) ? UINT32_MAX : x;
}

INLINE(uint32_t,cvwzqi) (QUAD_ITYPE x)
{
    if (x >= UINT32_MAX) return UINT32_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE(uint32_t,cvwzqf) (QUAD_FTYPE x)
{
    if (x >= UINT32_MAX) return UINT32_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE( Wwu,       WWI_CVWZ) ( Wwi m)
{
    float32x2_t f = vdup_n_f32(m);
    int64x2_t   q = vmovl_s32(vreinterpret_s32_f32(f));
    uint32x2_t  d = vqmovun_s64(q);
    f = vreinterpret_f32_u32(d);
    return vget_lane_f32(f, V2_K0);
}

INLINE( Wwu, WWF_CVWZ) ( Wwf m)
{
    float32x2_t d = vset_lane_f32(m, d, V2_K0);
    uint32x2_t  u = vcvt_u32_f32(d);
    d = vreinterpret_f32_u32(u);
    return vget_lane_f32(d, V2_K0);
}

INLINE(Vqwu,VWBU_CVWZ) (Vwbu x) {return VWBU_CVWU(x);}
INLINE(Vqwu,VWBI_CVWZ) (Vwbi x)
{
/*  This is much simpler than it looks.
*/
    float32x2_t m = vdup_n_f32(VWBI_ASTM(x));
    uint8x8_t   b = vreinterpret_u8_f32(m);
    uint8x8_t   s = vtst_u8(b, vdup_n_u8(0x80));
    uint8x16_t  t = vcombine_u8(b, s);
    uint8x16_t  l = vqtbl1q_u8(
        t,
        vcombine_u8(
            vcreate_u8(UINT64_C(0x0000000000000008)),
            vcreate_u8(UINT64_C(0x0101010101010109))
        )
    );
    uint8x16_t  r = vqtbl1q_u8(
        t,
        vcombine_u8(
            vcreate_u8(UINT64_C(0x020202020202020a)),
            vcreate_u8(UINT64_C(0x030303030303030b))
        )
    );
    int64x2_t   lq = vreinterpretq_s64_u8(l);
    uint32x2_t  lo = vqmovun_s64(lq);
    int64x2_t   rq = vreinterpretq_s64_u8(r);
    uint32x2_t  hi = vqmovun_s64(rq);
    return vcombine_u32(lo, hi);
}

INLINE(Vqwu,VWBC_CVWZ) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVWZ(VWBC_ASBI(x));
#else
    return  VWBU_CVWZ(VWBC_ASBU(x));
#endif
}

INLINE(Vdwu,VWHU_CVWZ) (Vwhu x) {return VWHU_CVWU(x);}
INLINE(Vdwu,VWHI_CVWZ) (Vwhi x)
{
    float32x2_t dwf = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   dhi = vreinterpret_s16_f32(dwf);
    int32x4_t   qwi = vmovl_s16(dhi);
    int64x2_t   qdi = vmovl_s32(vget_low_s32(qwi));
    return  vqmovun_s64(qdi);
}
INLINE(Vdwu,VWHF_CVWZ) (Vwhf x) {return VWHF_CVWU(x);}

INLINE(Vwwu,VWWU_CVWZ) (Vwwu x) {return x;}
INLINE(Vwwu,VWWI_CVWZ) (Vwwi v)
{
#define     VWWI_CVWZ(V)    WWU_ASTV(WWI_CVWZ(VWWI_ASTM(V)))
    return  VWWI_CVWZ(v);
}

INLINE(Vwwu,VWWF_CVWZ) (Vwwf v)
{
#define     VWWF_CVWZ(V)    WWU_ASTV(WWF_CVWZ(VWWF_ASTM(V)))
    return  VWWF_CVWZ(v);
}

INLINE(Vqwu,VDHU_CVWZ) (Vdhu x) {return VDHU_CVWU(x);}
INLINE(Vqwu,VDHI_CVWZ) (Vdhi x)
{
    int32x4_t m = vmovl_s16(x);
    int64x2_t l = vmovl_s32(vget_low_s32(m));
    int64x2_t r = vmovl_s32(vget_high_s32(m));
    return  vcombine_u32(
        vqmovun_s64(l),
        vqmovun_s64(r)
    );
}

INLINE(Vqwu,VDHF_CVWZ) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmovl_u16(vcvt_u16_f16(x));
#else
    return  vcvtq_u32_f32(vcvt_f32_f16(x));
#endif
}

INLINE(Vdwu,VDWU_CVWZ) (Vdwu x) {return x;}
INLINE(Vdwu,VDWI_CVWZ) (Vdwi x) {return vqmovun_s64(vmovl_s32(x));}
INLINE(Vdwu,VDWF_CVWZ) (Vdwf x) {return vcvt_u32_f32(x);}

INLINE(Vwwu,VDDU_CVWZ) (Vddu v)
{
#define     DDU_CVWZ(V)     \
vget_lane_f32(              \
    vreinterpret_f32_u32(   \
        vqmovn_u64(         \
            vcombine_u64(   \
                V,          \
                VDDU_VOID   \
            )               \
        )                   \
    ),                      \
    V2_K0                   \
)

#define     VDDU_CVWZ(V)    WWU_ASTV(DDU_CVWZ(V))
    return  VDDU_CVWZ(v);
}

INLINE(Vwwu,VDDI_CVWZ) (Vddi v)
{
#define     DDI_CVWZ(M)     \
vget_lane_f32(              \
    vreinterpret_f32_u32(   \
        vqmovun_s64(        \
            vcombine_s64(   \
                M,          \
                VDDI_VOID   \
            )               \
        )                   \
    ),                      \
    V2_K0                   \
)

#define     VDDI_CVWZ(V)    WWU_ASTV(DDI_CVWZ(V))
    return  VDDI_CVWZ(v);
}

INLINE(Vwwu,VDDF_CVWZ) (Vddf x)
{
    uint64x1_t  m = vcvt_u64_f64(x);
    return UINT_ASTV(vqmovnd_u64(vget_lane_u64(m, 0)));
}
INLINE(Vqwu,VQWU_CVWZ) (Vqwu x) {return x;}
INLINE(Vqwu,VQWI_CVWZ) (Vqwi x)
{
    return vcombine_s32(
        vqmovun_s64(vmovl_s32(vget_low_s32(x))),
        vqmovun_s64(vmovl_s32(vget_high_s32(x)))
    );
}
INLINE(Vqwu,VQWF_CVWZ) (Vqwf x) {return vcvtq_u32_f32(x);}

INLINE(Vdwu,VQDU_CVWZ) (Vqdu x) {return vqmovn_u64(x);}
INLINE(Vdwu,VQDI_CVWZ) (Vqdi x) {return vqmovun_s64(x);}
INLINE(Vdwu,VQDF_CVWZ) (Vqdf x) {return vqmovn_u64(vcvtq_u64_f64(x));}

#if _LEAVE_ARM_CVWZ
}
#endif

#if _ENTER_ARM_CVWI
{
#endif

#define     WHU_CVWI(X)         \
vreinterpret_s32_u16(           \
    vzip1_u16(                  \
        vreinterpret_u16_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        ),                      \
        vdup_n_u16(0)           \
    )                           \
)


#define     WHI_CVWI(X)         \
vget_low_s32(                   \
    vmovl_s16(                  \
        vreinterpret_s16_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        )                       \
    )                           \
)

INLINE(int32_t,   BOOL_CVWI)     (_Bool x) {return x;}
INLINE(int32_t,  UCHAR_CVWI)     (uchar x) {return x;}
INLINE(int32_t,  SCHAR_CVWI)     (schar x) {return x;}
INLINE(int32_t,   CHAR_CVWI)      (char x) {return x;}
INLINE(int32_t,  USHRT_CVWI)    (ushort x) {return x;}
INLINE(int32_t,   SHRT_CVWI)     (short x) {return x;}
INLINE(int32_t,    INT_CVWI)       (int x) {return x;}
INLINE(int32_t,   UINT_CVWI)      (uint x) {return x;}
INLINE(int32_t,  ULONG_CVWI)     (ulong x) {return x;}
INLINE(int32_t,   LONG_CVWI)      (long x) {return x;}
INLINE(int32_t, ULLONG_CVWI)    (ullong x) {return x;}
INLINE(int32_t,  LLONG_CVWI)     (llong x) {return x;}
INLINE(int32_t,  FLT16_CVWI)   (flt16_t x) {return x;}
INLINE(int32_t,    FLT_CVWI)     (float x) {return x;}
INLINE(int32_t,    DBL_CVWI)    (double x) {return x;}
#if QUAD_NLLONG == 2
INLINE(int32_t,cvwiqu) (QUAD_UTYPE x) {return x;}
INLINE(int32_t,cvwiqi) (QUAD_ITYPE x) {return x;}
INLINE(int32_t,cvwiqf) (QUAD_FTYPE x) {return x;}
#endif

INLINE(int32_t,cvwiac) (void volatile const *x)
{
    return  (intptr_t) x;
}

INLINE(Vqwi,VWBU_CVWI) (Vwbu x)
{
/*  TODO: test the perf difference between VWBU_CVWU and
    VWBU_CVWI.
*/
    float32x2_t dwf = vdup_n_f32(VWBU_ASTM(x));
    uint8x8_t   dbu = vreinterpret_u8_f32(dwf);
    uint16x8_t  qhu = vmovl_u8(dbu);
    uint16x4_t  dhu = vget_low_u16(qhu);
    uint32x4_t  qwu = vmovl_u16(dhu);
    return  vreinterpretq_s32_u32(qwu);
}

INLINE(Vqwi,VWBI_CVWI) (Vwbi x)
{
    float32x2_t dwf = vdup_n_f32(VWBI_ASTM(x));
    int8x8_t    dbi = vreinterpret_s8_f32(dwf);
    int16x8_t   qhi = vmovl_s8(dbi);
    return  vmovl_s16(vget_low_s16(qhi));
}

INLINE(Vqwi,VWBC_CVWI) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVWI(VWBC_ASBI(x));
#else
    return  VWBU_CVWI(VWBC_ASBU(x));
#endif
}


INLINE(Vdwi,VWHU_CVWI) (Vwhu x)
{
    float32x2_t dwf = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  dhu = vreinterpret_u16_f32(dwf);
    uint32x4_t  qwu = vmovl_u16(dhu);
    return  vreinterpret_s32_u32(vget_low_u32(qwu));
}

INLINE(Vdwi,VWHI_CVWI) (Vwhi x)
{
    float32x2_t dwf = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   dhi = vreinterpret_s16_f32(dwf);
    int32x4_t   qwi = vmovl_s16(dhi);
    return  vget_low_u32(qwi);
}

INLINE(Vdwi,VWHF_CVWI) (Vwhf x)
{
    float32x2_t dwf = vdup_n_f32(VWHF_ASTM(x));
    float16x4_t dhf = vreinterpret_f16_f32(dwf);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_low_s32(vmovl_s16(vcvt_s16_f16(dhf)));
#else
    return  vget_low_s32(vcvtq_s32_f32(vcvt_f32_f16(dhf)));
#endif
}


INLINE(Vwwi,VWWU_CVWI) (Vwwu x) {return VWWU_ASTI(x);}
INLINE(Vwwi,VWWI_CVWI) (Vwwi x) {return x;}
INLINE(Vwwi,VWWF_CVWI) (Vwwf x)
{
    return  INT_ASTV(vcvts_s32_f32(VWWF_ASTM(x)));
}

INLINE(Vqwi,VDHU_CVWI) (Vdhu x) {return VQWU_ASTI(vmovl_u16(x));}
INLINE(Vqwi,VDHI_CVWI) (Vdhi x) {return           vmovl_s16(x);}
INLINE(Vqwi,VDHF_CVWI) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmovl_s16(vcvt_s16_f16(x));
#else
    return  vcvtq_u32_f32(vcvt_f32_f16(x));
#endif
}

INLINE(Vdwi,VDWU_CVWI) (Vdwu x) {return vreinterpret_s32_u32(x);}
INLINE(Vdwi,VDWI_CVWI) (Vdwi x) {return x;}
INLINE(Vdwi,VDWF_CVWI) (Vdwf x) {return vcvt_s32_f32(x);}

INLINE(Vwwi,VDDU_CVWI) (Vddu x)
{
    return  INT_ASTV(vget_lane_u64(x, 0));
}

INLINE(Vwwi,VDDI_CVWI) (Vddi x)
{
    return  INT_ASTV(vget_lane_s64(x, 0));
}

INLINE(Vwwi,VDDF_CVWI) (Vddf x)
{
    return  INT_ASTV(vget_lane_f64(x, 0));
}

INLINE(Vqwi,VQWU_CVWI) (Vqwu x) {return vreinterpretq_s32_u32(x);}
INLINE(Vqwi,VQWI_CVWI) (Vqwi x) {return x;}
INLINE(Vqwi,VQWF_CVWI) (Vqwf x) {return vcvtq_u32_f32(x);}

INLINE(Vdwi,VQDU_CVWI) (Vqdu x) {return vreinterpret_s32_u32(vmovn_u64(x));}
INLINE(Vdwi,VQDI_CVWI) (Vqdi x) {return                      vmovn_s64(x);}
INLINE(Vdwi,VQDF_CVWI) (Vqdu x) {return vmovn_s64(vcvtq_s64_f64(x));}


#if _LEAVE_ARM_CVWI
}
#endif

#if _ENTER_ARM_CVWS
{
#endif

INLINE(int32_t,  BOOL_CVWS)   (_Bool x) {return x;}
INLINE(int32_t, UCHAR_CVWS)   (uchar x) {return x;}
INLINE(int32_t, SCHAR_CVWS)   (schar x) {return x;}
INLINE(int32_t,  CHAR_CVWS)    (char x) {return x;}
INLINE(int32_t, USHRT_CVWS)  (ushort x) {return x;}
INLINE(int32_t,  SHRT_CVWS)   (short x) {return x;}
INLINE(int32_t,  UINT_CVWS)    (uint x)
{
    uint64_t m = vtstd_u64((UINT32_MAX-INT32_MAX), x)>>33;
    return  (x&INT32_MAX)|m;
}
INLINE(int32_t,   INT_CVWS)     (int x) {return x;}
INLINE(int32_t, ULONG_CVWS)   (ulong x) {return UINT_CVWS(x);}
INLINE(int32_t,  LONG_CVWS)    (long x)
{
#if DWRD_NLONG == 2
    return  x;
#else
    return  vqmovnd_s64(x);
#endif
}

INLINE(int32_t,ULLONG_CVWS)  (ullong x)
{
    uint64_t m = vtstd_u64((UINT64_MAX-INT32_MAX), x)>>33;
    return  (x&INT32_MAX)|m;
}

INLINE(int32_t, LLONG_CVWS)   (llong x)
{
#if QUAD_NLLONG == 2
    return  vqmovnd_s64(x);
#else
// TODO
#endif
}

INLINE(int32_t, FLT16_CVWS)  (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvth_s16_f16(x);
#else
    return  vcvts_s32_f32(x);
#endif
}

INLINE(int32_t,   FLT_CVWS) (float x) {return vcvts_s32_f32(x);}
INLINE(int32_t,   DBL_CVWS) (double x)
{
    return vqmovnd_s64(vcvtd_s64_f64(x));
}

INLINE(int32_t,cvwsqu) (QUAD_UTYPE x)
{
    return (x >= INT32_MAX) ? INT32_MAX : x;
}

INLINE(int32_t,cvwsqi) (QUAD_ITYPE x)
{
    if (x >= INT32_MAX) return INT32_MAX;
    if (x <= INT32_MIN) return INT32_MIN;
    return x;
}

INLINE(int32_t,cvwsqf) (QUAD_FTYPE x)
{
    if (x >= INT32_MAX) return INT32_MAX;
    if (x <= INT32_MIN) return INT32_MIN;
    return x;
}


INLINE(Vqwi,VWBU_CVWS) (Vwbu x) {return VWBU_CVWI(x);}
INLINE(Vqwi,VWBI_CVWS) (Vwbi x) {return VWBI_CVWI(x);}
INLINE(Vqwi,VWBC_CVWS) (Vwbc x) {return VWBC_CVWI(x);}

INLINE(Vdwi,VWHU_CVWS) (Vwhu x) {return VWHU_CVWI(x);}
INLINE(Vdwi,VWHI_CVWS) (Vwhi x) {return VWHI_CVWI(x);}
INLINE(Vdwi,VWHF_CVWS) (Vwhf x) {return VWHF_CVWI(x);}

INLINE(Vwwi,VWWU_CVWS) (Vwwu x)
{
    float32x2_t dwf = vdup_n_f32(VWWU_ASTM(x));
    uint32x2_t  dwu = vreinterpret_u32_f32(dwf);
    uint32x2_t  tmp = vtst_u32(vdup_n_u32(0x80000000u), dwu);
    tmp = vorr_u32(dwu, tmp);
    int32x2_t   dwi = vand_s32(
        vdup_n_s32(INT32_MAX),
        vreinterpret_s32_u32(tmp)
    );
    dwf = vreinterpret_f32_s32(dwi);
    return  WWI_ASTV(vget_lane_f32(dwf, V2_K0));
}

INLINE(Vwwi,VWWI_CVWS) (Vwwi x) {return x;}
INLINE(Vwwi,VWWF_CVWS) (Vwwf x) {return VWWF_CVWI(x);}

INLINE(Vqwi,VDHU_CVWS) (Vdhu x) {return VDHU_CVWI(x);}
INLINE(Vqwi,VDHI_CVWS) (Vdhi x) {return VDHU_CVWI(x);}
INLINE(Vqwi,VDHF_CVWS) (Vdhf x) {return VDHF_CVWI(x);}


INLINE(Vdwi,VDWU_CVWS) (Vdwu x)
{
    uint32x2_t  tmp = vtst_u32(vdup_n_u32(0x80000000u), x);
    tmp = vorr_u32(x, tmp);
    return vand_s32(
        vdup_n_s32(INT32_MAX),
        vreinterpret_s32_u32(tmp)
    );
}

INLINE(Vdwi,VDWI_CVWS) (Vdwi x) {return x;}
INLINE(Vdwi,VDWF_CVWS) (Vdwf x) {return vcvt_s32_f32(x);}

INLINE(Vwwi,VDDU_CVWS) (Vddu x)
{
    uint64x1_t m = vtst_u64(
        vdup_n_u64(UINT64_MAX-INT32_MAX),
        x
    );
    x = vorr_u64(x, m);
    x = vand_u64(x, vdup_n_u64(INT32_MAX));
    return  INT_ASTV(vget_lane_u64(x, 0));
}

INLINE(Vwwi,VDDI_CVWS) (Vddi x)
{
    return  INT_ASTV(
        vqmovnd_s64(vget_lane_s64(x, 0))
    );
}

INLINE(Vwwi,VDDF_CVWS) (Vddf x)
{
    double  f = vget_lane_f64(x, 0);
    int64_t i = vcvtd_s64_f64(f);
    return  INT_ASTV(vqmovnd_s64(i));
}


INLINE(Vqwi,VQWU_CVWS) (Vqwu x)
{
    uint32x4_t  tmp = vtstq_u32(vdupq_n_u32(0x80000000u), x);
    tmp = vorrq_u32(x, tmp);
    return vandq_s32(
        vdupq_n_s32(INT32_MAX),
        vreinterpretq_s32_u32(tmp)
    );
}

INLINE(Vqwi,VQWI_CVWS) (Vqwi x) {return x;}
INLINE(Vqwi,VQWF_CVWS) (Vqwf x) {return vcvtq_s32_f32(x);}

INLINE(Vdwi,VQDU_CVWS) (Vqdu x) {return vqmovn_u64(x);}
INLINE(Vdwi,VQDI_CVWS) (Vqdi x) {return vqmovn_s64(x);}
INLINE(Vdwi,VQDF_CVWS) (Vqdf x) {return vqmovn_s64(vcvtq_s64_f64(x));}


#if _LEAVE_ARM_CVWS
}
#endif

#if _ENTER_ARM_CVWF
{
#endif
/*  Convert each operand element to an IEEE 754 single
    precision floating point value.
*/
INLINE(float,  BOOL_CVWF)      (_Bool x) {return x;}
INLINE(float, UCHAR_CVWF)      (uchar x) {return x;}
INLINE(float, SCHAR_CVWF)      (schar x) {return x;}
INLINE(float,  CHAR_CVWF)       (char x) {return x;}
INLINE(float, USHRT_CVWF)     (ushort x) {return x;}
INLINE(float,  SHRT_CVWF)      (short x) {return x;}
INLINE(float,  UINT_CVWF)       (uint x) {return x;}
INLINE(float,   INT_CVWF)        (int x) {return x;}
INLINE(float, ULONG_CVWF)      (ulong x) {return x;}
INLINE(float,  LONG_CVWF)       (long x) {return x;}
INLINE(float,ULLONG_CVWF)     (ullong x) {return x;}
INLINE(float, LLONG_CVWF)      (llong x) {return x;}
//INLINE(float, FLT16_CVWF)    (flt16_t x) {return x;}
INLINE(float,   FLT_CVWF)      (float x) {return x;}
INLINE(float,   DBL_CVWF)     (double x) {return x;}
#if QUAD_NLLONG == 2
INLINE(float,cvwfqu) (QUAD_UTYPE x) {return x;}
INLINE(float,cvwfqi) (QUAD_ITYPE x) {return x;}
INLINE(float,cvwfqf) (QUAD_FTYPE x) {return x;}
#endif

INLINE(Vqwf,VWBU_CVWF) (Vwbu x)
{
    float32x2_t m = vdup_n_f32(VWBU_ASTM(x));
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint8x16_t  q = vcombine_u8(
        vtbl1_u8(d, vcreate_u8(0xffffff01ffffff00ull)),
        vtbl1_u8(d, vcreate_u8(0xffffff03ffffff02ull))
    );
    uint32x4_t  w = vreinterpretq_u32_u8(q);
    return  vcvtq_f32_u32(w);
}

INLINE(Vqwf,VWBI_CVWF) (Vwbi x)
{
    int32x4_t   m = VWBI_CVWI(x);
    return vcvtq_f32_s32(m);
}

INLINE(Vqwf,VWBC_CVWF) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_CVWF(VWBC_ASBI(x));
#else
    return  VWBU_CVWF(VWBC_ASBU(x));
#endif
}


INLINE(Vdwf,VWHU_CVWF) (Vwhu x)
{
    float32x2_t m = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  u = vreinterpret_u16_f32(m);
    uint32x4_t  q = vmovl_u16(u);
    uint32x2_t  r = vget_low_u32(q);
    return  vcvt_f32_u32(r);
}

INLINE(Vdwf,VWHI_CVWF) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   i = vreinterpret_s16_f32(m);
    int32x4_t   q = vmovl_s16(i);
    int32x2_t   r = vget_low_s32(q);
    return  vcvt_f32_s32(r);
}

INLINE(Vdwf,VWHF_CVWF) (Vwhf x)
{
    float32x2_t m = vdup_n_f32(VWHF_ASTM(x));
    float16x4_t f = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(f);
    return  vget_low_f32(q);
}

INLINE(Vwwf,VWWU_CVWF) (Vwwu x)
{
    float32x2_t m = vdup_n_f32(VWWU_ASTM(x));
    uint32x2_t  z = vreinterpret_u32_f32(m);
    m = vcvt_f32_u32(z);
    return  WWF_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vwwf,VWWI_CVWF) (Vwwi x)
{
    float32x2_t m = vdup_n_f32(VWWI_ASTM(x));
    int32x2_t   z = vreinterpret_s32_f32(m);
    m = vcvt_f32_s32(z);
    return  WWF_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vwwf,VWWF_CVWF) (Vwwf x) {return x;}

INLINE(Vqwf,VDHU_CVWF) (Vdhu x) {return vcvtq_f32_u32(vmovl_u16(x));}
INLINE(Vqwf,VDHI_CVWF) (Vdhi x) {return vcvtq_f32_s32(vmovl_s16(x));}
INLINE(Vqwf,VDHF_CVWF) (Vdhf x) {return  vcvt_f32_f16(x);}

INLINE(Vdwf,VDWU_CVWF) (Vdwu x) {return vcvt_f32_u32(x);}
INLINE(Vdwf,VDWI_CVWF) (Vdwi x) {return vcvt_f32_s32(x);}
INLINE(Vdwf,VDWF_CVWF) (Vdwf x) {return x;}

INLINE(Vwwf,VDDU_CVWF) (Vddu x) {return WWF_ASTV(vget_lane_u64(x, 0));}
INLINE(Vwwf,VDDI_CVWF) (Vddi x) {return WWF_ASTV(vget_lane_s64(x, 0));}
INLINE(Vwwf,VDDF_CVWF) (Vddf x) {return WWF_ASTV(vget_lane_f64(x, 0));}

INLINE(Vqwf,VQWU_CVWF) (Vqwu x) {return vcvtq_f32_u32(x);}
INLINE(Vqwf,VQWI_CVWF) (Vqwi x) {return vcvtq_f32_u32(x);}
INLINE(Vqwf,VQWF_CVWF) (Vqwf x) {return x;}

INLINE(Vdwf,VQDU_CVWF) (Vqdu x) {return vcvt_f32_f64(vcvtq_f64_u64(x));}
INLINE(Vdwf,VQDI_CVWF) (Vqdi x) {return vcvt_f32_f64(vcvtq_f64_s64(x));}
INLINE(Vdwf,VQDF_CVWF) (Vqdf x) {return vcvt_f32_f64(x);}

#if _LEAVE_ARM_CVWF
}
#endif

#if _ENTER_ARM_CVDU
{
#endif

#define     WWU_CVDU(X)         \
vreinterpret_u64_u32(           \
    vzip1_u32(                  \
        vreinterpret_u32_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        ),                      \
        vdup_n_u32(0)           \
    )                           \
)

INLINE(uint64_t,   BOOL_CVDU)     (_Bool x) {return x;}
INLINE(uint64_t,  UCHAR_CVDU)     (uchar x) {return x;}
INLINE(uint64_t,  SCHAR_CVDU)     (schar x) {return x;}
INLINE(uint64_t,   CHAR_CVDU)      (char x) {return x;}
INLINE(uint64_t,  USHRT_CVDU)    (ushort x) {return x;}
INLINE(uint64_t,   SHRT_CVDU)     (short x) {return x;}
INLINE(uint64_t,    INT_CVDU)       (int x) {return x;}
INLINE(uint64_t,   UINT_CVDU)      (uint x) {return x;}
INLINE(uint64_t,  ULONG_CVDU)     (ulong x) {return x;}
INLINE(uint64_t,   LONG_CVDU)      (long x) {return x;}
INLINE(uint64_t, ULLONG_CVDU)    (ullong x) {return x;}
INLINE(uint64_t,  LLONG_CVDU)     (llong x) {return x;}
INLINE(uint64_t,  FLT16_CVDU)   (flt16_t x) {return x;}
INLINE(uint64_t,    FLT_CVDU)     (float x) {return x;}
INLINE(uint64_t,    DBL_CVDU)    (double x) {return x;}
#if QUAD_NLLONG == 2
INLINE(uint64_t,cvduqu) (QUAD_UTYPE x) {return x;}
INLINE(uint64_t,cvduqi) (QUAD_ITYPE x) {return x;}
INLINE(uint64_t,cvduqf) (QUAD_FTYPE x) {return x;}
#endif

INLINE(uint64_t,cvduac) (void volatile const *x)
{
    return  (uintptr_t) x;
}

INLINE(Vqdu,VWHU_CVDU) (Vwhu x)
{
    float32x2_t m = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  h = vreinterpret_u16_f32(m);
    uint32x4_t  w = vmovl_u16(h);
    uint32x2_t  l = vget_low_u32(w);
    return  vmovl_u32(l);
}

INLINE(Vqdu,VWHI_CVDU) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   h = vreinterpret_s16_f32(m);
    int32x4_t   w = vmovl_s16(h);
    int32x2_t   l = vget_low_s32(w);
    int64x2_t   q = vmovl_s32(l);
    return  vreinterpretq_u64_s64(q);
}

INLINE(Vqdu,VWHF_CVDU) (Vwhf x)
{
    float32x2_t m = vdup_n_f32(VWHF_ASTM(x));
    float32x4_t w = vcvt_f32_f16(m);
    int32x4_t   s = vcvtq_s32_f32(w);
    int32x2_t   l = vget_low_s32(s);
    int64x2_t   q = vmovl_s32(l);
    return  vreinterpretq_u64_s64(q);
}


INLINE(Vddu,VWWU_CVDU) (Vwwu x)
{
    return UINT64_ASTV(VWWU_ASTV(x));
}

INLINE(Vddu,VWWI_CVDU) (Vwwi x)
{
    return UINT64_ASTV(VWWI_ASTV(x));
}

INLINE(Vddu,VWWF_CVDU) (Vwwf x)
{
    return UINT64_ASTV(VWWF_ASTM(x));
}


INLINE(Vqdu,VDWU_CVDU) (Vdwu x) {return vmovl_u32(x);}
INLINE(Vqdu,VDWI_CVDU) (Vdwi x) {return VQDI_ASTU(vmovl_s32(x));}
INLINE(Vqdu,VDWF_CVDU) (Vdwf x) {return vcvtq_u64_f64(vcvt_f64_f32(x));}

INLINE(Vddu,VDDU_CVDU) (Vddu x) {return x;}
INLINE(Vddu,VDDI_CVDU) (Vddi x) {return vreinterpret_u64_s64(x);}
INLINE(Vddu,VDDF_CVDU) (Vddf x) {return vcvt_u64_f64(x);}

INLINE(Vqdu,VQDU_CVDU) (Vqdu x) {return x;}
INLINE(Vqdu,VQDI_CVDU) (Vqdi x) {return vreinterpretq_u64_s64(x);}
INLINE(Vqdu,VQDF_CVDU) (Vqdf x) {return vcvtq_u64_f64(x);}


#if _LEAVE_ARM_CVDU
}
#endif

#if _ENTER_ARM_CVDI
{
#endif

#define     WWU_CVDI(X)         \
vreinterpret_s64_u32(           \
    vzip1_u32(                  \
        vreinterpret_u32_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        ),                      \
        vdup_n_u32(0)           \
    )                           \
)

#define     WWI_CVDI(X)         \
vget_low_s64(                   \
    vmovl_s32(                  \
        vreinterpret_s32_f32(   \
            vdup_n_f32(         \
                _Generic(       \
                    X,          \
                    float:X     \
                )               \
            )                   \
        )                       \
    )                           \
)

INLINE(int64_t,   BOOL_CVDI)   (_Bool x) {return x;}
INLINE(int64_t,  UCHAR_CVDI)   (uchar x) {return x;}
INLINE(int64_t,  SCHAR_CVDI)   (schar x) {return x;}
INLINE(int64_t,   CHAR_CVDI)    (char x) {return x;}
INLINE(int64_t,  USHRT_CVDI)  (ushort x) {return x;}
INLINE(int64_t,   SHRT_CVDI)   (short x) {return x;}
INLINE(int64_t,    INT_CVDI)     (int x) {return x;}
INLINE(int64_t,   UINT_CVDI)    (uint x) {return x;}
INLINE(int64_t,  ULONG_CVDI)   (ulong x) {return x;}
INLINE(int64_t,   LONG_CVDI)    (long x) {return x;}
INLINE(int64_t, ULLONG_CVDI)  (ullong x) {return x;}
INLINE(int64_t,  LLONG_CVDI)   (llong x) {return x;}
INLINE(int64_t,  FLT16_CVDI) (flt16_t x) {return x;}
INLINE(int64_t,    FLT_CVDI)   (float x) {return x;}
INLINE(int64_t,    DBL_CVDI)  (double x) {return x;}
#if QUAD_NLLONG == 2
INLINE(int64_t,cvdiqu) (QUAD_UTYPE x) {return x;}
INLINE(int64_t,cvdiqi) (QUAD_ITYPE x) {return x;}
INLINE(int64_t,cvdiqf) (QUAD_FTYPE x) {return x;}
#endif

INLINE(int64_t,cvdiac) (void volatile const *x)
{
    return  (intptr_t) x;
}


INLINE(Vqdi,VWHU_CVDI) (Vwhu x)
{
    float32x2_t m = vdup_n_f32(VWHU_ASTM(x));
    uint16x4_t  h = vreinterpret_u16_f32(m);
    uint32x4_t  w = vmovl_u16(h);
    uint32x2_t  l = vget_low_u32(w);
    int32x2_t   i = vreinterpret_s32_u32(l);
    return  vmovl_s32(l);
}

INLINE(Vqdi,VWHI_CVDI) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   h = vreinterpret_s16_f32(m);
    int32x4_t   w = vmovl_s16(h);
    int32x2_t   l = vget_low_s32(w);
    int64x2_t   q = vmovl_s32(l);
    return  q;
}

INLINE(Vqdi,VWHF_CVDI) (Vwhf x)
{
    float32x2_t m = vdup_n_f32(VWHF_ASTM(x));
    float32x4_t w = vcvt_f32_f16(m);
    int32x4_t   s = vcvtq_s32_f32(w);
    int32x2_t   l = vget_low_s32(s);
    return vmovl_s32(l);
}


INLINE(Vddi,VWWU_CVDI) (Vwwu x)
{
    return INT64_ASTV(VWWU_ASTV(x));
}

INLINE(Vddi,VWWI_CVDI) (Vwwi x)
{
    return INT64_ASTV(VWWI_ASTV(x));
}

INLINE(Vddi,VWWF_CVDI) (Vwwf x)
{
    return INT64_ASTV(VWWF_ASTM(x));
}


INLINE(Vqdi,VDWU_CVDI) (Vdwu x) {return vreinterpretq_s64_u64(vmovl_u32(x));}
INLINE(Vqdi,VDWI_CVDI) (Vdwi x) {return vmovl_s32(x);}
INLINE(Vqdi,VDWF_CVDI) (Vdwf x) {return vcvtq_s64_f64(vcvt_f64_f32(x));}

INLINE(Vddi,VDDU_CVDI) (Vddu x) {return vreinterpret_s64_u64(x);}
INLINE(Vddi,VDDI_CVDI) (Vddi x) {return x;}
INLINE(Vddi,VDDF_CVDI) (Vddf x) {return vcvt_s64_f64(x);}

INLINE(Vqdi,VQDU_CVDI) (Vqdu x) {return vreinterpretq_s64_u64(x);}
INLINE(Vqdi,VQDI_CVDI) (Vqdi x) {return x;}
INLINE(Vqdi,VQDF_CVDI) (Vqdf x) {return vcvtq_s64_f64(x);}

#if _LEAVE_ARM_CVDI
}
#endif

#if _ENTER_ARM_CVDZ
{
#endif

INLINE(uint64_t,  BOOL_CVDZ)   (_Bool x) {return x;}
INLINE(uint64_t, UCHAR_CVDZ)   (uchar x) {return x;}
INLINE(uint64_t,  CHAR_CVDZ)    (char x)
{
#if CHAR_MIN
    return  vqmovunh_s32(x);
#else
    return  x;
#endif
}

INLINE(uint64_t, SCHAR_CVDZ)   (schar x) {return vqmovunh_s16(x);}
INLINE(uint64_t, USHRT_CVDZ)  (ushort x) {return x;}
INLINE(uint64_t,  SHRT_CVDZ)   (short x) {return vqmovuns_s32(x);}
INLINE(uint64_t,  UINT_CVDZ)    (uint x) {return x;}
INLINE(uint64_t,   INT_CVDZ)     (int x) {return vqmovund_s64(x);}
INLINE(uint64_t, ULONG_CVDZ)   (ulong x) {return x;}
INLINE(uint64_t,  LONG_CVDZ)    (long x)
{
#if DWRD_NLONG == 2
    return  vqmovnd_s64(x);
#else
    if (x >= INT64_MAX) return INT64_MAX;
    if (x <= 0) return 0;
    return  x;
#endif
}

INLINE(uint64_t,ULLONG_CVDZ)  (ullong x)
{
#if QUAD_NLLONG == 2
    return  x;
#else
    return  (x >= UINT64_MAX) ? UINT64_MAX : x;
#endif
}

INLINE(uint64_t, LLONG_CVDZ)   (llong x)
{
    if (x >= INT64_MAX) return INT64_MAX;
    if (x <= 0) return 0;
    return  x;
}

#if QUAD_NLLONG == 2

INLINE(uint64_t,cvdzqu)   (QUAD_UTYPE x)
{
    return x > UINT64_MAX ? UINT64_MAX : x;
}

INLINE(uint64_t,cvdzqi)   (QUAD_ITYPE x)
{
    if (x >= UINT64_MAX) return UINT64_MAX;
    if (x <= 0) return 0;
    return x;
}

INLINE(uint64_t,cvdzqf)   (QUAD_FTYPE x)
{
    if (x >= UINT64_MAX) return UINT64_MAX;
    if (x <= 0) return 0;
    return x;
}

#endif

INLINE(uint64_t,FLT16_CVDZ) (flt16_t x) {return  vcvtd_u64_f64(x);}
INLINE(uint64_t,  FLT_CVDZ)   (float x) {return  vcvts_u32_f32(x);}
INLINE(uint64_t,  DBL_CVDZ)  (double x) {return  vcvtd_u64_f64(x);}

INLINE(Vqdu,VWHU_CVDZ) (Vwhu x) {return VWHU_CVDU(x);}
INLINE(Vqdu,VWHI_CVDZ) (Vwhi x)
{
    float32x2_t dwf = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   dhi = vreinterpret_s16_f32(dwf);
    int32x4_t   qwi = vmovl_s16(dhi);
    uint16x4_t  dhu = vqmovun_s32(qwi);
    uint8x8_t   dbu = vreinterpret_u8_u16(dhu);
    uint8x8_t   lhs = vtbl1_u8(
        dbu,
        vcreate_u8(0xffffffffffff0100ULL)
    );
    uint8x8_t   rhs = vtbl1_u8(
        dbu,
        vcreate_u8(0xffffffffffff0302ULL)
    );
    uint8x16_t  qbu = vcombine_u8(lhs, rhs);
    return  vreinterpretq_u64_u8(qbu);
}

INLINE(Vqdu,VWHF_CVDZ) (Vwhf x)
{
    // TODO
    return  VQDU_VOID;
}

INLINE(Vddu,VWWU_CVDZ) (Vwwu x) {return VWWU_CVDU(x);}
INLINE(Vddu,VWWI_CVDZ) (Vwwi x)
{
#define     VWWI_CVDZ(X) UINT64_ASTV(vqmovund_s64(VWWI_ASTV(X)))
    return  VWWI_CVDZ(x);
}

INLINE(Vddu,VWWF_CVDZ) (Vwwf x)
{
#define     VWWF_CVDZ(X) UINT64_ASTV(vcvtd_u64_f64(VWWF_ASTM(X)))
    return  VWWF_CVDZ(x);
}

INLINE(Vqdu,VDWU_CVDZ) (Vdwu x) {return VDWU_CVDU(x);}
INLINE(Vqdu,VDWI_CVDZ) (Vdwi x)
{
#define     VDWI_CVDZ(X) vmovl_u32(vqmovun_s64(vmovl_s32(X)))
    return  VDWI_CVDZ(x);
}

INLINE(Vqdu,VDWF_CVDZ) (Vdwf x)
{
#define     VDWF_CVDZ(X) vcvtq_u64_f64(vcvt_f64_f32(X))
    return  VDWF_CVDZ(x);
}

INLINE(Vddu,VDDU_CVDZ) (Vddu x) {return x;}
INLINE(Vddu,VDDI_CVDZ) (Vddi x)
{
    uint64x1_t m = vtst_u64(
        vreinterpret_u64_s64(x),
        vdup_n_u64(0x8000000000000000ULL)
    );
    m = vreinterpret_u64_u32(
        vmvn_u32(vreinterpret_u32_u64(m))
    );
    m = vshr_n_u64(m, 1);
    return  vand_u64(m, vreinterpret_u64_s64(x));
}

INLINE(Vddu,VDDF_CVDZ) (Vddf x) {return vcvt_u64_f64(x);}

INLINE(Vqdu,VQDU_CVDZ) (Vqdu x) {return x;}
INLINE(Vqdu,VQDI_CVDZ) (Vqdi x)
{
    uint64x2_t m = vtstq_u64(
        vreinterpretq_u64_s64(x),
        vdupq_n_u64(0x8000000000000000ULL)
    );
    m = vreinterpretq_u64_u32(
        vmvnq_u32(vreinterpretq_u32_u64(m))
    );
    m = vshrq_n_u64(m, 1);
    return  vandq_u64(m, vreinterpretq_u64_s64(x));
}

INLINE(Vqdu,VQDF_CVDZ) (Vqdf x) {return vcvtq_u64_f64(x);}

#if _LEAVE_ARM_CVDZ
}
#endif

#if _ENTER_ARM_CVDS
{
#endif

INLINE(int64_t,   BOOL_CVDS)   (_Bool x) {return x;}
INLINE(int64_t,  UCHAR_CVDS)   (uchar x) {return x;}
INLINE(int64_t,  SCHAR_CVDS)   (schar x) {return x;}
INLINE(int64_t,   CHAR_CVDS)    (char x) {return x;}
INLINE(int64_t,  USHRT_CVDS)  (ushort x) {return x;}
INLINE(int64_t,   SHRT_CVDS)   (short x) {return x;}
INLINE(int64_t,   UINT_CVDS)    (uint x) {return x;}
INLINE(int64_t,    INT_CVDS)     (int x) {return x;}

#if DWRD_NLONG == 2

INLINE(int64_t,  ULONG_CVDS)   (ulong x) {return x;}
INLINE(int64_t,   LONG_CVDS)    (long x) {return x;}

#else

INLINE(int64_t,ULONG_CVDS) (ulong x)
{
    return x >= INT64_MAX ? INT64_MAX : x;
}

INLINE(int64_t,LONG_CVDS) (long x) {return x;}

#endif

INLINE(int64_t,ULLONG_CVDS) (ullong x)
{
    return x >= INT64_MAX ? INT64_MAX : x;
}

INLINE(int64_t,LLONG_CVDS) (llong x)
{
#if QUAD_NLLONG == 2
    return x ;
#else
    if (x >= INT64_MAX) return INT64_MAX;
    if (x <= INT64_MIN) return INT64_MIN;
    return x;
#endif
}

INLINE(int64_t, FLT16_CVDS) (flt16_t x) {return x;}
INLINE(int64_t,   FLT_CVDS)   (float x) {return vcvtd_s64_f64(x);}
INLINE(int64_t,   DBL_CVDS)  (double x) {return vcvtd_s64_f64(x);}

#if QUAD_NLLONG == 2

INLINE(int64_t,cvdsqu) (QUAD_UTYPE x)
{
    return x > INT64_MAX ? INT64_MAX : x;
}

INLINE(int64_t,cvdsqi) (QUAD_ITYPE x)
{
    if (x >= INT64_MAX) return INT64_MAX;
    if (x <= INT64_MIN) return INT64_MIN;
    return  x;
}

INLINE(int64_t,cvdsqf) (QUAD_FTYPE x)
{
    if (x >= INT64_MAX) return INT64_MAX;
    if (x <= INT64_MIN) return INT64_MIN;
    return  x;
}

#endif

INLINE(Vqdi,VWHU_CVDS) (Vwhu x)
{
#define     VWHU_CVDS(X) VWHU_CVDI(X)
    return  VWHU_CVDS(x);
}

INLINE(Vqdi,VWHI_CVDS) (Vwhi x)
{
#define     VWHI_CVDS(X) VWHI_CVDI(X)
    return  VWHI_CVDS(x);
}

INLINE(Vqdi,VWHF_CVDS) (Vwhf x)
{
// TODO
    return  VQDI_VOID;
}

INLINE(Vddi,VWWU_CVDS) (Vwwu x) {return VWWU_CVDI(x);}
INLINE(Vddi,VWWI_CVDS) (Vwwi x) {return VWWI_CVDI(x);}
INLINE(Vddi,VWWF_CVDS) (Vwwf x)
{
#define     VWWF_CVDS(X) INT64_ASTV(vcvtd_s64_f64(VWWF_ASTM(X)))
    return  VWWF_CVDS(x);
}

INLINE(Vqdi,VDWU_CVDS) (Vdwu x) {return VDWU_CVDI(x);}
INLINE(Vqdi,VDWI_CVDS) (Vdwi x) {return VDWI_CVDI(x);}
INLINE(Vqdi,VDWF_CVDS) (Vdwf x) {return vcvtq_s64_f64(vcvt_f64_f32(x));}


INLINE(Vddi,VDDU_CVDS) (Vddu x)
{
    uint64x1_t m = vtst_u64(
        vdup_n_u64(0x8000000000000000ull),
        x
    );
    return  vand_s64(
        vdup_n_s64(INT64_MAX),
        vreinterpret_s64_u64(m)
    );
}

INLINE(Vddi,VDDI_CVDS) (Vddi x) {return x;}
INLINE(Vddi,VDDF_CVDS) (Vddf x) {return vcvt_s64_f64(x);}

INLINE(Vqdi,VQDU_CVDS) (Vqdu x)
{
    uint64x2_t m = vtstq_u64(
        vdupq_n_u64(0x8000000000000000ull),
        x
    );
    return  vandq_s64(
        vdupq_n_s64(INT64_MAX),
        vreinterpretq_s64_u64(m)
    );
}

INLINE(Vqdi,VQDI_CVDS) (Vqdi x) {return x;}
INLINE(Vqdi,VQDF_CVDS) (Vqdf x) {return vcvtq_s64_f64(x);}

#if _LEAVE_ARM_CVDS
}
#endif

#if _ENTER_ARM_CVDF
{
#endif
/*  Convert each operand element to an IEEE 754 double
    precision floating point value.
*/
INLINE(double,   BOOL_CVDF)   (_Bool x) {return x;}
INLINE(double,  UCHAR_CVDF)   (uchar x) {return x;}
INLINE(double,  SCHAR_CVDF)   (schar x) {return x;}
INLINE(double,   CHAR_CVDF)    (char x) {return x;}
INLINE(double,  USHRT_CVDF)  (ushort x) {return x;}
INLINE(double,   SHRT_CVDF)   (short x) {return x;}
INLINE(double,   UINT_CVDF)    (uint x) {return x;}
INLINE(double,    INT_CVDF)     (int x) {return x;}
INLINE(double,  ULONG_CVDF)   (ulong x) {return x;}
INLINE(double,   LONG_CVDF)    (long x) {return x;}
INLINE(double, ULLONG_CVDF)  (ullong x) {return x;}
INLINE(double,  LLONG_CVDF)   (llong x) {return x;}
INLINE(double,  FLT16_CVDF) (flt16_t x) {return x;}
INLINE(double,    FLT_CVDF)   (float x) {return x;}
INLINE(double,    DBL_CVDF)  (double x) {return x;}

#if QUAD_NLLONG == 2

INLINE(double,cvdfqu) (QUAD_UTYPE x) {return x;}
INLINE(double,cvdfqi) (QUAD_ITYPE x) {return x;}
INLINE(double,cvdfqf) (QUAD_FTYPE x) {return x;}
#endif

INLINE(Vqdf,VWHU_CVDF) (Vwhu x)
{
    float32x2_t m = vdup_n_f32(VWHU_ASTM(x));
    uint8x8_t   d = vreinterpret_u8_f32(m);
    d = vtbl1_u8(
        d,
        vcreate_u8(0xffff0302ffff0100ULL)
    );
    uint64x2_t  q = vmovl_u32(vreinterpret_u32_u8(d));
    return  vcvtq_f64_u64(q);
}

INLINE(Vqdf,VWHI_CVDF) (Vwhi x)
{
//  TODO: optimize
    return  vcvtq_f64_s64(VWHI_CVDI(x));
}

INLINE(Vqdf,VWHF_CVDF) (Vwhf x)
{
//  TODO
    return  VQDF_VOID;
}


INLINE(Vddf,VWWU_CVDF) (Vwwu x) {return vdup_n_f64(VWWU_ASTV(x));}
INLINE(Vddf,VWWI_CVDF) (Vwwi x) {return vdup_n_f64(VWWI_ASTV(x));}
INLINE(Vddf,VWWF_CVDF) (Vwwf x) {return vdup_n_f64(VWWF_ASTV(x));}

INLINE(Vqdf,VDWU_CVDF) (Vdwu x) {return vcvtq_f64_u64(vmovl_u32(x));}
INLINE(Vqdf,VDWI_CVDF) (Vdwi x) {return vcvtq_f64_u64(vmovl_s32(x));}
INLINE(Vqdf,VDWF_CVDF) (Vdwf x) {return vcvt_f64_f32(x);}

INLINE(Vddf,VDDU_CVDF) (Vddu x) {return vcvt_f64_u64(x);}
INLINE(Vddf,VDDI_CVDF) (Vddi x) {return vcvt_f64_s64(x);}
INLINE(Vddf,VDDF_CVDF) (Vddf x) {return x;}

INLINE(Vqdf,VQDU_CVDF) (Vqdu x) {return vcvtq_f64_u64(x);}
INLINE(Vqdf,VQDI_CVDF) (Vqdi x) {return vcvtq_f64_s64(x);}
INLINE(Vqdf,VQDF_CVDF) (Vqdf x) {return x;}

#if _LEAVE_ARM_CVDF
}
#endif


#if _ENTER_ARM_CVQU
{
#endif

INLINE(QUAD_UTYPE,  BOOL_CVQU)      (_Bool x) {return x;}
INLINE(QUAD_UTYPE, UCHAR_CVQU)      (uchar x) {return x;}
INLINE(QUAD_UTYPE, SCHAR_CVQU)      (schar x) {return x;}
INLINE(QUAD_UTYPE,  CHAR_CVQU)       (char x) {return x;}
INLINE(QUAD_UTYPE, USHRT_CVQU)     (ushort x) {return x;}
INLINE(QUAD_UTYPE,  SHRT_CVQU)      (short x) {return x;}
INLINE(QUAD_UTYPE,   INT_CVQU)        (int x) {return x;}
INLINE(QUAD_UTYPE,  UINT_CVQU)       (uint x) {return x;}
INLINE(QUAD_UTYPE, ULONG_CVQU)      (ulong x) {return x;}
INLINE(QUAD_UTYPE,  LONG_CVQU)       (long x) {return x;}
INLINE(QUAD_UTYPE,ULLONG_CVQU)     (ullong x) {return x;}
INLINE(QUAD_UTYPE, LLONG_CVQU)      (llong x) {return x;}
INLINE(QUAD_UTYPE, FLT16_CVQU)    (flt16_t x) {return x;}
INLINE(QUAD_UTYPE,   FLT_CVQU)      (float x) {return x;}
INLINE(QUAD_UTYPE,   DBL_CVQU)     (double x) {return x;}
INLINE(QUAD_UTYPE,cvququ) (QUAD_UTYPE x) {return x;}
INLINE(QUAD_UTYPE,cvquqi) (QUAD_ITYPE x) {return x;}
INLINE(QUAD_UTYPE,cvquqf) (QUAD_FTYPE x) {return x;}
INLINE(QUAD_ITYPE,cvquac) (void volatile const *x)
{
    return  (uintptr_t) x;
}

#if _LEAVE_ARM_CVQU
}
#endif

#if _ENTER_ARM_CVQI
{
#endif

INLINE(QUAD_ITYPE,  BOOL_CVQI)      (_Bool x) {return x;}
INLINE(QUAD_ITYPE, UCHAR_CVQI)      (uchar x) {return x;}
INLINE(QUAD_ITYPE, SCHAR_CVQI)      (schar x) {return x;}
INLINE(QUAD_ITYPE,  CHAR_CVQI)       (char x) {return x;}
INLINE(QUAD_ITYPE, USHRT_CVQI)     (ushort x) {return x;}
INLINE(QUAD_ITYPE,  SHRT_CVQI)      (short x) {return x;}
INLINE(QUAD_ITYPE,   INT_CVQI)        (int x) {return x;}
INLINE(QUAD_ITYPE,  UINT_CVQI)       (uint x) {return x;}
INLINE(QUAD_ITYPE, ULONG_CVQI)      (ulong x) {return x;}
INLINE(QUAD_ITYPE,  LONG_CVQI)       (long x) {return x;}
INLINE(QUAD_ITYPE,ULLONG_CVQI)     (ullong x) {return x;}
INLINE(QUAD_ITYPE, LLONG_CVQI)      (llong x) {return x;}
INLINE(QUAD_ITYPE, FLT16_CVQI)    (flt16_t x) {return x;}
INLINE(QUAD_ITYPE,   FLT_CVQI)      (float x) {return x;}
INLINE(QUAD_ITYPE,   DBL_CVQI)     (double x) {return x;}
INLINE(QUAD_ITYPE,cvqiqu) (QUAD_UTYPE x) {return x;}
INLINE(QUAD_ITYPE,cvqiqi) (QUAD_ITYPE x) {return x;}
INLINE(QUAD_ITYPE,cvqiqf) (QUAD_FTYPE x) {return x;}
INLINE(QUAD_ITYPE,cvqiac) (void volatile const *x)
{
    return (intptr_t) x;
}

#if _LEAVE_ARM_CVQI
}
#endif

#if _ENTER_ARM_CVQF
{
#endif

INLINE(QUAD_FTYPE,  BOOL_CVQF)      (_Bool x) {return x;}
INLINE(QUAD_FTYPE, UCHAR_CVQF)      (uchar x) {return x;}
INLINE(QUAD_FTYPE, SCHAR_CVQF)      (schar x) {return x;}
INLINE(QUAD_FTYPE,  CHAR_CVQF)       (char x) {return x;}
INLINE(QUAD_FTYPE, USHRT_CVQF)     (ushort x) {return x;}
INLINE(QUAD_FTYPE,  SHRT_CVQF)      (short x) {return x;}
INLINE(QUAD_FTYPE,   INT_CVQF)        (int x) {return x;}
INLINE(QUAD_FTYPE,  UINT_CVQF)       (uint x) {return x;}
INLINE(QUAD_FTYPE, ULONG_CVQF)      (ulong x) {return x;}
INLINE(QUAD_FTYPE,  LONG_CVQF)       (long x) {return x;}
INLINE(QUAD_FTYPE,ULLONG_CVQF)     (ullong x) {return x;}
INLINE(QUAD_FTYPE, LLONG_CVQF)      (llong x) {return x;}
INLINE(QUAD_FTYPE, FLT16_CVQF)    (flt16_t x) {return x;}
INLINE(QUAD_FTYPE,   FLT_CVQF)      (float x) {return x;}
INLINE(QUAD_FTYPE,   DBL_CVQF)     (double x) {return x;}
INLINE(QUAD_FTYPE,cvqfqu) (QUAD_UTYPE x) {return x;}
INLINE(QUAD_FTYPE,cvqfqi) (QUAD_ITYPE x) {return x;}
INLINE(QUAD_FTYPE,cvqfqf) (QUAD_FTYPE x) {return x;}

#if _LEAVE_ARM_CVQF
}
#endif


#if _ENTER_ARM_DUPW
{
#endif

INLINE(Vwyu,BOOL_DUPW) (_Bool x)
{
#define     BOOL_DUPW(X)        \
WYU_ASTV(                       \
    vget_lane_f32(              \
        vreinterpret_f32_u32(   \
            vdup_n_u32(X?~0u:0u)\
        ),                      \
        V2_K0                   \
    )                           \
)
    return  BOOL_DUPW(x);
}

INLINE(Vwyu,BOOL_DUPWAC) (void const *a)
{
#define     BOOL_DUPWAC(A)   BOOL_DUPW(*(_Bool const *) A)
    return  BOOL_DUPWAC(a);
}

INLINE(Vwyu,VWYU_DUPW) (Vwyu v, Rc(0, 31) k)
{
#define     WYU_DUPW(M, K)              \
vget_lane_f32(                          \
    vreinterpret_f32_u32(               \
        vtst_u32(                       \
            vreinterpret_u32_f32(       \
                vdup_n_f32(M)           \
            ),                          \
            vreinterpret_u32_u64(       \
                vdup_n_u64(1<<(31&(K))) \
            )                           \
        )                               \
    ),                                  \
    V2_K0                               \
)
#define     VWYU_DUPW(V, K) WYU_ASTV(WYU_DUPW(VWYU_ASTM(V), K))
    return  VWYU_DUPW(v, k);
}

INLINE(Vwyu,VDYU_DUPW) (Vdyu v, Rc(0, 63) k)
{
#define     DYU_DUPW(M, K)          \
vget_lane_f32(                      \
    vreinterpret_f32_u64(           \
        vtst_u64(                   \
            M,                      \
            vdup_n_u64(1<<(63&(K))) \
        )                           \
    ),                              \
    V2_K0                           \
)
#define     VDYU_DUPW(V, K) WYU_ASTV(DYU_DUPW(VDYU_ASTM(V),K))
    return  VDYU_DUPW(v, k);
}

INLINE(Vwyu,VQYU_DUPW) (Vqyu v, Rc(0,127) k)
{
#define     QYU_DUPW(M, K)                          \
vget_lane_f32(                                      \
    vreinterpret_f32_u64(                           \
        vdup_lane_u64(                              \
            (                                       \
                K > 63                              \
                ?   vtst_u64(                       \
                        vget_high_u64(M),           \
                        vdup_n_u64(1<<(63&K))       \
                    )                               \
                :   vtst_u64(                       \
                        vget_low_u64(M),            \
                        vdup_n_u64(1<<(63&K))       \
                    )                               \
            ),                                      \
            0                                       \
        )                                           \
    ),                                              \
    V2_K0                                           \
)
#define     VQYU_DUPW(V, K) WYU_ASTV(QYU_DUPW(VQYU_ASTM(V),K))
    return  VQYU_DUPW(v, k);
}


INLINE(Vwbu,UCHAR_DUPW) (unsigned char x)
{
#define     UCHAR_DUPW(X)   \
WBU_ASTV(                   \
    vget_lane_f32(          \
        vreinterpret_f32_u8(\
            vdup_n_u8(X)    \
        ),                  \
        V2_K0               \
    )                       \
)
    return  UCHAR_DUPW(x);
}

INLINE(Vwbu,UCHAR_DUPWAC) (void const *a)
{
#define     UCHAR_DUPWAC(A)      \
WBU_ASTV(                       \
    vget_lane_f32(              \
        vreinterpret_f32_u8(    \
            vld1_dup_u8(        \
                (void const *) A\
            )                   \
       ),                       \
       V2_K0                    \
    )                           \
)

    return  UCHAR_DUPWAC(a);
}

INLINE(Vwbu,VWBU_DUPW) (Vwbu v, Rc(0, 3) k)
{
#define     WBU_DUPW(M, K)  \
vget_lane_f32(              \
    vdup_lane_u8(           \
        vreinterpret_u8_f32(\
            vdup_n_f32(M)   \
        ),                  \
        3&(K)               \
    ),                      \
    V2_K0                   \
)
#define     VWBU_DUPW(V, K) WBU_ASTV(WBU_DUPW(VWBU_ASTM(V), K))
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(
                vtbl1_u8(
                    vreinterpret_u8_f32(
                        vdup_n_f32(VWBU_ASTM(v))
                    ),
                    vdup_n_u8(3&k)
                )
            ),
            V2_K0
        )
    );
}

INLINE(Vwbu,VDBU_DUPW) (Vdbu v, Rc(0, 7) k)
{
#define     DBU_DUPW(M, K)      \
vget_lane_f32(                  \
    vreinterpret_f32_u8(        \
        vdup_lane_u8(M, (7&K))  \
    ),                          \
    V2_K0                       \
)

#define     VDBU_DUPW(V, K) WBU_ASTV(DBU_DUPW(V,K))
    return  WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(
                vtbl1_u8(v, vdup_n_u8(k))
            ),
            V2_K0
        )
    );
}

INLINE(Vwbu,VQBU_DUPW) (Vqbu v, Rc(0, 15) k)
{
#define     QBU_DUPW(M, K)      \
vget_lane_f32(                  \
    vreinterpret_f32_u8(        \
        vdup_laneq_u8(M, 15&(K))\
    ),                          \
    V2_K0                       \
)

#define     VQBU_DUPW(V, K) WBU_ASTV(QBU_DUPW(V, K))
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(
                vqtbl1_u8(v, vdup_n_u8(15&k))
            ),
            V2_K0
        )
    );
}


INLINE(Vwbi,SCHAR_DUPW) (signed char x)
{
#define     SCHAR_DUPW(X)   \
WBI_ASTV(                   \
    vget_lane_f32(          \
        vreinterpret_f32_s8(\
            vdup_n_s8(X)    \
        ),                  \
        V2_K0               \
    )                       \
)
    return  SCHAR_DUPW(x);
}

INLINE(Vwbi,SCHAR_DUPWAC) (void const *a)
{
#define     SCHAR_DUPWAC(A)      \
WBI_ASTV(                       \
    vget_lane_f32(              \
        vreinterpret_f32_s8(    \
            vld1_dup_s8(        \
                (void const *) A\
            )                   \
       ),                       \
       V2_K0                    \
    )                           \
)

    return  SCHAR_DUPWAC(a);
}

INLINE(Vwbi,VWBI_DUPW) (Vwbi v, Rc(0, 3) k)
{
#define     WBI_DUPW(M, K)  \
vget_lane_f32(              \
    vdup_lane_s8(           \
        vreinterpret_s8_f32(\
            vdup_n_f32(M)   \
        ),                  \
        3&(K)               \
    ),                      \
    V2_K0                   \
)
#define     VWBI_DUPW(V, K) WBI_ASTV(WBI_DUPW(VWBI_ASTM(V), K))
    return WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_s8(
                vtbl1_s8(
                    vreinterpret_s8_f32(
                        vdup_n_f32(VWBI_ASTM(v))
                    ),
                    vdup_n_u8(3&k)
                )
            ),
            V2_K0
        )
    );
}

INLINE(Vwbi,VDBI_DUPW) (Vdbi v, Rc(0, 7) k)
{
#define     DBI_DUPW(M, K)      \
vget_lane_f32(                  \
    vreinterpret_f32_s8(        \
        vdup_lane_s8(M, (7&K))  \
    ),                          \
    V2_K0                       \
)

#define     VDBI_DUPW(V, K) WBI_ASTV(DBI_DUPW(V,K))
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_s8(
                vtbl1_s8(v, vdup_n_s8(k))
            ),
            V2_K0
        )
    );
}

INLINE(Vwbi,VQBI_DUPW) (Vqbi v, Rc(0, 15) k)
{
#define     QBI_DUPW(M, K)      \
vget_lane_f32(                  \
    vreinterpret_f32_s8(        \
        vdup_laneq_s8(M, 15&(K))\
    ),                          \
    V2_K0                       \
)

#define     VQBI_DUPW(V, K) WBI_ASTV(QBI_DUPW(V,K))
    return WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_s8(
                vqtbl1_s8(v, vdup_n_u8(15&k))
            ),
            V2_K0
        )
    );
}


INLINE(Vwbc,CHAR_DUPW) (char x)
{
    return  WBC_ASTV(
        vget_lane_f32(
#if CHAR_MIN
            vreinterpret_f32_s8(vdup_n_s8(x)),
#else
            vreinterpret_f32_s8(vdup_n_u8(x)),
#endif
            V2_K0
        )
    );
}

INLINE(Vwbc,CHAR_DUPWAC) (void const *a)
{
#define     CHAR_DUPWAC(A)   \
WBC_ASTV(                   \
    vget_lane_f32(          \
        vreinterpret_f32_u8(\
            vld1_dup_u8(a)  \
       ),                   \
       V2_K0                \
    )                       \
)

    return  CHAR_DUPWAC(a);
}

INLINE(Vwbc,VWBC_DUPW) (Vwbc v, Rc(0, 3) k)
{
#define     VWBC_DUPW(V, K)     WBC_ASTV(WBU_DUPW(VWBC_ASTM(V), K))
    return  WBC_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(
                vtbl1_u8(
                    vreinterpret_u8_f32(
                        vdup_n_f32(VWBC_ASTM(v))
                    ),
                    vdup_n_u8(3&k)
                )
            ),
            V2_K0
        )
    );
}

INLINE(Vwbc,VDBC_DUPW) (Vdbc v, Rc(0,  7) k)
{
#define     VDBC_DUPW(V, K) WBC_ASTV(DBC_DUPW(VDBC_ASTM(V), K))
    Dbc m = VDBC_ASTM(v);
    return   WBC_ASTV(
        vget_lane_f32(
#if CHAR_MIN
#   define  DBC_DUPW(M, K)  DBI_DUPW(M, K)
            vreinterpret_f32_s8(
                vtbl1_s8(m, vdup_n_u8(k))
            ),
#else
#   define  DBC_DUPW(M, K)  DBU_DUPW(M, K)
            vreinterpret_f32_u8(
                vtbl1_u8(m, vdup_n_u8(k))
            ),
#endif
            V2_K0
        )
    );
}

INLINE(Vwbc,VQBC_DUPW) (Vqbc v, Rc(0, 15) k)
{
    Qbc m = VQBC_ASTM(v);
    return   WBC_ASTV(
        vget_lane_f32(
#if CHAR_MIN
#   define  QBC_DUPW(M, K)  QBI_DUPW(M,K)
            vreinterpret_f32_s8(
                vqtbl1_s8(m, vdupq_n_u8(15&k))
            ),
#else
#   define  QBC_DUPW(M, K)  QBU_DUPW(M,K)
            vreinterpret_f32_s8(
                vqtbl1_u8(m, vdup_n_u8(15&k))
            ),
#endif
            V2_K0
        )
    );
#define     VQBC_DUPW(V, K) WBC_ASTV(QBC_DUPW(VQBC_ASTM(V),K))
}


INLINE(Vwhu,USHRT_DUPW) (unsigned short x)
{
#define     USHRT_DUPW(X)       \
WHU_ASTV(                       \
    vget_lane_f32(              \
        vreinterpret_f32_u16(   \
            vdup_n_u16(X)       \
        ),                      \
        V2_K0                   \
    )                           \
)
    return  USHRT_DUPW(x);
}

INLINE(Vwhu,USHRT_DUPWAC) (void const *a)
{
#define     USHRT_DUPWAC(A)      \
WHU_ASTV(                       \
    vget_lane_f32(              \
        vreinterpret_f32_u16(   \
            vld1_dup_u16(a)     \
       ),                       \
       V2_K0                    \
    )                           \
)

    return  USHRT_DUPWAC(a);
}

INLINE(Vwhu,VWHU_DUPW) (Vwhu v, Rc(0, 1) k)
{
/*  TODO:
    Runtime variant won't work in big endian mode and
    testing needs to be done to confirm zip-shuffling is
    faster than shift-and-dupping
*/
#define     WHU_DUPW(M, K)          \
vget_lane_f32(                      \
    vreinterpret_f32_u16(           \
        vdup_lane_u16(              \
            vreinterpret_u16_f32(   \
                vdup_n_f32(M)       \
            ),                      \
            3&(K)                   \
        )                           \
    ),                              \
    V2_K0                           \
)

#define     VWHU_DUPW(V, K)     WHU_ASTV(WHU_DUPW(VWHU_ASTM(V),K))
    float32x2_t m = vset_lane_f32(VWHU_ASTM(v), m, V2_K0);
    uint8x8_t   t = vzip1_u8(vdup_n_u8(k*2), vdup_n_u8(k*2+1));
    t = vtbl1_u8(vreinterpret_u8_f32(m), t);
    return  WHU_ASTV(vget_lane_f32(vreinterpret_f32_u8(t), V2_K0));
}

INLINE(Vwhu,VDHU_DUPW) (Vdhu v, Rc(0, 3) k)
{
#define     DHU_DUPW(M, K)      \
vget_lane_f32(                  \
    vreinterpret_f32_u16(       \
        vdup_lane_u16(M, 3&(K)) \
    ),                          \
    V2_K0                       \
)
#define     VDHU_DUPW(V, K) WHU_ASTV(DHU_DUPW(V,K))
    uint8x8_t   m = vreinterpret_u8_u16(v);
    uint8x8_t   t = vzip1_u8(vdup_n_u8(k*2), vdup_n_u8(k*2+1));
    t = vtbl1_u8(m, t);
    return  WHU_ASTV(
        vget_lane_f32(vreinterpret_f32_u8(t), V2_K0)
    );
}

INLINE(Vwhu,VQHU_DUPW) (Vqhu v, Rc(0, 7) k)
{
#define     QHU_DUPW(M, K)      \
vget_lane_f32(                  \
    vreinterpret_f32_u16(       \
        vdup_laneq_u16(M, 7&(K))\
    ),                          \
    V2_K0                       \
)
/*
    l = {4,4,4,4, 4,4,4,4}
    r = {5,5,5,5, 5,5,5,5}
    t = {4,5,4,5, 4,5,4,5}
    t = {m[4], m[5], m[4], m[5], ...}
*/
#define     VQHU_DUPW(V, K) WHU_ASTV(QHU_DUPW(V,K))
    uint8x8_t   l = vdup_n_u8(2*k);
    uint8x8_t   r = vdup_n_u8(2*k+1);
    uint8x8_t   t = vzip1_u8(l, r);
    t = vqtbl1_u8(vreinterpretq_u8_u16(v), t);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(t),
            0
        )
    );
}


INLINE(Vwhi,SHRT_DUPW) (short x)
{
#define     SHRT_DUPW(X)        \
WHI_ASTV(                       \
    vget_lane_f32(              \
        vreinterpret_f32_s16(   \
            vdup_n_s16(X)       \
        ),                      \
        0                       \
    )                           \
)
    return  SHRT_DUPW(x);
}

INLINE(Vwhi,SHRT_DUPWAC) (void const *a)
{
#define     SHRT_DUPWAC(A)      \
WHI_ASTV(                       \
    vget_lane_f32(              \
        vreinterpret_f32_s16(   \
            vld1_dup_s16(a)     \
       ),                       \
       0                        \
    )                           \
)

    return  SHRT_DUPWAC(a);
}

INLINE(Vwhi,VWHI_DUPW) (Vwhi v, Rc(0, 1) k)
{
#define     WHI_DUPW(M, K)          \
vget_lane_f32(                      \
    vreinterpret_f32_s16(           \
        vdup_lane_s16(              \
            vreinterpret_s16_f32(   \
                vdup_n_f32(M)       \
            ),                      \
            3&(K)                   \
        )                           \
    ),                              \
    0                           \
)

#define     VWHI_DUPW(V, K)     WHI_ASTV(WHI_DUPW(VWHI_ASTM(V),K))
    float32x2_t m = vset_lane_f32(VWHI_ASTM(v), m, 0);
    uint8x8_t   t = vzip1_u8(
        vdup_n_u8(2*k),
        vdup_n_u8(2*k+1)
    );
    t = vtbl1_u8(vreinterpret_u8_f32(m), t);
    return  WHI_ASTV(
        vget_lane_f32(vreinterpret_f32_u8(t), 0)
    );
}

INLINE(Vwhi,VDHI_DUPW) (Vdhi v, Rc(0, 3) k)
{
#define     DHI_DUPW(M, K)      \
vget_lane_f32(                  \
    vreinterpret_f32_s16(       \
        vdup_lane_s16(M, 3&(K)) \
    ),                          \
    0                       \
)
#define     VDHI_DUPW(V, K) WHI_ASTV(DHI_DUPW(V,K))
    uint8x8_t   m = vreinterpret_u8_s16(v);
    uint8x8_t   t = vzip1_u8(
        vdup_n_u8(2*k),
        vdup_n_u8(2*k+1)
    );
    t = vtbl1_u8(m, t);
    return  WHI_ASTV(
        vget_lane_f32(vreinterpret_f32_u8(t), 0)
    );
}

INLINE(Vwhi,VQHI_DUPW) (Vqhi v, Rc(0, 7) k)
{
#define     QHI_DUPW(M, K)      \
vget_lane_f32(                  \
    vreinterpret_f32_s16(       \
        vdup_laneq_s16(M, 7&(K))\
    ),                          \
    0                       \
)
#define     VQHI_DUPW(V, K) WHI_ASTV(QHI_DUPW(V,K))
    uint8x8_t   l = vdup_n_u8(2*k);
    uint8x8_t   r = vdup_n_u8(2*k+1);
    uint8x8_t   t = vzip1_u8(l, r);
    t = vqtbl1_u8(vreinterpretq_u8_s16(v), t);
    return WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(t),
            0
        )
    );
}


INLINE(Vwhf,FLT16_DUPW) (flt16_t x)
{
#define     FLT16_DUPW(X)       \
WHF_ASTV(                       \
    vget_lane_f32(              \
        vreinterpret_f32_f16(   \
            vdup_n_f16(X)       \
        ),                      \
        0                   \
    )                           \
)
    return  FLT16_DUPW(x);
}

INLINE(Vwhf,FLT16_DUPWAC) (void const *a)
{
#define     FLT16_DUPWAC(A)      \
WHF_ASTV(                       \
    vget_lane_f32(              \
        vreinterpret_f32_u16(   \
            vld1_dup_u16(a)     \
       ),                       \
       0                    \
    )                           \
)
    return  FLT16_DUPWAC(a);
}

INLINE(Vwhf,VWHF_DUPW) (Vwhf v, Rc(0, 1) k)
{
#define     WHF_DUPW(M, K)          \
vget_lane_f32(                      \
    vreinterpret_f32_f16(           \
        vdup_lane_f16(              \
            vreinterpret_f16_f32(   \
                vdup_n_f32(M)       \
            ),                      \
            3&(K)                   \
        )                           \
    ),                              \
    0                           \
)

#define     VWHF_DUPW(V, K)     WHF_ASTV(WHF_DUPW(VWHF_ASTM(V),K))
    float32x2_t m = vset_lane_f32(VWHF_ASTM(v), m, 0);
    uint8x8_t   t = vzip1_u8(
        vdup_n_u8(2*k),
        vdup_n_u8(2*k+1)
    );
    t = vtbl1_u8(vreinterpret_u8_f32(m), t);
    return  WHF_ASTV(
        vget_lane_f32(vreinterpret_f32_u8(t), 0)
    );
}

INLINE(Vwhf,VDHF_DUPW) (Vdhf v, Rc(0, 3) k)
{
#define     DHF_DUPW(M, K)      \
vget_lane_f32(                  \
    vreinterpret_f32_f16(       \
        vdup_lane_f16(M, 3&(K)) \
    ),                          \
    0                       \
)
#define     VDHF_DUPW(V, K) WHF_ASTV(DHF_DUPW(V,K))
    uint8x8_t   m = vreinterpret_u8_f16(v);
    uint8x8_t   t = vzip1_u8(
        vdup_n_u8(2*k),
        vdup_n_u8(2*k+1)
    );
    t = vtbl1_u8(m, t);
    return  WHF_ASTV(
        vget_lane_f32(vreinterpret_f32_u8(t), 0)
    );
}

INLINE(Vwhf,VQHF_DUPW) (Vqhf v, Rc(0, 7) k)
{
#define     QHF_DUPW(M, K)      \
vget_lane_f32(                  \
    vreinterpret_f32_f16(       \
        vdup_laneq_f16(M, 7&(K))\
    ),                          \
    0                       \
)
#define     VQHF_DUPW(V, K) WHF_ASTV(QHF_DUPW(V,K))
    uint8x8_t   l = vdup_n_u8(2*k);
    uint8x8_t   r = vdup_n_u8(2*k+1);
    uint8x8_t   t = vzip1_u8(l, r);
    t = vqtbl1_u8(vreinterpretq_u8_f16(v), t);
    return WHF_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(t),
            0
        )
    );
}

INLINE(Vwwu,VWWU_DUPW) (Vwwu v, int const k) {return v;}
INLINE(Vwwi,VWWI_DUPW) (Vwwi v, int const k) {return v;}
INLINE(Vwwf,VWWF_DUPW) (Vwwf v, int const k) {return v;}

#if _LEAVE_ARM_DUPW
}
#endif

#if _ENTER_ARM_DUPD
{
#endif

INLINE(Vdyu,  BOOL_DUPD)   (_Bool x)
{
#define     BOOL_DUPD(X)    VDDU_ASYU(vdup_n_u64((X?1ull:0)))
    return  BOOL_DUPD(x);
}

INLINE(Vdbu, UCHAR_DUPD)   (uchar x) {return vdup_n_u8(x);}
INLINE(Vdbi, SCHAR_DUPD)   (schar x) {return vdup_n_s8(x);}
INLINE(Vdbc,  CHAR_DUPD)    (char x)
{
#if CHAR_MIN
#   define  CHAR_DUPD(X)    VDBI_ASBC(vdup_n_s8(X))
#else
#   define  CHAR_DUPD(X)    VDBU_ASBC(vdup_n_u8(X))
#endif
    return  CHAR_DUPD(x);
}

INLINE(Vdhu, USHRT_DUPD)  (ushort x) {return vdup_n_u16(x);}
INLINE(Vdhi,  SHRT_DUPD)   (short x) {return vdup_n_s16(x);}
INLINE(Vdwu,  UINT_DUPD)    (uint x) {return vdup_n_u32(x);}
INLINE(Vdwi,   INT_DUPD)     (int x) {return vdup_n_s32(x);}
#if DWRD_NLONG == 2
INLINE(Vdwu, ULONG_DUPD)   (ulong x) {return vdup_n_u32(x);}
INLINE(Vdwi,  LONG_DUPD)    (long x) {return vdup_n_s32(x);}
#else
INLINE(Vddu, ULONG_DUPD)   (ulong x) {return vdup_n_u64(x);}
INLINE(Vddi,  LONG_DUPD)    (long x) {return vdup_n_s64(x);}
#endif

#if QUAD_NLLONG == 2
INLINE(Vddu,ULLONG_DUPD)   (ulong x) {return vdup_n_u64(x);}
INLINE(Vddi, LLONG_DUPD)    (long x) {return vdup_n_s64(x);}
#endif

INLINE(Vdhf, FLT16_DUPD) (flt16_t x) {return vdup_n_f16(x);}
INLINE(Vdwf,   FLT_DUPD)   (float x) {return vdup_n_f32(x);}
INLINE(Vddf,   DBL_DUPD)  (double x) {return vdup_n_f64(x);}

INLINE(Vdyu,  VWYU_DUPD) (Vwyu v, Rc(0, 31) k)
{
#define     WYU_DUPD(M, K)          \
vdup_n_u64(                         \
    vtstd_u64(                      \
        (UINT64_C(1)<<(K)),         \
        vget_lane_u32(              \
            vreinterpret_u32_f32(   \
                vdup_n_f32(M)       \
            ),                      \
            0                   \
        )                           \
    )                               \
)

#define     VWYU_DUPD(V, K) DYU_ASTV(WYU_DUPD(VWYU_ASTM(V), K))
    return  VWYU_DUPD(v, k);
}

INLINE(Vdbu,  VWBU_DUPD) (Vwbu v, Rc(0,  3) k)
{
#define     WBU_DUPD(M, K)  \
vdup_lane_u8(               \
    vreinterpret_u8_f32(    \
        vdup_n_f32(M)       \
    ),                      \
    (7&(K))                 \
)
#define     VWBU_DUPD(V, K) WBU_DUPD(VWBU_ASTM(V), K)
    return  vtbl1_u8(
        vreinterpret_u8_f32(
            vdup_n_f32(VWBU_ASTM(v))
        ),
        vdup_n_u8(k)
    );
}

INLINE(Vdbi,  VWBI_DUPD) (Vwbi v, Rc(0,  3) k)
{
#define     WBI_DUPD(M, K) \
vdup_lane_s8(vreinterpret_s8_f32(vdup_n_f32(M)), K)

#define     VWBI_DUPD(V, K) WBI_DUPD(VWBI_ASTM(V), K)
    return  vtbl1_s8(
        vreinterpret_s8_f32(vdup_n_f32(VWBI_ASTM(v))),
        vdup_n_u8(k)
    );
}

INLINE(Vdbc,VWBC_DUPD) (Vwbc v, Rc(0,  3) k)
{
#define     VWBC_DUPD(V, K) DBC_ASTV(WBU_DUPD(VWBC_ASTM(V), K))
    return  VDBU_ASBC((VWBU_DUPD)(VWBC_ASBU(v), k));
}

INLINE(Vdhu,VWHU_DUPD) (Vwhu x, Rc(0, 1) k)
{
#define     WHU_DUPD(M, K)  \
vdup_lane_u16(              \
    vreinterpret_u16_f32(   \
        vdup_n_f32(M)       \
    ),                      \
    1&(K)                   \
)

#define     VWHU_DUPD(V, K) WHU_DUPD(VWHU_ASTM(V), K)
    return vreinterpret_u16_u8(
        vtbl1_u8(
            vreinterpret_u8_f32(vdup_n_f32(VWHU_ASTM(x))),
            vzip1_u8(
                vdup_n_u8(2*k),
                vdup_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vdhi,VWHI_DUPD) (Vwhi x, Rc(0, 1) k)
{
#define     WHI_DUPD(M, K)  \
vdup_lane_s16(vreinterpret_s16_f32(vdup_n_f32(M)),1&(K))

#define     VWHI_DUPD(V, K) WHI_DUPD(VWHI_ASTM(V), K)
    return vreinterpret_s16_u8(
        vtbl1_u8(
            vreinterpret_u8_f32(vdup_n_f32(VWHI_ASTM(x))),
            vzip1_u8(
                vdup_n_u8(2*k),
                vdup_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vdhf,VWHF_DUPD) (Vwhf x, Rc(0, 1) k)
{
#define     WHF_DUPD(M, K)  \
vdup_lane_f16(vreinterpret_f16_f32(vdup_n_f32(M)),1&(K))

#define     VWHF_DUPD(V, K) WHF_DUPD(VWHF_ASTM(V), K)
    return vreinterpret_f16_u8(
        vtbl1_u8(
            vreinterpret_u8_f32(vdup_n_f32(VWHF_ASTM(x))),
            vzip1_u8(
                vdup_n_u8(2*k),
                vdup_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vdwu,VWWU_DUPD) (Vwwu v, int const k)
{
#define     WWU_DUPD(M, K)  vreinterpret_u32_f32(vdup_n_f32(M))
#define     VWWU_DUPD(V, K) WWU_DUPD(VWWU_ASTM(V), K)
    return  VWWU_DUPD(v, k);
}

INLINE(Vdwi,VWWI_DUPD) (Vwwi v, int const k)
{
#define     WWI_DUPD(M, K)  vreinterpret_s32_f32(vdup_n_f32(M))
#define     VWWI_DUPD(V, K) WWI_DUPD(VWWI_ASTM(V), K)
    return  VWWI_DUPD(v, k);
}

INLINE(Vdwf,VWWF_DUPD) (Vwwf v, int const k)
{
#define     WWF_DUPD        vdup_n_f32
#define     VWWF_DUPD(V, K) WWF_DUPD(VWWF_ASTM(V))
    return  VWWF_DUPD(v, k);
}


INLINE(Vdyu,VDYU_DUPD) (Vdyu v, Rc(0, 63) k)
{
#define     DYU_DUPD(M, K)  \
vdup_n_u64(                 \
    vtstd_u64(              \
        UINT64_C(1)<<(K),   \
        vget_lane_u64(      \
            M,              \
            0           \
        )                   \
    )                       \
)

#define     VDYU_DUPD(V, K) DYU_ASTV(DYU_DUPD(VDYU_ASTM(V), K))
    return  VDYU_DUPD(v, k);
}

INLINE(Vdbu,VDBU_DUPD) (Vdbu v, Rc(0, 7) k)
{
#define     DBU_DUPD(M, K)  vdup_lane_u8(M, (7&(K)))
#define     VDBU_DUPD(V, K) DBU_DUPD(V, K)
    return  vtbl1_u8(v, vdup_n_u8(k));
}

INLINE(Vdbi,VDBI_DUPD) (Vdbi v, Rc(0, 7) k)
{
#define     DBI_DUPD(M, K)  vdup_lane_s8(M, (7&K))
#define     VDBI_DUPD(V, K) DBI_DUPD(V, K)
    return  vtbl1_s8(v, vdup_n_s8(k));
}

INLINE(Vdbc,VDBC_DUPD) (Vdbc v, Rc(0, 7) k)
{
#define     VDBC_DUPD(V, K) VDBU_ASBC(DBU_DUPD(VDBC_ASBU(V), K))
    return VDBU_ASBC((VDBU_DUPD)(VDBC_ASBU(v), k));
}

INLINE(Vdhu,VDHU_DUPD) (Vdhu x, Rc(0, 3) k)
{
#define     DHU_DUPD        vdup_lane_u16
#define     VDHU_DUPD(V, K) DHU_DUPD(V, K)
    return vreinterpret_u16_u8(
        vtbl1_u8(
            vreinterpret_u8_u16(x),
            vzip1_u8(
                vdup_n_u8(2*k),
                vdup_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vdhi,VDHI_DUPD) (Vdhi x, Rc(0, 3) k)
{
#define     DHI_DUPD(M, K)  vdup_lane_s16(M, (3&(K)))
#define     VDHI_DUPD(V, K) DHI_DUPD(V, K)
    return vreinterpret_s16_u8(
        vtbl1_u8(
            vreinterpret_u8_s16(x),
            vzip1_u8(
                vdup_n_u8(2*k),
                vdup_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vdhf,VDHF_DUPD) (Vdhf x, Rc(0, 3) k)
{
#define     DHF_DUPD(M, K)  vdup_lane_f16(M, (3&(K)))
#define     VDHF_DUPD(V, K) DHF_DUPD(V, K)
    return vreinterpret_f16_u8(
        vtbl1_u8(
            vreinterpret_u8_f16(x),
            vzip1_u8(
                vdup_n_u8(2*k),
                vdup_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vdwu,VDWU_DUPD) (Vdwu v, Rc(0, 1) k)
{
#define     DWU_DUPD(M, K)  vdup_lane_u32(M, (1&(K)))
#define     VDWU_DUPD(V, K) DWU_DUPD(V, K)
    uint64x1_t m = vreinterpret_u64_u32(v);
    m = vshl_u64(m, vdup_n_s64(k*INT64_C(-32)));
    return  vdup_lane_u32(
        vreinterpret_u32_u64(m),
        0
    );
}

INLINE(Vdwi,VDWI_DUPD) (Vdwi v, Rc(0, 1) k)
{
#define     DWI_DUPD(M, K)  vdup_lane_s32(M, (1&K))
#define     VDWI_DUPD(V, K) DWI_DUPD(V, K)
    uint64x1_t m = vreinterpret_u64_s32(v);
    m = vshl_u64(m, vdup_n_s64(0-32*k));
    return  vdup_lane_s32(
        vreinterpret_s32_u64(m),
        0
    );
}

INLINE(Vdwf,VDWF_DUPD) (Vdwf v, Rc(0, 1) k)
{
#define     DWF_DUPD(M, K)  vdup_lane_f32(M, (1&K))
#define     VDWF_DUPD(V, K) DWF_DUPD(V, K)
    uint64x1_t m = vreinterpret_u64_f32(v);
    m = vshl_u64(m, vdup_n_s64(0-32*k));
    return  vdup_lane_f32(
        vreinterpret_f32_u64(m),
        0
    );
}

INLINE(Vddu,VDDU_DUPD) (Vddu v, int const k)
{
#   define  VDDU_DUPD(V, K) vdup_lane_u64(V, 0)
    return  vdup_lane_u64(v, 0);
}

INLINE(Vddi,VDDI_DUPD) (Vddi v, int const k)
{
#   define  VDDI_DUPD(V, K) vdup_lane_s64(V, 0)
    return  vdup_lane_s64(v, 0);
}

INLINE(Vddf,VDDF_DUPD) (Vddf v, int const k)
{
#   define  VDDF_DUPD(V, K) vdup_lane_f64(V, 0)
    return  vdup_lane_f64(v, 0);
}


INLINE(Vdyu,VQYU_DUPD) (Vqyu v, Rc(0,127) k)
{
#define     VQYU_DUPD   VQYU_DUPD
    uint64x2_t  q = VQYU_ASTM(v);
    uint64_t    d;
    if (k > 63) {
        d = vget_lane_u64(vget_high_u64(q), 0);
        d = vtstd_u64(d, UINT64_C(1)<<(k-64));
    }
    else {
        d = vget_lane_u64(vget_low_u64(q), 0);
        d = vtstd_u64(d, UINT64_C(1)<<k);
    }
    return DYU_ASTV(vdup_n_u64(d));
}

INLINE(Vdbu,VQBU_DUPD) (Vqbu v, Rc(0, 15) k)
{
#define     QBU_DUPD(M, K)  vdup_laneq_u8(M, (15&(K)))
#define     VQBU_DUPD(V, K) QBU_DUPD(V, K)
    return  vqtbl1_u8(v, vdup_n_u8(k));
}

INLINE(Vdbi,VQBI_DUPD) (Vqbi v, Rc(0, 15) k)
{
#define     QBI_DUPD(M, K)  vdup_laneq_u8(M, (15&K))
#define     VQBI_DUPD(V, K) QBI_DUPD(V, K)
    return  vqtbl1_u8(v, vdup_n_u8(k));
}

INLINE(Vdbc,VQBC_DUPD) (Vqbc v, Rc(0, 15) k)
{
#define     VQBC_DUPD(V, K) VDBU_ASBC(QBU_DUPD(VQBC_ASBU(V), K))
    return  VDBU_ASBC((VQBU_DUPD)(VQBC_ASBU(v), k));
}

INLINE(Vdhu,VQHU_DUPD) (Vqhu x, Rc(0, 7) k)
{
#define     QHU_DUPD        vdup_laneq_u16
#define     VQHU_DUPD(V, K) QHU_DUPD(V, K)
    return vreinterpret_u16_u8(
        vqtbl1_u8(
            vreinterpretq_u8_u16(x),
            vzip1_u8(
                vdup_n_u8(2*k),
                vdup_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vdhi,VQHI_DUPD) (Vqhi x, Rc(0, 7) k)
{
#define     QHI_DUPD(M, K)  vdup_laneq_s16(M, (7&(K)))
#define     VQHI_DUPD(V, K) QHI_DUPD(V, K)
    return  vreinterpret_s16_u8(
        vqtbl1_u8(
            vreinterpretq_u8_s16(x),
            vzip1_u8(
                vdup_n_u8(2*k),
                vdup_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vdhf,VQHF_DUPD) (Vqhf x, Rc(0, 7) k)
{
#define     QHF_DUPD(M, K)  vdup_laneq_f16(M, (7&(K)))
#define     VQHF_DUPD(V, K) QHF_DUPD(V, K)
    return  vreinterpret_f16_u8(
        vqtbl1_u8(
            vreinterpretq_u8_f16(x),
            vzip1_u8(
                vdup_n_u8(2*k),
                vdup_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vdwu,VQWU_DUPD) (Vqwu v, Rc(0, 3) k)
{
#define     QWU_DUPD(M, K)  vdup_laneq_u32(M, (3&K))
#define     VQWU_DUPD(V, K) QWU_DUPD(V, K)
    uint64x1_t  m;
    if (k > 1) {
        m = vshl_u64(
            vget_high_u64(vreinterpretq_u64_u32(v)),
            vdup_n_s64(0-(k-2)*32)
        );
    }
    else {
        m = vshl_u64(
            vget_low_u64(vreinterpretq_u64_u32(v)),
            vdup_n_s64(0-k*32)
        );
    }
    return  vdup_lane_u32(
        vreinterpret_u32_u64(m),
        0
    );
}

INLINE(Vdwi,VQWI_DUPD) (Vqwi v, Rc(0, 3) k)
{
#define     QWI_DUPD(M, K)  vdup_laneq_s32(V, (3&K))
#define     VQWI_DUPD(V, K) QWI_DUPD(V, K)
    uint64x1_t  m;
    if (k > 1) {
        m = vshl_u64(
            vget_high_u64(vreinterpretq_u64_s32(v)),
            vdup_n_s64(0-(k-2)*32)
        );
    }
    else {
        m = vshl_u64(
            vget_low_u64(vreinterpretq_u64_s32(v)),
            vdup_n_s64(0-k*32)
        );
    }
    return  vdup_lane_s32(
        vreinterpret_s32_u64(m),
        0
    );
}

INLINE(Vdwf,VQWF_DUPD) (Vqwf v, Rc(0, 3) k)
{
#define     QWF_DUPD(M, K)  vdup_laneq_f32(M, (3&K))
#define     VQWF_DUPD(V, K) QWF_DUPD(V, K)
    uint64x1_t  m;
    uint64x2_t  q = vreinterpretq_u64_f32(v);
    if (k > 1)  m = vshl_u64(vget_high_u64(q), vdup_n_s64((k-2)*INT64_C(-32)));
    else        m = vshl_u64(vget_low_u64( q), vdup_n_s64(k*INT64_C(-32)));
    return  vdup_lane_f32(vreinterpret_f32_u64(m), 0);
}

INLINE(Vddu,VQDU_DUPD) (Vqdu v, Rc(0, 1) k)
{
#define     QDU_DUPD(M, K)      vdup_laneq_u64(M, (1&K))
#define     VQDU_DUPD(V, K)     QDU_DUPD(V, K)
    return  k ? vget_high_u64(v) : vget_low_u64(v);
}

INLINE(Vddi,VQDI_DUPD) (Vqdi v, Rc(0, 1) k)
{
#define     QDI_DUPD(M, K)      vdup_laneq_s64(V, (1&K))
#define     VQDI_DUPD(V, K)     QDI_DUPD(V, K)
    return  k ? vget_high_s64(v) : vget_low_s64(v);
}

INLINE(Vddf,VQDF_DUPD) (Vqdf v, Rc(0, 1) k)
{
#define     QDF_DUPD(M, K)      vdup_laneq_f64(M, (1&K))
#define     VQDF_DUPD(V, K)     QDF_DUPD(V, K)
    return  k ? vget_high_f64(v) : vget_low_f64(v);

}


INLINE(Vdyu,  BOOL_DUPDAC)   (_Bool const a[1])
{
    return  BOOL_DUPD( (*(_Bool const *) a) );
}

INLINE(Vdbu, UCHAR_DUPDAC)   (uchar const a[1]) {return vld1_dup_u8(a);}
INLINE(Vdbi, SCHAR_DUPDAC)   (schar const a[1]) {return vld1_dup_s8(a);}
INLINE(Vdbc,  CHAR_DUPDAC)    (char const a[1])
{
    return  VDBU_ASBC(vld1_dup_u8( ((void const *) a) ) );
}

INLINE(Vdhu, USHRT_DUPDAC)  (ushort const a[1]) {return vld1_dup_u16(a);}
INLINE(Vdhi,  SHRT_DUPDAC)   (short const a[1]) {return vld1_dup_s16(a);}
INLINE(Vdwu,  UINT_DUPDAC)    (uint const a[1]) {return vld1_dup_u32(a);}
INLINE(Vdwi,   INT_DUPDAC)     (int const a[1]) {return vld1_dup_s32(a);}

#if DWRD_NLONG == 2

INLINE(Vdwu, ULONG_DUPDAC)   (ulong const a[1])
{
    return  vld1_dup_u32( ((uint32_t const *) a) );
}

INLINE(Vdwi,  LONG_DUPDAC)    (long const a[1])
{
    return  vld1_dup_s32( ((int32_t const *) a) );
}

#else

INLINE(Vddu, ULONG_DUPDAC)   (ulong const a[1]) {return  vld1_dup_u64(a);}
INLINE(Vddi,  LONG_DUPDAC)    (long const a[1]) {return  vld1_dup_s64(a);}

#endif

#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_DUPDAC)  (ullong const a[1])
{
    return  vld1_dup_u64( ((uint64_t const *) a) );
}

INLINE(Vddi, LLONG_DUPDAC)   (llong const a[1])
{
    return  vld1_dup_s64( ((int64_t const *) a) );
}

#endif

INLINE(Vdhf, FLT16_DUPDAC) (flt16_t const a[1]) {return vld1_dup_f16(a);}
INLINE(Vdwf,   FLT_DUPDAC)   (float const a[1]) {return vld1_dup_f32(a);}
INLINE(Vddf,   DBL_DUPDAC)  (double const a[1]) {return vld1_dup_f64(a);}

#if _LEAVE_ARM_DUPD
}
#endif

#if _ENTER_ARM_DUPQ
{
#endif

INLINE(Vqyu,  BOOL_DUPQ)   (_Bool x)
{
#define     BOOL_DUPQ(X)   QYU_ASTV(vdupq_n_u64( (0ull-(X?1:0)) ))
    return  BOOL_DUPQ(x);
}

INLINE(Vqbu, UCHAR_DUPQ)   (uchar x) {return vdupq_n_u8(x);}
INLINE(Vqbi, SCHAR_DUPQ)   (schar x) {return vdupq_n_s8(x);}
INLINE(Vqbc,  CHAR_DUPQ)    (char x)
{
#if CHAR_MIN
#   define  CHAR_DUPQ(X)    VQBI_ASBC(vdupq_n_s8(X))
#else
#   define  CHAR_DUPQ(X)    VQBU_ASBC(vdupq_n_u8(X))
#endif
    return  CHAR_DUPQ(x);
}

INLINE(Vqhu, USHRT_DUPQ)  (ushort x) {return vdupq_n_u16(x);}
INLINE(Vqhi,  SHRT_DUPQ)   (short x) {return vdupq_n_s16(x);}
INLINE(Vqwu,  UINT_DUPQ)    (uint x) {return vdupq_n_u32(x);}
INLINE(Vqwi,   INT_DUPQ)     (int x) {return vdupq_n_s32(x);}
#if QUAD_NLONG == 2
INLINE(Vqwu, ULONG_DUPQ)   (ulong x) {return vdupq_n_u32(x);}
INLINE(Vqwi,  LONG_DUPQ)    (long x) {return vdupq_n_s32(x);}
#else
INLINE(Vqdu, ULONG_DUPQ)   (ulong x) {return vdupq_n_u64(x);}
INLINE(Vqdi,  LONG_DUPQ)    (long x) {return vdupq_n_s64(x);}
#endif

#if QUAD_NLLONG == 2
INLINE(Vqdu,ULLONG_DUPQ)   (ulong x) {return vdupq_n_u64(x);}
INLINE(Vqdi, LLONG_DUPQ)    (long x) {return vdupq_n_s64(x);}
#endif

INLINE(Vqhf, FLT16_DUPQ) (flt16_t x) {return vdupq_n_f16(x);}
INLINE(Vqwf,   FLT_DUPQ)   (float x) {return vdupq_n_f32(x);}
INLINE(Vqdf,   DBL_DUPQ)  (double x) {return vdupq_n_f64(x);}


INLINE(Vqyu,  BOOL_DUPQAC)   (_Bool const a[1]) {return BOOL_DUPQ( (*a));}
INLINE(Vqbu, UCHAR_DUPQAC)   (uchar const a[1]) {return vld1q_dup_u8(a);}
INLINE(Vqbi, SCHAR_DUPQAC)   (schar const a[1]) {return vld1q_dup_s8(a);}
INLINE(Vqbc,  CHAR_DUPQAC)    (char const a[1])
{
    return  VQBU_ASBC(vld1q_dup_u8( ((void const *) a) ) );
}

INLINE(Vqhu, USHRT_DUPQAC)  (ushort const a[1]) {return vld1q_dup_u16(a);}
INLINE(Vqhi,  SHRT_DUPQAC)   (short const a[1]) {return vld1q_dup_s16(a);}
INLINE(Vqwu,  UINT_DUPQAC)    (uint const a[1]) {return vld1q_dup_u32(a);}
INLINE(Vqwi,   INT_DUPQAC)     (int const a[1]) {return vld1q_dup_s32(a);}

#if DWRD_NLONG == 2

INLINE(Vqwu, ULONG_DUPQAC)   (ulong const a[1])
{
    return  vld1q_dup_u32( ((uint32_t const *) a) );
}

INLINE(Vqwi,  LONG_DUPQAC)    (long const a[1])
{
    return  vld1q_dup_s32( ((int32_t const *) a) );
}

#else

INLINE(Vqdu, ULONG_DUPQAC)   (ulong const a[1]) {return vld1q_dup_u64(a);}
INLINE(Vqdi,  LONG_DUPQAC)    (long const a[1]) {return vld1q_dup_s64(a);}

#endif

#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_DUPQAC)  (ullong const a[1])
{
    return  vld1q_dup_u64( ((uint64_t const *) a) );
}

INLINE(Vqdi, LLONG_DUPQAC)   (llong const a[1])
{
    return  vld1q_dup_s64( ((int64_t const *) a) );
}

#endif

INLINE(Vqhf, FLT16_DUPQAC) (flt16_t const a[1]) {return vld1q_dup_f16(a);}
INLINE(Vqwf,   FLT_DUPQAC)   (float const a[1]) {return vld1q_dup_f32(a);}
INLINE(Vqdf,   DBL_DUPQAC)  (double const a[1]) {return vld1q_dup_f64(a);}


INLINE(Vqyu,VWYU_DUPQ) (Vwyu v, Rc(0, 31) k)
{
#define     WYU_DUPQ(M, K)          \
vdupq_n_u64(                        \
    vtstd_u64(                      \
        (UINT64_C(1)<<(K)),         \
        vget_lane_u32(              \
            vreinterpret_u32_f32(   \
                vdup_n_f32(M)       \
            ),                      \
            0                   \
        )                           \
    )                               \
)

#define     VWYU_DUPQ(V, K) QYU_ASTV(WYU_DUPQ(VWYU_ASTM(V), K))
    return  VWYU_DUPQ(v, k);
}


INLINE(Vqbu,VWBU_DUPQ) (Vwbu v, Rc(0,  3) k)
{
#define     WBU_DUPQ(M, K)  \
vdupq_lane_u8(              \
    vreinterpret_u8_f32(    \
        vdup_n_f32(M)       \
    ),                      \
    (3&(K))                 \
)

#define     VWBU_DUPQ(V, K) WBU_DUPQ(VWBU_ASTM(V), K)
    float32x2_t m = vdup_n_f32(VWBU_ASTM(v));
    uint8x8_t   t = vreinterpret_u8_f32(m);
    t = vtbl1_u8(t, vdup_n_u8(k));
    return  vcombine_u8(t, t);
}

INLINE(Vqbi,VWBI_DUPQ) (Vwbi v, Rc(0,  3) k)
{
#define     WBI_DUPQ(M, K)  \
vdupq_lane_s8(              \
    vreinterpret_s8_f32(    \
        vdup_n_f32(M)       \
    ),                      \
    (3&(K))                 \
)
#define     VWBI_DUPQ(V, K) WBI_DUPQ(VWBI_ASTM(V)
    float32x2_t m = vdup_n_f32(VWBI_ASTM(v));
    int8x8_t    t = vreinterpret_s8_f32(m);
    t = vtbl1_s8(t, vdup_n_u8(k));
    return  vcombine_s8(t, t);
}

INLINE(Vqbc,VWBC_DUPQ) (Vwbc v, Rc(0,  3) k)
{
#define     VWBC_DUPQ(V, K) VQBU_ASBC(WBU_DUPQ(VWBC_ASTM(V), K))
    float32x2_t m = vdup_n_f32(VWBC_ASTM(v));
    uint8x8_t   t = vdup_n_u8(k);
#if CHAR_MIN
    int8x8_t    d = vtbl1_s8(vreinterpret_s8_f32(m), t);
    int8x16_t   q = vcombine_s8(d, d);
#else
    uint8x8_t   d = vtbl1_u8(vreinterpret_u8_f32(m), t);
    uint8x16_t  q = vcombine_u8(d, d);
#endif
    return  QBC_ASTV(q);
}


INLINE(Vqhu,VWHU_DUPQ) (Vwhu v, Rc(0, 1) k)
{
#define     WHU_DUPQ(M, K)  \
vdupq_lane_u16(             \
    vreinterpret_u16_f32(   \
        vdup_n_f32(M)       \
    ),                      \
    1&(K)                   \
)

#define     VWHU_DUPQ(V, K) WHU_DUPQ(VWHU_ASTM(V), K)
    return  vreinterpretq_u16_u8(
        vqtbl1q_u8(
            vreinterpretq_u8_f32(
                vdupq_n_f32(VWHU_ASTM(v))
            ),
            vzip1q_u8(
                vdupq_n_u8(2*k),
                vdupq_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vqhi,VWHI_DUPQ) (Vwhi v, Rc(0, 1) k)
{
#define     WHI_DUPQ(M, K)  \
vdupq_lane_s16(             \
    vreinterpret_s16_f32(   \
        vdup_n_f32(M)       \
    ),                      \
    1&(K)                   \
)

#define     VWHI_DUPQ(V, K) WHI_DUPQ(VWHI_ASTM(V), K)
    return  vreinterpretq_s16_u8(
        vqtbl1q_u8(
            vreinterpretq_u8_f32(
                vdupq_n_f32(VWHI_ASTM(v))
            ),
            vzip1q_u8(
                vdupq_n_u8(2*k),
                vdupq_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vqhf,VWHF_DUPQ) (Vwhf v, Rc(0, 1) k)
{
#define     WHF_DUPQ(M, K)      \
vreinterpret_f16_u16(           \
    vdupq_lane_u16(             \
        vreinterpret_u16_f32(   \
            vdup_n_f32(M)       \
        ),                      \
        1&(K)                   \
    )                           \
)

#define     VWHF_DUPQ(V, K) WHF_DUPQ(VWHF_ASTM(V), K)
    return  vreinterpretq_f16_u8(
        vqtbl1q_u8(
            vreinterpretq_u8_f32(
                vdupq_n_f32(VWHF_ASTM(v))
            ),
            vzip1q_u8(
                vdupq_n_u8(2*k),
                vdupq_n_u8(2*k+1)
            )
        )
    );
}


INLINE(Vqwu,VWWU_DUPQ) (Vwwu v, int const k)
{
#define     WWU_DUPQ(M, K)  vreinterpretq_u32_f32(vdupq_n_f32(M))
#define     VWWU_DUPQ(V, K) WWU_DUPQ(VWWU_ASTM(V), K)
    return  VWWU_DUPQ(v, k);
}

INLINE(Vqwi,VWWI_DUPQ) (Vwwi v, int const k)
{
#define     WWI_DUPQ(M, K)  vreinterpretq_s32_f32(vdupq_n_f32(M))
#define     VWWI_DUPQ(V, K) WWI_DUPQ(VWWI_ASTM(V), K)
    return  VWWI_DUPQ(v, k);
}

INLINE(Vqwf,VWWF_DUPQ) (Vwwf v, int const k)
{
#define     WWF_DUPQ(M, K)  vdupq_n_f32(M)
#define     VWWF_DUPQ(V, K) WWF_DUPQ(VWWF_ASTM(V), K)
    return  VWWF_DUPQ(v, k);
}



INLINE(Vqyu,VDYU_DUPQ) (Vdyu v, Rc(0, 63) k)
{
#define     DYU_DUPQ(M, K)  \
vdupq_n_u64(                \
    vtstd_u64(              \
        (UINT64_C(1)<<(K)), \
        vget_lane_u64(      \
            M,              \
            0           \
        )                   \
    )                       \
)

#define     VDYU_DUPQ(V, K) QYU_ASTV(DYU_DUPQ(VDYU_ASTM(V), K))
    return  VDYU_DUPQ(v, k);
}

INLINE(Vqbu,VDBU_DUPQ) (Vdbu v, Rc(0,  7) k)
{
#define     DBU_DUPQ(M, K)  vdupq_lane_u8(M, 7&(K))
/*  TODO: compare the outputs of VDBU_DUPQ && VDBI_DUPQ */
#define     VDBU_DUPQ(V,K)  DBU_DUPQ(V,K)
    return vqtbl1q_u8(
        vcombine_u8(v, v),
        vdupq_n_u8(k)
    );
}

INLINE(Vqbi,VDBI_DUPQ) (Vdbi v, Rc(0,  7) k)
{
#define     DBI_DUPQ(M, K)  vdupq_lane_s8(M, 7&(K))
#define     VDBI_DUPQ(V,K)  vdupq_lane_s8(V, 7&(K))
    v = vtbl1_s8(v, vdup_n_u8(k));
    return  vcombine_s8(v, v);
}

INLINE(Vqbc,VDBC_DUPQ) (Vdbc v, Rc(0,  7) k)
{
#define     VDBC_DUPQ(V, K) VQBU_ASBC(DBU_DUPQ(VDBC_ASBU(V), K))
    return  VQBU_ASBC(
        (VDBU_DUPQ)(VDBC_ASBU(v), k)
    );
}


INLINE(Vqhu,VDHU_DUPQ) (Vdhu v, Rc(0, 3) k)
{
#define     DHU_DUPQ(M, K)  vdupq_lane_u16(M, (3&(K)))
#define     VDHU_DUPQ(V, K) DHU_DUPQ(V, K)
    return  vreinterpretq_u16_u8(
        vqtbl1q_u8(
            vreinterpretq_u8_u16(
                vcombine_u16(v, v)
            ),
            vzip1q_u8(
                vdupq_n_u8(2*k),
                vdupq_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vqhi,VDHI_DUPQ) (Vdhi v, Rc(0, 3) k)
{
#define     DHI_DUPQ(M, K)  vdupq_lane_s16(M, (3&(K)))
#define     VDHI_DUPQ(V, K) DHI_DUPQ(V, K)
    return  vreinterpretq_s16_u8(
        vqtbl1q_u8(
            vreinterpretq_u8_s16(
                vcombine_s16(v, v)
            ),
            vzip1q_u8(
                vdupq_n_u8(2*k),
                vdupq_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vqhf,VDHF_DUPQ) (Vdhf v, Rc(0, 3) k)
{
#define     DHF_DUPQ(M, K)          \
vreinterpretq_f16_u16(              \
    vdupq_lane_u16(                 \
        vreinterpretq_u16_f16(M),   \
        (3&(K))                     \
    )                               \
)
#define     VDHF_DUPQ(V, K) DHF_DUPQ(V, K)
    return  vreinterpretq_f16_u16(
        (VDHU_DUPQ)(vreinterpret_u16_f16(v), k)
    );
}


INLINE(Vqwu,VDWU_DUPQ) (Vdwu v, Rc(0, 1) k)
{
#define     DWU_DUPQ(M, K)  vdupq_lane_u32(M, (1&(K)))
#define     VDWU_DUPQ(V, K) DWU_DUPQ(V, K)
    uint64x1_t  m = vreinterpret_u64_u32(v);
    m = vshl_u64(m, vdup_n_s64(-32*k));
    return  vdupq_lane_u32(
        vreinterpret_u32_u64(m),
        0
    );
}

INLINE(Vqwi,VDWI_DUPQ) (Vdwi v, Rc(0, 1) k)
{
#define     DWI_DUPQ(M, K)  vdupq_lane_s32(M, (1&(K)))
#define     VDWI_DUPQ(V, K) DWI_DUPQ(V, K)
    uint64x1_t  m = vreinterpret_u64_s32(v);
    m = vshl_u64(m, vdup_n_s64(-32*k));
    return  vdupq_lane_s32(
        vreinterpret_s32_u64(m),
        0
    );
}

INLINE(Vqwf,VDWF_DUPQ) (Vdwf v, Rc(0, 1) k)
{
#define     DWF_DUPQ(M, K)  vdupq_lane_f32(M, (1&(K)))
#define     VDWF_DUPQ(V, K) DWF_DUPQ(V, K)
    uint64x1_t  m = vreinterpret_u64_f32(v);
    m = vshl_u64(m, vdup_n_s64(-32*k));
    return  vdupq_lane_f32(
        vreinterpret_f32_u64(m),
        0
    );
}


INLINE(Vqdu,VDDU_DUPQ) (Vddu v, int const k)
{
#   define  VDDU_DUPQ(V, K) vdupq_lane_u64(V, 0)
    return  vdupq_lane_u64(v, 0);
}

INLINE(Vqdi,VDDI_DUPQ) (Vddi v, int const k)
{
#   define  VDDI_DUPQ(V, K) vdupq_lane_s64(V, 0)
    return  vdupq_lane_s64(v, 0);
}

INLINE(Vqdf,VDDF_DUPQ) (Vddf v, int const k)
{
#   define  VDDF_DUPQ(V, K) vdupq_lane_f64(V, 0)
    return  vdupq_lane_f64(v, 0);
}


INLINE(Vqyu,VQYU_DUPQ) (Vqyu v, Rc(0,127) k)
{
    uint64x2_t  q = VQYU_ASTM(v);
    uint64_t    d;
    if (k > 63)
    {
        d = vget_lane_u64(vget_high_u64(q), 0);
        d = vtstd_u64(d, UINT64_C(1)<<(k-64));
    }
    else
    {
        d = vget_lane_u64(vget_low_u64(q), 0);
        d = vtstd_u64(d, UINT64_C(1)<<k);
    }
    return  QYU_ASTV(vdupq_n_u64(d));
}


INLINE(Vqbu,VQBU_DUPQ) (Vqbu v, Rc(0, 15) k)
{
#define     QBU_DUPQ(M, K)  vdupq_laneq_u8(M, 15&(K))
#define     VQBU_DUPQ(V, K) vdupq_laneq_u8(V, 15&(K))
    return  vqtbl1q_u8(v, vdupq_n_u8(k));
}

INLINE(Vqbi,VQBI_DUPQ) (Vqbi v, Rc(0, 15) k)
{
#define     QBI_DUPQ(M, K)  vdupq_laneq_s8(M,(15&(K)))
#define     VQBI_DUPQ(V, K) vdupq_laneq_s8(V,(15&(K)))
    return  vqtbl1q_s8(v, vdupq_n_u8(k));
}

INLINE(Vqbc,VQBC_DUPQ) (Vqbc v, Rc(0, 15) k)
{
#define     VQBC_DUPQ(V, K) VQBU_ASBC(VQBU_DUPQ(VQBC_ASBU(V), K))
    return  VQBU_ASBC((VQBU_DUPQ)(VQBC_ASBU(v), k));
}


INLINE(Vqhu,VQHU_DUPQ) (Vqhu v, Rc(0, 7) k)
{
#define     QHU_DUPQ(M, K)  vdupq_laneq_u16(M,(7&(K)))
#define     VQHU_DUPQ(V, K) QHU_DUPQ(V, K)
    return vreinterpretq_u16_u8(
        vqtbl1q_u8(
            vreinterpretq_u8_u16(v),
            vzip1q_u8(
                vdupq_n_u8(2*k),
                vdupq_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vqhi,VQHI_DUPQ) (Vqhi v, Rc(0, 7) k)
{
#define     QHI_DUPQ(M, K)  vdupq_laneq_s16(M,(7&(K)))
#define     VQHI_DUPQ(V, K) QHI_DUPQ(V, K)
    return vreinterpretq_s16_u8(
        vqtbl1q_u8(
            vreinterpretq_u8_s16(v),
            vzip1q_u8(
                vdupq_n_u8(2*k),
                vdupq_n_u8(2*k+1)
            )
        )
    );
}

INLINE(Vqhf,VQHF_DUPQ) (Vqhf v, Rc(0, 7) k)
{
#define     QHF_DUPQ(M, K)  vdupq_laneq_f16(M, (7&(K)))
#define     VQHF_DUPQ(V, K) QHF_DUPQ(V, K)
    return  vreinterpretq_f16_u8(
        vqtbl1q_u8(
            vreinterpretq_u8_f16(v),
            vzip1q_u8(
                vdupq_n_u8(2*k),
                vdupq_n_u8(2*k+1)
            )
        )
    );
}


INLINE(Vqwu,VQWU_DUPQ) (Vqwu v, Rc(0, 3) k)
{
#define     QWU_DUPQ(V, K)  vdupq_laneq_u32(V, (3&(K)))
#define     VQWU_DUPQ(V, K) QWU_DUPQ(V, K)
    uint64x1_t  m;
    if (k > 1) {
        m = vshl_u64(
            vget_high_u64(vreinterpretq_u64_u32(v)),
            vdup_n_s64((k-2)*INT64_C(-32))
        );
    }
    else {
        m = vshl_u64(
            vget_low_u64(vreinterpretq_u64_u32(v)),
            vdup_n_s64(k*INT64_C(-32))
        );
    }
    return  vdupq_lane_u32(
        vreinterpret_s32_u64(m),
        0
    );
}

INLINE(Vqwi,VQWI_DUPQ) (Vqwi v, Rc(0, 3) k)
{
#define     QWI_DUPQ(V, K)  vdupq_laneq_s32(V, (3&(K)))
#define     VQWI_DUPQ(V, K) QWI_DUPQ(V, K)
    uint64x1_t  m;
    if (k > 1) {
        m = vshl_u64(
            vget_high_u64(vreinterpretq_u64_s32(v)),
            vdup_n_s64((k-2)*INT64_C(-32))
        );
    }
    else {
        m = vshl_u64(
            vget_low_u64(vreinterpretq_u64_s32(v)),
            vdup_n_s64(k*INT64_C(-32))
        );
    }
    return  vdupq_lane_s32(
        vreinterpret_s32_u64(m),
        0
    );
}

INLINE(Vqwf,VQWF_DUPQ) (Vqwf v, Rc(0, 3) k)
{
#define     QWF_DUPQ(V, K)  vdupq_laneq_f32(V, (3&(K)))
#define     VQWF_DUPQ(V, K) QWF_DUPQ(V, K)
    uint64x1_t  m;
    if (k > 1) {
        m = vshl_u64(
            vget_high_u64(vreinterpretq_u64_f32(v)),
            vdup_n_s64((k-2)*INT64_C(-32))
        );
    }
    else {
        m = vshl_u64(
            vget_low_u64(vreinterpretq_u64_f32(v)),
            vdup_n_s64(k*INT64_C(-32))
        );
    }
    return  vdupq_lane_f32(
        vreinterpret_f32_u64(m),
        0
    );
}


INLINE(Vqdu,VQDU_DUPQ) (Vqdu v, Rc(0, 1) k)
{
#   define  VQDU_DUPQ(V, K) vdupq_laneq_u64(V, (1&K))
    return  k ? VQDU_DUPQ(v, 1) : VQDU_DUPQ(v, 0);
}

INLINE(Vqdi,VQDI_DUPQ) (Vqdi v, Rc(0, 1) k)
{
#   define  VQDI_DUPQ(V, K) vdupq_laneq_s64(V, 0)
    return  k ? VQDI_DUPQ(v, 1) : VQDI_DUPQ(v, 0);
}

INLINE(Vqdf,VQDF_DUPQ) (Vqdf v, Rc(0, 1) k)
{
#   define  VQDF_DUPQ(V, K) vdupq_laneq_f64(V, 0)
    return  k ? VQDF_DUPQ(v, 1) : VQDF_DUPQ(v, 0);
}


#if _LEAVE_ARM_DUPQ
}
#endif

#if _ENTER_ARM_DUPL
{
#endif

INLINE(Vwyu,VWYU_DUPL) (Vwyu x)
{
#define     VWYU_DUPL(X) VWYU_DUPW(X, 0)
    return  VWYU_DUPL(x);
}


INLINE(Vwbu,VWBU_DUPL) (Vwbu x) {return VWBU_DUPW(x, 0);}
INLINE(Vwbi,VWBI_DUPL) (Vwbi x) {return VWBI_DUPW(x, 0);}
INLINE(Vwbc,VWBC_DUPL) (Vwbc x) {return VWBC_DUPW(x, 0);}
INLINE(Vwhu,VWHU_DUPL) (Vwhu x) {return VWHU_DUPW(x, 0);}
INLINE(Vwhi,VWHI_DUPL) (Vwhi x) {return VWHI_DUPW(x, 0);}
INLINE(Vwhf,VWHF_DUPL) (Vwhf x) {return VWHF_DUPW(x, 0);}
INLINE(Vwwu,VWWU_DUPL) (Vwwu x) {return VWWU_DUPW(x, 0);}
INLINE(Vwwi,VWWI_DUPL) (Vwwi x) {return VWWI_DUPW(x, 0);}
INLINE(Vwwf,VWWF_DUPL) (Vwwf x) {return VWWF_DUPW(x, 0);}

INLINE(Vdyu,VDYU_DUPL) (Vdyu x)
{
#define     VDYU_DUPL(X) VDDU_ASYU(vtst_u64(vdup_n_u64(1ull),VDYU_ASDU(X)))
    return  VDYU_DUPL(x);
}

INLINE(Vdbu,VDBU_DUPL) (Vdbu x) {return vdup_lane_u8(x, 0);}
INLINE(Vdbi,VDBI_DUPL) (Vdbi x) {return vdup_lane_s8(x, 0);}
INLINE(Vdbc,VDBC_DUPL) (Vdbc x)
{
#   define  VDBC_DUPL(X) VDBU_ASBC(vdup_lane_u8(VDBC_ASBU(X),0))
    return  VDBC_DUPL(x);
}

INLINE(Vdhu,VDHU_DUPL) (Vdhu x) {return vdup_lane_u16(x, 0);}
INLINE(Vdhi,VDHI_DUPL) (Vdhi x) {return vdup_lane_s16(x, 0);}
INLINE(Vdhf,VDHF_DUPL) (Vdhf x)
{
// TODO: verify this convolution is necessary
#if defined(SPC_ARM_FP16_SIMD)
#   define  VDHF_DUPL(X) vdup_lane_f16(x, 0);
#else
#   define  VDHF_DUPL(X) \
vreinterpret_f16_u16(vdup_lane_u16(vreinterpret_u16_f16(X), 0))
#endif
    return VDHF_DUPL(x);
}

INLINE(Vdwu,VDWU_DUPL) (Vdwu x) {return vdup_lane_u32(x, 0);}
INLINE(Vdwi,VDWI_DUPL) (Vdwi x) {return vdup_lane_s32(x, 0);}
INLINE(Vdwf,VDWF_DUPL) (Vdwf x) {return vdup_lane_f32(x, 0);}
INLINE(Vddu,VDDU_DUPL) (Vddu x) {return vdup_lane_u64(x, 0);}
INLINE(Vddi,VDDI_DUPL) (Vddi x) {return vdup_lane_s64(x, 0);}
INLINE(Vddf,VDDF_DUPL) (Vddf x) {return vdup_lane_f64(x, 0);}


INLINE(Vqyu,VQYU_DUPL) (Vqyu x)
{
#define     QYU_DUPL(M) \
vdupq_lane_u64(         \
    vtst_u64(           \
        vget_low_u64(M),\
        vdup_n_u64(1ull)\
    ),                  \
    0                   \
)

#define     VQYU_DUPL(X) VQDU_ASYU(QYU_DUPL(VQYU_ASTM(X)))
    return  VQYU_DUPL(x);
}

INLINE(Vqbu,VQBU_DUPL) (Vqbu x) {return vdupq_laneq_u8(x, 0);}
INLINE(Vqbi,VQBI_DUPL) (Vqbi x) {return vdupq_laneq_s8(x, 0);}
INLINE(Vqbc,VQBC_DUPL) (Vqbc x)
{
#   define  VQBC_DUPL(X) VQBU_ASBC(vdupq_laneq_u8(VQBC_ASBU(X),0))
    return  VQBC_DUPL(x);
}

INLINE(Vqhu,VQHU_DUPL) (Vqhu x) {return vdupq_laneq_u16(x, 0);}
INLINE(Vqhi,VQHI_DUPL) (Vqhi x) {return vdupq_laneq_s16(x, 0);}
INLINE(Vqhf,VQHF_DUPL) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
#   define  VQHF_DUPL(X) vdupq_laneq_f16(x, 0);
#else
#   define  VQHF_DUPL(X) \
vreinterpretq_f16_u16(vdupq_laneq_u16(vreinterpretq_u16_f16(X), 0))
#endif
    return VQHF_DUPL(x);
}

INLINE(Vqwu,VQWU_DUPL) (Vqwu x) {return vdupq_laneq_u32(x, 0);}
INLINE(Vqwi,VQWI_DUPL) (Vqwi x) {return vdupq_laneq_s32(x, 0);}
INLINE(Vqwf,VQWF_DUPL) (Vqwf x) {return vdupq_laneq_f32(x, 0);}
INLINE(Vqdu,VQDU_DUPL) (Vqdu x) {return vdupq_laneq_u64(x, 0);}
INLINE(Vqdi,VQDI_DUPL) (Vqdi x) {return vdupq_laneq_s64(x, 0);}
INLINE(Vqdf,VQDF_DUPL) (Vqdf x) {return vdupq_laneq_f64(x, 0);}


#if _LEAVE_ARM_DUPL
}
#endif

#if _ENTER_ARM_ZIPL
{
#endif

INLINE(float,WBU_ZIPL) (float a, float b)
{
    uint8x8_t l = vreinterpret_u8_f32(vdup_n_f32(a));
    uint8x8_t r = vreinterpret_u8_f32(vdup_n_f32(b));
    uint8x8_t c = vzip1_u8(l, r);
    return  vget_lane_f32(vreinterpret_f32_u8(c), 0);
}

INLINE(float,WHU_ZIPL) (float a, float b)
{
    uint16x4_t l = vreinterpret_u16_f32(vdup_n_f32(a));
    uint16x4_t r = vreinterpret_u16_f32(vdup_n_f32(b));
    uint16x4_t c = vzip1_u16(l, r);
    return  vget_lane_f32(vreinterpret_f32_u16(c), 0);
}

INLINE(uint16_t, UCHAR_ZIPP) (uchar a, uchar b)
{
    unsigned x = a;
    x = 0x0f0f&(x|(x<<4));
    x = 0x3333&(x|(x<<2));
    x = 0x5555&(x|(x<<1));
    unsigned y = b;
    y = 0x0f0f&(y|(y<<4));
    y = 0x3333&(y|(y<<2));
    y = 0x5555&(y|(y<<1));
    return x|(y<<1);
}

INLINE(uint16_t, CHAR_ZIPP) (char a, char b)
{
    return  UCHAR_ZIPP(a, b);
}

INLINE(uint32_t, USHRT_ZIPP) (ushort a, ushort b)
{
    uint32_t x = a;
    x = UINT32_C(0x00ff00ff)&(x|(x<<8));
    x = UINT32_C(0x0f0f0f0f)&(x|(x<<4));
    x = UINT32_C(0x33333333)&(x|(x<<2));
    x = UINT32_C(0x55555555)&(x|(x<<1));
    uint32_t y = b;
    y = UINT32_C(0x00ff00ff)&(y|(y<<8));
    y = UINT32_C(0x0f0f0f0f)&(y|(y<<4));
    y = UINT32_C(0x33333333)&(y|(y<<2));
    y = UINT32_C(0x55555555)&(y|(y<<1));
    return x|(y<<1);
}

INLINE(uint64_t, UINT_ZIPP) (uint a, uint b)
{
    uint64_t x = a;
    x = UINT64_C(0x0000ffff0000ffff)&(x|(x<<16));
    x = UINT64_C(0x00ff00ff00ff00ff)&(x|(x<<8));
    x = UINT64_C(0x0f0f0f0f0f0f0f0f)&(x|(x<<4));
    x = UINT64_C(0x3333333333333333)&(x|(x<<2));
    x = UINT64_C(0x5555555555555555)&(x|(x<<1));
    uint64_t y = b;
    y = UINT64_C(0x0000ffff0000ffff)&(y|(y<<16));
    y = UINT64_C(0x00ff00ff00ff00ff)&(y|(y<<8));
    y = UINT64_C(0x0f0f0f0f0f0f0f0f)&(y|(y<<4));
    y = UINT64_C(0x3333333333333333)&(y|(y<<2));
    y = UINT64_C(0x5555555555555555)&(y|(y<<1));
    return x|(y<<1);
}

#if DWRD_NLONG == 2

INLINE(uint64_t, ULONG_ZIPP) (ulong a, ulong b)
{
    return  UINT_ZIPP(a, b);
}

#endif

INLINE(uint64_t, UINT64_ZIPL) (uint64_t a, uint64_t b)
{
    return  UINT_ZIPP(a, b);
}

INLINE(uint64_t, UINT64_ZIPR) (uint64_t a, uint64_t b)
{
    return UINT_ZIPP((a>>32), (b>>32));
}


INLINE(Vwyu,VWYU_ZIPL) (Vwyu a, Vwyu b)
{
    uint32_t x = FLT_ASTU(VWYU_ASTM(a));
    uint32_t y = FLT_ASTU(VWYU_ASTM(b));
    uint32x2_t z = vdup_n_u32(USHRT_ZIPP(x, y));
    return WYU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(z),
            0
        )
    );
}


INLINE(Vwbu,VWBU_ZIPL) (Vwbu a, Vwbu b)
{
    return  WBU_ASTV(WBU_ZIPL(VWBU_ASTM(b), VWBU_ASTM(b)));
}

INLINE(Vwbi,VWBI_ZIPL) (Vwbi a, Vwbi b)
{
    return  WBI_ASTV(WBU_ZIPL(VWBI_ASTM(a), VWBI_ASTM(a)));
}

INLINE(Vwbc,VWBC_ZIPL) (Vwbc a, Vwbc b)
{
    return  WBC_ASTV(WBU_ZIPL(VWBC_ASTM(a), VWBC_ASTM(a)));
}


INLINE(Vwhu,VWHU_ZIPL) (Vwhu a, Vwhu b)
{
    return  WHU_ASTV(WHU_ZIPL(VWHU_ASTM(b), VWHU_ASTM(b)));
}

INLINE(Vwhi,VWHI_ZIPL) (Vwhi a, Vwhi b)
{
    return  WHI_ASTV(WHU_ZIPL(VWHI_ASTM(b), VWHI_ASTM(b)));
}

INLINE(Vwhf,VWHF_ZIPL) (Vwhf a, Vwhf b)
{
    return  WHF_ASTV(WHU_ZIPL(VWHF_ASTM(b), VWHF_ASTM(b)));
}


INLINE(Vdyu,VDYU_ZIPL) (Vdyu a, Vdyu b)
{
    return  VDDU_ASYU(
        vdup_n_u64(
            UINT_ZIPP(
                vget_lane_u64(VDYU_ASDU(a), 0),
                vget_lane_u64(VDYU_ASDU(b), 0)
            )
        )
    );
}

INLINE(Vdbu,VDBU_ZIPL) (Vdbu a, Vdbu b) {return vzip1_u8(a, b);}
INLINE(Vdbi,VDBI_ZIPL) (Vdbi a, Vdbi b) {return vzip1_s8(a, b);}
INLINE(Vdbc,VDBC_ZIPL) (Vdbc a, Vdbc b)
{
    return  VDBU_ASBC(vzip1_u8(VDBC_ASBU(a), VDBC_ASBU(b)));
}

INLINE(Vdhu,VDHU_ZIPL) (Vdhu a, Vdhu b) {return vzip1_u16(a, b);}
INLINE(Vdhi,VDHI_ZIPL) (Vdhi a, Vdhi b) {return vzip1_s16(a, b);}
INLINE(Vdhf,VDHF_ZIPL) (Vdhf a, Vdhf b)
{
    return  VDHU_ASHF(vzip1_u16(VDHF_ASHU(a), VDHF_ASHU(b)));
}

INLINE(Vdwu,VDWU_ZIPL) (Vdwu a, Vdwu b) {return vzip1_u32(a, b);}
INLINE(Vdwi,VDWI_ZIPL) (Vdwi a, Vdwi b) {return vzip1_s32(a, b);}
INLINE(Vdwf,VDWF_ZIPL) (Vdwf a, Vdwf b) {return vzip1_f32(a, b);}


INLINE(Vqyu,VQYU_ZIPL) (Vqyu a, Vqyu b)
{
    uint64_t p = vget_lane_u64(vget_low_u64(VQYU_ASTM(a)), 0);
    uint64_t q = vget_lane_u64(vget_low_u64(VQYU_ASTM(b)), 0);
    uint64_t l = UINT_ZIPP(p, q);
    uint64_t r = UINT_ZIPP(p>>32, q>>32);
    return  VQDU_ASYU(
        vcombine_u64(
            vdup_n_u64(l),
            vdup_n_u64(r)
        )
    );
}

INLINE(Vqbu,VQBU_ZIPL) (Vqbu a, Vqbu b) {return vzip1q_u8(a, b);}
INLINE(Vqbi,VQBI_ZIPL) (Vqbi a, Vqbi b) {return vzip1q_s8(a, b);}
INLINE(Vqbc,VQBC_ZIPL) (Vqbc a, Vqbc b)
{
    return  VQBU_ASBC(vzip1q_u8(VQBC_ASBU(a), VQBC_ASBU(b)));
}

INLINE(Vqhu,VQHU_ZIPL) (Vqhu a, Vqhu b) {return vzip1q_u16(a, b);}
INLINE(Vqhi,VQHI_ZIPL) (Vqhi a, Vqhi b) {return vzip1q_s16(a, b);}
INLINE(Vqhf,VQHF_ZIPL) (Vqhf a, Vqhf b)
{
    return  VQHU_ASHF(vzip1q_u16(VQHF_ASHU(a), VQHF_ASHU(b)));
}

INLINE(Vqwu,VQWU_ZIPL) (Vqwi a, Vqwi b) {return vzip1q_u32(a, b);}
INLINE(Vqwi,VQWI_ZIPL) (Vqwu a, Vqwu b) {return vzip1q_s32(a, b);}
INLINE(Vqwf,VQWF_ZIPL) (Vqwf a, Vqwf b) {return vzip1q_f32(a, b);}

INLINE(Vqdu,VQDU_ZIPL) (Vqdi a, Vqdi b) {return vzip1q_u64(a, b);}
INLINE(Vqdi,VQDI_ZIPL) (Vqdu a, Vqdu b) {return vzip1q_s64(a, b);}
INLINE(Vqdf,VQDF_ZIPL) (Vqdf a, Vqdf b) {return vzip1q_f64(a, b);}

#if _LEAVE_ARM_ZIPL
}
#endif

#if _ENTER_ARM_ZIPR
{
#endif

INLINE(float, MY_ZIPRW) (float a, float b)
{
    float32x2_t c;
    c = vset_lane_f32(a, c, 0);
    c = vset_lane_f32(b, c, V2_K1);
    return vget_lane_f32(vzip2_f32(c, c), 0);
}

//efine     WYU_ZIPR
#define     WBU_ZIPR    MY_ZIPRW
#define     WBI_ZIPR    MY_ZIPRW
#define     WBC_ZIPR    MY_ZIPRW
#define     WHU_ZIPR    MY_ZIPRW
#define     WHI_ZIPR    MY_ZIPRW
#define     WHF_ZIPR    MY_ZIPRW
//efine     WWU_ZIPR
//efine     WWI_ZIPR
//efine     WWF_ZIPR

#define     DBU_ZIPR    vzip2_u8
#define     DBI_ZIPR    vzip2_s8
#if CHAR_MIN
#   define  DBC_ZIPR    vzip2_s8
#else
#   define  DBC_ZIPR    vzip2_u8
#endif

#define     DHU_ZIPR    vzip2_u16
#define     DHI_ZIPR    vzip2_s16
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_ZIPR    vzip2_f16
#else
#   define  DHF_ZIPR(A, B)      \
vreinterpret_f16_u16(           \
    vzip2_u16(                  \
        vreinterpret_u16_f16(A),\
        vreinterpret_u16_f16(A) \
    )                           \
)
#endif

#define     DWU_ZIPR    vzip2_u32
#define     DWI_ZIPR    vzip2_s32
#define     DWF_ZIPR    vzip2_f32


#define     QBU_ZIPR    vzip2q_u8
#define     QBI_ZIPR    vzip2q_s8
#if CHAR_MIN
#   define  QBC_ZIPR    vzip2q_s8
#else
#   define  QBC_ZIPR    vzip2q_u8
#endif

#define     QHU_ZIPR    vzip2q_u16
#define     QHI_ZIPR    vzip2q_s16

#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_ZIPR    vzip2q_f16
#else
#   define  QHF_ZIPR(A, B)          \
vreinterpretq_f16_u16(              \
    vzip2q_u16(                     \
        vreinterpretq_u16_f16(A),   \
        vreinterpretq_u16_f16(A)    \
    )                               \
)
#endif

#define     QWU_ZIPR    vzip2q_u32
#define     QWI_ZIPR    vzip2q_s32
#define     QWF_ZIPR    vzip2q_f32
#define     QDU_ZIPR    vzip2q_u64
#define     QDI_ZIPR    vzip2q_s64
#define     QDF_ZIPR    vzip2q_f64


INLINE(Vwbu,VWBU_ZIPR) (Vwbu a, Vwbu b)
{
#define     VWBU_ZIPR(A, B) WBU_ASTV(WBU_ZIPR(VWBU_ASTM(A), VWBU_ASTM(B)))
    return  VWBU_ZIPR(a, b);
}

INLINE(Vwbi,VWBI_ZIPR) (Vwbi a, Vwbi b)
{
#define     VWBI_ZIPR(A, B) WBI_ASTV(WBI_ZIPR(VWBI_ASTM(A), VWBI_ASTM(B)))
    return  VWBI_ZIPR(a, b);
}

INLINE(Vwbc,VWBC_ZIPR) (Vwbc a, Vwbc b)
{
#define     VWBC_ZIPR(A, B) WBC_ASTV(WBC_ZIPR(VWBC_ASTM(A), VWBC_ASTM(B)))
    return  VWBC_ZIPR(a, b);
}


INLINE(Vwhu,VWHU_ZIPR) (Vwhu a, Vwhu b)
{
#define     VWHU_ZIPR(A, B) WHU_ASTV(WHU_ZIPR(VWHU_ASTM(A), VWHU_ASTM(B)))
    return  VWHU_ZIPR(a, b);
}

INLINE(Vwhi,VWHI_ZIPR) (Vwhi a, Vwhi b)
{
#define     VWHI_ZIPR(A, B) WHI_ASTV(WHI_ZIPR(VWHI_ASTM(A), VWHI_ASTM(B)))
    return  VWHI_ZIPR(a, b);
}

INLINE(Vwhf,VWHF_ZIPR) (Vwhf a, Vwhf b)
{
#define     VWHF_ZIPR(A, B) WHF_ASTV(WHF_ZIPR(VWHF_ASTM(A), VWHF_ASTM(B)))
    return  VWHF_ZIPR(a, b);
}


INLINE(Vdbu,VDBU_ZIPR) (Vdbu a, Vdbu b) {return vzip2_u8(a, b);}
INLINE(Vdbi,VDBI_ZIPR) (Vdbi a, Vdbi b) {return vzip2_s8(a, b);}
INLINE(Vdbc,VDBC_ZIPR) (Vdbc a, Vdbc b)
{
    return  VDBU_ASBC(
        vzip2_u8(
            VDBC_ASBU(a),
            VDBC_ASBU(b)
        )
    );
}

INLINE(Vdhu,VDHU_ZIPR) (Vdhu a, Vdhu b) {return vzip2_u16(a, b);}
INLINE(Vdhi,VDHI_ZIPR) (Vdhi a, Vdhi b) {return vzip2_s16(a, b);}
INLINE(Vdhf,VDHF_ZIPR) (Vdhf a, Vdhf b)
{
    return vreinterpret_f16_u16(
        vzip2_u16(
            vreinterpret_u16_f16(a),
            vreinterpret_u16_f16(b)
        )
    );
}

INLINE(Vdwu,VDWU_ZIPR) (Vdwu a, Vdwu b) {return vzip2_u32(a, b);}
INLINE(Vdwi,VDWI_ZIPR) (Vdwi a, Vdwi b) {return vzip2_s32(a, b);}
INLINE(Vdwf,VDWF_ZIPR) (Vdwf a, Vdwf b) {return vzip2_f32(a, b);}

INLINE(Vqbu,VQBU_ZIPR) (Vqbu a, Vqbu b) {return vzip2q_u8(a, b);}
INLINE(Vqbi,VQBI_ZIPR) (Vqbi a, Vqbi b) {return vzip2q_s8(a, b);}
INLINE(Vqbc,VQBC_ZIPR) (Vqbc a, Vqbc b)
{
    return  VQBU_ASBC(
        vzip2q_u8(
            VQBC_ASBU(a),
            VQBC_ASBU(b)
        )
    );
}

INLINE(Vqhu,VQHU_ZIPR) (Vqhu a, Vqhu b) {return vzip2q_u16(a, b);}
INLINE(Vqhi,VQHI_ZIPR) (Vqhi a, Vqhi b) {return vzip2q_s16(a, b);}
INLINE(Vqhf,VQHF_ZIPR) (Vqhf a, Vqhf b)
{
    return vreinterpretq_f16_u16(
        vzip2q_u16(
            vreinterpretq_u16_f16(a),
            vreinterpretq_u16_f16(b)
        )
    );
}

INLINE(Vqwu,VQWU_ZIPR) (Vqwu a, Vqwu b) {return vzip2q_u32(a, b);}
INLINE(Vqwi,VQWI_ZIPR) (Vqwi a, Vqwi b) {return vzip2q_s32(a, b);}
INLINE(Vqwf,VQWF_ZIPR) (Vqwf a, Vqwf b) {return vzip2q_f32(a, b);}

INLINE(Vqdu,VQDU_ZIPR) (Vqdu a, Vqdu b) {return vzip2q_u64(a, b);}
INLINE(Vqdi,VQDI_ZIPR) (Vqdi a, Vqdi b) {return vzip2q_s64(a, b);}
INLINE(Vqdf,VQDF_ZIPR) (Vqdf a, Vqdf b) {return vzip2q_f64(a, b);}

#if _LEAVE_ARM_ZIPR
}
#endif

#if _ENTER_ARM_ZIPP
{
#endif

INLINE(Vdyu,VWYU_ZIPP) (Vwyu a, Vwyu b)
{
    unsigned p = VWWU_ASTV(VWYU_ASWU(a));
    unsigned q = VWWU_ASTV(VWYU_ASWU(b));
    return  VDDU_ASYU(UINT64_ASTV(UINT_ZIPP(p, q)));
}

INLINE(Vdbu,VWBU_ZIPP) (Vwbu a, Vwbu b)
{
    float32x2_t l = vdup_n_f32(VWBU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBU_ASTM(b));
    return  vzip1_u8(VDWF_ASBU(l), VDWF_ASBU(r));
}

INLINE(Vdbi,VWBI_ZIPP) (Vwbi a, Vwbi b)
{
    float32x2_t l = vdup_n_f32(VWBI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBI_ASTM(b));
    return  vzip1_s8(VDWF_ASBI(l), VDWF_ASBI(r));
}

INLINE(Vdbc,VWBC_ZIPP) (Vwbc a, Vwbc b)
{
    float32x2_t l = vdup_n_f32(VWBC_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBC_ASTM(b));
    return  VDBU_ASBC(vzip1_u8(VDWF_ASBU(l), VDWF_ASBU(r)));
}


INLINE(Vdhu,VWHU_ZIPP) (Vwhu a, Vwhu b)
{
    float32x2_t l = vdup_n_f32(VWHU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHU_ASTM(b));
    return  vzip1_u16(VDWF_ASHU(l), VDWF_ASHU(r));
}

INLINE(Vdhi,VWHI_ZIPP) (Vwhi a, Vwhi b)
{
    float32x2_t l = vdup_n_f32(VWHI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHI_ASTM(b));
    return  vzip1_s16(VDWF_ASHI(l), VDWF_ASHI(r));
}

INLINE(Vdhf,VWHF_ZIPP) (Vwhf a, Vwhf b)
{
    float32x2_t l = vdup_n_f32(VWHF_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHF_ASTM(b));
    return  VDHU_ASHF(vzip1_u16(VDWF_ASHU(l), VDWF_ASHU(r)));
}


INLINE(Vdwf,VWWF_ZIPP) (Vwwf a, Vwwf b)
{
#define     WWF_ZIPP(A, B) vset_lane_f32(B,vdup_n_f32(A),0)
#define     VWWF_ZIPP(A, B) WWF_ZIPP(VWWF_ASTM(A),VWWF_ASTM(B))
    return  VWWF_ZIPP(a, b);
}

INLINE(Vdwu,VWWU_ZIPP) (Vwwu a, Vwwu b)
{
#define     VWWU_ZIPP(A, B) VDWF_ASWU(WWF_ZIPP(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_ZIPP(a, b);
}

INLINE(Vdwi,VWWI_ZIPP) (Vwwi a, Vwwi b)
{
#define     VWWI_ZIPP(A, B) VDWF_ASWI(WWF_ZIPP(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_ZIPP(a, b);
}


INLINE(Vqyu,VDYU_ZIPP) (Vdyu a, Vdyu b)
{
    uint64_t x = vget_lane_u64(VDYU_ASDU(a), 0);
    uint64_t y = vget_lane_u64(VDYU_ASDU(b), 0);
    uint64x1_t l = vdup_n_u64(UINT64_ZIPL(x, y));
    uint64x1_t r = vdup_n_u64(UINT64_ZIPR(x, y));
    return  QYU_ASTV(vcombine_u64(l, r));
}

INLINE(Vqbu,VDBU_ZIPP) (Vdbu a, Vdbu b)
{
    return  vcombine_u8(vzip1_u8(a, b), vzip2_u8(a, b));
}

INLINE(Vqbi,VDBI_ZIPP) (Vdbi a, Vdbi b)
{
    return  vcombine_s8(vzip1_s8(a, b), vzip2_s8(a, b));
}

INLINE(Vqbc,VDBC_ZIPP) (Vdbc a, Vdbc b)
{
#define     VDBC_ZIPP(A, B)  VQBU_ASBC(VDBU_ZIPP(VDBC_ASBU(A),VDBC_ASBU(B)))
    return  VDBC_ZIPP(a, b);
}

INLINE(Vqhu,VDHU_ZIPP) (Vdhu a, Vdhu b)
{
    return  vcombine_u16(vzip1_u16(a, b), vzip2_u16(a, b));
}

INLINE(Vqhi,VDHI_ZIPP) (Vdhi a, Vdhi b)
{
    return  vcombine_s16(vzip1_s16(a, b), vzip2_s16(a, b));
}

INLINE(Vqhf,VDHF_ZIPP) (Vdhf a, Vdhf b)
{
#define     VDHF_ZIPP(A, B)  VQHU_ASHF(VDHU_ZIPP(VDHF_ASHU(A),VDHF_ASHU(B)))
    return  VDHF_ZIPP(a, b);
}


INLINE(Vqwu,VDWU_ZIPP) (Vdwu a, Vdwu b)
{
    return  vcombine_u32(vzip1_u32(a, b), vzip2_u32(a, b));
}

INLINE(Vqwi,VDWI_ZIPP) (Vdwi a, Vdwi b)
{
    return  vcombine_s32(vzip1_s32(a, b), vzip2_s32(a, b));
}

INLINE(Vqwf,VDWF_ZIPP) (Vdwf a, Vdwf b)
{
    return  vcombine_f32(vzip1_f32(a, b), vzip2_f32(a, b));
}

INLINE(Vqdu,VDDU_ZIPP) (Vddu a, Vddu b) {return vcombine_u64(a, b);}
INLINE(Vqdi,VDDI_ZIPP) (Vddi a, Vddi b) {return vcombine_s64(a, b);}
INLINE(Vqdf,VDDF_ZIPP) (Vddf a, Vddf b) {return vcombine_f64(a, b);}

#if _LEAVE_ARM_ZIPP
}
#endif

#if _ENTER_ARM_UZPL
{
#endif

INLINE(Vdyu,VDYU_UZPL) (Vdyu a, Vdyu b)
{
    uint64x1_t  x = VDYU_ASTM(a);

    // clear a's odd bits
    x = vand_u64(vdup_n_u64(0x5555555555555555ull), x);

    // x = 0x3333333333333333&(x|(x>>1));
    x = vand_u64(
        vdup_n_u64(0x3333333333333333ull),
        vorr_u64(x, vshr_n_u64(x, 1))
    );

    // x = 0x0f0f0f0f0f0f0f0f&(x|(x>>2));
    x = vand_u64(
        vdup_n_u64(0x0f0f0f0f0f0f0f0full),
        vorr_u64(x, vshr_n_u64(x, 2))
    );

    // x = 0x00ff00ff00ff00ff&(x|(x>>4));
    x = vand_u64(
        vdup_n_u64(0x00ff00ff00ff00ffull),
        vorr_u64(x, vshr_n_u64(x, 4))
    );

    // x = 0x0000ffff0000ffff&(x|(x>>8));
    x = vand_u64(
        vdup_n_u64(0x0000ffff0000ffffull),
        vorr_u64(x, vshr_n_u64(x, 8))
    );

    uint32x2_t w = vreinterpret_u32_u64(x);
    x = VDYU_ASTM(b);

    // clear b's odd bits
    x = vand_u64(vdup_n_u64(0x5555555555555555ull), x);

    x = vand_u64(
        vdup_n_u64(0x3333333333333333ull),
        vorr_u64(x, vshr_n_u64(x, 1))
    );

    x = vand_u64(
        vdup_n_u64(0x0f0f0f0f0f0f0f0full),
        vorr_u64(x, vshr_n_u64(x, 2))
    );

    x = vand_u64(
        vdup_n_u64(0x00ff00ff00ff00ffull),
        vorr_u64(x, vshr_n_u64(x, 4))
    );

    x = vand_u64(
        vdup_n_u64(0x0000ffff0000ffffull),
        vorr_u64(x, vshr_n_u64(x, 8))
    );
    w = vset_lane_u32(
        vget_lane_u64(x, 0),
        w,
        V2_K1
    );
    x = vreinterpret_u64_u32(w);
    return  VDDU_ASYU(x);
}

INLINE(Vdbu,VDBU_UZPL) (Vdbu a, Vdbi b) {return vuzp1_u8(a, b);}
INLINE(Vdbi,VDBI_UZPL) (Vdbi a, Vdbi b) {return vuzp1_s8(a, b);}

INLINE(Vdbc,VDBC_UZPL) (Vdbc a, Vdbc b)
{
    return  VDBU_ASBC(vuzp1_u8(VDBC_ASBU(a),VDBC_ASBU(b)));
}

INLINE(Vdhu,VDHU_UZPL) (Vdhu a, Vdhu b) {return vuzp1_u16(a, b);}
INLINE(Vdhi,VDHI_UZPL) (Vdhi a, Vdhi b) {return vuzp1_s16(a, b);}
INLINE(Vdhf,VDHF_UZPL) (Vdhf a, Vdhf b)
{
    return  vreinterpret_f16_u16(
        vuzp1_u16(
            vreinterpret_u16_f16(a),
            vreinterpret_u16_f16(b)
        )
    );
}

INLINE(Vdwu,VDWU_UZPL) (Vdwu a, Vdwu b) {return vuzp1_u32(a, b);}
INLINE(Vdwi,VDWI_UZPL) (Vdwi a, Vdwi b) {return vuzp1_s32(a, b);}
INLINE(Vdwf,VDWF_UZPL) (Vdwf a, Vdwf b) {return vuzp1_f32(a, b);}

INLINE(Vqbu,VQBU_UZPL) (Vqbu a, Vqbu b) {return vuzp1q_u8(a, b);}
INLINE(Vqbi,VQBI_UZPL) (Vqbi a, Vqbi b) {return vuzp1q_s8(a, b);}

INLINE(Vqbc,VQBC_UZPL) (Vqbc a, Vqbc b)
{
    return  VQBU_ASBC(vuzp1q_u8(VQBC_ASBU(a),VQBC_ASBU(b)));
}

INLINE(Vqhu,VQHU_UZPL) (Vqhu a, Vqhu b) {return vuzp1q_u16(a, b);}
INLINE(Vqhi,VQHI_UZPL) (Vqhi a, Vqhi b) {return vuzp1q_s16(a, b);}
INLINE(Vqhf,VQHF_UZPL) (Vqhf a, Vqhf b)
{
    return  vreinterpretq_f16_u16(
        vuzp1q_u16(
            vreinterpretq_u16_f16(a),
            vreinterpretq_u16_f16(b)
        )
    );
}

INLINE(Vqwu,VQWU_UZPL) (Vqwu a, Vqwu b) {return vuzp1q_u32(a, b);}
INLINE(Vqwi,VQWI_UZPL) (Vqwi a, Vqwi b) {return vuzp1q_s32(a, b);}
INLINE(Vqwf,VQWF_UZPL) (Vqwf a, Vqwf b) {return vuzp1q_f32(a, b);}

INLINE(Vqdu,VQDU_UZPL) (Vqdu a, Vqdu b) {return vuzp1q_u64(a, b);}
INLINE(Vqdi,VQDI_UZPL) (Vqdi a, Vqdi b) {return vuzp1q_s64(a, b);}
INLINE(Vqdf,VQDF_UZPL) (Vqdf a, Vqdf b) {return vuzp1q_f64(a, b);}

#if _LEAVE_ARM_UZPL
}
#endif

#if _ENTER_ARM_UZPR
{
#endif

INLINE(Vdbu,VDBU_UZPR) (Vdbu a, Vdbu b) {return vuzp2_u8(a, b);}
INLINE(Vdbi,VDBI_UZPR) (Vdbi a, Vdbi b) {return vuzp2_s8(a, b);}

INLINE(Vdbc,VDBC_UZPR) (Vdbc a, Vdbc b)
{
    return  VDBU_ASBC(vuzp2_u8(VDBC_ASBU(a),VDBC_ASBU(b)));
}

INLINE(Vdhu,VDHU_UZPR) (Vdhu a, Vdhu b) {return vuzp2_u16(a, b);}
INLINE(Vdhi,VDHI_UZPR) (Vdhi a, Vdhi b) {return vuzp2_s16(a, b);}
INLINE(Vdhf,VDHF_UZPR) (Vdhf a, Vdhf b)
{
    return  vreinterpret_f16_u16(
        vuzp2_u16(
            vreinterpret_u16_f16(a),
            vreinterpret_u16_f16(b)
        )
    );
}

INLINE(Vdwu,VDWU_UZPR) (Vdwu a, Vdwu b) {return vuzp2_u32(a, b);}
INLINE(Vdwi,VDWI_UZPR) (Vdwi a, Vdwi b) {return vuzp2_s32(a, b);}
INLINE(Vdwf,VDWF_UZPR) (Vdwf a, Vdwf b) {return vuzp2_f32(a, b);}

INLINE(Vqbu,VQBU_UZPR) (Vqbu a, Vqbu b) {return vuzp2q_u8(a, b);}
INLINE(Vqbi,VQBI_UZPR) (Vqbi a, Vqbi b) {return vuzp2q_s8(a, b);}

INLINE(Vqbc,VQBC_UZPR) (Vqbc a, Vqbc b)
{
    return  VQBU_ASBC(vuzp2q_u8(VQBC_ASBU(a),VQBC_ASBU(b)));
}

INLINE(Vqhu,VQHU_UZPR) (Vqhu a, Vqhu b) {return vuzp2q_u16(a, b);}
INLINE(Vqhi,VQHI_UZPR) (Vqhi a, Vqhi b) {return vuzp2q_s16(a, b);}
INLINE(Vqhf,VQHF_UZPR) (Vqhf a, Vqhf b)
{
    return  vreinterpretq_f16_u16(
        vuzp2q_u16(
            vreinterpretq_u16_f16(a),
            vreinterpretq_u16_f16(b)
        )
    );
}

INLINE(Vqwu,VQWU_UZPR) (Vqwu a, Vqwu b) {return vuzp2q_u32(a, b);}
INLINE(Vqwi,VQWI_UZPR) (Vqwi a, Vqwi b) {return vuzp2q_s32(a, b);}
INLINE(Vqwf,VQWF_UZPR) (Vqwf a, Vqwf b) {return vuzp2q_f32(a, b);}

INLINE(Vqdu,VQDU_UZPR) (Vqdu a, Vqdu b) {return vuzp2q_u64(a, b);}
INLINE(Vqdi,VQDI_UZPR) (Vqdi a, Vqdi b) {return vuzp2q_s64(a, b);}
INLINE(Vqdf,VQDF_UZPR) (Vqdf a, Vqdf b) {return vuzp2q_f64(a, b);}

#if _LEAVE_ARM_UZPR
}
#endif



#if _ENTER_ARM_SETL
{
#endif

INLINE(ushort, USHRT_SETL) (ushort x,  uint8_t lo)
{
    HALF_TYPE v = {.U=x};
    v.Lo.U = lo;
    return v.U;
}

INLINE( short,  SHRT_SETL)  (short x,  uint8_t lo)
{
    HALF_TYPE v = {.I=x};
    v.Lo.U = lo;
    return v.I;
}

INLINE(  uint,  UINT_SETL)   (uint x, uint16_t lo)
{
    WORD_TYPE v = {.U=x};
    v.Lo.U = lo;
    return v.U;
}

INLINE(   int,   INT_SETL)    (int x, uint16_t lo)
{
    WORD_TYPE v = {.I=x};
    v.Lo.U = lo;
    return v.I;
}

#if DWRD_NLONG == 2

INLINE( ulong, ULONG_SETL)  (ulong x, uint16_t lo)
{
    WORD_TYPE v = {.U=x};
    v.Lo.U = lo;
    return v.U;
}

INLINE(   int,  LONG_SETL)   (long x, uint16_t lo)
{
    WORD_TYPE v = {.I=x};
    v.Lo.U = lo;
    return v.I;
}

#else

INLINE( ulong, ULONG_SETL)  (ulong x, uint32_t lo)
{
    DWRD_TYPE v = {.U=x};
    v.Lo.U = lo;
    return v.U;
}

INLINE(  long,  LONG_SETL)   (long x, uint32_t lo)
{
    DWRD_TYPE v = {.I=x};
    v.Lo.U = lo;
    return v.I;
}

#endif

#if QUAD_NLLONG == 2

INLINE(ullong,ULLONG_SETL) (ullong x, uint32_t lo)
{
    DWRD_TYPE v = {.U=x};
    v.Lo.U = lo;
    return v.U;
}

INLINE( llong, LLONG_SETL)  (llong x, uint32_t lo)
{
    DWRD_TYPE v = {.I=x};
    v.Lo.U = lo;
    return v.I;
}

#else

INLINE(ullong,ULLONG_SETL) (ullong x, uint64_t lo)
{
    QUAD_TYPE v = {.U=x};
    v.Lo.U = lo;
    return v.U;
}

INLINE(llong,LLONG_SETL) (llong x, uint32_t lo)
{
    QUAD_TYPE v = {.I=x};
    v.Lo.U = lo;
    return v.I;
}

#endif

INLINE(Vdyu,VDYU_SETL) (Vdyu c, Vwyu l)
{
#define     VDYU_SETL(C, L) \
VDWF_ASYU(vset_lane_f32(VWYU_ASTM(L), VDYU_ASWF(C), V2_K0))
    return  VDYU_SETL(c, l);
}


INLINE(Vdbu,VDBU_SETL) (Vdbu c, Vwbu l)
{
#define     VDBU_SETL(C, L) \
VDWF_ASBU(vset_lane_f32(VWBU_ASTM(L), VDBU_ASWF(C), V2_K0))
    return  VDBU_SETL(c, l);
}

INLINE(Vdbi,VDBI_SETL) (Vdbi c, Vwbi l)
{
#define     VDBI_SETL(C, L) \
VDWF_ASBI(vset_lane_f32(VWBI_ASTM(L), VDBI_ASWF(C), V2_K0))
    return  VDBI_SETL(c, l);
}

INLINE(Vdbc,VDBC_SETL) (Vdbc c, Vwbc l)
{
#define     VDBC_SETL(C, L) \
VDWF_ASBC(vset_lane_f32(VWBC_ASTM(L), VDBC_ASWF(C), V2_K0))
    return  VDBC_SETL(c, l);
}


INLINE(Vdhu,VDHU_SETL) (Vdhu c, Vwhu l)
{
#define     VDHU_SETL(C, L) \
VDWF_ASHU(vset_lane_f32(VWHU_ASTM(L), VDHU_ASWF(C), V2_K0))
    return  VDHU_SETL(c, l);
}

INLINE(Vdhi,VDHI_SETL) (Vdhi c, Vwhi l)
{
#define     VDHI_SETL(C, L) \
VDWF_ASHI(vset_lane_f32(VWHI_ASTM(L), VDHI_ASWF(C), V2_K0))
    return  VDHI_SETL(c, l);
}

INLINE(Vdhf,VDHF_SETL) (Vdhf c, Vwhf l)
{
#define     VDHF_SETL(C, L) \
VDWF_ASHF(vset_lane_f32(VWHF_ASTM(L), VDHF_ASWF(C), V2_K0))
    return  VDHF_SETL(c, l);
}


INLINE(Vdwu,VDWU_SETL) (Vdwu c, Vwwu l)
{
#define     VDWU_SETL(C, L) \
VDWF_ASWU(vset_lane_f32(VWWU_ASTM(L), VDWU_ASWF(C), V2_K0))
    return  VDWU_SETL(c, l);
}

INLINE(Vdwi,VDWI_SETL) (Vdwi c, Vwwi l)
{
#define     VDWI_SETL(C, L) \
VDWF_ASWI(vset_lane_f32(VWWI_ASTM(L), VDWI_ASWF(C), V2_K0))
    return  VDWI_SETL(c, l);
}

INLINE(Vdwf,VDWF_SETL) (Vdwf c, Vwwf l)
{
#define     VDWF_SETL(C, L) vset_lane_f32(VWWF_ASTM(L), C, V2_K0)
    return  VDWF_SETL(c, l);
}


INLINE(Vqyu,VQYU_SETL) (Vqyu c, Vdyu l)
{
#define     VQYU_SETL(C, L) \
QYU_ASTV(vcombine_u64(VDYU_ASTM(L), vget_low_u64(VQYU_ASTM(C))))
    return  VQYU_SETL(c, l);
}

INLINE(Vqbu,VQBU_SETL) (Vqbu c, Vdbu l)
{
#define     VQBU_SETL(C, L) vcombine_u8(L, vget_high_u8(C))
    return  VQBU_SETL(c, l);
}

INLINE(Vqbi,VQBI_SETL) (Vqbi c, Vdbi l)
{
#define     VQBI_SETL(C, L) vcombine_s8(L, vget_high_s8(C))
    return  VQBI_SETL(c, l);
}

INLINE(Vqbc,VQBC_SETL) (Vqbc c, Vdbc l)
{
#define     VQBC_SETL(C, L)         \
VQDU_ASBC(                          \
    vcombine_u64(                   \
        VDBC_ASDU(L),               \
        vget_high_u64(VQBC_ASDU(C)) \
    )                               \
)
    return  VQBC_SETL(c, l);
}


INLINE(Vqhu,VQHU_SETL) (Vqhu c, Vdhu l)
{
#define     VQHU_SETL(C, L) vcombine_u16(L, vget_high_u16(C))
    return  VQHU_SETL(c, l);
}

INLINE(Vqhi,VQHI_SETL) (Vqhi c, Vdhi l)
{
#define     VQHI_SETL(C, L) vcombine_s16(L, vget_high_s16(C))
    return  VQHI_SETL(c, l);
}

INLINE(Vqhf,VQHF_SETL) (Vqhf c, Vdhf l)
{
#define     VQHF_SETL(C, L) vcombine_f16(L, vget_high_f16(C))
    return  VQHF_SETL(c, l);
}


INLINE(Vqwu,VQWU_SETL) (Vqwu c, Vdwu l)
{
#define     VQWU_SETL(C, L) vcombine_u32(L, vget_high_u32(C))
    return  VQWU_SETL(c, l);
}

INLINE(Vqwi,VQWI_SETL) (Vqwi c, Vdwi l)
{
#define     VQWI_SETL(C, L) vcombine_s32(L, vget_high_s32(C))
    return  VQWI_SETL(c, l);
}

INLINE(Vqwf,VQWF_SETL) (Vqwf c, Vdwf l)
{
#define     VQWF_SETL(C, L) vcombine_f32(L, vget_high_f32(C))
    return  VQWF_SETL(c, l);
}


INLINE(Vqdu,VQDU_SETL) (Vqdu c, Vddu l)
{
#define     VQDU_SETL(C, L) vcombine_u64(L, vget_high_u64(C))
    return  VQDU_SETL(c, l);
}

INLINE(Vqdi,VQDI_SETL) (Vqdi c, Vddi l)
{
#define     VQDI_SETL(C, L) vcombine_s64(L, vget_high_s64(C))
    return  VQDI_SETL(c, l);
}

INLINE(Vqdf,VQDF_SETL) (Vqdf c, Vddf l)
{
#define     VQDF_SETL(C, L) vcombine_f64(L, vget_high_f64(C))
    return  VQDF_SETL(c, l);
}

#if _LEAVE_ARM_SETL
}
#endif

#if _ENTER_ARM_SETR
{
#endif

INLINE(ushort,USHRT_SETR) (ushort x,  uint8_t hi)
{
    HALF_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE( short, SHRT_SETR)  (short x,  uint8_t hi)
{
    HALF_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}


INLINE(  uint, UINT_SETR)   (uint x, uint16_t hi)
{
    WORD_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE(   int,  INT_SETR)    (int x, uint16_t hi)
{
    WORD_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}

#if DWRD_NLONG == 2

INLINE( ulong, ULONG_SETR) (ulong x, uint16_t hi)
{
    WORD_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE(  long,  LONG_SETR)  (long x, uint16_t hi)
{
    WORD_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}

#else

INLINE( ulong, ULONG_SETR) (ulong x, uint32_t hi)
{
    DWRD_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE(  long,  LONG_SETR)  (long x, uint32_t hi)
{
    DWRD_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}

#endif

#if QUAD_NLLONG == 2

INLINE(ullong,ULLONG_SETR)(ullong x, uint32_t hi)
{
    DWRD_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE( llong, LLONG_SETR) (llong x, uint32_t hi)
{
    DWRD_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}

#else

INLINE(ullong,ULLONG_SETR)(ullong x, uint64_t hi)
{
    QUAD_TYPE v = {.U=x};
    v.Hi.U = hi;
    return v.U;
}

INLINE( llong, LLONG_SETR) (llong x, uint64_t hi)
{
    QUAD_TYPE v = {.I=x};
    v.Hi.U = hi;
    return v.I;
}

#endif

INLINE(Vdyu,VDYU_SETR) (Vdyu c, Vwyu r)
{
#define     VDYU_SETR(C, R) \
VDWF_ASYU(vset_lane_f32(VWYU_ASTM(R), VDYU_ASWF(C), V2_K1))
    return  VDYU_SETR(c, r);
}

INLINE(Vdbu,VDBU_SETR) (Vdbu c, Vwbu r)
{
#define     VDBU_SETR(C, R) \
VDWF_ASBU(vset_lane_f32(VWBU_ASTM(R), VDBU_ASWF(C), V2_K1))
    return  VDBU_SETR(c, r);
}

INLINE(Vdbi,VDBI_SETR) (Vdbi c, Vwbi r)
{
#define     VDBI_SETR(C, R) \
VDWF_ASBI(vset_lane_f32(VWBI_ASTM(R), VDBI_ASWF(C), V2_K1))
    return  VDBI_SETR(c, r);
}

INLINE(Vdbc,VDBC_SETR) (Vdbc c, Vwbc r)
{
#define     VDBC_SETR(C, R) \
VDWF_ASBC(vset_lane_f32(VWBC_ASTM(R), VDBC_ASWF(C), V2_K1))
    return  VDBC_SETR(c, r);
}


INLINE(Vdhu,VDHU_SETR) (Vdhu c, Vwhu r)
{
#define     VDHU_SETR(C, R) \
VDWF_ASHU(vset_lane_f32(VWHU_ASTM(R), VDHU_ASWF(C), V2_K1))
    return  VDHU_SETR(c, r);
}

INLINE(Vdhi,VDHI_SETR) (Vdhi c, Vwhi r)
{
#define     VDHI_SETR(C, R) \
VDWF_ASHI(vset_lane_f32(VWHI_ASTM(R), VDHI_ASWF(C), V2_K1))
    return  VDHI_SETR(c, r);
}

INLINE(Vdhf,VDHF_SETR) (Vdhf c, Vwhf r)
{
#define     VDHF_SETR(C, R) \
VDWF_ASHF(vset_lane_f32(VWHF_ASTM(R), VDHF_ASWF(C), V2_K1))
    return  VDHF_SETR(c, r);
}


INLINE(Vdwu,VDWU_SETR) (Vdwu c, Vwwu r)
{
#define     VDWU_SETR(C, R) \
VDWF_ASWU(vset_lane_f32(VWWU_ASTM(R), VDWU_ASWF(C), V2_K1))
    return  VDWU_SETR(c, r);
}

INLINE(Vdwi,VDWI_SETR) (Vdwi c, Vwwi r)
{
#define     VDWI_SETR(C, R) \
VDWF_ASWI(vset_lane_f32(VWWI_ASTM(R), VDWI_ASWF(C), V2_K1))
    return  VDWI_SETR(c, r);
}

INLINE(Vdwf,VDWF_SETR) (Vdwf c, Vwwf r)
{
#define     VDWF_SETR(C, R) vset_lane_f32(VWWF_ASTM(R), C, V2_K1)
    return  VDWF_SETR(c, r);
}


INLINE(Vqyu,VQYU_SETR) (Vqyu c, Vdyu r)
{
#define     VQYU_SETR(C, R)         \
QYU_ASTV(                           \
    vcombine_u64(                   \
        vget_low_u64(VQYU_ASTM(C)), \
        VDYU_ASTM(R)                \
    )                               \
)

    return  VQYU_SETR(c, r);
}

INLINE(Vqbu,VQBU_SETR) (Vqbu c, Vdbu r)
{
#define     VQBU_SETR(C, R)     vcombine_u8(vget_low_u8(C), R)
    return  VQBU_SETR(c, r);
}

INLINE(Vqbi,VQBI_SETR) (Vqbi c, Vdbi r)
{
#define     VQBI_SETR(C, R)     vcombine_s8(vget_low_s8(C), R)
    return  VQBI_SETR(c, r);
}

INLINE(Vqbc,VQBC_SETR) (Vqbc c, Vdbc r)
{
#define     VQBC_SETR(C, R)         \
VQDU_ASBC(                          \
    vcombine_u64(                   \
        vget_low_u64(VQBC_ASDU(C)), \
        VDBC_ASDU(R)                \
    )                               \
)
    return  VQBC_SETR(c, r);
}


INLINE(Vqhu,VQHU_SETR) (Vqhu c, Vdhu r)
{
#define     VQHU_SETR(C, R)     vcombine_u16(vget_low_u16(C), R)
    return  VQHU_SETR(c, r);
}

INLINE(Vqhi,VQHI_SETR) (Vqhi c, Vdhi r)
{
#define     VQHI_SETR(C, R)     vcombine_s16(vget_low_s16(C), R)
    return  VQHI_SETR(c, r);
}

INLINE(Vqhf,VQHF_SETR) (Vqhf c, Vdhf r)
{
#define     VQHF_SETR(C, R)     vcombine_f16(vget_low_f16(C), R)
    return  VQHF_SETR(c, r);
}


INLINE(Vqwu,VQWU_SETR) (Vqwu c, Vdwu r)
{
#define     VQWU_SETR(C, R)     vcombine_u32(vget_low_u32(C), R)
    return  VQWU_SETR(c, r);
}

INLINE(Vqwi,VQWI_SETR) (Vqwi c, Vdwi r)
{
#define     VQWI_SETR(C, R)     vcombine_s32(vget_low_s32(C), R)
    return  VQWI_SETR(c, r);
}

INLINE(Vqwf,VQWF_SETR) (Vqwf c, Vdwf r)
{
#define     VQWF_SETR(C, R)     vcombine_f32(vget_low_f32(C), R)
    return  VQWF_SETR(c, r);
}


INLINE(Vqdu,VQDU_SETR) (Vqdu c, Vddu r)
{
#define     VQDU_SETR(C, R)     vcombine_u64(vget_low_u64(C), R)
    return  VQDU_SETR(c, r);
}

INLINE(Vqdi,VQDI_SETR) (Vqdi c, Vddi r)
{
#define     VQDI_SETR(C, R)     vcombine_s64(vget_low_s64(C), R)
    return  VQDI_SETR(c, r);
}

INLINE(Vqdf,VQDF_SETR) (Vqdf c, Vddf r)
{
#define     VQDF_SETR(C, R)     vcombine_f64(vget_low_f64(C), R)
    return  VQDF_SETR(c, r);
}

#if _LEAVE_ARM_SETR
}
#endif

#if _ENTER_ARM_SET1
{
#endif

INLINE(Vwyu,VWYU_SET1) (Vwyu d, Rc(0,31) k, _Bool v)
{
#define     VWYU_SET1   VWYU_SET1
    float32x2_t w = vdup_n_f32(VWYU_ASTM(d));
    int64x1_t   s = vdup_n_s64(k);
    uint64x1_t  m = vshl_u64(vdup_n_u64(1), s);
    uint64x1_t  y = vbic_u64(VDWF_ASDU(w), m);
    y = vorr_u64(y, vshl_u64(vdup_n_u64(v), s));
    return  WYU_ASTV(vget_lane_f32(VDDU_ASWF(y), V2_K0));
}

INLINE(Vwbu,VWBU_SET1) (Vwbu d, Rc(0, 3) k, uint8_t v)
{
#define     WBU_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_u8(        \
        vset_lane_u8(           \
            (V),                \
            vreinterpret_u8_f32(\
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     VWBU_SET1(D, K, V)  WBU_ASTV(WBU_SET1(VWBU_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWBU_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*8);
    uint64x1_t  m = vdup_n_u64(0xffull);
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(vdup_n_u64(v), s));
    return  WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            V2_K0
        )
    );
}

INLINE(Vwbi,VWBI_SET1) (Vwbi d, Rc(0, 3) k, int8_t v)
{
#define     WBI_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_s8(        \
        vset_lane_s8(           \
            (V),                \
            vreinterpret_s8_f32(\
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     VWBI_SET1(D, K, V)  WBI_ASTV(WBI_SET1(VWBI_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWBI_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*8);
    uint64x1_t  m = vdup_n_u64(0xffull);
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(vdup_n_u64(v), s));
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            V2_K0
        )
    );
}

INLINE(Vwbc,VWBC_SET1) (Vwbc d, Rc(0, 3) k, char v)
{
#if CHAR_MIN
#   define  WBC_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_s8(        \
        vset_lane_s8(           \
            (V),                \
            vreinterpret_s8_f32(\
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    V2_K0                       \
)

#else
#   define  WBC_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_u8(        \
        vset_lane_u8(           \
            (V),                \
            vreinterpret_u8_f32(\
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    V2_K0                       \
)

#endif

#define     VWBC_SET1(D, K, V)  WBC_ASTV(WBC_SET1(VWBC_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWBC_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*8);
    uint64x1_t  m = vdup_n_u64(0xffull);
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(vdup_n_u64(v), s));
    return  WBC_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            V2_K0
        )
    );
}

INLINE(Vwhu,VWHU_SET1) (Vwhu d, Rc(0, 1) k, uint16_t v)
{
#define     WHU_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_u16(       \
        vset_lane_u16(          \
            (V),                \
            VDWF_ASHU(          \
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     VWHU_SET1(D, K, V)  WHU_ASTV(WHU_SET1(VWHU_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWHU_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*16);
    uint64x1_t  m = vdup_n_u64(0xffffull);
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(vdup_n_u64(v), s));
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            V2_K0
        )
    );
}

INLINE(Vwhi,VWHI_SET1) (Vwhi d, Rc(0, 1) k, int16_t v)
{
#define     WHI_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_s16(       \
        vset_lane_s16(          \
            (V),                \
            VDWF_ASHI(          \
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     VWHI_SET1(D, K, V)  WHI_ASTV(WHI_SET1(VWHI_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWHI_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*16);
    uint64x1_t  m = vdup_n_u64(0xffffull);
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(vdup_n_u64(v), s));
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            V2_K0
        )
    );
}

INLINE(Vwhf,VWHF_SET1) (Vwhf d, Rc(0, 1) k, flt16_t v)
{
#define     WHF_SET1(D, K, V)   \
vget_lane_f32(                  \
    vreinterpret_f32_f16(       \
        vset_lane_f16(          \
            (V),                \
            VDWF_ASHF(          \
                vdup_n_f32(D)   \
            ),                  \
            (K)                 \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     VWHF_SET1(D, K, V)  WHF_ASTV(WHF_SET1(VWHF_ASTM(D), K, V))
    float32x2_t w = vdup_n_f32(VWHF_ASTM(d));
    int64x1_t   s = vdup_n_s64(k*16);
    uint64x1_t  m = vdup_n_u64(0xffffull);
    uint64x1_t  h = vdup_n_u64(FLT16_ASHU(v));
    m = vshl_u64(m, s);
    m = vbic_u64(m, vreinterpret_u64_f32(w));
    m = vorr_u64(m, vshl_u64(h, s));
    return  WHF_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u64(m),
            V2_K0
        )
    );
}


INLINE(Vdyu,VDYU_SET1) (Vdyu d, Rc(0,63) k, _Bool v)
{
#define     VDYU_SET1   VDYU_SET1
    int64x1_t   s = vdup_n_s64(k);
    uint64x1_t  m = vshl_u64(vdup_n_u64(1), s);
    uint64x1_t  y = vbic_u64(m, VDYU_ASTM(d));
    return DYU_ASTV(vorr_u64(y, vshl_u64(vdup_n_u64(v), s)));
}

INLINE(Vdbu,VDBU_SET1) (Vdbu d, Rc(0, 7) k, uint8_t v)
{
#define     VDBU_SET1(D, K, V)  vset_lane_u8(V, D, K)
    int64x1_t  s = vdup_n_s64(k*8);
    uint64x1_t m = vdup_n_u64(0xffull);
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_u8(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(v), s), m);
    return  vreinterpret_u8_u64(m);
}

INLINE(Vdbi,VDBI_SET1) (Vdbi d, Rc(0, 7) k, int8_t v)
{
#define     VDBI_SET1(D, K, V)  vset_lane_u8(V, D, K)
    int64x1_t  s = vdup_n_s64(k*8);
    uint64x1_t m = vdup_n_u64(0xffull);
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_s8(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(v), s), m);
    return  vreinterpret_s8_u64(m);
}

INLINE(Vdbc,VDBC_SET1) (Vdbc d, Rc(0, 7) k, char v)
{
#if CHAR_MIN
#   define  VDBC_SET1(D, K, V)  DBC_ASTV(vset_lane_s8(V, VDBC_ASTM(D), K))
#else
#   define  VDBC_SET1(D, K, V)  DBC_ASTV(vset_lane_u8(V, VDBC_ASTM(D), K))
#endif

    int64x1_t   s = vdup_n_s64(k*8);
    uint64x1_t  m = VDBC_ASDU(d);
    uint64x1_t  x;
    x = vshl_u64(vdup_n_u64(0xffull), s);
    x = vbic_u64(m, x);
    x = vorr_u64(x, vshl_u64(vdup_n_u64(v), s));
    return VDDU_ASBC(x);
}


INLINE(Vdhu,VDHU_SET1) (Vdhu d, Rc(0, 3) k, uint16_t v)
{
#define     VDHU_SET1(D, K, V)  vset_lane_u16(V, D, K)
    int64x1_t  s = vdup_n_s64(k*16);
    uint64x1_t m = vdup_n_u64(0xffffull);
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_u16(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(v), s), m);
    return vreinterpret_u16_u64(m);
}

INLINE(Vdhi,VDHI_SET1) (Vdhi d, Rc(0, 3) k, int16_t v)
{
#define     VDHI_SET1(D, K, V)  vset_lane_s16(V, D, K)
    int64x1_t   s = vdup_n_s64(k*16);
    uint64x1_t  x = vdup_n_u64(0xffffull);
    x = vshl_u64(x, s);
    x = vbic_u64(x, vreinterpret_u64_s16(d));
    x = vorr_u64(x, vshl_u64(vdup_n_u64(0xffffull&v), s));
    return vreinterpret_s16_u64(x);
}

INLINE(Vdhf,VDHF_SET1) (Vdhf d, Rc(0, 3) k, flt16_t v)
{
#define     VDHF_SET1(D, K, V)  vset_lane_f16(V, D, K)
    int64x1_t   s = vdup_n_s64(k*16);
    uint64x1_t  m = vdup_n_u64(0xffffull);
    uint64x1_t  h = vdup_n_u64(FLT16_ASHU(v));
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_u16(d), m);
    m = vorr_u64(vshl_u64(h, s), m);
    return  vreinterpret_f16_u64(m);
}


INLINE(Vdwu,VDWU_SET1) (Vdwu d, Rc(0, 1) k, uint32_t v)
{
#define     VDWU_SET1(D, K, V)  vset_lane_u32(V, D, K)
    int64x1_t  s = vdup_n_s64(k*32);
    uint64x1_t m = vdup_n_u64(0xffffffffull);
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_u32(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(v), s), m);
    return  vreinterpret_u32_u64(m);
}

INLINE(Vdwi,VDWI_SET1) (Vdwi d, Rc(0, 1) k, int32_t v)
{
#define     VDWI_SET1(D, K, V)  vset_lane_s32(V, D, K)
    int64x1_t  s = vdup_n_s64(k*32);
    uint64x1_t m = vdup_n_u64(0xffffffffull);
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_s32(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(v), s), m);
    return  vreinterpret_s32_u64(m);
}

INLINE(Vdwf,VDWF_SET1) (Vdwf d, Rc(0, 1) k, float v)
{
#define     VDWF_SET1(D, K, V)  vset_lane_f32(V, D, K)
    int64x1_t  s = vdup_n_s64(k*32);
    uint64x1_t m = vdup_n_u64(0xffffffffull);
    WORD_TYPE  w = {.F=v};
    m = vshl_u64(m, s);
    m = vbic_u64(vreinterpret_u64_f32(d), m);
    m = vorr_u64(vshl_u64(vdup_n_u64(w.U), s), m);
    return  vreinterpret_f32_u64(m);
}


INLINE(Vqyu,VQYU_SET1) (Vqyu d, Rc(0,127) k, _Bool v)
{
#define     VQYU_SET1   VQYU_SET1
    Vqdu  q = VQYU_ASTM(d);
    Vddu  l =  vget_low_u64(q);
    Vddu  r =  vget_high_u64(q);
    if (k > 63)
        r = VDYU_ASTM(VDYU_SET1(DYU_ASTV(r), 63&k, v));
    else
        l = VDYU_ASTM(VDYU_SET1(DYU_ASTV(l), k,    v));
    q = vcombine_u64(l, r);
    return  QYU_ASTV(q);
}

INLINE(Vqbu,VQBU_SET1) (Vqbu d, Rc(0, 15) k, uint8_t v)
{
#define     VQBU_SET1(D, K, V)  vsetq_lane_u8(V, D, K)
    uint8x8_t l = vget_low_u8(d);
    uint8x8_t r = vget_high_u8(d);
    if (k > 7)
        r = (VDBU_SET1)(r, 7&k, v);
    else
        l = (VDBU_SET1)(l,   k, v);
    return  vcombine_u8(l, r);
}

INLINE(Vqbi,VQBI_SET1) (Vqbi d, Rc(0, 15) k, int8_t v)
{
#define     VQBI_SET1(D, K, V)  vsetq_lane_s8(V, D, K)
    int8x8_t l = vget_low_s8(d);
    int8x8_t r = vget_high_s8(d);
    if (k > 7)
        r = (VDBI_SET1)(r, 7&k, v);
    else
        l = (VDBI_SET1)(l,   k, v);
    return  vcombine_u8(l, r);
}

INLINE(Vqbc,VQBC_SET1) (Vqbc d, Rc(0, 15) k, char v)
{
#if CHAR_MIN
#   define  QBC_SET1(M, K, V)   vsetq_lane_s8(V, M, K)
#else
#   define  QBC_SET1(M, K, V)   vsetq_lane_u8(V, M, K)
#endif

#define     VQBC_SET1(D, K, V)  QBC_ASTV(QBC_SET1(VQBC_ASTM(D), K, V))
    uint8x16_t  q = VQBC_ASBU(d);
    uint8x8_t   l = vget_low_u8(q);
    uint8x8_t   r = vget_high_u8(q);
    if (k > 7)
        r = (VDBU_SET1)(r, 7&k, v);
    else
        l = (VDBU_SET1)(l,   k, v);
    return  VQBU_ASBC(vcombine_u8(l, r));
}


INLINE(Vqhu,VQHU_SET1) (Vqhu d, Rc(0, 7) k, uint16_t v)
{
#define     VQHU_SET1(D, K, V)  vsetq_lane_u16(V, D, K)
    uint16x4_t l = vget_low_u16(d);
    uint16x4_t r = vget_high_u16(d);
    if (k > 3)
        r = (VDHU_SET1)(r, 3&k, v);
    else
        l = (VDHU_SET1)(l,  k,  v);
    return  vcombine_u16(l, r);
}

INLINE(Vqhi,VQHI_SET1) (Vqhi d, Rc(0, 7) k, int16_t v)
{
#define     VQHI_SET1(D, K, V)  vsetq_lane_s16(V, D, K)
    int16x4_t l = vget_low_s16(d);
    int16x4_t r = vget_high_s16(d);
    if (k > 3)
        r = (VDHI_SET1)(r, 3&k, v);
    else
        l = (VDHI_SET1)(l,  k,  v);
    return  vcombine_s16(l, r);
}

INLINE(Vqhf,VQHF_SET1) (Vqhf d, Rc(0, 7) k, flt16_t v)
{
#define     VQHF_SET1(D, K, V)  vsetq_lane_f16(V, D, K)
    float16x4_t l = vget_low_f16(d);
    float16x4_t r = vget_high_f16(d);
    if (k > 3)
        r = (VDHF_SET1)(r, 3&k, v);
    else
        l = (VDHF_SET1)(l,  k,  v);
    return  vcombine_f16(l, r);
}



INLINE(Vqwu,VQWU_SET1) (Vqwu d, Rc(0, 3) k, uint32_t v)
{
#define     VQWU_SET1(D, K, V)  vsetq_lane_u32(V, D, K)
    uint32x2_t l = vget_low_u32(d);
    uint32x2_t r = vget_high_u32(d);
    if (k > 1)
        r = (VDWU_SET1)(r, 1&k, v);
    else
        l = (VDWU_SET1)(l,  k,  v);
    return  vcombine_u32(l, r);
}

INLINE(Vqwi,VQWI_SET1) (Vqwi d, Rc(0, 3) k, int32_t v)
{
#define     VQWI_SET1(D, K, V)  vsetq_lane_s32(V, D, K)
    int32x2_t l = vget_low_s32(d);
    int32x2_t r = vget_high_s32(d);
    if (k > 1)
        r = (VDWI_SET1)(r, 1&k, v);
    else
        l = (VDWI_SET1)(l,  k,  v);
    return  vcombine_s32(l, r);
}

INLINE(Vqwf,VQWF_SET1) (Vqwf d, Rc(0, 3) k, float v)
{
#define     VQWF_SET1(D, K, V)  vsetq_lane_f32(V, D, K)
    float32x2_t l = vget_low_f32(d);
    float32x2_t r = vget_high_f32(d);
    if (k > 1)
        r = (VDWF_SET1)(r, 1&k, v);
    else
        l = (VDWF_SET1)(l,  k,  v);
    return  vcombine_f32(l, r);
}


INLINE(Vqdu,VQDU_SET1) (Vqdu d, Rc(0, 1) k, uint64_t v)
{
#define     VQDU_SET1(D, K, V) vsetq_lane_u64(V, D, K)
    return k
    ?   vsetq_lane_u64(v, d, 1)
    :   vsetq_lane_u64(v, d, 0);
}

INLINE(Vqdi,VQDI_SET1) (Vqdi d, Rc(0, 1) k, int64_t v)
{
#define     VQDI_SET1(D, K, V) vsetq_lane_s64(V, D, K)
    return k
    ?   vsetq_lane_s64(v, d, 1)
    :   vsetq_lane_s64(v, d, 0);
}

INLINE(Vqdf,VQDF_SET1) (Vqdf d, Rc(0, 1) k, double v)
{
#define     VQDF_SET1(D, K, V) vsetq_lane_f64(V, D, K)
    return k
    ?   vsetq_lane_f64(v, d, 1)
    :   vsetq_lane_f64(v, d, 0);
}

#if _LEAVE_ARM_SET1
}
#endif


#if _ENTER_ARM_LDR1
{
#endif

#define MY_LDRVAC(C, S, L, T, A) C##S(L((T const *) A))

#define MY_LDRDAC(D, SRC)       \
MY_LDRVAC(                      \
    vreinterpret_##D,   /* C */ \
    _u64,               /* S */ \
    vld1_u64,           /* L */ \
    uint64_t,           /* T */ \
    SRC                 /* A */ \
)

#define MY_LDRQAC(D, SRC)       \
MY_LDRVAC(                      \
    vreinterpretq_##D,  /* C */ \
    _p128,              /* S */ \
    vldrq_p128,         /* L */ \
    unsigned __int128,  /* T */ \
    SRC                 /* A */ \
)


INLINE(Vwyu,VWYU_LDRK) (Vwyu v, Rc(0, 31) k, _Bool volatile const a[1])
{
#define     VWYU_LDRK   VWYU_LDRK
    uint32x2_t  m = vreinterpret_u32_f32(vdup_n_f32(VWYU_ASTM(v)));
    m = vbic_u32(m, vdup_n_u32(1u<<k));
    m = vorr_u32(m, vdup_n_u32((uint32_t) *a<<k));
    return WYU_ASTV(vget_lane_f32(vreinterpret_f32_u32(m), V2_K0));
}

INLINE(Vwbu,VWBU_LDRK) (Vwbu v, Rc(0, 3) k, uint8_t volatile const a[1])
{
#define     WBU_LDRK(M, K, A)       \
vget_lane_f32(                      \
    vreinterpret_f32_u8(             \
        vld1_lane_u8(               \
            (uint8_t *)(A),         \
            vreinterpret_u8_f32(    \
                vdup_n_f32(M)       \
            ),                      \
            K                       \
        )                           \
    ),                              \
    V2_K0                           \
)

#define     VWBU_LDRK(V,K,A)        \
WBU_ASTV(WBU_LDRK(VWBU_ASTM(V), K, A))

    int64x1_t   s = vdup_n_s64(0ll-8*k);
    float32x2_t m = vdup_n_f32(VWBU_ASTM(v));
    uint64x1_t  d = vreinterpret_u64_f32(m);
    uint8x8_t   b = vset_lane_u8(*a, vdup_n_u8(0), 0);
    d = vbic_u64(d, vshl_u64(vdup_n_u64(255ull), s));
    d = vorr_u64(d, vshl_u64(vreinterpret_u64_u8(b), s));
    m = vreinterpret_f32_u64(d);
    return WBU_ASTV(vget_lane_f32(m, V2_K0));
}

#if _LEAVE_ARM_LDRK
}
#endif

#if _ENTER_ARM_LDRW
{
#endif

INLINE(Vwbu,UCHAR_LDRWAC) (unsigned char const src[4])
{
#define     UCHAR_LDRWAC(SRC) (*(VWBU_TYPE const *) SRC)
    return  UCHAR_LDRWAC(src);
}

INLINE(Vwbi,SCHAR_LDRWAC) (signed char const src[4])
{
#define     SCHAR_LDRWAC(SRC) (*(VWBI_TYPE const *) SRC)
    return  SCHAR_LDRWAC(src);
}

INLINE(Vwbc,CHAR_LDRWAC) (char  const src[4])
{
#define     CHAR_LDRWAC(SRC) (*(VWBC_TYPE const *) SRC)
    return  CHAR_LDRWAC(src);
}


INLINE(Vwhu,USHRT_LDRWAC) (unsigned short const src[2])
{
#define     USHRT_LDRWAC(SRC) (*(VWHU_TYPE const *) SRC)
    return  USHRT_LDRWAC(src);
}

INLINE(Vwhi,SHRT_LDRWAC) (short const src[2])
{
#define     SHRT_LDRWAC(SRC) (*(VWHI_TYPE const *) SRC)
    return  SHRT_LDRWAC(src);
}


INLINE(Vwwu,UINT_LDRWAC) (unsigned int const src[1])
{
#define     UINT_LDRWAC(SRC) (*(VWWU_TYPE const *) SRC)
    return  UINT_LDRWAC(src);
}

INLINE(Vwwi,INT_LDRWAC) (int const src[1])
{
#define     INT_LDRWAC(SRC) (*(VWWI_TYPE const *) SRC)
    return  INT_LDRWAC(src);
}


#if DWRD_NLONG == 2

INLINE(Vwwu,ULONG_LDRWAC) (unsigned long const src[1])
{
#define     ULONG_LDRWAC(SRC) (*(VWWU_TYPE const *) SRC)
    return  ULONG_LDRWAC(src);
}

INLINE(Vwwi,LONG_LDRWAC) (long const src[1])
{
#define     LONG_LDRWAC(SRC) (*(VWWI_TYPE const *) SRC)
    return  LONG_LDRWAC(src);
}

#endif

INLINE(Vwhf,FLT16_LDRWAC) (flt16_t const src[2])
{
#define     FLT16_LDRWAC(SRC) (*(VWHF_TYPE const *) SRC)
    return  FLT16_LDRWAC(src);
}

INLINE(Vwwf,FLT_LDRWAC) (float const src[1])
{
#define     FLT_LDRWAC(SRC) (*(VWWF_TYPE const *) SRC)
    return  FLT_LDRWAC(src);
}

#if _LEAVE_ARM_LDRW
}
#endif

#if _ENTER_ARM_LDRD
{
#endif

INLINE(Vdbu,UCHAR_LDRDAC) (unsigned char const src[8])
{
#define     UCHAR_LDRDAC(SRC)   MY_LDRDAC(u8,SRC)
    return  UCHAR_LDRDAC(src);
}

INLINE(Vdbi,SCHAR_LDRDAC) (signed char const src[8])
{
#define     SCHAR_LDRDAC(SRC)   MY_LDRDAC(s8,SRC)
    return  SCHAR_LDRDAC(src);
}

INLINE(Vdbc,CHAR_LDRDAC) (char const src[8])
{
#define     CHAR_LDRDAC(SRC)   VDBU_ASBC(MY_LDRDAC(u8,SRC))
    return  CHAR_LDRDAC(src);
}


INLINE(Vdhu,USHRT_LDRDAC) (unsigned short const src[4])
{
#define     USHRT_LDRDAC(SRC)   MY_LDRDAC(u16,SRC)
    return  USHRT_LDRDAC(src);
}

INLINE(Vdhi,SHRT_LDRDAC) (short const src[4])
{
#define     SHRT_LDRDAC(SRC)   MY_LDRDAC(s16,SRC)
    return  SHRT_LDRDAC(src);
}


INLINE(Vdwu,UINT_LDRDAC) (unsigned int const src[2])
{
#define     UINT_LDRDAC(SRC)   MY_LDRDAC(u32,SRC)
    return  UINT_LDRDAC(src);
}

INLINE(Vdwi,INT_LDRDAC) (int const src[2])
{
#define     INT_LDRDAC(SRC)   MY_LDRDAC(s32,SRC)
    return  INT_LDRDAC(src);
}


#if DWRD_NLONG == 2

INLINE(Vdwu,ULONG_LDRDAC) (unsigned long const src[2])
{
#define     ULONG_LDRDAC(SRC)   MY_LDRDAC(u32,SRC)
    return  ULONG_LDRDAC(src);
}

INLINE(Vdwi, LONG_LDRDAC) (long const src[2])
{
#define     LONG_LDRDAC(SRC)   MY_LDRDAC(s32,SRC)
    return  LONG_LDRDAC(src);
}

#else

INLINE(Vddu,ULONG_LDRDAC) (unsigned long const src[1])
{
#define     ULONG_LDRDAC(SRC)   vld1_u64(SRC)
    return  vld1_u64(src);
}

INLINE(Vddi, LONG_LDRDAC) (long const src[1])
{
#define     LONG_LDRDAC(SRC)   vld1_s64(SRC)
    return  vld1_s64(src);
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_LDRDAC) (unsigned long long const src[1])
{
#define     ULLONG_LDRDAC(SRC)   vld1_u64((uint64_t const *) SRC)
    return  ULLONG_LDRDAC(src);
}

INLINE(Vddi,LLONG_LDRDAC) (long long const src[1])
{
#define     LLONG_LDRDAC(SRC)   vld1_s64((int64_t const *) SRC)
    return  LLONG_LDRDAC(src);
}

#endif

INLINE(Vdhf,FLT16_LDRDAC) (flt16_t const src[4])
{
#define     FLT16_LDRDAC(SRC)   MY_LDRDAC(f16,SRC)
    return  FLT16_LDRDAC(src);
}

INLINE(Vdwf,FLT_LDRDAC) (float const src[2])
{
#define     FLT_LDRDAC(SRC)   MY_LDRDAC(f32,SRC)
    return  FLT_LDRDAC(src);
}

INLINE(Vddf,DBL_LDRDAC) (double const src[1])
{
#define     DBL_LDRDAC(SRC)   MY_LDRDAC(f64,SRC)
    return  DBL_LDRDAC(src);
}

#if _LEAVE_ARM_LDRD
}
#endif

#if _ENTER_ARM_LDRQ
{
#endif

INLINE(Vqbu,UCHAR_LDRQAC) (unsigned char const src[16])
{
#define     UCHAR_LDRQAC(SRC)   MY_LDRQAC(u8,SRC)
    return  UCHAR_LDRQAC(src);
}

INLINE(Vqbi,SCHAR_LDRQAC) (signed char const src[16])
{
#define     SCHAR_LDRQAC(SRC)   MY_LDRQAC(s8,SRC)
    return  SCHAR_LDRQAC(src);
}

INLINE(Vqbc,CHAR_LDRQAC) (char const src[16])
{
#define     CHAR_LDRQAC(SRC)   VQBU_ASBC(MY_LDRQAC(u8,SRC))
    return  CHAR_LDRQAC(src);
}


INLINE(Vqhu,USHRT_LDRQAC) (unsigned short const src[8])
{
#define     USHRT_LDRQAC(SRC)   MY_LDRQAC(u16,SRC)
    return  USHRT_LDRQAC(src);
}

INLINE(Vqhi,SHRT_LDRQAC) (short const src[8])
{
#define     SHRT_LDRQAC(SRC)   MY_LDRQAC(s16,SRC)
    return  SHRT_LDRQAC(src);
}


INLINE(Vqwu,UINT_LDRQAC) (unsigned int const src[4])
{
#define     UINT_LDRQAC(SRC)   MY_LDRQAC(u32,SRC)
    return  UINT_LDRQAC(src);
}

INLINE(Vqwi,INT_LDRQAC) (int const src[4])
{
#define     INT_LDRQAC(SRC)   MY_LDRQAC(s32,SRC)
    return  INT_LDRQAC(src);
}


#if DWRD_NLONG == 2

INLINE(Vqwu,ULONG_LDRQAC) (unsigned long const src[4])
{
#define     ULONG_LDRQAC(SRC)   MY_LDRQAC(u32,SRC)
    return  ULONG_LDRQAC(src);
}

INLINE(Vqwi,LONG_LDRQAC) (long const src[4])
{
#define     LONG_LDRQAC(SRC)   MY_LDRQAC(s32,SRC)
    return  LONG_LDRQAC(src);
}

#else

INLINE(Vqdu,ULONG_LDRQAC) (unsigned long const src[2])
{
#define     ULONG_LDRQAC(SRC)   MY_LDRQAC(u64,SRC)
    return  ULONG_LDRQAC(src);
}

INLINE(Vqdi,LONG_LDRQAC) (long const src[2])
{
#define     LONG_LDRQAC(SRC)   MY_LDRQAC(s64,SRC)
    return  LONG_LDRQAC(src);
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_LDRQAC) (unsigned long long const src[2])
{
#define     ULLONG_LDRQAC(SRC)   MY_LDRQAC(u64,SRC)
    return  ULLONG_LDRQAC(src);
}

INLINE(Vqdi,LLONG_LDRQAC) (long long const src[2])
{
#define     LLONG_LDRQAC(SRC)   MY_LDRQAC(s64,SRC)
    return  LLONG_LDRQAC(src);
}

#else

INLINE(Vqqu,ULLONG_LDRQAC) (unsigned long long const src[1])
{
#define     ULLONG_LDRQAC(SRC)   vldrq_p128(SRC)
    return  ULLONG_LDRQAC(src);
}

INLINE(Vqqi,LLONG_LDRQAC) (long long const src[1])
{
#define     LLONG_LDRQAC(SRC)   VQQU_ASQI(vldrq_p128((void const *) SRC))
    return  LLONG_LDRQAC(src);
}

#endif

INLINE(Vqhf,FLT16_LDRQAC) (flt16_t const src[8])
{
#define     FLT16_LDRQAC(SRC)   MY_LDRQAC(f16,SRC)
    return  FLT16_LDRQAC(src);
}

INLINE(Vqwf,FLT_LDRQAC) (float const src[4])
{
#define     FLT_LDRQAC(SRC)   MY_LDRQAC(f32,SRC)
    return  FLT_LDRQAC(src);
}

INLINE(Vqdf,DBL_LDRQAC) (double const src[2])
{
#define     DBL_LDRQAC(SRC)   MY_LDRQAC(f64,SRC)
    return  DBL_LDRQAC(src);
}

#if QUAD_NLLONG == 1
INLINE(Vqqf,LDBL_LDRQAC) (long double src[1])
{
#define     LDBL_LDRQAC(SRC)   VQQU_ASQF(vldrq_p128((void const *) SRC))
    return  LDBL_LDRQAC(src);
}
#endif

#if _LEAVE_ARM_LDRQ
}
#endif

#if _ENTER_ARM_LDRO
{
#endif

#if CHAR_SIGNEDNESS
#   define  ABC_LDRO(p)     ((VOBI_TYPE){vld1q_s8_x2( (void const *)(p))})
#else
#   define  ABC_LDRO(p)     ((VOBU_TYPE){vld1q_u8_x2( (void const *)(p))})
#endif

#define     ABU_LDRO(p)     ((VOBU_TYPE){vld1q_u8_x2( (void const *)(p))})
#define     ABI_LDRO(p)     ((VOBI_TYPE){vld1q_s8_x2( (void const *)(p))})

#define     AHU_LDRO(p)     ((VOHU_TYPE){vld1q_u16_x2((void const *)(p))})
#define     AHI_LDRO(p)     ((VOHI_TYPE){vld1q_s16_x2((void const *)(p))})
#define     AHF_LDRO(p)     ((VOHF_TYPE){vld1q_f16_x2((void const *)(p))})

#define     AWU_LDRO(p)     ((VOWU_TYPE){vld1q_u32_x2((void const *)(p))})
#define     AWI_LDRO(p)     ((VOWI_TYPE){vld1q_s32_x2((void const *)(p))})
#define     AWF_LDRO(p)     ((VOWF_TYPE){vld1q_f32_x2((void const *)(p))})

#define     ADU_LDRO(p)     ((VODU_TYPE){vld1q_u64_x2((void const *)(p))})
#define     ADI_LDRO(p)     ((VODI_TYPE){vld1q_s64_x2((void const *)(p))})
#define     ADF_LDRO(p)     ((VODF_TYPE){vld1q_f64_x2((void const *)(p))})

#if _LEAVE_ARM_LDRO
}
#endif

#if _ENTER_ARM_LDRS
{
#endif

#if CHAR_SIGNEDNESS
#   define  ABC_LDRS(p)     ((VOBI_TYPE){vld1q_s8_x4( (void const *)(p))})
#else
#   define  ABC_LDRS(p)     ((VOBU_TYPE){vld1q_u8_x4( (void const *)(p))})
#endif
#define     ABU_LDRS(p)     ((VOBU_TYPE){vld1q_u8_x4( (void const *)(p))})
#define     ABI_LDRS(p)     ((VOBI_TYPE){vld1q_s8_x4( (void const *)(p))})

#define     AHU_LDRS(p)     ((VOHU_TYPE){vld1q_u16_x4((void const *)(p))})
#define     AHI_LDRS(p)     ((VOHI_TYPE){vld1q_s16_x4((void const *)(p))})
#define     AHF_LDRS(p)     ((VOHF_TYPE){vld1q_f16_x4((void const *)(p))})

#define     AWU_LDRS(p)     ((VOWU_TYPE){vld1q_u32_x4((void const *)(p))})
#define     AWI_LDRS(p)     ((VOWI_TYPE){vld1q_s32_x4((void const *)(p))})
#define     AWF_LDRS(p)     ((VOWF_TYPE){vld1q_f32_x4((void const *)(p))})

#define     ADU_LDRS(p)     ((VODU_TYPE){vld1q_u64_x4((void const *)(p))})
#define     ADI_LDRS(p)     ((VODI_TYPE){vld1q_s64_x4((void const *)(p))})
#define     ADF_LDRS(p)     ((VODF_TYPE){vld1q_f64_x4((void const *)(p))})

#if _LEAVE_ARM_LDRS
}
#endif

#if _ENTER_ARM_LUNN
{
#endif

/*  NOTE: keep the const qualifier for the macros; it will
    help prevent the possibility of doing something like:

        (SHRT_LUNN(dst)=(src))

*/

INLINE(  _Bool,  BOOL_LUNNAC) (void const *a) {return *(_Bool const *) a;}

INLINE(  uchar, UCHAR_LUNNAC) (void const *a) {return *(uchar const *) a;}
INLINE(  schar, SCHAR_LUNNAC) (void const *a) {return *(schar const *) a;}
INLINE(   char,  CHAR_LUNNAC) (void const *a) {return *(char const *) a;}

INLINE( ushort, USHRT_LUNNAC) (void const *a)
{
    return ((USHRT_STG(TYPE) const *) a)->M.U;
}

INLINE(   short, SHRT_LUNNAC) (void const *a)
{
    return ((SHRT_STG(TYPE) const *) a)->M.I;
}

INLINE(   uint,  UINT_LUNNAC) (void const *a)
{
    return ((UINT_STG(TYPE) const *) a)->M.U;
}

INLINE(    int,   INT_LUNNAC) (void const *a)
{
    return  ((INT_STG(TYPE) const *) a)->M.I;
}

INLINE(  ulong, ULONG_LUNNAC) (void const *a)
{
    return  ((ULONG_STG(TYPE) const *) a)->M.U;
}

INLINE(   long,  LONG_LUNNAC) (void const *a)
{
    return  ((LONG_STG(TYPE) const *) a)->M.I;
}

INLINE( ullong,ULLONG_LUNNAC) (void const *a)
{
    return  ((ULLONG_STG(TYPE) const *) a)->M.U;
}

INLINE(  llong, LLONG_LUNNAC) (void const *a)
{
    return  ((LLONG_STG(TYPE) const *) a)->M.I;
}

INLINE(flt16_t, FLT16_LUNNAC) (void const *a)
{
    return  ((FLT16_STG(TYPE) const *) a)->M.F;
}

INLINE(  float,   FLT_LUNNAC) (void const *a)
{
    return  ((FLT_STG(TYPE) const *) a)->M.F;
}

INLINE( double,   DBL_LUNNAC) (void const *a)
{
    return  ((DBL_STG(TYPE) const *) a)->M.F;
}

#if _LEAVE_ARM_LUNN
}
#endif

#if _ENTER_ARM_LUN1
{
#endif

INLINE(Vwbu,VWBU_LUN1) (Vwbu d, Rc(0, 3) k, void const *a)
{
#define     VWBU_LUN1(D, K, A) VWBU_SET1(D, K, (*(uint8_t *)(A)))
    return (VWBU_SET1)(d, k, *(uint8_t *) a);
}

INLINE(Vwbi,VWBI_LUN1) (Vwbi d, Rc(0, 3) k, void const *a)
{
#define     VWBI_LUN1(D, K, A) VWBI_SET1(D, K, (*(int8_t *)(A)))
    return (VWBI_SET1)(d, k, *(int8_t *) a);
}

INLINE(Vwbc,VWBC_LUN1) (Vwbc d, Rc(0, 3) k, void const *a)
{
#define     VWBC_LUN1(D, K, A) VWBC_SET1(D, K, (*(char *)(A)))
    return (VWBC_SET1)(d, k, *(char *) a);
}


INLINE(Vwhu,VWHU_LUN1) (Vwhu d, Rc(0, 1) k, void const *a)
{
#define     VWHU_LUN1(D, K, A) VWHU_SET1(D, K, ((HALF_TYPE *)(A)->M.U))
    return (VWHU_SET1)(d, k, ((HALF_TYPE *) a)->M.U);
}

INLINE(Vwhi,VWHI_LUN1) (Vwhi d, Rc(0, 1) k, void const *a)
{
#define     VWHI_LUN1(D, K, A) VWHI_SET1(D, K, ((HALF_TYPE *)(A)->M.I))
    return (VWHI_SET1)(d, k, ((HALF_TYPE *) a)->M.I);
}

INLINE(Vwhf,VWHF_LUN1) (Vwhf d, Rc(0, 1) k, void const *a)
{
#define     VWHF_LUN1(D, K, A) VWHF_SET1(D, K, ((HALF_TYPE *)(A)->M.F))
    return (VWHF_SET1)(d, k, ((HALF_TYPE *) a)->M.F);
}


INLINE(Vdbu,VDBU_LUN1) (Vdbu d, Rc(0, 7) k, void const *a)
{
#define     VDBU_LUN1(D, K, A) vld1_lane_u8((void *)(A), D, K)
    return (VDBU_SET1)(d,k, *(uint8_t *) a);
}

INLINE(Vdbi,VDBI_LUN1) (Vdbi d, Rc(0, 7) k, void const *a)
{
#define     VDBI_LUN1(D, K, A) vld1_lane_s8((void *)(A), D, K)
    return (VDBI_SET1)(d,k, *(int8_t *) a);
}

INLINE(Vdbc,VDBC_LUN1) (Vdbc d, Rc(0, 7) k, void const *a)
{
#define  VDBC_LUN1(D, K, A) \
VDBU_ASBC(vld1_lane_u8((void *)(A), VDBC_ASBU(D), K))
    return (VDBC_SET1)(d, k, *(char *) a);
}


INLINE(Vdhu,VDHU_LUN1) (Vdhu d, Rc(0, 3) k, void const *a)
{
#define     VDHU_LUN1(D, K, A) VDHU_SET1(D, K, (((HALF_TYPE *)(a))->M.U))
    return (VDHU_SET1)(d, k, ((HALF_TYPE *) a)->M.U);
}

INLINE(Vdhi,VDHI_LUN1) (Vdhi d, Rc(0, 3) k, void const *a)
{
#define     VDHI_LUN1(D, K, A) VDHI_SET1(D, K, (((HALF_TYPE *)(A))->M.I))
    return (VDHI_SET1)(d, k, ((HALF_TYPE *) a)->M.I);
}

INLINE(Vdhf,VDHF_LUN1) (Vdhf d, Rc(0, 3) k, void const *a)
{
#define     VDHF_LUN1(D, K, A) VDHF_SET1(D, K, (((HALF_TYPE *)(A))->M.F))
    return (VDHF_SET1)(d, k, ((HALF_TYPE *) a)->M.F);
}


INLINE(Vdwu,VDWU_LUN1) (Vdwu d, Rc(0, 1) k, void const *a)
{
#define     VDWU_LUN1(D, K, A) VDWU_SET1(D, K, (((WORD_TYPE *)(A))->M.U))
    return (VDWU_SET1)(d, k, ((WORD_TYPE *) a)->M.U);
}

INLINE(Vdwi,VDWI_LUN1) (Vdwi d, Rc(0, 1) k, void const *a)
{
#define     VDWI_LUN1(D, K, A) VDWI_SET1(D, K, (((WORD_TYPE *)(A))->M.I))
    return (VDWI_SET1)(d, k, ((WORD_TYPE *) a)->M.I);
}

INLINE(Vdwf,VDWF_LUN1) (Vdwf d, Rc(0, 1) k, void const *a)
{
#define     VDWF_LUN1(D, K, A) VDWF_SET1(D, K, (((WORD_TYPE *)(A))->M.F))
    return (VDWF_SET1)(d, k, ((WORD_TYPE *) a)->M.F);
}


INLINE(Vqbu,VQBU_LUN1) (Vqbu d, Rc(0, 15) k, void const *a)
{
#define     VQBU_LUN1(D, K, A) vld1q_lane_u8((void *)(A), D, K)
    return (VQBU_SET1)(d,k, *(uint8_t *) a);
}

INLINE(Vqbi,VQBI_LUN1) (Vqbi d, Rc(0, 15) k, void const *a)
{
#define     VQBI_LUN1(D, K, A) vld1q_lane_s8((void *)(A), D, K)
    return (VQBI_SET1)(d,k, *(int8_t *) a);
}

INLINE(Vqbc,VQBC_LUN1) (Vqbc d, Rc(0, 15) k, void const *a)
{
#define  VQBC_LUN1(D, K, A) \
VQBU_ASBC(vld1q_lane_u8((void *)(A), VQBC_ASBU(D), K))
    return (VQBC_SET1)(d, k, *(char *) a);
}


INLINE(Vqhu,VQHU_LUN1) (Vqhu d, Rc(0, 7) k, void const *a)
{
#define     VQHU_LUN1(D, K, A) VQHU_SET1(D, K, (((HALF_TYPE *)(a))->M.U))
    return (VQHU_SET1)(d, k, ((HALF_TYPE *) a)->M.U);
}

INLINE(Vqhi,VQHI_LUN1) (Vqhi d, Rc(0, 7) k, void const *a)
{
#define     VQHI_LUN1(D, K, A) VQHI_SET1(D, K, (((HALF_TYPE *)(A))->M.I))
    return (VQHI_SET1)(d, k, ((HALF_TYPE *) a)->M.I);
}

INLINE(Vqhf,VQHF_LUN1) (Vqhf d, Rc(0, 7) k, void const *a)
{
#define     VQHF_LUN1(D, K, A) VQHF_SET1(D, K, (((HALF_TYPE *)(A))->M.F))
    return (VQHF_SET1)(d, k, ((HALF_TYPE *) a)->M.F);
}


INLINE(Vqwu,VQWU_LUN1) (Vqwu d, Rc(0, 3) k, void const *a)
{
#define     VQWU_LUN1(D, K, A) VQWU_SET1(D, K, (((WORD_TYPE *)(A))->M.U))
    return (VQWU_SET1)(d, k, ((WORD_TYPE *) a)->M.F);
}

INLINE(Vqwi,VQWI_LUN1) (Vqwi d, Rc(0, 3) k, void const *a)
{
#define     VQWI_LUN1(D, K, A) VQWI_SET1(D, K, (((WORD_TYPE *)(A))->M.I))
    return (VQWI_SET1)(d, k, ((WORD_TYPE *) a)->M.I);
}

INLINE(Vqwf,VQWF_LUN1) (Vqwf d, Rc(0, 3) k, void const *a)
{
#define     VQWF_LUN1(D, K, A) VQWF_SET1(D, K, (((WORD_TYPE *)(A))->M.F))
    return (VQWI_SET1)(d, k, ((WORD_TYPE *) a)->M.F);
}


INLINE(Vqdu,VQDU_LUN1) (Vqdu d, Rc(0, 1) k, void const *a)
{
#define     VQDU_LUN1(D, K, A) VQDU_SET1(D, K, (((DWRD_TYPE *)(A))->M.U))
    return (VQDU_SET1)(d, k, ((DWRD_TYPE *) a)->M.U);
}

INLINE(Vqdi,VQDI_LUN1) (Vqdi d, Rc(0, 1) k, void const *a)
{
#define     VQDI_LUN1(D, K, A) VQDI_SET1(D, K, (((DWRD_TYPE *)(A))->M.I))
    return (VQDI_SET1)(d, k, ((DWRD_TYPE *) a)->M.I);
}

INLINE(Vqdf,VQDF_LUN1) (Vqdf d, Rc(0, 1) k, void const *a)
{
#define     VQDF_LUN1(D, K, A) VQDF_SET1(D, K, (((DWRD_TYPE *)(A))->M.F))
    return (VQDI_SET1)(d, k, ((DWRD_TYPE *) a)->M.F);
}

#if _LEAVE_ARM_LUN1
}
#endif

#if _ENTER_ARM_LUNW
{
#endif

INLINE(Vwbu,UCHAR_LUNWAC) (void const *a)
{
#define     UCHAR_LUNWAC(A)     WBU_ASTV( (((WORD_TYPE const *) A)->M.F) )
    return  UCHAR_LUNWAC(a);
}

INLINE(Vwbi,SCHAR_LUNWAC) (void const *a)
{
#define     SCHAR_LUNWAC(A)     WBI_ASTV( (((WORD_TYPE const *) A)->M.F) )
    return  SCHAR_LUNWAC(a);
}

INLINE(Vwbc,CHAR_LUNWAC) (void const *a)
{
#define     CHAR_LUNWAC(A)      WBC_ASTV( (((WORD_TYPE const *) A)->M.F) )
    return  CHAR_LUNWAC(a);
}

INLINE(Vwhu,USHRT_LUNWAC) (void const *a)
{
#define     USHRT_LUNWAC(A)     WHU_ASTV( (((WORD_TYPE const *) A)->M.F) )
    return  USHRT_LUNWAC(a);
}

INLINE(Vwhi,SHRT_LUNWAC) (void const *a)
{
#define     SHRT_LUNWAC(A)      WHI_ASTV( (((WORD_TYPE const *) A)->M.F) )
    return  SHRT_LUNWAC(a);
}

INLINE(Vwwu,UINT_LUNWAC) (void const *a)
{
#define     UINT_LUNWAC(A)      WWU_ASTV( (((WORD_TYPE const *) A)->M.F) )
    return  UINT_LUNWAC(a);
}

INLINE(Vwwi,INT_LUNWAC) (void const *a)
{
#define     INT_LUNWAC(A)       WWI_ASTV( (((WORD_TYPE const *) A)->M.F) )
    return  INT_LUNWAC(a);
}


#if DWRD_NLONG == 2

INLINE(Vwwu,ULONG_LUNWAC) (void const *a)
{
#   define  ULONG_LUNWAC(A)     WWU_ASTV( (((WORD_TYPE const *) A)->M.F) )
}

INLINE(Vdwi,LONG_LUNDAC) (void const *a)
{
#   define  LONG_LUNWAC(A)      WWI_ASTV( (((WORD_TYPE const *) A)->M.F) )
    return  LONG_LUNWAC(a);
}

#endif

INLINE(Vwhf,FLT16_LUNWAC) (void const *a)
{
#define     FLT16_LUNWAC(A)     WHF_ASTV( (((WORD_TYPE const *) A)->M.F) )
    return  FLT16_LUNWAC(a);
}

INLINE(Vwwf,FLT_LUNWAC) (void const *a)
{
#define     FLT_LUNWAC(A)       WWF_ASTV( (((WORD_TYPE const *) A)->M.F) )
    return  FLT_LUNWAC(a);
}

#if _LEAVE_ARM_LUNW
}
#endif

#if _ENTER_ARM_LUND
{
#endif

/*  
TODO: verify that there's any difference between e.g.
LD1.8B and LD1.4H in armv8. If they're identical, we can
dispense with this reinterpret cast convolution, both here
and in sun*.
*/

INLINE(Vdbu,UCHAR_LUNDAC) (void const *a)
{
#define     UCHAR_LUNDAC(A)   vld1_u8( ((void const *) A) )
    return  UCHAR_LUNDAC(a);
}

INLINE(Vdbi,SCHAR_LUNDAC) (void const *a)
{
#define     SCHAR_LUNDAC(A)   vld1_s8( ((void const *) A) )
    return  SCHAR_LUNDAC(a);
}

INLINE(Vdbc, CHAR_LUNDAC) (void const *a)
{
#define     CHAR_LUNDAC(A)    VDBU_ASBC(vld1_u8( ((void const *) A) ))
    return  CHAR_LUNDAC(a);
}

INLINE(Vdhu,USHRT_LUNDAC) (void const *a)
{
    return  vreinterpret_u16_u8(vld1_u8(a));
}

INLINE(Vdhi, SHRT_LUNDAC) (void const *a)
{
    return  vreinterpret_s16_u8(vld1_u8(a));
}

INLINE(Vdwu, UINT_LUNDAC) (void const *a)
{
    return  vreinterpret_u32_u8(vld1_u8(a));
}

INLINE(Vdwi,  INT_LUNDAC) (void const *a)
{
    return  vreinterpret_s32_u8(vld1_u8(a));
}

#if DWRD_NLONG == 2

INLINE(Vdwu,ULONG_LUNDAC) (void const *a)
{
    return  vreinterpret_u32_u8(vld1_u8(a));
}

INLINE(Vdwi,LONG_LUNDAC) (void const *a)
{
    return  vreinterpret_s32_u8(vld1_u8(a));
}

#else

INLINE(Vddu,ULONG_LUNDAC) (void const *a)
{
    return  vreinterpret_u64_u8(vld1_u8(a));
}

INLINE(Vddi,LONG_LUNDAC) (void const *a)
{
    return  vreinterpret_s64_u8(vld1_u8(a));
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_LUNDAC) (void const *a)
{
    return  vreinterpret_u64_u8(vld1_u8(a));
}

INLINE(Vddi,LLONG_LUNDAC) (void const *a)
{
    return  vreinterpret_s64_u8(vld1_u8(a));
}

#endif

INLINE(Vdhf,FLT16_LUNDAC) (void const *a)
{
    return  vreinterpret_f16_u8(vld1_u8(a));
}

INLINE(Vdwf,FLT_LUNDAC) (void const *a)
{
    return  vreinterpret_f32_u8(vld1_u8(a));
}

INLINE(Vddf,DBL_LUNDAC) (void const *a)
{
    return  vreinterpret_f64_u8(vld1_u8(a));
}

#if _LEAVE_ARM_LUND
}
#endif

#if _ENTER_ARM_LUNQ
{
#endif

INLINE(Vqbu, UCHAR_LUNQAC) (void const *a) {return vld1q_u8(a);}
INLINE(Vqbi, SCHAR_LUNQAC) (void const *a) {return vld1q_s8(a);}
INLINE(Vqbc,  CHAR_LUNQAC) (void const *a)
{
#define     CHAR_LUNQAC(A) VQBU_ASBC(vld1q_u8(((void const *) A)))
    return  CHAR_LUNQAC(a);
}

INLINE(Vqhu, USHRT_LUNQAC) (void const *a)
{
    return  vreinterpretq_u16_u8(vld1q_u8(a));
}

INLINE(Vqhi,  SHRT_LUNQAC) (void const *a)
{
    return  vreinterpretq_s16_u8(vld1q_u8(a));
}

INLINE(Vqwu,  UINT_LUNQAC) (void const *a)
{
    return  vreinterpretq_u32_u8(vld1q_u8(a));
}

INLINE(Vqwi,   INT_LUNQAC) (void const *a)
{
    return  vreinterpretq_s32_u8(vld1q_u8(a));
}

#if DWRD_NLONG == 2

{
    return  vreinterpretq_u32_u8(vld1q_u8(a));
}

INLINE(Vqwu, ULONG_LUNQAC) (void const *a)
INLINE(Vqwi,  LONG_LUNQAC) (void const *a)
{
    return  vreinterpretq_s32_u8(vld1q_u8(a));
}

#else

INLINE(Vqdu, ULONG_LUNQAC) (void const *a)
{
    return  vreinterpretq_u64_u8(vld1q_u8(a));
}

INLINE(Vqdi,  LONG_LUNQAC) (void const *a)
{
    return  vreinterpretq_s64_u8(vld1q_u8(a));
}

#endif

#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_LUNQAC) (void const *a)
{
    return  vreinterpretq_u64_u8(vld1q_u8(a));
}

INLINE(Vqdi, LLONG_LUNQAC) (void const *a)
{
    return  vreinterpretq_s64_u8(vld1q_u8(a));
}

#endif

INLINE(Vqhf, FLT16_LUNQAC) (void const *a)
{
    return  vreinterpretq_f16_u8(vld1q_u8(a));
}

INLINE(Vqwf,   FLT_LUNQAC) (void const *a)
{
    return  vreinterpretq_f32_u8(vld1q_u8(a));
}

INLINE(Vqdf,   DBL_LUNQAC) (void const *a)
{
    return  vreinterpretq_f64_u8(vld1q_u8(a));
}

#if _LEAVE_ARM_LUNQ
}
#endif


#if _ENTER_ARM_SUNW
{
#endif
/*
WORD_TYPE.M is a packed float
*/

INLINE(Vwbu, UCHAR_SUNWA)   (void *dst, Vwbu src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWBU_ASTM(src)), src;
}

INLINE(Vwbi, SCHAR_SUNWA)   (void *dst, Vwbi src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWBI_ASTM(src)), src;
}

INLINE(Vwbc,  CHAR_SUNWA)   (void *dst, Vwbc src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWBC_ASTM(src)), src;
}


INLINE(Vwhu, USHRT_SUNWA)   (void *dst, Vwhu src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWHU_ASTM(src)), src;
}

INLINE(Vwhi,  SHRT_SUNWA)   (void *dst, Vwhi src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWHI_ASTM(src)), src;
}


INLINE(Vwwu,  UINT_SUNWA)   (void *dst, Vwwu src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWWU_ASTM(src)), src;
}

INLINE(Vwwi,   INT_SUNWA)   (void *dst, Vwwi src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWWI_ASTM(src)), src;
}


#if DWRD_NLONG == 2

INLINE(Vwwu, ULONG_SUNWA)   (void *dst, Vwwu src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWWU_ASTM(src)), src;
}

INLINE(Vwwi,  LONG_SUNWA)   (void *dst, Vwwi src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWWI_ASTM(src)), src;
}

#endif


INLINE(Vwhf, FLT16_SUNWA)   (void *dst, Vwhf src)
{
    return  (((WORD_TYPE *) dst)->M.F=VWHF_ASTM(src)), src;
}

INLINE(Vwwf, FLT_SUNWA)   (void *dst, Vwwf src)
{
    ((WORD_TYPE *) dst)->M.F = VWWF_ASTM(src);
    return  src;
}


#if _LEAVE_ARM_SUNW
}
#endif

#if _ENTER_ARM_SUND
{
#endif

INLINE(Vdbu, UCHAR_SUNDA) (void *dst, Vdbu src)
{
    vst1_u8(dst, src);
    return  src;
}

INLINE(Vdbi, SCHAR_SUNDA) (void *dst, Vdbi src)
{
    vst1_s8(dst, src);
    return  src;
}

INLINE(Vdbc,  CHAR_SUNDA) (void *dst, Vdbc src)
{
    vst1_u8(dst, VDBC_ASBU(src));
    return  src;
}


INLINE(Vdhu, USHRT_SUNDA) (void *dst, Vdhu src)
{
    vst1_u8(dst, vreinterpret_u8_u16(src));
    return  src;
}

INLINE(Vdhi,  SHRT_SUNDA) (void *dst, Vdhi src)
{
    vst1_u8(dst, vreinterpret_u8_s16(src));
    return  src;
}



INLINE(Vdwu,  UINT_SUNDA) (void *dst, Vdwu src)
{
    vst1_u8(dst, vreinterpret_u8_u32(src));
    return  src;
}

INLINE(Vdwi,   INT_SUNDA) (void *dst, Vdwi src)
{
    vst1_u8(dst, vreinterpret_u8_s32(src));
    return  src;
}

#if DWRD_NLONG == 2

INLINE(Vdwu, ULONG_SUNDA) (void *dst, Vdwu src)
{
    vst1_u8(dst, vreinterpret_u8_u32(src));
    return  src;
}

INLINE(Vdwi,  LONG_SUNDA) (void *dst, Vdwi src)
{
    vst1_u8(dst, vreinterpret_u8_s32(src));
    return  src;
}

#else

INLINE(Vddu, ULONG_SUNDA) (void *dst, Vddu src)
{
    vst1_u8(dst, vreinterpret_u8_u64(src));
    return  src;
}

INLINE(Vddi,  LONG_SUNDA) (void *dst, Vddi src)
{
    vst1_u8(dst, vreinterpret_u8_s64(src));
    return  src;
}

#endif


#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_SUNDA) (void *dst, Vddu src)
{
    vst1_u8(dst, vreinterpret_u8_u64(src));
    return  src;
}

INLINE(Vddi, LLONG_SUNDA) (void *dst, Vddi src)
{
    vst1_u8(dst, vreinterpret_u8_s64(src));
    return  src;
}

#endif

INLINE(Vdhf, FLT16_SUNDA) (void *dst, Vdhf src)
{
    vst1_u8(dst, vreinterpret_u8_f16(src));
    return  src;
}

INLINE(Vdwf,   FLT_SUNDA) (void *dst, Vdwf src)
{
    vst1_u8(dst, vreinterpret_u8_f32(src));
    return  src;
}

INLINE(Vddf,   DBL_SUNDA) (void *dst, Vddf src)
{
    vst1_u8(dst, vreinterpret_u8_f64(src));
    return  src;
}

#if _LEAVE_ARM_SUND
}
#endif


#if _ENTER_ARM_SUNQ
{
#endif

INLINE(Vqbu, UCHAR_SUNQA) (void *dst, Vqbu src)
{
    vst1q_u8(dst, src);
    return  src;
}

INLINE(Vqbi, SCHAR_SUNQA) (void *dst, Vqbi src)
{
    vst1q_s8(dst, src);
    return  src;
}

INLINE(Vqbc,  CHAR_SUNQA) (void *dst, Vqbc src)
{
    vst1q_u8(dst, VQBC_ASBU(src));
    return  src;
}


INLINE(Vqhu, USHRT_SUNQA) (void *dst, Vqhu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u16(src));
    return  src;
}

INLINE(Vqhi,  SHRT_SUNQA) (void *dst, Vqhi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s16(src));
    return  src;
}



INLINE(Vqwu,  UINT_SUNQA) (void *dst, Vqwu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u32(src));
    return  src;
}

INLINE(Vqwi,   INT_SUNQA) (void *dst, Vqwi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s32(src));
    return  src;
}

#if DWRD_NLONG == 2

INLINE(Vqwu, ULONG_SUNQA) (void *dst, Vqwu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u32(src));
    return  src;
}

INLINE(Vqwi,  LONG_SUNQA) (void *dst, Vqwi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s32(src));
    return  src;
}

#else

INLINE(Vqdu, ULONG_SUNQA) (void *dst, Vqdu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u64(src));
    return  src;
}

INLINE(Vqdi,  LONG_SUNQA) (void *dst, Vqdi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s64(src));
    return  src;
}

#endif


#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_SUNQA) (void *dst, Vqdu src)
{
    vst1q_u8(dst, vreinterpretq_u8_u64(src));
    return  src;
}

INLINE(Vqdi, LLONG_SUNQA) (void *dst, Vqdi src)
{
    vst1q_u8(dst, vreinterpretq_u8_s64(src));
    return  src;
}

#endif

INLINE(Vqhf, FLT16_SUNQA) (void *dst, Vqhf src)
{
    vst1q_u8(dst, vreinterpretq_u8_f16(src));
    return  src;
}

INLINE(Vqwf,   FLT_SUNQA) (void *dst, Vqwf src)
{
    vst1q_u8(dst, vreinterpretq_u8_f32(src));
    return  src;
}

INLINE(Vqdf,   DBL_SUNQA) (void *dst, Vqdf src)
{
    vst1q_u8(dst, vreinterpretq_u8_f64(src));
    return  src;
}

#if _LEAVE_ARM_SUNQ
}
#endif


#if _ENTER_ARM_STRW
{
#endif

INLINE(Vwbu,UCHAR_STRWA)    (uchar dst[4], Vwbu src)
{
    ((WORD_TYPE *) dst)->F = VWBU_ASTM(src);
    return  src;
}

INLINE(Vwbi,SCHAR_STRWA)    (schar dst[4], Vwbi src)
{
    ((WORD_TYPE *) dst)->F = VWBI_ASTM(src);
    return  src;
}

INLINE(Vwbc, CHAR_STRWA)     (char dst[4], Vwbc src)
{
    ((WORD_TYPE *) dst)->F = VWBC_ASTM(src);
    return  src;
}


INLINE(Vwhu,USHRT_STRWA)   (ushort dst[2], Vwhu src)
{
    ((WORD_TYPE *) dst)->F = VWHU_ASTM(src);
    return  src;
}

INLINE(Vwhi, SHRT_STRWA)    (short dst[2], Vwhi src)
{
    ((WORD_TYPE *) dst)->F = VWHI_ASTM(src);
    return  src;
}


INLINE(Vwwu, UINT_STRWA)     (uint dst[1], Vwwu src)
{
    ((WORD_TYPE *) dst)->F = VWWU_ASTM(src);
    return  src;
}

INLINE(Vwwi,  INT_STRWA)      (int dst[1], Vwwi src)
{
    ((WORD_TYPE *) dst)->F = VWWI_ASTM(src);
    return  src;
}

#if DWRD_NLONG == 2

INLINE(Vwwu,ULONG_STRWA)    (ulong dst[1], Vwwu src)
{
    ((WORD_TYPE *) dst)->F = VWWU_ASTM(src);
    return  dst;
}

INLINE(Vwwi, LONG_STRWA)     (long dst[1], Vwwi src)
{
    ((WORD_TYPE *) dst)->F = VWWI_ASTM(src);
    return  dst;
}

#endif

INLINE(Vwhf, FLT16_STRWA) (flt16_t dst[2], Vwhf src)
{
    ((WORD_TYPE *) dst)->F = VWHF_ASTM(src);
    return  src;
}

INLINE(Vwwf,   FLT_STRWA)   (float dst[1], Vwwf src)
{
    ((WORD_TYPE *) dst)->F = VWWF_ASTM(src);
    return  src;
}

#if _LEAVE_ARM_STRW
}
#endif

#if _ENTER_ARM_STRD
{
#endif

INLINE(Vdbu, UCHAR_STRDA)   (uchar dst[8], Vdbu src)
{
    vst1_u8(dst, src);
    return  src;
}

INLINE(Vdbi, SCHAR_STRDA)   (schar dst[8], Vdbi src)
{
    vst1_s8(dst, src);
    return  src;
}

INLINE(Vdbc,  CHAR_STRDA)    (char dst[8], Vdbc src)
{
    vst1_u8(((void *) dst), VDBC_ASBU(src));
    return  src;
}


INLINE(Vdhu, USHRT_STRDA)  (ushort dst[4], Vdhu src)
{
    vst1_u16(dst, src);
    return  src;
}

INLINE(Vdhi,  SHRT_STRDA)   (short dst[4], Vdhi src)
{
    vst1_s16(dst, src);
    return  src;
}



INLINE(Vdwu,  UINT_STRDA)    (uint dst[2], Vdwu src)
{
    vst1_u32(dst, src);
    return  src;
}

INLINE(Vdwi,   INT_STRDA)     (int dst[2], Vdwi src)
{
    vst1_s32(dst, src);
    return  src;
}

#if DWRD_NLONG == 2

INLINE(Vdwu, ULONG_STRDA)   (ulong dst[2], Vdwu src)
{
    vst1_u32(((uint32_t *) dst), src);
    return  src;
}

INLINE(Vdwi,  LONG_STRDA)    (long dst[2], Vdwi src)
{
    vst1_s32(((int32_t *) dst), src);
    return  src;
}

#else

INLINE(Vddu, ULONG_STRDA)   (ulong dst[1], Vddu src)
{
    vst1_u64(((uint64_t *) dst), src);
    return  src;
}

INLINE(Vddi,  LONG_STRDA)    (long dst[1], Vddi src)
{
    vst1_s64(((int64_t *) dst), src);
    return  src;
}

#endif


#if QUAD_NLLONG == 2

INLINE(Vddu,ULLONG_STRDA)  (ullong dst[1], Vddu src)
{
    vst1_u64(((uint64_t *) dst), src);
    return  src;
}

INLINE(Vddi, LLONG_STRDA)   (llong dst[1], Vddi src)
{
    vst1_s64(((int64_t *) dst), src);
    return  src;
}

#endif

INLINE(Vdhf, FLT16_STRDA) (flt16_t dst[4], Vdhf src)
{
    vst1_f16(dst, src);
    return  src;
}

INLINE(Vdwf,   FLT_STRDA)   (float dst[2], Vdwf src)
{
    vst1_f32(dst, src);
    return  src;
}

INLINE(Vddf,   DBL_STRDA)  (double dst[1], Vddf src)
{
    vst1_f64(dst, src);
    return  src;
}

#if _LEAVE_ARM_STRD
}
#endif

#if _ENTER_ARM_STRQ
{
#endif

INLINE(Vqbu, UCHAR_STRQA)   (uchar dst[16], Vqbu src)
{
    vst1q_u8(dst, src);
    return  src;
}

INLINE(Vqbi, SCHAR_STRQA)   (schar dst[16], Vqbi src)
{
    vst1q_s8(dst, src);
    return  src;
}

INLINE(Vqbc,  CHAR_STRQA)    (char dst[16], Vqbc src)
{
    vst1q_u8(((void *) dst), VQBC_ASBU(src));
    return  src;
}


INLINE(Vqhu, USHRT_STRQA)  (ushort dst[8], Vqhu src)
{
    vst1q_u16(dst, src);
    return  src;
}

INLINE(Vqhi,  SHRT_STRQA)   (short dst[8], Vqhi src)
{
    vst1q_s16(dst, src);
    return  src;
}



INLINE(Vqwu,  UINT_STRQA)    (uint dst[4], Vqwu src)
{
    vst1q_u32(dst, src);
    return  src;
}

INLINE(Vqwi,   INT_STRQA)     (int dst[4], Vqwi src)
{
    vst1q_s32(dst, src);
    return  src;
}

#if DWRD_NLONG == 2

INLINE(Vqwu, ULONG_STRQA)   (ulong dst[4], Vqwu src)
{
    vst1q_u32(((uint32_t *) dst), src);
    return  src;
}

INLINE(Vqwi,  LONG_STRQA)    (long dst[4], Vqwi src)
{
    vst1q_s32(((int32_t *) dst), src);
    return  src;
}

#else

INLINE(Vqdu, ULONG_STRQA)   (ulong dst[2], Vqdu src)
{
    vst1q_u64(((uint64_t *) dst), src);
    return  src;
}

INLINE(Vqdi,  LONG_STRQA)    (long dst[2], Vqdi src)
{
    vst1q_s64(((int64_t *) dst), src);
    return  src;
}

#endif


#if QUAD_NLLONG == 2

INLINE(Vqdu,ULLONG_STRQA)  (ullong dst[2], Vqdu src)
{
    vst1q_u64(((uint64_t *) dst), src);
    return  src;
}

INLINE(Vqdi, LLONG_STRQA)   (llong dst[2], Vqdi src)
{
    vst1q_s64(((int64_t *) dst), src);
    return  src;
}

#endif

INLINE(Vqhf, FLT16_STRQA) (flt16_t dst[8], Vqhf src)
{
    vst1q_f16(dst, src);
    return  src;
}

INLINE(Vqwf,   FLT_STRQA)   (float dst[4], Vqwf src)
{
    vst1q_f32(dst, src);
    return  src;
}

INLINE(Vqdf,   DBL_STRQA)  (double dst[2], Vqdf src)
{
    vst1q_f64(dst, src);
    return  src;
}

#if _LEAVE_ARM_STRQ
}
#endif


#if _ENTER_ARM_NEWL
{
#endif

#define     WBZ_NEWL(K0, K1, K2, K3)    \
vget_lane_f32(                          \
    vreinterpret_f32_u64(               \
        vdup_n_u64(                     \
            ((0xffull&(K0))<<000)       \
        |   ((0xffull&(K1))<<010)       \
        |   ((0xffull&(K2))<<020)       \
        |   ((0xffull&(K3))<<030)       \
        )                               \
    ),                                  \
    V2_K0                               \
)
#define     WBU_NEWL(K0, K1, K2, K3) WBZ_NEWL(K0, K1, K2, K3)
#define     WBI_NEWL(K0, K1, K2, K3) WBZ_NEWL(K0, K1, K2, K3)
#define     WBC_NEWL(K0, K1, K2, K3) WBZ_NEWL(K0, K1, K2, K3)

#define     WHZ_NEWL(K0, K1)        \
vget_lane_f32(                      \
    vreinterpret_f32_u64(           \
        vdup_n_u64(                 \
            ((0xffffull&(K0))<<000) \
        |   ((0xffffull&(K1))<<020) \
        )                           \
    ),                              \
    V2_K0                           \
)
#define     WHU_NEWL(...)   WHZ_NEWL(__VA_ARGS__)
#define     WHI_NEWL(...)   WHZ_NEWL(__VA_ARGS__)
#define     WHF_NEWL(K0, K1)    \
vget_lane_f32(                  \
    vreinterpret_f32_f16(       \
        MY_VSET2(               \
            V4,                 \
            vset_lane_f16,      \
            DHF_VOID,           \
            K0, K1              \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     WWU_NEWL(K0)    ((union Word){.U=K0}).F
#define     WWI_NEWL(K0)    ((union Word){.I=K0}).F
#define     WWF_NEWL(K0)    ((union Word){.F=K0}).F

#define     DBZ_NEWL(T, K0, K1, K2, K3, K4, K5, K6, K7) \
vreinterpret##T(                \
    vdup_n_u64(                 \
        ((0xffull&(K0))<<000)   \
    |   ((0xffull&(K1))<<010)   \
    |   ((0xffull&(K2))<<020)   \
    |   ((0xffull&(K3))<<030)   \
    |   ((0xffull&(K4))<<040)   \
    |   ((0xffull&(K5))<<050)   \
    |   ((0xffull&(K6))<<060)   \
    |   ((0xffull&(K7))<<070)   \
    )\
)

#define     DBU_NEWL(...)   DBZ_NEWL(_u8_u64, __VA_ARGS__)
#define     DBI_NEWL(...)   DBZ_NEWL(_s8_u64, __VA_ARGS__)
#if CHAR_MIN
#   define  DBC_NEWL(...)   DBI_NEWL(__VA_ARGS__)
#else
#   define  DBC_NEWL(...)   DBU_NEWL(__VA_ARGS__)
#endif

#define     DHZ_NEWL(T, K0, K1, K2, K3) \
vreinterpret_##T(                       \
    vdup_n_u64(                         \
        ((0xffffull&(K0))<<000)         \
    |   ((0xffffull&(K1))<<020)         \
    |   ((0xffffull&(K2))<<040)         \
    |   ((0xffffull&(K3))<<060)         \
    )\
)


#define     DHU_NEWL(...)   DHZ_NEWL(u16_u64, __VA_ARGS__)
#define     DHI_NEWL(...)   DHZ_NEWL(s16_u64, __VA_ARGS__)
#define     DHF_NEWL(...)   MY_VSET4(V4, vset_lane_f16, DHF_VOID, __VA_ARGS__)

#define     DWZ_NEWL(T, K0, K1)     \
vreinterpret_##T(                   \
    vdup_n_u64(                     \
        ((0xffffffffull&(K0))<<000) \
    |   ((0xffffffffull&(K1))<<040) \
    )\
)

#define     DWU_NEWL(...)   DWZ_NEWL(u32_u64, __VA_ARGS__)
#define     DWI_NEWL(...)   DWZ_NEWL(s32_u64, __VA_ARGS__)
#define     DWF_NEWL(...)   MY_VSET2(V2, vset_lane_f32, DWF_VOID, __VA_ARGS__)

#define     DDU_NEWL        vdup_n_u64
#define     DDI_NEWL        vdup_n_s64
#define     DDF_NEWL        vdup_n_f64


#define     QBZ_NEWL(T,             \
    K0, K1, K2, K3, K4, K5, K6, K7, \
    K8, K9, K10,K11,K12,K13,K14,K15 \
)                                   \
vreinterpretq_##T(                  \
    vcombine_u64(                   \
        vdup_n_u64(                 \
            ((0xffull&(K0 ))<<000)  \
        |   ((0xffull&(K1 ))<<010)  \
        |   ((0xffull&(K2 ))<<020)  \
        |   ((0xffull&(K3 ))<<030)  \
        |   ((0xffull&(K4 ))<<040)  \
        |   ((0xffull&(K5 ))<<050)  \
        |   ((0xffull&(K6 ))<<060)  \
        |   ((0xffull&(K7 ))<<070)  \
        ),                          \
        vdup_n_u64(                 \
            ((0xffull&(K8 ))<<000)  \
        |   ((0xffull&(K9 ))<<010)  \
        |   ((0xffull&(K10))<<020)  \
        |   ((0xffull&(K11))<<030)  \
        |   ((0xffull&(K12))<<040)  \
        |   ((0xffull&(K13))<<050)  \
        |   ((0xffull&(K14))<<060)  \
        |   ((0xffull&(K15))<<070)  \
        )                           \
    )                               \
)
#define     QBU_NEWL(...)       QBZ_NEWL(u8_u64, __VA_ARGS__)
#define     QBI_NEWL(...)       QBZ_NEWL(s8_u64, __VA_ARGS__)
#if CHAR_MIN
#   define  QBC_NEWL(...)   QBI_NEWL(__VA_ARGS__)
#else
#   define  QBC_NEWL(...)   QBU_NEWL(__VA_ARGS__)
#endif

#define     QHZ_NEWL(T, K0, K1, K2, K3, K4, K5, K6, K7) \
vreinterpretq_##T(                  \
    vcombine_u64(                   \
        vdup_n_u64(                 \
            ((0xffffull&(K0))<<000) \
        |   ((0xffffull&(K1))<<020) \
        |   ((0xffffull&(K2))<<040) \
        |   ((0xffffull&(K3))<<060) \
        ),                          \
        vdup_n_u64(                 \
            ((0xffffull&(K4))<<000) \
        |   ((0xffffull&(K5))<<020) \
        |   ((0xffffull&(K6))<<040) \
        |   ((0xffffull&(K7))<<060) \
        )                           \
    )                               \
)
#define     QHU_NEWL(...)       QHZ_NEWL(u16_u64, __VA_ARGS__)
#define     QHI_NEWL(...)       QHZ_NEWL(s16_u64, __VA_ARGS__)
#define     QHF_NEWL(...)       \
MY_VSET8(V8, vsetq_lane_f16, QHF_VOID, __VA_ARGS__)

#define     QWZ_NEWL(T, K0, K1, K2, K3) \
vreinterpretq_##T(                      \
    vcombine_u64(                       \
        vdup_n_u64(                     \
            ((0xffffffffull&(K0))<<000) \
        |   ((0xffffffffull&(K1))<<040) \
        ),                              \
        vdup_n_u64(                     \
            ((0xffffffffull&(K2))<<000) \
        |   ((0xffffffffull&(K3))<<040) \
        )                               \
    )                                   \
)
#define     QWU_NEWL(...)       QWZ_NEWL(u32_u64, __VA_ARGS__)
#define     QWI_NEWL(...)       QWZ_NEWL(s32_u64, __VA_ARGS__)
#define     QWF_NEWL(...)       \
MY_VSET4(V4, vsetq_lane_f32, QWF_VOID, __VA_ARGS__)

#define     QDU_NEWL(K0, K1)    vcombine_u64(vdup_n_u64(K0), vdup_n_u64(K1))
#define     QDI_NEWL(K0, K1)    vcombine_s64(vdup_n_s64(K0), vdup_n_s64(K1))
#define     QDF_NEWL(K0, K1)    vcombine_f64(vdup_n_f64(K0), vdup_n_f64(K1))

INLINE(Vwbu,VWBU_NEWL) (uint8_t  k0, uint8_t  k1, uint8_t k2, uint8_t k3)
{
    return (VWBU_TYPE){WBU_NEWL(k0, k1, k2, k3)};
}

INLINE(Vwbi,VWBI_NEWL) ( int8_t  k0,  int8_t  k1,  int8_t k2,  int8_t k3)
{
    return (VWBI_TYPE){WBI_NEWL(k0, k1, k2, k3)};
}

INLINE(Vwbc,VWBC_NEWL) (   char  k0,    char  k1,    char k2,    char k3)
{
    return (VWBC_TYPE){WBC_NEWL(k0, k1, k2, k3)};
}


INLINE(Vwhu,VWHU_NEWL) (uint16_t k0, uint16_t k1)
{
    return (VWHU_TYPE){WHU_NEWL(k0, k1)};
}

INLINE(Vwhi,VWHI_NEWL) ( int16_t k0,  int16_t k1)
{
    return (VWHI_TYPE){WHI_NEWL(k0, k1)};
}

INLINE(Vwhf,VWHF_NEWL) ( flt16_t k0,  flt16_t k1)
{
    return (VWHF_TYPE){WHF_NEWL(k0, k1)};
}

INLINE(Vwwu,VWWU_NEWL) (uint32_t k0)
{
    return  UINT32_ASTV(k0);
}

INLINE(Vwwi,VWWI_NEWL) ( int32_t k0)
{
    return  INT32_ASTV(k0);
}

INLINE(Vwwf,VWWF_NEWL) (   float k0)
{
    return (VWWF_TYPE){k0};
}


INLINE(Vdbu,VDBU_NEWL)
(
    uint8_t k0, uint8_t k1, uint8_t k2, uint8_t k3,
    uint8_t k4, uint8_t k5, uint8_t k6, uint8_t k7
)
{
#define     VDBU_NEWL(K0, K1, K2, K3, K4, K5, K6, K7) \
vcreate_u8(\
    (\
        ((0xffULL&K0)<<000) \
    |   ((0xffULL&K1)<<010) \
    |   ((0xffULL&K2)<<020) \
    |   ((0xffULL&K3)<<030) \
    |   ((0xffULL&K4)<<040) \
    |   ((0xffULL&K5)<<050) \
    |   ((0xffULL&K6)<<060) \
    |   ((0xffULL&K7)<<070) \
    )                       \
)
    return  DBU_NEWL(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vdbi,VDBI_NEWL)
(
    int8_t k0, int8_t k1, int8_t k2, int8_t k3,
    int8_t k4, int8_t k5, int8_t k6, int8_t k7
)
{
#define     VDBI_NEWL(K0, K1, K2, K3, K4, K5, K6, K7) \
vcreate_s8(\
    (\
        ((0xffULL&K0)<<000) \
    |   ((0xffULL&K1)<<010) \
    |   ((0xffULL&K2)<<020) \
    |   ((0xffULL&K3)<<030) \
    |   ((0xffULL&K4)<<040) \
    |   ((0xffULL&K5)<<050) \
    |   ((0xffULL&K6)<<060) \
    |   ((0xffULL&K7)<<070) \
    )                       \
)
    return  DBI_NEWL(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vdbc,VDBC_NEWL)
(
    char k0, char k1, char k2, char k3,
    char k4, char k5, char k6, char k7
)
{
    return  DBC_ASTV(DBC_NEWL(k0, k1, k2, k3, k4, k5, k6, k7));
}


INLINE(Vdhu,VDHU_NEWL)
(
    uint16_t k0, uint16_t k1, uint16_t k2, uint16_t k3
)
{
    return  DHU_NEWL(k0, k1, k2, k3);
}

INLINE(Vdhi,VDHI_NEWL)
(
    int16_t k0, int16_t k1, int16_t k2, int16_t k3
)
{
    return  DHI_NEWL(k0, k1, k2, k3);
}

INLINE(Vdhf,VDHF_NEWL)
(
    flt16_t k0, flt16_t k1, flt16_t k2, flt16_t k3
)
{
    return  DHF_NEWL(k0, k1, k2, k3);
}


INLINE(Vdwu,VDWU_NEWL)
(
    uint32_t k0, uint32_t k1
)
{
    return  DWU_NEWL(k0, k1);
}

INLINE(Vdwi,VDWI_NEWL)
(
    int32_t k0, int32_t k1
)
{
    return  DWI_NEWL(k0, k1);
}

INLINE(Vdwf,VDWF_NEWL)
(
    float k0, float k1
)
{
    return  DWF_NEWL(k0, k1);
}


INLINE(Vddu,VDDU_NEWL) (uint64_t k0) {return UINT64_ASTV(k0);}
INLINE(Vddi,VDDI_NEWL) ( int64_t k0) {return  INT64_ASTV(k0);}
INLINE(Vddf,VDDF_NEWL) (  double k0) {return    DBL_ASTV(k0);}


INLINE(Vqbu,VQBU_NEWL)
(
    uint8_t  k0, uint8_t  k1, uint8_t  k2, uint8_t  k3,
    uint8_t  k4, uint8_t  k5, uint8_t  k6, uint8_t  k7,
    uint8_t  k8, uint8_t  k9, uint8_t k10, uint8_t k11,
    uint8_t k12, uint8_t k13, uint8_t k14, uint8_t k15
)
{
    return  QBU_NEWL(
        k0,  k1, k2,  k3,  k4,  k5,  k6,  k7,
        k8,  k9, k10, k11, k12, k13, k14, k15
    );
}

INLINE(Vqbi,VQBI_NEWL)
(
    int8_t  k0, int8_t  k1, int8_t  k2, int8_t  k3,
    int8_t  k4, int8_t  k5, int8_t  k6, int8_t  k7,
    int8_t  k8, int8_t  k9, int8_t k10, int8_t k11,
    int8_t k12, int8_t k13, int8_t k14, int8_t k15
)
{
    return  QBI_NEWL(
        k0,  k1, k2,  k3,  k4,  k5,  k6,  k7,
        k8,  k9, k10, k11, k12, k13, k14, k15
    );
}

INLINE(Vqbc,VQBC_NEWL)
(
    char  k0, char  k1, char  k2, char  k3,
    char  k4, char  k5, char  k6, char  k7,
    char  k8, char  k9, char k10, char k11,
    char k12, char k13, char k14, char k15
)
{
    return  QBC_ASTV(
        QBC_NEWL(
            k0,  k1, k2,  k3,  k4,  k5,  k6,  k7,
            k8,  k9, k10, k11, k12, k13, k14, k15
        )
    );
}

INLINE(Vqhu,VQHU_NEWL)
(
    uint16_t k0, uint16_t k1, uint16_t k2, uint16_t k3,
    uint16_t k4, uint16_t k5, uint16_t k6, uint16_t k7
)
{
    return QHU_NEWL(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vqhi,VQHI_NEWL)
(
    int16_t k0, int16_t k1, int16_t k2, int16_t k3,
    int16_t k4, int16_t k5, int16_t k6, int16_t k7
)
{
    return QHI_NEWL(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vqhf,VQHF_NEWL)
(
    flt16_t k0, flt16_t k1, flt16_t k2, flt16_t k3,
    flt16_t k4, flt16_t k5, flt16_t k6, flt16_t k7
)
{
    return QHF_NEWL(k0, k1, k2, k3, k4, k5, k6, k7);
}

INLINE(Vqwu,VQWU_NEWL)
(
    uint32_t k0, uint32_t k1, uint32_t k2, uint32_t k3
)
{
    return  QWU_NEWL(k0, k1, k2, k3);
}

INLINE(Vqwi,VQWI_NEWL)
(
    int32_t k0, int32_t k1, int32_t k2, int32_t k3
)
{
    return  QWI_NEWL(k0, k1, k2, k3);
}

INLINE(Vqwf,VQWF_NEWL)
(
    float k0, float k1, float k2, float k3
)
{
    return  QWF_NEWL(k0, k1, k2, k3);
}

INLINE(Vqdu,VQDU_NEWL) (uint64_t k0, uint64_t k1)
{
    return  QDU_NEWL(k0, k1);
}

INLINE(Vqdi,VQDI_NEWL) (int64_t k0, int64_t k1)
{
    return  QDI_NEWL(k0, k1);
}

INLINE(Vqdf,VQDF_NEWL) (double k0, double k1)
{
    return  QDF_NEWL(k0, k1);
}

#if _LEAVE_ARM_NEWL
}
#endif

#if _ENTER_ARM_NEWR
{
#endif

#define     WBZ_NEWR(K3, K2, K1, K0)    \
vget_lane_f32(                          \
    vreinterpret_f32_u64(               \
        vdup_n_u64(                     \
            ((0xffULL&(K0))<<000)       \
        |   ((0xffULL&(K1))<<010)       \
        |   ((0xffULL&(K2))<<020)       \
        |   ((0xffULL&(K3))<<030)       \
        )                               \
    ),                                  \
    V2_K0                               \
)

#define     WHZ_NEWR(K1, K0)        \
vget_lane_f32(                      \
    vreinterpret_f32_u64(           \
        vdup_n_u64(                 \
            ((0xffffULL&(K0))<<000) \
        |   ((0xffffULL&(K1))<<020) \
        )                           \
    ),                              \
    V2_K0                           \
)

#define     WWU_NEWR(K0)    ((WORD_TYPE){.U=K0}).F
#define     WWI_NEWR(K0)    ((WORD_TYPE){.I=K0}).F
#define     WWF_NEWR(K0)    ((WORD_TYPE){.F=K0}).F

#define     DBZ_NEWR(T, K7, K6, K5, K4, K3, K2, K1, K0) \
vcreate_##T(            \
    ((0xffULL&K0)<<000) \
|   ((0xffULL&K1)<<010) \
|   ((0xffULL&K2)<<020) \
|   ((0xffULL&K3)<<030) \
|   ((0xffULL&K4)<<040) \
|   ((0xffULL&K5)<<050) \
|   ((0xffULL&K6)<<060) \
|   ((0xffULL&K7)<<070) \
)

#define     DBU_NEWR(...) DBZ_NEWR(u8, __VA_ARGS__)
#define     DBI_NEWR(...) DBZ_NEWR(s8, __VA_ARGS__)
#if CHAR_MIN
#   define  DBC_NEWR    DBI_NEWR
#else
#   define  DBC_NEWR    DBU_NEWR
#endif

#define     DHZ_NEWR(T, K3, K2, K1, K0) \
vcreate_##T(                \
    ((0xffffULL&K0)<<000)   \
|   ((0xffffULL&K1)<<020)   \
|   ((0xffffULL&K2)<<040)   \
|   ((0xffffULL&K3)<<060)   \
)

#define     DHU_NEWR(...)   DHZ_NEWR(u16,__VA_ARGS__)
#define     DHI_NEWR(...)   DHZ_NEWR(s16,__VA_ARGS__)

#define     DWZ_NEWR(T, K1, K0) \
vcreate_##T(                    \
    ((0xffffffffULL&K0)<<000)   \
|   ((0xffffffffULL&K1)<<040)   \
)


#define     DWU_NEWR(...)   DWZ_NEWR(u32, __VA_ARGS__)
#define     DWI_NEWR(...)   DWZ_NEWR(s32, __VA_ARGS__)

#define     DDU_NEWR    vdup_n_u64
#define     DDI_NEWR    vdup_n_s64
#define     DDF_NEWR    vdup_n_f64

#define     QBZ_NEWR(T,             \
    K15,K14,K13,K12,K11,K10,K9, K8, \
    K7, K6, K5, K4, K3, K2, K1, K0  \
)                                   \
vcombine_##T(                       \
    DBZ_NEWR(T,K15,K14,K13,K12,K11,K10,K9,K8),  \
    DBZ_NEWR(T,K7, K6, K5, K4, K3, K2, K1,K0)   \
)

#define     QBU_NEWR(...)   QBZ_NEWR(u8,__VA_ARGS__)
#define     QBI_NEWR(...)   QBZ_NEWR(s8,__VA_ARGS__)
#if CHAR_MIN
#   define  QBC_NEWR(...)   QBI_NEWR(__VA_ARGS__)
#else
#   define  QBC_NEWR(...)   QBU_NEWR(__VA_ARGS__)
#endif

#define     QHZ_NEWR(T,K7,K6,K5,K4,K3,K2,K1,K0) \
vcombine_##T(               \
    DHZ_NEWR(T,K7,K6,K5,K4),\
    DHZ_NEWR(T,K3,K2,K1,K0) \
)

#define     QHU_NEWR(...)   QHZ_NEWR(u16,__VA_ARGS__)
#define     QHI_NEWR(...)   QHZ_NEWR(s16,__VA_ARGS__)

#define     QWZ_NEWR(T, K3,K2,K1,K0) \
vcombine_##T(DWZ_NEWR(T,K3,K2),DWZ_NEWR(T,K1,K0))

#define     QWU_NEWR(...)   QWZ_NEWR(u32,__VA_ARGS__)
#define     QWI_NEWR(...)   QWZ_NEWR(s32,__VA_ARGS__)

#define     QDZ_NEWR(T, K1, K0) vcombine_##T(vdup_n_##T(K1),vdup_n_##T(K0))

#define     QDU_NEWR(...)    QDZ_NEWR(u64,__VA_ARGS__)
#define     QDI_NEWR(...)    QDZ_NEWR(s64,__VA_ARGS__)
#define     QDF_NEWR(...)    QDZ_NEWR(f64,__VA_ARGS__)

INLINE(Vwbu,VWBU_NEWR) (uint8_t  k3, uint8_t  k2, uint8_t k1, uint8_t k0)
{
#define     VWBU_NEWR(...) ((VWBU_TYPE){WBZ_NEWR(__VA_ARGS__)})
    return  VWBU_NEWR(k3, k2, k1, k0);
}

INLINE(Vwbi,VWBI_NEWR) ( int8_t  k3,  int8_t  k2,  int8_t k1,  int8_t k0)
{
#define     VWBI_NEWR(...) ((VWBI_TYPE){WBZ_NEWR(__VA_ARGS__)})
    return  VWBI_NEWR(k3, k2, k1, k0);
}

INLINE(Vwbc,VWBC_NEWR) (   char  k3,    char  k2,    char k1,    char k0)
{
#define     VWBC_NEWR(...) ((VWBC_TYPE){WBZ_NEWR(__VA_ARGS__)})
    return  VWBC_NEWR(k3, k2, k1, k0);
}


INLINE(Vwhu,VWHU_NEWR) (uint16_t k1, uint16_t k0)
{
#define     VWHU_NEWR(...) ((VWHU_TYPE){WHZ_NEWR(__VA_ARGS__)})
    return  VWHU_NEWR(k1, k0);
}

INLINE(Vwhi,VWHI_NEWR)  (int16_t k1,  int16_t k0)
{
#define     VWHI_NEWR(...) ((VWHI_TYPE){WHZ_NEWR(__VA_ARGS__)})
    return  VWHI_NEWR(k1, k0);
}

INLINE(Vwhf,VWHF_NEWR) ( flt16_t k1,  flt16_t k0)
{
    float16x4_t r = vdup_n_f16(0);
    r = vset_lane_f16(k0, r, 0);
    r = vset_lane_f16(k1, r, 1);
    float32x2_t m = vreinterpret_f32_f16(r);
    return  WHF_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwwu,VWWU_NEWR) (uint32_t k0) {return  UINT32_ASTV(k0);}

INLINE(Vwwi,VWWI_NEWR) ( int32_t k0) {return   INT32_ASTV(k0);}

INLINE(Vwwf,VWWF_NEWR) (   float k0) {return     FLT_ASTV(k0);}


INLINE(Vdbu,VDBU_NEWR)
(
    uint8_t k7, uint8_t k6, uint8_t k5, uint8_t k4,
    uint8_t k3, uint8_t k2, uint8_t k1, uint8_t k0
)
{
#define     VDBU_NEWR(...) DBU_NEWR(__VA_ARGS__)
    return  VDBU_NEWR(k7, k6, k5, k4, k3, k2, k1, k0);
}

INLINE(Vdbi,VDBI_NEWR)
(
    int8_t k7, int8_t k6, int8_t k5, int8_t k4,
    int8_t k3, int8_t k2, int8_t k1, int8_t k0
)
{
#define     VDBI_NEWR(...) DBI_NEWR(__VA_ARGS__)
    return  VDBI_NEWR(k7, k6, k5, k4, k3, k2, k1, k0);
}

INLINE(Vdbc,VDBC_NEWR)
(
    char k0, char k1, char k2, char k3,
    char k4, char k5, char k6, char k7
)
{
#define     VDBC_NEWR(...) DBC_ASTV(DBC_NEWR(__VA_ARGS__))
    return  VDBC_NEWR(k7, k6, k5, k4, k3, k2, k1, k0);
}


INLINE(Vdhu,VDHU_NEWR)
(
    uint16_t k3, uint16_t k2, uint16_t k1, uint16_t k0
)
{
#define     VDHU_NEWR(...) DHU_NEWR(__VA_ARGS__)
    return  VDHU_NEWR(k3, k2, k1, k0);
}

INLINE(Vdhi,VDHI_NEWR)
(
    int16_t k3, int16_t k2, int16_t k1, int16_t k0
)
{
#define     VDHI_NEWR(...) DHI_NEWR(__VA_ARGS__)
    return  VDHI_NEWR(k3, k2, k1, k0);
}

INLINE(Vdhf,VDHF_NEWR)
(
    flt16_t k3, flt16_t k2, flt16_t k1, flt16_t k0
)
{
    float16x4_t r = VDHF_VOID;
    r = vset_lane_f16(k3, r, 3);
    r = vset_lane_f16(k2, r, 2);
    r = vset_lane_f16(k1, r, 1);
    r = vset_lane_f16(k0, r, 0);
    return  r;
}


INLINE(Vdwu,VDWU_NEWR) (uint32_t k1, uint32_t k0)
{
#define     VDWU_NEWR(...) DWU_NEWR(__VA_ARGS__)
    return  DWU_NEWR(k1, k0);
}

INLINE(Vdwi,VDWI_NEWR) (int32_t k1, int32_t k0)
{
#define     VDWI_NEWR(...) DWI_NEWR(__VA_ARGS__)
    return  DWI_NEWR(k1, k0);
}

INLINE(Vdwf,VDWF_NEWR) (float k1, float k0)
{
    float32x2_t r = VDWF_VOID;
    r = vset_lane_f32(k1, r, 1);
    r = vset_lane_f32(k0, r, 0);
    return  r;
}

INLINE(Vddu,VDDU_NEWR) (uint64_t k0) {return UINT64_ASTV(k0);}
INLINE(Vddi,VDDI_NEWR) ( int64_t k0) {return  INT64_ASTV(k0);}
INLINE(Vddf,VDDF_NEWR) (  double k0) {return    DBL_ASTV(k0);}


INLINE(Vqbu,VQBU_NEWR)
(
    uint8_t k15, uint8_t k14, uint8_t k13, uint8_t k12,
    uint8_t k11, uint8_t k10, uint8_t k9,  uint8_t k8,
    uint8_t k7,  uint8_t k6,  uint8_t k5,  uint8_t k4,
    uint8_t k3,  uint8_t k2,  uint8_t k1,  uint8_t k0
)
{
#define     VQBU_NEWR(...) QBU_NEWR(__VA_ARGS__)
    return  VQBU_NEWR(k15,k14,k13,k12,k11,k10,k9,k8,k7,k6,k5,k4,k3,k2,k1,k0);
}

INLINE(Vqbi,VQBI_NEWR)
(
    int8_t k15, int8_t k14, int8_t k13, int8_t k12,
    int8_t k11, int8_t k10, int8_t k9,  int8_t k8,
    int8_t k7,  int8_t k6,  int8_t k5,  int8_t k4,
    int8_t k3,  int8_t k2,  int8_t k1,  int8_t k0
)
{
#define     VQBI_NEWR(...) QBI_NEWR(__VA_ARGS__)
    return  VQBI_NEWR(k15,k14,k13,k12,k11,k10,k9,k8,k7,k6,k5,k4,k3,k2,k1,k0);
}


INLINE(Vqbc,VQBC_NEWR)
(
    char k15, char k14, char k13, char k12,
    char k11, char k10, char k9,  char k8,
    char k7,  char k6,  char k5,  char k4,
    char k3,  char k2,  char k1,  char k0
)
{
#define     VQBC_NEWR(...)  QBC_ASTV(QBC_NEWR(__VA_ARGS__))
    return  VQBC_NEWR(k15,k14,k13,k12,k11,k10,k9,k8,k7,k6,k5,k4,k3,k2,k1,k0);
}

INLINE(Vqhu,VQHU_NEWR)
(
    uint16_t k7, uint16_t k6, uint16_t k5, uint16_t k4,
    uint16_t k3, uint16_t k2, uint16_t k1, uint16_t k0
)
{
#define     VQHU_NEWR(...)  QHU_NEWR(__VA_ARGS__)
    return  VQHU_NEWR(k7, k6, k5, k4, k3, k2, k1, k0);
}

INLINE(Vqhi,VQHI_NEWR)
(
    int16_t k7, int16_t k6, int16_t k5, int16_t k4,
    int16_t k3, int16_t k2, int16_t k1, int16_t k0
)
{
#define     VQHI_NEWR(...)  QHU_NEWR(__VA_ARGS__)
    return  VQHI_NEWR(k7, k6, k5, k4, k3, k2, k1, k0);
}

INLINE(Vqhf,VQHF_NEWR)
(
    flt16_t k7, flt16_t k6, flt16_t k5, flt16_t k4,
    flt16_t k3, flt16_t k2, flt16_t k1, flt16_t k0
)
{
    float16x8_t r = VQHF_VOID;
    r = vsetq_lane_f16(k7, r, 7);
    r = vsetq_lane_f16(k6, r, 6);
    r = vsetq_lane_f16(k5, r, 5);
    r = vsetq_lane_f16(k4, r, 4);
    r = vsetq_lane_f16(k3, r, 3);
    r = vsetq_lane_f16(k2, r, 2);
    r = vsetq_lane_f16(k1, r, 1);
    r = vsetq_lane_f16(k0, r, 0);
    return  r;
}

INLINE(Vqwu,VQWU_NEWR)
(
    uint32_t k3, uint32_t k2, uint32_t k1, uint32_t k0
)
{
#define     VQWU_NEWR(...)  QWU_NEWR(__VA_ARGS__)
    return  VQWU_NEWR(k3, k2, k1, k0);
}

INLINE(Vqwi,VQWI_NEWR)
(
    int32_t k3, int32_t k2, int32_t k1, int32_t k0
)
{
#define     VQWI_NEWR(...)  QWI_NEWR(__VA_ARGS__)
    return  VQWI_NEWR(k3, k2, k1, k0);
}

INLINE(Vqwf,VQWF_NEWR)
(
    float k3, float k2, float k1, float k0
)
{
    float32x4_t r = VQWF_VOID;
    r = vsetq_lane_f32(k3, r, 3);
    r = vsetq_lane_f32(k2, r, 2);
    r = vsetq_lane_f32(k1, r, 1);
    r = vsetq_lane_f32(k0, r, 0);
    return  r;
}

INLINE(Vqdu,VQDU_NEWR) (uint64_t k1, uint64_t k0)
{
#define     VQDU_NEWR(...)  QDU_NEWR(__VA_ARGS__)
    return  VQDU_NEWR(k1, k0);
}

INLINE(Vqdi,VQDI_NEWR) (int64_t k1, int64_t k0)
{
#define     VQDI_NEWR(...)  QDI_NEWR(__VA_ARGS__)
    return  VQDI_NEWR(k1, k0);
}

INLINE(Vqdf,VQDF_NEWR) (double k1, double k0)
{
#define     VQDF_NEWR(...)  QDF_NEWR(__VA_ARGS__)
    return  VQDF_NEWR(k1, k0);
}

#if _LEAVE_ARM_NEWR
}
#endif


#if _ENTER_ARM_SEQW
{
#endif

INLINE(Vwbu, UCHAR_SEQW)  (uchar a, int8_t b)
{
    uint8x8_t   v = vmla_u8(
        vdup_n_u8(a),
        vcreate_u8(0x0706050403020100ULL),
        vreinterpret_u8_s8(vdup_n_s8(b))
    );
    float32x2_t m = vreinterpret_f32_u8(v);
    float       f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi, SCHAR_SEQW)  (schar a, int8_t b)
{
    int8x8_t    v =  vmla_s8(
        vdup_n_s8(a),
        vcreate_s8(0x0706050403020100ULL),
        vdup_n_s8(b)
    );
    float32x2_t m = vreinterpret_f32_s8(v);
    float       f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,  CHAR_SEQW)   (char a, int8_t b)
{
#if CHAR_MIN
#   define  CHAR_SEQW(A, B) VWBI_ASBC(SCHAR_SEQW(A,B))
#else
#   define  CHAR_SEQW(A, B) VWBU_ASBC(UCHAR_SEQW(A,B))
#endif
    return  CHAR_SEQW(a, b);
}


INLINE(Vwhu, USHRT_SEQW) (ushort a, int16_t b)
{
    uint16x4_t v =  vmla_u16(
        vdup_n_u16(a),
        vcreate_u16(0x0003000200010000ULL),
        vreinterpret_u16_s16(vdup_n_s16(b))
    );
    float32x2_t m = vreinterpret_f32_u16(v);
    float       f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,  SHRT_SEQW)  (short a, int16_t b)
{
    int16x4_t v =  vmla_s16(
        vdup_n_s16(a),
        vcreate_s16(0x0003000200010000ULL),
        vdup_n_s16(b)
    );
    float32x2_t m = vreinterpret_f32_s16(v);
    float       f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


#if _LEAVE_ARM_SEQW
}
#endif

#if _ENTER_ARM_SEQD
{
#endif

INLINE(Vdbu, UCHAR_SEQD)  (uchar a, int8_t b)
{
    return  vmla_u8(
        vdup_n_u8(a),
        vcreate_u8(0x0706050403020100ULL),
        vreinterpret_u8_s8(vdup_n_s8(b))
    );
}

INLINE(Vdbi, SCHAR_SEQD)  (schar a, int8_t b)
{
    return  vmla_s8(
        vdup_n_s8(a),
        vcreate_s8(0x0706050403020100ULL),
        vdup_n_s8(b)
    );
}

INLINE(Vdbc,  CHAR_SEQD)   (char a, int8_t b)
{
#if CHAR_MIN
#   define  CHAR_SEQD(A, B) VDBI_ASBC(SCHAR_SEQD(A,B))
#else
#   define  CHAR_SEQD(A, B) VDBU_ASBC(UCHAR_SEQD(A,B))
#endif
    return  CHAR_SEQD(a, b);
}


INLINE(Vdhu, USHRT_SEQD) (ushort a, int16_t b)
{
    return  vmla_u16(
        vdup_n_u16(a),
        vcreate_u16(0x0003000200010000ULL),
        vreinterpret_u16_s16(vdup_n_s16(b))
    );
}

INLINE(Vdhi,  SHRT_SEQD)  (short a, int16_t b)
{
    return  vmla_s16(
        vdup_n_s16(a),
        vcreate_s16(0x0003000200010000ULL),
        vdup_n_s16(b)
    );
}


INLINE(Vdwu,  UINT_SEQD)   (uint a, int32_t b)
{
    return  vmla_u32(
        vdup_n_u32(a),
        vcreate_u32(0x0000000100000000ULL),
        vreinterpret_u32_s32(vdup_n_s32(b))
    );
}

INLINE(Vdwi,   INT_SEQD)    (int a, int32_t b)
{
    return  vmla_s32(
        vdup_n_s32(a),
        vcreate_s32(0x0000000100000000ULL),
        vdup_n_s32(b)
    );
}


#if DWRD_NLONG == 2

INLINE(Vdwu, ULONG_SEQD)  (ulong a, int32_t b) {return UINT_SEQD(a, b);}
INLINE(Vdwi,  LONG_SEQD)   (long a, int32_t b) {return  INT_SEQD(a, b);}

#endif

#if _LEAVE_ARM_SEQD
}
#endif

#if _ENTER_ARM_SEQQ
{
#endif

INLINE(Vqbu, UCHAR_SEQQ)  (uchar a, int8_t b)
{
    return  vmlaq_u8(
        vdupq_n_u8(a),
        vcombine_u8(
            vcreate_u8(0x0706050403020100ULL),
            vcreate_u8(0x0f0e0d0c0b0a0908ULL)
        ),
        vreinterpretq_u8_s8(vdupq_n_s8(b))
    );
}

INLINE(Vqbi, SCHAR_SEQQ)  (schar a, int8_t b)
{
    return  vmlaq_s8(
        vdupq_n_s8(a),
        vcombine_s8(
            vcreate_s8(0x0706050403020100ULL),
            vcreate_s8(0x0f0e0d0c0b0a0908ULL)
        ),
        vdupq_n_s8(b)
    );
}

INLINE(Vqbc,  CHAR_SEQQ)   (char a, int8_t b)
{
#if CHAR_MIN
#   define  CHAR_SEQQ(A, B) VQBI_ASBC(SCHAR_SEQQ(A,B))
#else
#   define  CHAR_SEQQ(A, B) VQBU_ASBC(UCHAR_SEQQ(A,B))
#endif
    return  CHAR_SEQQ(a, b);
}


INLINE(Vqhu, USHRT_SEQQ) (ushort a, int16_t b)
{
    return  vmlaq_u16(
        vdupq_n_u16(a),
        vcombine_u16(
            vcreate_u16(0x0003000200010000ULL),
            vcreate_u16(0x0007000600050004ULL)
        ),
        vreinterpretq_u16_s16(vdupq_n_s16(b))
    );
}

INLINE(Vqhi,  SHRT_SEQQ)  (short a, int16_t b)
{
    return  vmlaq_s16(
        vdupq_n_s16(a),
        vcombine_s16(
            vcreate_s16(0x0003000200010000ULL),
            vcreate_s16(0x0007000600050004ULL)
        ),
        vdupq_n_s16(b)
    );
}


INLINE(Vqwu,  UINT_SEQQ)   (uint a, int32_t b)
{
    return  vmlaq_u32(
        vdupq_n_u32(a),
        vcombine_u32(
            vcreate_u32(0x0000000100000000ULL),
            vcreate_u32(0x0000000300000002ULL)
        ),
        vreinterpretq_u32_s32(vdupq_n_s32(b))
    );
}

INLINE(Vqwi,   INT_SEQQ)    (int a, int32_t b)
{
    return  vmlaq_s32(
        vdupq_n_s32(a),
        vcombine_s32(
            vcreate_s32(0x0000000100000000ULL),
            vcreate_s32(0x0000000300000002ULL)
        ),
        vdupq_n_s32(b)
    );
}

#if DWRD_NLONG == 2

INLINE(Vqwu, ULONG_SEQQ)  (ulong a, int32_t b) {return UINT_SEQQ(a, b);}
INLINE(Vqwi,  LONG_SEQQ)   (long a, int32_t b) {return  INT_SEQQ(a, b);}

#else

INLINE(Vqdu, ULONG_SEQQ)  (ulong a, int64_t b)
{
    return  vaddq_u64(
        vdupq_n_u64(a),
        vcombine_u64(
            vdup_n_u64(0),
            vreinterpret_u64_s64(vdup_n_s64(b))
        )
    );
}

INLINE(Vqdi,  LONG_SEQQ)   (long a, int64_t b)
{
    return  vaddq_s64(
        vdupq_n_s64(a),
        vcombine_u64(vdup_n_s64(0), vdup_n_s64(b))
    );
}

#endif

#if QUAD_NLONG == 2

INLINE(Vqdu,ULLONG_SEQQ) (ullong a, int64_t b)
{
    return  vaddq_u64(
        vdupq_n_u64(a),
        vcombine_u64(
            vdup_n_u64(0),
            vreinterpret_u64_s64(vdup_n_s64(b))
        )
    );
}

INLINE(Vqdi, LLONG_SEQQ)  (llong a, int64_t b)
{
    return  vaddq_s64(
        vdupq_n_s64(a),
        vcombine_u64(vdup_n_s64(0), vdup_n_s64(b))
    );
}

#endif

#if _LEAVE_ARM_SEQQ
}
#endif

#if _ENTER_ARM_CATL
{
#endif

INLINE(uint16_t,UCHAR_CATL)  (uchar l, uchar r)
{
#define     UCHAR_CATL(L, R) ((HALF_TYPE){.Lo.U=L,.Hi.U=R}).U
    return  UCHAR_CATL(l, r);
}

INLINE( int16_t,SCHAR_CATL)  (schar l, schar r)
{
#define     SCHAR_CATL(L, R) ((HALF_TYPE){.Lo.I=L,.Hi.I=R}).I
    return  SCHAR_CATL(l, r);
}

#if CHAR_MIN

INLINE( int16_t, CHAR_CATL)   (char l,  char r)
{
#define     CHAR_CATL(L, R) ((HALF_TYPE){.Lo.C=L,.Hi.C=R}).I
    return  CHAR_CATL(l, r);
}
#else

INLINE(uint16_t, CHAR_CATL)   (char l,  char r)
{
#define     CHAR_CATL(L, R) ((HALF_TYPE){.Lo.C=L,.Hi.C=R}).U
    return  CHAR_CATL(l, r);
}
#endif

INLINE(uint32_t,USHRT_CATL) (ushort l, ushort r)
{
#define     USHRT_CATL(L, R) ((WORD_TYPE){.Lo.U=L,.Hi.U=R}).U
    return  USHRT_CATL(l, r);
}

INLINE( int32_t, SHRT_CATL)  (short l, short r)
{
#define     SHRT_CATL(L, R) ((WORD_TYPE){.Lo.I=L,.Hi.I=R}).I
    return  SHRT_CATL(l, r);
}

INLINE(uint64_t, UINT_CATL)   (uint l,  uint r)
{
#define     UINT_CATL(L, R) ((DWRD_TYPE){.Lo.U=L,.Hi.U=R}).U
    return  UINT_CATL(l, r);
}

INLINE( int64_t,  INT_CATL)    (int l,   int r)
{
#define     INT_CATL(L, R) ((DWRD_TYPE){.Lo.I=L,.Hi.I=R}).I
    return  INT_CATL(l, r);
}

#if DWRD_NLONG == 2

INLINE(uint64_t,ULONG_CATL)  (ulong l, ulong r) {return UINT_CATL(l, r);}
INLINE( int64_t, LONG_CATL)   (long l,  long r) {return  INT_CATL(l, r);}

#else

INLINE(QUAD_UTYPE, ULONG_CATL)  (ulong l,  ulong r)
{
#define     ULONG_CATL(L, R) ((QUAD_TYPE){.Lo.U=L,.Hi.U=R}).U
    return  ULONG_CATL(l, r);
}

INLINE(QUAD_ITYPE,  LONG_CATL)   (long l,   long r)
{
#define     LONG_CATL(L, R) ((QUAD_TYPE){.Lo.I=L,.Hi.I=R}).I
    return  LONG_CATL(l, r);
}

#endif

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,ULLONG_CATL) (ullong l, ullong r)
{
#define     ULLONG_CATL(L, R) ((QUAD_TYPE){.Lo.U=L,.Hi.U=R}).U
    return  ULLONG_CATL(l, r);
}

INLINE(QUAD_ITYPE, LLONG_CATL)  (llong l,  llong r)
{
#define     LLONG_CATL(L, R) ((QUAD_TYPE){.Lo.I=L,.Hi.I=R}).I
    return  LLONG_CATL(l, r);
}

#else

#endif

INLINE( Dwf,WWF_CATL) ( Wwf l, Wwf r)
{
#define     WWF_CATL        WWF_CATL
    Dwf m = vset_lane_f32(l, m, V2_K0);
    return  vset_lane_f32(r, m, V2_K1);
}

#define     WYU_CATL(L, R)  vreinterpret_u64_f32(WWF_CATL(L,R))
#define     WBU_CATL(L, R)  vreinterpret_u8_f32( WWF_CATL(L,R))
#define     WBI_CATL(L, R)  vreinterpret_s8_f32( WWF_CATL(L,R))
#if CHAR_MIN
#   define  WBC_CATL(L, R)  WBI_CATL(L,R)
#else
#   define  WBC_CATL(L, R)  WBU_CATL(L,R)
#endif

#define     WHU_CATL(L, R)  vreinterpret_u16_f32(WWF_CATL(L,R))
#define     WHI_CATL(L, R)  vreinterpret_s16_f32(WWF_CATL(L,R))
#define     WHF_CATL(L, R)  vreinterpret_f16_f32(WWF_CATL(L,R))
#define     WWU_CATL(L, R)  vreinterpret_u32_f32(WWF_CATL(L,R))
#define     WWI_CATL(L, R)  vreinterpret_s32_f32(WWF_CATL(L,R))

#define     DYU_CATL    vcombine_u64
#define     DBU_CATL    vcombine_u8
#define     DBI_CATL    vcombine_s8
#if CHAR_MIN
#   define  DBC_CATL    vcombine_s8
#else
#   define  DBC_CATL    vcombine_u8
#endif

#define     DHU_CATL    vcombine_u16
#define     DHI_CATL    vcombine_s16
#define     DHF_CATL    vcombine_f16
#define     DWU_CATL    vcombine_u32
#define     DWI_CATL    vcombine_s32
#define     DWF_CATL    vcombine_f32
#define     DDU_CATL    vcombine_u64
#define     DDI_CATL    vcombine_s64
#define     DDF_CATL    vcombine_f64

INLINE(Vdyu,VWYU_CATL) (Vwyu a, Vwyu b)
{
#   define  VWYU_CATL(L, R) DYU_ASTV(WYU_CATL(VWYU_ASTM(L),VWYU_ASTM(R)))
    return  VWYU_CATL(a, b);
}

INLINE(Vdbu,VWBU_CATL) (Vwbu a, Vwbu b)
{
#   define  VWBU_CATL(L, R)          WBU_CATL(VWBU_ASTM(L),VWBU_ASTM(R))
    return  VWBU_CATL(a, b);
}

INLINE(Vdbi,VWBI_CATL) (Vwbi a, Vwbi b)
{
#   define  VWBI_CATL(L, R)          WBI_CATL(VWBI_ASTM(L),VWBI_ASTM(R))
    return  VWBI_CATL(a, b);
}

INLINE(Vdbc,VWBC_CATL) (Vwbc a, Vwbc b)
{
#   define  VWBC_CATL(L, R) DBC_ASTV(WBC_CATL(VWBC_ASTM(L),VWBC_ASTM(R)))
    return  VWBC_CATL(a, b);
}

INLINE(Vdhu,VWHU_CATL) (Vwhu a, Vwhu b)
{
#   define  VWHU_CATL(L, R)          WHU_CATL(VWHU_ASTM(L),VWHU_ASTM(R))
    return  VWHU_CATL(a, b);
}

INLINE(Vdhi,VWHI_CATL) (Vwhi a, Vwhi b)
{
#   define  VWHI_CATL(L, R)          WHI_CATL(VWHI_ASTM(L),VWHI_ASTM(R))
    return  VWHI_CATL(a, b);
}

INLINE(Vdhf,VWHF_CATL) (Vwhf a, Vwhf b)
{
#   define  VWHF_CATL(L, R)          WHF_CATL(VWHF_ASTM(L),VWHF_ASTM(R))
    return  VWHF_CATL(a, b);
}

INLINE(Vdwu,VWWU_CATL) (Vwwu a, Vwwu b)
{
#   define  VWWU_CATL(L, R)          WWU_CATL(VWWU_ASTM(L),VWWU_ASTM(R))
    return  VWWU_CATL(a, b);
}

INLINE(Vdwi,VWWI_CATL) (Vwwi a, Vwwi b)
{
#   define  VWWI_CATL(L, R)          WWI_CATL(VWWI_ASTM(L),VWWI_ASTM(R))
    return  VWWI_CATL(a, b);
}

INLINE(Vdwf,VWWF_CATL) (Vwwf a, Vwwf b)
{
#   define  VWWF_CATL(L, R)          WWF_CATL(VWWF_ASTM(L),VWWF_ASTM(R))
    return  VWWF_CATL(a, b);
}

INLINE(Vqyu,VDYU_CATL) (Vdyu a, Vdyu b)
{
#   define  VDYU_CATL(L, R) QYU_ASTV(DYU_CATL(VDYU_ASTM(L),VDYU_ASTM(R)))
    return  VDYU_CATL(a, b);
}

INLINE(Vqbu,VDBU_CATL) (Vdbu a, Vdbu b) {return vcombine_u8(a, b);}
INLINE(Vqbi,VDBI_CATL) (Vdbi a, Vdbi b) {return vcombine_s8(a, b);}
INLINE(Vqbc,VDBC_CATL) (Vdbc a, Vdbc b)
{
#   define  VDBC_CATL(L, R) QBC_ASTV(DBC_CATL(VDBC_ASTM(L),VDBC_ASTM(R)))
    return  VDBC_CATL(a, b);
}

INLINE(Vqhu,VDHU_CATL) (Vdhu a, Vdhu b) {return vcombine_u16(a, b);}
INLINE(Vqhi,VDHI_CATL) (Vdhi a, Vdhi b) {return vcombine_s16(a, b);}
INLINE(Vqhi,VDHF_CATL) (Vdhf a, Vdhf b) {return vcombine_f16(a, b);}
INLINE(Vqwu,VDWU_CATL) (Vdwu a, Vdwu b) {return vcombine_u32(a, b);}
INLINE(Vqwi,VDWI_CATL) (Vdwi a, Vdwi b) {return vcombine_s32(a, b);}
INLINE(Vqwi,VDWF_CATL) (Vdwf a, Vdwf b) {return vcombine_f32(a, b);}
INLINE(Vqdu,VDDU_CATL) (Vddu a, Vddu b) {return vcombine_u64(a, b);}
INLINE(Vqdi,VDDI_CATL) (Vddi a, Vddi b) {return vcombine_s64(a, b);}
INLINE(Vqdi,VDDF_CATL) (Vddf a, Vddf b) {return vcombine_f64(a, b);}

#if _LEAVE_ARM_CATL
}
#endif

#if _ENTER_ARM_CATR
{
#endif

INLINE(uint64x1_t,WYU_CATR) (float l, float r)
{
    float32x2_t m = VDWF_VOID;
    m = vset_lane_f32(l, m, 0);
    m = vset_lane_f32(r, m, 1);
    uint8x8_t   x = vreinterpret_u8_f32(m);
    x = vrbit_u8(x);
    x = vrev64_u8(x);
    return  vreinterpret_u64_u8(x);
}

INLINE(uint8x8_t,WBU_CATR) (float l, float r)
{
    float32x2_t m = VDWF_VOID;
    m = vset_lane_f32(l, m, 0);
    m = vset_lane_f32(r, m, 1);
    uint8x8_t   x = vreinterpret_u8_f32(m);
    return  vrev64_u8(x);
}

#define     WBI_CATR(L, R) vreinterpret_s8_u8(WBU_CATR(L,R))
#define     WBC_CATR(L, R) VDBU_ASBC(WBU_CATR(L,R))

INLINE(uint16x4_t,WHU_CATR) (float l, float r)
{
    float32x2_t m = VDWF_VOID;
    m = vset_lane_f32(l, m, 0);
    m = vset_lane_f32(r, m, 1);
    uint16x4_t   x = vreinterpret_u16_f32(m);
    return  vrev64_u16(x);
}

#define     WHI_CATR(L, R) vreinterpret_s16_u16(WBU_CATR(L,R))
#define     WHF_CATR(L, R) vreinterpret_f16_u16(WBU_CATR(L,R))

INLINE(uint32x2_t,WWU_CATR) (float l, float r)
{
    float32x2_t m = VDWF_VOID;
    m = vset_lane_f32(l, m, 1);
    m = vset_lane_f32(r, m, 0);
    return  vreinterpret_u32_f32(m);
}

#define     WWI_CATR(L, R) vreinterpret_s32_u32(WBU_CATR(L,R))
#define     WWF_CATR(L, R) vreinterpret_f32_u32(WBU_CATR(L,R))


INLINE(uint64x2_t,DYU_CATR) (uint64x1_t l, uint64x1_t r)
{
    uint8x8_t   a = vreinterpret_u8_u64(l);
    uint8x8_t   b = vreinterpret_u8_u64(r);
    uint8x16_t  c = vcombine_u8(
        vrev64_u8(b),
        vrev64_u8(a)
    );
    c = vrbitq_u8(c);
    return  vreinterpretq_u64_u8(c);
}

#define     DBU_CATR(L, R)  vcombine_u8( vrev64_u8(R), vrev64_u8(L))
#define     DBI_CATR(L, R)  vcombine_s8( vrev64_s8(R), vrev64_s8(L))
#if CHAR_MIN
#   define  DBC_CATR    DBI_CATR
#else
#   define  DBC_CATR    DBU_CATR
#endif

#define     DHU_CATR(L, R)  vcombine_u16(vrev64_u16(R),vrev64_u16(L))
#define     DHI_CATR(L, R)  vcombine_s16(vrev64_s16(R),vrev64_s16(L))
#define     DHF_CATR(L, R)      \
vreinterpretq_f16_u16(          \
    DHU_CATR(                   \
        vreinterpret_u16_f16(L),\
        vreinterpret_u16_f16(R) \
    )                           \
)

#define     DWU_CATR(L, R)  vcombine_u32(vrev64_u32(R),vrev64_u32(L))
#define     DWI_CATR(L, R)  vcombine_s32(vrev64_s32(R),vrev64_s32(L))
#define     DWF_CATR(L, R)  vcombine_f32(vrev64_f32(R),vrev64_f32(L))
#define     DDU_CATR(L, R)  vcombine_u64(R,L)
#define     DDI_CATR(L, R)  vcombine_s64(R,L)
#define     DDF_CATR(L, R)  vcombine_f64(R,L)

INLINE(Vdyu,VWYU_CATR) (Vwyu l, Vwyu r)
{
#define     VWYU_CATR(L, R) DYU_ASTV(WYU_CATR(VWYU_ASTM(L),VWYU_ASTM(R)))
    return  VWYU_CATR(l, r);
}

INLINE(Vdbu,VWBU_CATR) (Vwbu l, Vwbu r)
{
#define     VWBU_CATR(L, R) WBU_CATR(VWBU_ASTM(L),VWBU_ASTM(R))
    return  VWBU_CATR(l, r);
}

INLINE(Vdbi,VWBI_CATR) (Vwbi l, Vwbi r)
{
#define     VWBI_CATR(L, R) VDBU_ASBI(WBU_CATR(VWBI_ASTM(L),VWBI_ASTM(R)))
    return  VWBI_CATR(l, r);
}

INLINE(Vdbc,VWBC_CATR) (Vwbc l, Vwbc r)
{
#define     VWBC_CATR(L, R) VDBU_ASBC(WBU_CATR(VWBC_ASTM(L),VWBC_ASTM(R)))
    return  VWBC_CATR(l, r);
}


INLINE(Vdhu,VWHU_CATR) (Vwhu l, Vwhu r)
{
#define     VWHU_CATR(L, R) WHU_CATR(VWHU_ASTM(L),VWHU_ASTM(R))
    return  VWHU_CATR(l, r);
}

INLINE(Vdhi,VWHI_CATR) (Vwhi l, Vwhi r)
{
#define     VWHI_CATR(L, R) VDHU_ASHI(WHU_CATR(VWHI_ASTM(L),VWHI_ASTM(R)))
    return  VWHI_CATR(l, r);
}

INLINE(Vdhf,VWHF_CATR) (Vwhf l, Vwhf r)
{
#define     VWHF_CATR(L, R) VDHU_ASHF(WHU_CATR(VWHF_ASTM(L),VWHF_ASTM(R)))
    return  VWHF_CATR(l, r);
}


INLINE(Vdwu,VWWU_CATR) (Vwwu l, Vwwu r)
{
#define     VWWU_CATR(L, R) WWU_CATR(VWWU_ASTM(L),VWWU_ASTM(R))
    return  VWWU_CATR(l, r);
}

INLINE(Vdwi,VWWI_CATR) (Vwwi l, Vwwi r)
{
#define     VWWI_CATR(L, R) VDWU_ASWI(WWU_CATR(VWWI_ASTM(L),VWWI_ASTM(R)))
    return  VWWI_CATR(l, r);
}

INLINE(Vdwf,VWWF_CATR) (Vwwf l, Vwwf r)
{
#define     VWWF_CATR(L, R) VDWU_ASWF(WWU_CATR(VWWF_ASTM(L),VWWF_ASTM(R)))
    return  VWWF_CATR(l, r);
}

INLINE(Vqyu,VDYU_CATR) (Vdyu l, Vdyu r)
{
#define     VDYU_CATR(L, R) QYU_ASTV(DYU_CATR(VDYU_ASTM(L),VDYU_ASTM(R)))
    return  VDYU_CATR(l, r);
}

INLINE(Vqbu,VDBU_CATR) (Vdbu l, Vdbu r)
{
#define     VDBU_CATR(L, R) DBU_CATR(L,R)
    return  VDBU_CATR(l, r);
}

INLINE(Vqbi,VDBI_CATR) (Vdbi l, Vdbi r)
{
#define     VDBI_CATR(L, R) DBI_CATR(L,R)
    return  VDBI_CATR(l, r);
}

INLINE(Vqbc,VDBC_CATR) (Vdbc l, Vdbc r)
{
#define     VDBC_CATR(L, R) QBC_ASTV(DBC_CATR(VDBC_ASTM(L),VDBC_ASTM(R)))
    return  VDBC_CATR(l, r);
}

INLINE(Vqhu,VDHU_CATR) (Vdhu l, Vdhu r)
{
#define     VDHU_CATR(L, R) DHU_CATR(L,R)
    return  VDHU_CATR(l, r);
}

INLINE(Vqhi,VDHI_CATR) (Vdhi l, Vdhi r)
{
#define     VDHI_CATR(L, R) DHI_CATR(L,R)
    return  VDHI_CATR(l, r);
}

INLINE(Vqhf,VDHF_CATR) (Vdhf l, Vdhf r)
{
#define     VDHF_CATR(L, R) DHF_CATR(L,R)
    return  VDHF_CATR(l, r);
}

INLINE(Vqwu,VDWU_CATR) (Vdwu l, Vdwu r)
{
#define     VDWU_CATR(L, R) DWU_CATR(L,R)
    return  VDWU_CATR(l, r);
}

INLINE(Vqwi,VDWI_CATR) (Vdwi l, Vdwi r)
{
#define     VDWI_CATR(L, R) DWI_CATR(L,R)
    return  VDWI_CATR(l, r);
}

INLINE(Vqwf,VDWF_CATR) (Vdwf l, Vdwf r)
{
#define     VDWF_CATR(L, R) DWF_CATR(L,R)
    return  VDWF_CATR(l, r);
}

INLINE(Vqdu,VDDU_CATR) (Vddu l, Vddu r)
{
#define     VDDU_CATR(L, R) DDU_CATR(L,R)
    return  VDDU_CATR(l, r);
}

INLINE(Vqdi,VDDI_CATR) (Vddi l, Vddi r)
{
#define     VDDI_CATR(L, R) DDI_CATR(L,R)
    return  VDDI_CATR(l, r);
}

INLINE(Vqdf,VDDF_CATR) (Vddf l, Vddf r)
{
#define     VDDF_CATR(L, R) DDF_CATR(L,R)
    return  VDDF_CATR(l, r);
}

#if _LEAVE_ARM_CATR
}
#endif

#if _ENTER_ARM_REVY
{
#endif

INLINE( uchar, UCHAR_REVY)  (uchar x)
{
#define     UCHAR_REVY(X) vget_lane_u8(vrbit_u8(vdup_n_u8(X)), V8_K0)
    return  UCHAR_REVY(x);
}

INLINE( schar, SCHAR_REVY)  (schar x)
{
#define     SCHAR_REVY(X) vget_lane_s8(vrbit_s8(vdup_n_s8(X)), V8_K0)
    return  SCHAR_REVY(x);
}

INLINE(  char,  CHAR_REVY)   (char x) {return UCHAR_REVY(x);}
INLINE(ushort, USHRT_REVY) (ushort x) {return __rbit(x)>>16;}
INLINE( short,  SHRT_REVY)  (short x) {return __rbit(x)>>16;}
INLINE(  uint,  UINT_REVY)   (uint x) {return __rbit(x);}
INLINE(   int,   INT_REVY)    (int x) {return __rbit(x);}
INLINE( ulong, ULONG_REVY)  (ulong x) {return __rbitl(x);}
INLINE(  long,  LONG_REVY)   (long x) {return __rbitl(x);}
INLINE(ullong,ULLONG_REVY) (ullong x) {return __rbitll(x);}
INLINE( llong, LLONG_REVY)  (llong x) {return __rbitll(x);}

INLINE(float,WBU_REVY) (float v)
{
    float32x2_t m = vdup_n_f32(v);
    uint8x8_t   r = vreinterpret_u8_f32(m);
    r = vrbit_u8(r);
    m = vreinterpret_f32_u8(r);
    return vget_lane_f32(m, 0);
}

INLINE(float,WHU_REVY) (float v)
{
    float32x2_t m = vdup_n_f32(v);
    uint8x8_t   r = vreinterpret_u8_f32(m);
    r = vrbit_u8(r);
    r = vrev16_u8(r);
    m = vreinterpret_f32_u8(r);
    return vget_lane_f32(m, 0);
}

INLINE(float,WWU_REVY) (float v)
{
    float32x2_t m = vdup_n_f32(v);
    uint8x8_t   r = vreinterpret_u8_f32(m);
    r = vrbit_u8(r);
    r = vrev32_u8(r);
    m = vreinterpret_f32_u8(r);
    return vget_lane_f32(m, 0);
}

INLINE(Vwbu,VWBU_REVY) (Vwbu v)
{
#define     VWBU_REVY(V) WBU_ASTV(WBU_REVY(VWBU_ASTM(v)))
    return  VWBU_REVY(v);
}

INLINE(Vwbi,VWBI_REVY) (Vwbi v)
{
#define     VWBI_REVY(V) WBI_ASTV(WBU_REVY(VWBI_ASTM(v)))
    return  VWBI_REVY(v);
}

INLINE(Vwbc,VWBC_REVY) (Vwbc v)
{
#define     VWBC_REVY(V) WBC_ASTV(WBU_REVY(VWBC_ASTM(v)))
    return  VWBC_REVY(v);
}


INLINE(Vwhu,VWHU_REVY) (Vwhu v)
{
#define     VWHU_REVY(V) WHU_ASTV(WHU_REVY(VWHU_ASTM(v)))
    return  VWHU_REVY(v);
}

INLINE(Vwhi,VWHI_REVY) (Vwhi v)
{
#define     VWHI_REVY(V) WHI_ASTV(WHU_REVY(VWHI_ASTM(v)))
    return  VWHI_REVY(v);
}


INLINE(Vwwu,VWWU_REVY) (Vwwu v)
{
#define     VWWU_REVY(V) WWU_ASTV(WWU_REVY(VWWU_ASTM(v)))
    return  VWWU_REVY(v);
}

INLINE(Vwwi,VWWI_REVY) (Vwwi v)
{
#define     VWWI_REVY(V) WWI_ASTV(WWU_REVY(VWWI_ASTM(v)))
    return  VWWI_REVY(v);
}


INLINE(Vdbu,VDBU_REVY) (Vdbu v) {return vrbit_u8(v);}
INLINE(Vdbi,VDBI_REVY) (Vdbi v) {return vrbit_u8(v);}
INLINE(Vdbc,VDBC_REVY) (Vdbc v)
{
    return  VDBU_ASBC(VDBU_REVY(VDBC_ASBU(v)));
}


INLINE(Vdhu,VDHU_REVY) (Vdhu v)
{
    return  vreinterpret_u16_u8(
        vrev16_u8(
            vrbit_u8(
                vreinterpret_u8_u16(v)
            )
        )
    );
}

INLINE(Vdhi,VDHI_REVY) (Vdhi v)
{
    return  VDHU_ASHI(VDHU_REVY(VDHI_ASHU(v)));
}


INLINE(Vdwu,VDWU_REVY) (Vdwu v)
{
    return  vreinterpret_u32_u8(
        vrev32_u8(
            vrbit_u8(
                vreinterpret_u8_u32(v)
            )
        )
    );
}

INLINE(Vdwi,VDWI_REVY) (Vdwi v)
{
    return  VDWU_ASWI(VDWU_REVY(VDWI_ASWU(v)));
}

INLINE(Vddu,VDDU_REVY) (Vddu v)
{
    return  vreinterpret_u64_u8(
        vrev64_u8(
            vrbit_u8(
                vreinterpret_u8_u64(v)
            )
        )
    );
}


INLINE(Vddi,VDDI_REVY) (Vddi v)
{
    return  vreinterpret_s64_u8(
        vrev64_u8(
            vrbit_u8(
                vreinterpret_u8_s64(v)
            )
        )
    );
}


INLINE(Vqbu,VQBU_REVY) (Vqbu v) {return  vrbitq_u8(v);}
INLINE(Vqbi,VQBI_REVY) (Vqbi v) {return  vrbitq_s8(v);}
INLINE(Vqbc,VQBC_REVY) (Vqbc v) {return  VQBU_ASBC(vrbitq_u8(VQBC_ASBU(v)));}


INLINE(Vqhu,VQHU_REVY) (Vqhu v)
{
    return  VQBU_ASHU(vrev16q_u8(vrbitq_u8(VQHU_ASBU(v))));
}

INLINE(Vqhi,VQHI_REVY) (Vqhi v)
{
    return  VQBU_ASHI(vrev16q_u8(vrbitq_u8(VQHI_ASBU(v))));
}


INLINE(Vqwu,VQWU_REVY) (Vqwu v)
{
    return  VQBU_ASWU(vrev32q_u8(vrbitq_u8(VQWU_ASBU(v))));
}

INLINE(Vqwi,VQWI_REVY) (Vqwi v)
{
    return  VQBU_ASWU(vrev32q_u8(vrbitq_u8(VQWI_ASBU(v))));
}

INLINE(Vqdu,VQDU_REVY) (Vqdu v)
{
    return  VQBU_ASDU(vrev64q_u8(vrbitq_u8(VQDU_ASBU(v))));
}

INLINE(Vqdi,VQDI_REVY) (Vqdi v)
{
    return  VQBU_ASDI(vrev64q_u8(vrbitq_u8(VQDI_ASBU(v))));
}

#if _LEAVE_ARM_REVY
}
#endif

#if _ENTER_ARM_REVS
{
#endif

#define     WYU_REVS(X) \
vget_lane_f32(VDBU_ASWF(vrev32_u8(vrbit_u8(VDWF_ASBU(vdup_n_f32(X))))),0)

#define     WBU_REVS(X) \
vget_lane_f32(VDBU_ASWF(vrev32_u8(VDWF_ASBU(vdup_n_f32(X)))),0)

#define     WHU_REVS(X) \
vget_lane_f32(VDHU_ASWF(vrev32_u16(VDWF_ASHU(vdup_n_f32(X)))),0)


#define     DYU_REVS(X) \
vreinterpret_u64_u8(vrev64_u8(vrbit_u8(vreinterpret_u8_u64(X))))

#define     DBU_REVS        vrev64_u8
#define     DBI_REVS        vrev64_s8
#if CHAR_MIN
#   define  DBC_REVS        vrev64_s8
#else
#   define  DBC_REVS        vrev64_u8
#endif

#define     DHU_REVS        vrev64_u16
#define     DHI_REVS        vrev64_s16
#define     DHF_REVS(M)     \
vreinterpret_f16_u16(vrev64_u16(vreinterpret_u16_f16(M)))

#define     DWU_REVS        vrev64_u32
#define     DWI_REVS        vrev64_s32
#define     DWF_REVS        vrev64_f32

INLINE(uint64x2_t,QYU_REVS) (uint64x2_t x)
{
    uint8x16_t  m = vreinterpretq_u8_u64(x);
    m = vrbitq_u8(m);
    m = vrev64q_u8(m);
    return  vcombine_u8(vget_high_u8(m), vget_low_u8(m));
}

#define     QBU_REVS(M) \
vrev64q_u8(vcombine_u8(vget_high_u8(M),vget_low_u8(M)))

#define     QBI_REVS(M) \
vrev64q_s8(vcombine_s8(vget_high_s8(M),vget_low_s8(M)))

#if CHAR_MIN
#   define  QBC_REVS(M)  \
vrev64q_s8(vcombine_s8(vget_high_s8(M),vget_low_s8(M)))

#else
#   define  QBC_REVS(M) \
vrev64q_u8(vcombine_u8(vget_high_u8(M),vget_low_u8(M)))

#endif

#define     QHU_REVS(M) \
vrev64q_u16(vcombine_u16(vget_high_u16(M),vget_low_u16(M)))

#define     QHI_REVS(M) \
vrev64q_s16(vcombine_s16(vget_high_s16(M),vget_low_s16(M)))

// fuggit
#define     QHF_REVS(M)             \
vreinterpretq_f16_u16(              \
    vrev64q_u16(                    \
        vreinterpretq_u16_f16(      \
            vcombine_f16(           \
                vget_high_f16(M),   \
                vget_low_f16( M)    \
            )                       \
        )                           \
    )                               \
)

#define     QWU_REVS(M) \
vrev64q_u32(vcombine_u32(vget_high_u32(M),vget_low_u32(M)))

#define     QWI_REVS(M) \
vrev64q_s32(vcombine_s32(vget_high_s32(M),vget_low_s32(M)))

#define     QWF_REVS(M) \
vrev64q_f32(vcombine_f32(vget_high_f32(M),vget_low_f32(M)))


#define     QDU_REVS(M) vcombine_u64(vget_high_u64(M),vget_low_u64(M))
#define     QDI_REVS(M) vcombine_s64(vget_high_s64(M),vget_low_s64(M))
#define     QDF_REVS(M) vcombine_f64(vget_high_f64(M),vget_low_f64(M))

INLINE(Vwyu,VWYU_REVS) (Vwyu v)
{
#define     VWYU_REVS(V) WYU_ASTV(WYU_REVS(VWYU_ASTM(V)))
    return  VWYU_REVS(v);
}

INLINE(Vwbu,VWBU_REVS) (Vwbu v)
{
#define     VWBU_REVS(V) WBU_ASTV(WBU_REVS(VWBU_ASTM(V)))
    return  VWBU_REVS(v);
}

INLINE(Vwbi,VWBI_REVS) (Vwbi v)
{
#define     VWBI_REVS(V) WBI_ASTV(WBU_REVS(VWBI_ASTM(V)))
    return  VWBI_REVS(v);
}

INLINE(Vwbc,VWBC_REVS) (Vwbc v)
{
#define     VWBC_REVS(V) WBC_ASTV(WBU_REVS(VWBC_ASTM(V)))
    return  VWBC_REVS(v);
}

INLINE(Vwhu,VWHU_REVS) (Vwhu v)
{
#define     VWHU_REVS(V) WHU_ASTV(WHU_REVS(VWHU_ASTM(V)))
    return  VWHU_REVS(v);
}

INLINE(Vwhi,VWHI_REVS) (Vwhi v)
{
#define     VWHI_REVS(V) WHI_ASTV(WHU_REVS(VWHI_ASTM(V)))
    return  VWHI_REVS(v);
}

INLINE(Vwhf,VWHF_REVS) (Vwhf v)
{
#define     VWHF_REVS(V) WHF_ASTV(WHU_REVS(VWHF_ASTM(V)))
    return  VWHF_REVS(v);
}


INLINE(Vdyu,VDYU_REVS) (Vdyu v)
{
#define     VDYU_REVS(V)    DYU_ASTV(DYU_REVS(VDYU_ASTM(V)))
    return  VDYU_REVS(v);
}

INLINE(Vdbu,VDBU_REVS) (Vdbu v)
{
#define     VDBU_REVS(V)    DBU_REVS(V)
    return  VDBU_REVS(v);
}

INLINE(Vdbi,VDBI_REVS) (Vdbi v)
{
#define     VDBI_REVS(V)    DBI_REVS(V)
    return  VDBI_REVS(v);
}

INLINE(Vdbc,VDBC_REVS) (Vdbc v)
{
#define     VDBC_REVS(V) DBC_ASTV(DBC_REVS(VDBC_ASTM(V)))
    return  VDBC_REVS(v);
}

INLINE(Vdhu,VDHU_REVS) (Vdhu v)
{
#define     VDHU_REVS(V)    DHU_REVS(V)
    return  VDHU_REVS(v);
}

INLINE(Vdhi,VDHI_REVS) (Vdhi v)
{
#define     VDHI_REVS(V)    DHI_REVS(V)
    return  VDHI_REVS(v);
}

INLINE(Vdhf,VDHF_REVS) (Vdhf v)
{
#define     VDHF_REVS(V)    DHF_REVS(V)
    return  VDHF_REVS(v);
}

INLINE(Vdwu,VDWU_REVS) (Vdwu v)
{
#define     VDWU_REVS(V)    DWU_REVS(V)
    return  VDWU_REVS(v);
}

INLINE(Vdwi,VDWI_REVS) (Vdwi v)
{
#define     VDWI_REVS(V)    DWI_REVS(V)
    return  VDWI_REVS(v);
}

INLINE(Vdwf,VDWF_REVS) (Vdwf v)
{
#define     VDWF_REVS(V)    DWF_REVS(V)
    return  VDWF_REVS(v);
}

INLINE(Vqyu,VQYU_REVS) (Vqyu v)
{
#define     VQYU_REVS(V)    QYU_ASTV(QYU_REVS(VQYU_ASTM(V)))
    return  VQYU_REVS(v);
}

INLINE(Vqbu,VQBU_REVS) (Vqbu v)
{
#define     VQBU_REVS(V)    QBU_REVS(V)
    return  VQBU_REVS(v);
}

INLINE(Vqbi,VQBI_REVS) (Vqbi v)
{
#define     VQBI_REVS(V)    QBI_REVS(V)
    return  VQBI_REVS(v);
}

INLINE(Vqbc,VQBC_REVS) (Vqbc v)
{
#define     VQBC_REVS(V)    QBC_ASTV(QBC_REVS(VQBC_ASTM(V)))
    return  VQBC_REVS(v);
}

INLINE(Vqhu,VQHU_REVS) (Vqhu v)
{
#define     VQHU_REVS(V)    QHU_REVS(V)
    return  VQHU_REVS(v);
}

INLINE(Vqhi,VQHI_REVS) (Vqhi v)
{
#define     VQHI_REVS(V)    QHI_REVS(V)
    return  VQHI_REVS(v);
}

INLINE(Vqhf,VQHF_REVS) (Vqhf v)
{
#define     VQHF_REVS(V)    QHF_REVS(V)
    return  VQHF_REVS(v);
}

INLINE(Vqwu,VQWU_REVS) (Vqwu v)
{
#define     VQWU_REVS(V)    QWU_REVS(V)
    return  VQWU_REVS(v);
}

INLINE(Vqwi,VQWI_REVS) (Vqwi v)
{
#define     VQWI_REVS(V)    QWI_REVS(V)
    return  VQWI_REVS(v);
}

INLINE(Vqwf,VQWF_REVS) (Vqwf v)
{
#define     VQWF_REVS(V)    QWF_REVS(V)
    return  VQWF_REVS(v);
}

INLINE(Vqdu,VQDU_REVS) (Vqdu v)
{
#define     VQDU_REVS(V)    QDU_REVS(V)
    return  VQDU_REVS(v);
}

INLINE(Vqdi,VQDI_REVS) (Vqdi v)
{
#define     VQDI_REVS(V)    QDI_REVS(V)
    return  VQDI_REVS(v);
}

INLINE(Vqdf,VQDF_REVS) (Vqdf v)
{
#define     VQDF_REVS(V)    QDF_REVS(V)
    return  VQDF_REVS(v);
}

#if _LEAVE_ARM_REVS
}
#endif

#if _ENTER_ARM_REVB
{
#endif

INLINE( ushort, USHRT_REVB)  (ushort x) {return __revsh(x);}
INLINE(  short,  SHRT_REVB)   (short x) {return __revsh(x);}
INLINE(   uint,  UINT_REVB)    (uint x) {return __rev(x);}
INLINE(    int,   INT_REVB)     (int x) {return __rev(x);}
INLINE(  ulong, ULONG_REVB)   (ulong x) {return __revl(x);}
INLINE(   long,  LONG_REVB)    (long x) {return __revl(x);}
INLINE( ullong,ULLONG_REVB)  (ullong x) {return __revll(x);}
INLINE(  llong, LLONG_REVB)   (llong x) {return __revll(x);}
INLINE(flt16_t, FLT16_REVB) (flt16_t x)
{
    float16x4_t m = vdup_n_f16(x);
    uint8x8_t   v = vreinterpret_u8_f16(m);
    v = vrev16_u8(v);
    return  vget_lane_f16(v, 0);
}

INLINE(  float,   FLT_REVB)   (float x)
{
    float32x2_t v = vdup_n_f32(x);
    v = vrev32_u8(vreinterpret_u8_f32(v));
    return  vget_lane_f32(v, 0);
}

INLINE( double,   DBL_REVB)  (double x)
{
    float64x1_t m = vdup_n_f64(x);
    uint8x8_t   v = vreinterpret_u8_f64(m);
    v = vrev64_u8(v);
    return  vget_lane_f64(v, 0);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,revbqu) (QUAD_UTYPE x)
{
    QUAD_TYPE l = {.U=x};
    QUAD_TYPE r = {
        .Lo.U = __revll(l.Hi.U),
        .Hi.U = __revll(l.Lo.U),
    };
    return  r.U;
}

INLINE(QUAD_ITYPE,revbqi) (QUAD_ITYPE x)
{
    QUAD_TYPE l = {.I=x};
    QUAD_TYPE r = {
        .Lo.U = __revll(l.Hi.U),
        .Hi.U = __revll(l.Lo.U),
    };
    return  r.I;
}

INLINE(QUAD_FTYPE,revbqf) (QUAD_FTYPE x)
{
    QUAD_TYPE l = {.F=x};
    QUAD_TYPE r = {
        .Lo.U = __revll(l.Hi.U),
        .Hi.U = __revll(l.Lo.U),
    };
    return  r.F;
}

#endif

INLINE(float,WHU_REVB) (float v)
{
    float32x2_t m = vdup_n_f32(v);
    uint8x8_t   r = vreinterpret_u8_f32(m);
    r = vrev16_u8(r);
    m = vreinterpret_f32_u8(r);
    return  vget_lane_f32(m, 0);
}

INLINE(float,WWU_REVB) (float v)
{
    float32x2_t m = vdup_n_f32(v);
    uint8x8_t   r = vreinterpret_u8_f32(m);
    r = vrev32_u8(r);
    m = vreinterpret_f32_u8(r);
    return  vget_lane_f32(m, 0);
}

INLINE(Vwhu,VWHU_REVB) (Vwhu v)
{
    return  WHU_ASTV(WHU_REVB(VWHU_ASTM(v)));
}

INLINE(Vwhi,VWHI_REVB) (Vwhi v)
{
    return  WHI_ASTV(WHU_REVB(VWHI_ASTM(v)));
}

INLINE(Vwhf,VWHF_REVB) (Vwhf v)
{
    return  WHF_ASTV(WHU_REVB(VWHF_ASTM(v)));
}


INLINE(Vwwu,VWWU_REVB) (Vwwu v)
{
    return  WWU_ASTV(WWU_REVB(VWWU_ASTM(v)));
}

INLINE(Vwwi,VWWI_REVB) (Vwwi v)
{
    return  WWI_ASTV(WWU_REVB(VWWI_ASTM(v)));
}

INLINE(Vwwf,VWWF_REVB) (Vwwf v)
{
    return  WWF_ASTV(WWU_REVB(VWWF_ASTM(v)));
}


INLINE(Vdhu,VDHU_REVB) (Vdhu v)
{
    return  vreinterpret_u16_u8(vrev16_u8(vreinterpret_u8_u16(v)));
}

INLINE(Vdhi,VDHI_REVB) (Vdhi v)
{
    return  vreinterpret_s16_u8(vrev16_u8(vreinterpret_u8_s16(v)));
}

INLINE(Vdhf,VDHF_REVB) (Vdhf v)
{
    return  vreinterpret_f16_u8(vrev16_u8(vreinterpret_u8_f16(v)));
}


INLINE(Vdwu,VDWU_REVB) (Vdwu v)
{
    return  vreinterpret_u32_u8(vrev32_u8(vreinterpret_u8_u32(v)));
}

INLINE(Vdwi,VDWI_REVB) (Vdwi v)
{
    return  vreinterpret_s32_u8(vrev32_u8(vreinterpret_u8_s32(v)));
}

INLINE(Vdwf,VDWF_REVB) (Vdwf v)
{
    return  vreinterpret_f32_u8(vrev32_u8(vreinterpret_u8_f32(v)));
}


INLINE(Vddu,VDDU_REVB) (Vddu v)
{
    return  vreinterpret_u64_u8(vrev64_u8(vreinterpret_u8_u64(v)));
}

INLINE(Vddi,VDDI_REVB) (Vddi v)
{
    return  vreinterpret_s64_u8(vrev64_u8(vreinterpret_u8_s64(v)));
}

INLINE(Vddf,VDDF_REVB) (Vddf v)
{
    return  vreinterpret_f64_u8(vrev64_u8(vreinterpret_u8_f64(v)));
}


INLINE(Vqhu,VQHU_REVB) (Vqhu v)
{
    return  vreinterpretq_u16_u8(vrev16q_u8(vreinterpretq_u8_u16(v)));
}

INLINE(Vqhi,VQHI_REVB) (Vqhi v)
{
    return  vreinterpretq_s16_u8(vrev16q_u8(vreinterpretq_u8_s16(v)));
}

INLINE(Vqhf,VQHF_REVB) (Vqhf v)
{
    return  vreinterpretq_f16_u8(vrev16q_u8(vreinterpretq_u8_f16(v)));
}


INLINE(Vqwu,VQWU_REVB) (Vqwu v)
{
    return  vreinterpretq_u32_u8(vrev32q_u8(vreinterpretq_u8_u32(v)));
}

INLINE(Vqwi,VQWI_REVB) (Vqwi v)
{
    return  vreinterpretq_s32_u8(vrev32q_u8(vreinterpretq_u8_s32(v)));
}

INLINE(Vqwf,VQWF_REVB) (Vqwf v)
{
    return  vreinterpretq_f32_u8(vrev32q_u8(vreinterpretq_u8_f32(v)));
}


INLINE(Vqdu,VQDU_REVB) (Vqdu v)
{
    return  vreinterpretq_u64_u8(vrev64q_u8(vreinterpretq_u8_u64(v)));
}

INLINE(Vqdi,VQDI_REVB) (Vqdi v)
{
    return  vreinterpretq_s64_u8(vrev64q_u8(vreinterpretq_u8_s64(v)));
}

INLINE(Vqdf,VQDF_REVB) (Vqdf v)
{
    return  vreinterpretq_f64_u8(vrev64q_u8(vreinterpretq_u8_f64(v)));
}


#if _LEAVE_ARM_REVB
}
#endif


#if _ENTER_ARM_GETL
{
#endif

INLINE( uint8_t, USHRT_GETL) (ushort x) 
{
    return  (USHRT_MAX>>(USHRT_WIDTH>>1))&x;
}

INLINE(  int8_t,  SHRT_GETL)  (short x)
{
    return  (USHRT_MAX>>(SHRT_WIDTH>>1))&x;
}


INLINE(uint16_t,  UINT_GETL)   (uint x)
{
    return  (UINT_MAX>>(UINT_WIDTH>>1))&x;
}

INLINE( int16_t,   INT_GETL)    (int x)
{
    return  (UINT_MAX>>(INT_WIDTH>>1))&x;
}


#if DWRD_NLONG == 2

INLINE(uint16_t, ULONG_GETL)  (ulong x)
{
    return  (ULONG_MAX>>(ULONG_WIDTH>>1))&x;
}

INLINE( int16_t,  LONG_GETL)   (long x)
{
    return  (ULONG_MAX>>(LONG_WIDTH>>1))&x;
}

#else

INLINE(uint32_t, ULONG_GETL)  (ulong x)
{
    return  (ULONG_MAX>>(ULONG_WIDTH>>1))&x;
}

INLINE( int32_t,  LONG_GETL)   (long x)
{
    return  (ULONG_MAX>>(LONG_WIDTH>>1))&x;
}

#endif


#if QUAD_NLLONG == 2

INLINE(uint32_t,ULLONG_GETL) (ullong x)
{
    return  (ULLONG_MAX>>(ULLONG_WIDTH>>1))&x;
}

INLINE( int32_t, LLONG_GETL)  (llong x)
{
    return  (ULLONG_MAX>>(LLONG_WIDTH>>1))&x;
}

INLINE(uint64_t,getlqu) (QUAD_UTYPE x)
{
#define     getlqu(X) (((QUAD_TYPE){.U=X}).Lo.U)
    return  getlqu(x);
}

INLINE( int64_t,getlqi) (QUAD_ITYPE x)
{
#define     getlqi(X) (((QUAD_TYPE){.I=X}).Lo.I)
    return  getlqi(x);
}

#else


INLINE(uint32_t,ULLONG_GETL) (ullong x)
{
    return  (ULLONG_MAX>>(ULLONG_WIDTH>>1))&x;
}

INLINE( int32_t, LLONG_GETL)  (llong x)
{
    return  (ULLONG_MAX>>(LLONG_WIDTH>>1))&x;
}

#endif

#define     DWF_GETL(x) vget_lane_f32(x,0)

INLINE(Vwyu,VDYU_GETL) (Vdyu x)
{
#define     VDYU_GETL(X) WYU_ASTV(DWF_GETL(VDYU_ASWF(X)))
    return  VDYU_GETL(x);
}

INLINE(Vwbu,VDBU_GETL) (Vdbu x)
{
#define     VDBU_GETL(X) WBU_ASTV(DWF_GETL(VDBU_ASWF(X)))
    return  VDBU_GETL(x);
}

INLINE(Vwbi,VDBI_GETL) (Vdbi x)
{
#define     VDBI_GETL(X) WBI_ASTV(DWF_GETL(VDBI_ASWF(X)))
    return  VDBI_GETL(x);
}

INLINE(Vwbc,VDBC_GETL) (Vdbc x)
{
#define     VDBC_GETL(X) WBC_ASTV(DWF_GETL(VDBC_ASWF(X)))
    return  VDBC_GETL(x);
}

INLINE(Vwhu,VDHU_GETL) (Vdhu x)
{
#define     VDHU_GETL(X) WHU_ASTV(DWF_GETL(VDHU_ASWF(X)))
    return  VDHU_GETL(x);
}

INLINE(Vwhi,VDHI_GETL) (Vdhi x)
{
#define     VDHI_GETL(X) WHI_ASTV(DWF_GETL(VDHI_ASWF(X)))
    return  VDHI_GETL(x);
}

INLINE(Vwhf,VDHF_GETL) (Vdhf x)
{
#define     VDHF_GETL(X) WHF_ASTV(DWF_GETL(VDHF_ASWF(X)))
    return  VDHF_GETL(x);
}

INLINE(Vwwu,VDWU_GETL) (Vdwu x)
{
#define     VDWU_GETL(X) WWU_ASTV(DWF_GETL(VDWU_ASWF(X)))
    return  VDWU_GETL(x);
}

INLINE(Vwwi,VDWI_GETL) (Vdwi x)
{
#define     VDWI_GETL(X) WWI_ASTV(DWF_GETL(VDWI_ASWF(X)))
    return  VDWI_GETL(x);
}

INLINE(Vwwf,VDWF_GETL) (Vdwf x)
{
#define     VDWF_GETL(X) WWF_ASTV(DWF_GETL(x))
    return  VDWF_GETL(x);
}


INLINE(Vdyu,VQYU_GETL) (Vqyu x)
{
#define     VQYU_GETL(X)    VDDU_ASYU(vget_low_u64(VQYU_ASDU(X)))
    return  VQYU_GETL(x);
}

INLINE(Vdbu,VQBU_GETL) (Vqbu x) {return vget_low_u8(x);}
INLINE(Vdbi,VQBI_GETL) (Vqbi x) {return vget_low_s8(x);}
INLINE(Vdbc,VQBC_GETL) (Vqbc x)
{
#if CHAR_MIN
#   define  VQBC_GETL(X) DBC_ASTV(vget_low_s8(VQBC_ASTM(X)))
#else
#   define  VQBC_GETL(X) DBC_ASTV(vget_low_u8(VQBC_ASTM(X)))
#endif
    return  VQBC_GETL(x);
}

INLINE(Vdhu,VQHU_GETL) (Vqhu x) {return vget_low_u16(x);}
INLINE(Vdhi,VQHI_GETL) (Vqhi x) {return vget_low_s16(x);}
INLINE(Vdhf,VQHF_GETL) (Vqhf x) {return vget_low_f16(x);}
INLINE(Vdwu,VQWU_GETL) (Vqwu x) {return vget_low_u32(x);}
INLINE(Vdwi,VQWI_GETL) (Vqwi x) {return vget_low_s32(x);}
INLINE(Vdwf,VQWF_GETL) (Vqwf x) {return vget_low_f32(x);}
INLINE(Vddu,VQDU_GETL) (Vqdu x) {return vget_low_u64(x);}
INLINE(Vddi,VQDI_GETL) (Vqdi x) {return vget_low_s64(x);}
INLINE(Vddf,VQDF_GETL) (Vqdf x) {return vget_low_f64(x);}

#if _LEAVE_ARM_GETL
}
#endif

#if _ENTER_ARM_GETR
{
#endif

INLINE( uint8_t, USHRT_GETR) (ushort x) 
{
#define     USHRT_GETR(X) ((uint8_t) (((unsigned) X)>>8))
    return  x>>8;
}

INLINE(  int8_t,  SHRT_GETR)  (short x)
{
#define     SHRT_GETR(X) ((int8_t) (((unsigned) X)>>8))
    return  ((unsigned) x)>>8;
}


INLINE(uint16_t,  UINT_GETR)   (uint x)
{
#define     UINT_GETR(X) ((uint16_t) (((unsigned) X)>>16))
    return  x>>16;
}

INLINE( int16_t,   INT_GETR)    (int x)
{
#define     INT_GETR(X) ((int16_t) (((unsigned) X)>>16))
    return  ((unsigned) x)>>16;
}


#if DWRD_NLONG == 2

INLINE(uint16_t, ULONG_GETR)  (ulong x)
{
#define     ULONG_GETR(X) ((uint16_t) (((unsigned) X)>>16))
    return  x>>16;
}

INLINE( int16_t,  LONG_GETR)   (long x)
{
#define     LONG_GETR(X) ((int16_t) (((unsigned) X)>>16))
    return  ((unsigned) x)>>16;
}

#else

INLINE(uint32_t, ULONG_GETR)  (ulong x)
{
#define     ULONG_GETR(X) ((uint32_t) (((ulong) X)>>32))
    return  x>>32;
}

INLINE( int32_t,  LONG_GETR)   (long x)
{
#define     LONG_GETR(X) ((int32_t) (((ulong) X)>>32))
    return  ((ulong) x)>>32;
}

#endif


#if QUAD_NLLONG == 2

INLINE(uint32_t, ULLONG_GETR)  (ullong x)
{
#define     ULLONG_GETR(X) ((uint32_t) (((ullong) X)>>32))
    return  x>>32;
}

INLINE( int32_t,  LLONG_GETR)   (llong x)
{
#define     LLONG_GETR(X) ((int32_t) (((ullong) X)>>32))
    return  ((ullong) x)>>32;
}


INLINE(uint64_t,getrqu) (QUAD_UTYPE x)
{
#define     getrqu(X) ((uint64_t) (((QUAD_UTYPE) X)>>64))
    return  x>>64;
}

INLINE( int64_t,getrqi) (QUAD_ITYPE x)
{
#define     getrqi(X) ((int64_t) (((QUAD_UTYPE) X)>>64))
    return  ((QUAD_UTYPE) x)>>64;
}

#else

INLINE(uint64_t, ULLONG_GETR)  (ullong x)
{
#define     ULLONG_GETR(X) ((uint32_t) (((ullong) X)>>64))
    return  x>>64;
}

INLINE( int64_t,  LLONG_GETR)   (llong x)
{
#define     LLONG_GETR(X) ((int32_t) (((ullong) X)>>64))
    return  ((ullong) x)>>64;
}

#endif

#define     DWF_GETR(X)     vget_lane_f32(X,1)

INLINE(Vwyu,VDYU_GETR) (Vdyu x)
{
#define     VDYU_GETR(X) WYU_ASTV(DWF_GETR(VDYU_ASWF(X)))
    return  VDYU_GETR(x);
}

INLINE(Vwbu,VDBU_GETR) (Vdbu x)
{
#define     VDBU_GETR(X) WBU_ASTV(DWF_GETR(VDBU_ASWF(X)))
    return  VDBU_GETR(x);
}

INLINE(Vwbi,VDBI_GETR) (Vdbi x)
{
#define     VDBI_GETR(X) WBI_ASTV(DWF_GETR(VDBI_ASWF(X)))
    return  VDBI_GETR(x);
}

INLINE(Vwbc,VDBC_GETR) (Vdbc x)
{
#define     VDBC_GETR(X) WBC_ASTV(DWF_GETR(VDBC_ASWF(X)))
    return  VDBC_GETR(x);
}

INLINE(Vwhu,VDHU_GETR) (Vdhu x)
{
#define     VDHU_GETR(X) WHU_ASTV(DWF_GETR(VDHU_ASWF(X)))
    return  VDHU_GETR(x);
}

INLINE(Vwhi,VDHI_GETR) (Vdhi x)
{
#define     VDHI_GETR(X) WHI_ASTV(DWF_GETR(VDHI_ASWF(X)))
    return  VDHI_GETR(x);
}

INLINE(Vwhf,VDHF_GETR) (Vdhf x)
{
#define     VDHF_GETR(X) WHF_ASTV(DWF_GETR(VDHF_ASWF(X)))
    return  VDHF_GETR(x);
}

INLINE(Vwwu,VDWU_GETR) (Vdwu x)
{
#define     VDWU_GETR(X) WWU_ASTV(DWF_GETR(VDWU_ASWF(X)))
    return  VDWU_GETR(x);
}

INLINE(Vwwi,VDWI_GETR) (Vdwi x)
{
#define     VDWI_GETR(X) WWI_ASTV(DWF_GETR(VDWI_ASWF(X)))
    return  VDWI_GETR(x);
}

INLINE(Vwwf,VDWF_GETR) (Vdwf x)
{
#define     VDWF_GETR(X) WWF_ASTV(DWF_GETR(X))
    return  VDWF_GETR(x);
}

INLINE(Vdyu,VQYU_GETR) (Vqyu x)
{
#define     VQYU_GETR(X)    VDDU_ASYU(vget_high_u64(VQYU_ASDU(X)))
    return  VQYU_GETR(x);
}

INLINE(Vdbu,VQBU_GETR) (Vqbu x) {return vget_high_u8(x);}
INLINE(Vdbi,VQBI_GETR) (Vqbi x) {return vget_high_s8(x);}
INLINE(Vdbc,VQBC_GETR) (Vqbc x)
{
#if CHAR_MIN
#   define  VQBC_GETR(X) DBC_ASTV(vget_high_s8(VQBC_ASTM(X)))
#else
#   define  VQBC_GETR(X) DBC_ASTV(vget_high_u8(VQBC_ASTM(X)))
#endif
    return  VQBC_GETR(x);
}

INLINE(Vdhu,VQHU_GETR) (Vqhu x) {return vget_high_u16(x);}
INLINE(Vdhi,VQHI_GETR) (Vqhi x) {return vget_high_s16(x);}
INLINE(Vdhf,VQHF_GETR) (Vqhf x) {return vget_high_f16(x);}

INLINE(Vdwu,VQWU_GETR) (Vqwu x) {return vget_high_u32(x);}
INLINE(Vdwi,VQWI_GETR) (Vqwi x) {return vget_high_s32(x);}
INLINE(Vdwf,VQWF_GETR) (Vqwf x) {return vget_high_f32(x);}

INLINE(Vddu,VQDU_GETR) (Vqdu x) {return vget_high_u64(x);}
INLINE(Vddi,VQDI_GETR) (Vqdi x) {return vget_high_s64(x);}
INLINE(Vddf,VQDF_GETR) (Vqdf x) {return vget_high_f64(x);}

#if _LEAVE_ARM_GETR
}
#endif

#if _ENTER_ARM_GET1
{
#endif

INLINE(  _Bool, VWYU_GET1) (Vwyu v, Rc(0, 31) k)
{
#define     WYU_GET1(V, K) ((_Bool)(1&(FLT_ASTG(V).U>>(K))))
#define     VWYU_GET1(V, K)   WYU_GET1(VWYU_ASTM(V), K)
    return  VWYU_GET1(v, k);
}

INLINE( uint8_t,VWBU_GET1) (Vwbu v, Rc(0,3) k)
{
#define     WBU_GET1(A, B)  \
vget_lane_u8(vreinterpret_u8_f32(vdup_n_f32(A)),(3&B))

#define     VWBU_GET1(a, b)     WBU_GET1(VWBU_ASTM(a), b)
    float32x2_t t = vset_lane_f32(VWBU_ASTM(v),t, V2_K0);
    return  vget_lane_u32(vreinterpret_u32_f32(t),V2_K0)>>(k*8);
}

INLINE(  int8_t,VWBI_GET1) (Vwbi v, Rc(0,3) k)
{
#define     WBI_GET1(A, B)  \
vget_lane_s8(vreinterpret_s8_f32(vdup_n_f32(A)),(3&B))

#define     VWBI_GET1(a, b)     WBI_GET1(VWBI_ASTM(a), b)
    float32x2_t t = vset_lane_f32(VWBI_ASTM(v),t, V2_K0);
    return  vget_lane_u32(vreinterpret_u32_f32(t),V2_K0)>>(k*8);
}

INLINE(    char,VWBC_GET1) (Vwbc v, Rc(0,3) k)
{
#define     WBC_GET1(a, b)  \
((char) vget_lane_u8(vreinterpret_u8_f32(vdup_n_f32(a)),(3&(b))))

#define     VWBC_GET1(a, b)     WBC_GET1(VWBC_ASTM(a), b)
    float32x2_t f = vset_lane_f32(VWBC_ASTM(v),f, V2_K0);
    uint32x2_t  u = vreinterpret_u32_f32(f);
    u = vshl_u32(u, vdup_n_s32(-8*k));
    return  vget_lane_u32(u, V2_K0);
}


INLINE(uint16_t,VWHU_GET1) (Vwhu v, Rc(0,1) k)
{
#define     WHU_GET1(A, B)  \
vget_lane_u16(vreinterpret_u16_f32(vdup_n_f32(A)),(1&B))

#define     VWHU_GET1(a, b)     WHU_GET1(VWHU_ASTM(a), b)
    float32x2_t f = vset_lane_f32(VWHU_ASTM(v),f, V2_K0);
    uint32x2_t  u = vreinterpret_u32_f32(f);
    u = vshl_u32(u, vdup_n_s32(-16*k));
    return  vget_lane_u16(vreinterpret_u16_u32(u), V4_K0);
}

INLINE( int16_t,VWHI_GET1) (Vwhi v, Rc(0,1) k)
{
#define     WHI_GET1(A, B)  \
vget_lane_s16(vreinterpret_s16_f32(vdup_n_f32(A)),(1&B))

#define     VWHI_GET1(A, B)     WHI_GET1(VWHI_ASTM(A),B)
    float32x2_t f = vset_lane_f32(VWHI_ASTM(v),f, V2_K0);
    uint32x2_t  u = vreinterpret_u32_f32(f);
    u = vshl_u32(u, vdup_n_s32(-16*k));
    return  vget_lane_s16(vreinterpret_u16_u32(u), V4_K0);
}

INLINE( flt16_t,VWHF_GET1) (Vwhf v, Rc(0,1) k)
{
#define     WHF_GET1(A, B)  \
vget_lane_f16(vreinterpret_f16_f32(vdup_n_f32(A)), (1&B))

#define     VWHF_GET1(a, b)     WHF_GET1(VWHF_ASTM(a), b)
    float32x2_t f = vset_lane_f32(VWHF_ASTM(v),f, V2_K0);
    uint32x2_t  u = vreinterpret_u32_f32(f);
    u = vshl_u32(u, vdup_n_s32(-16*k));
    return  vget_lane_f16(vreinterpret_f16_u32(u), V4_K0);
}



INLINE(_Bool, VDYU_GET1) (Vdyu v, Rc(0, 63) k)
{
#define      DYU_GET1(V, K)                 \
(                                           \
    (_Bool)                                 \
    (1&(vget_lane_u64((V), 0)>>((K)%64)))   \
)

#define     VDYU_GET1(V, K)   DYU_GET1(VDYU_ASTM(V), K)
    return  VDYU_GET1(v, k);
}


INLINE( uint8_t,VDBU_GET1) (Vdbu v, Rc(0,7) k)
{
#define     VDBU_GET1(A, B) vget_lane_u8(A, (7&B))
    return  vget_lane_u64(vreinterpret_u64_u8(v),0)>>(k*8);
}

INLINE(  int8_t,VDBI_GET1) (Vdbi v, Rc(0,7) k)
{
#define     VDBI_GET1(A, B) vget_lane_s8(A, (7&B))
    uint64x1_t t = vreinterpret_u64_s8(v);
    t = vshl_u64(t, vdup_n_s64(0-8*k));
    return  vget_lane_s8(vreinterpret_s8_u64(t), 0);
}

INLINE(    char,VDBC_GET1) (Vdbc v, Rc(0,7) k)
{
#if CHAR_MIN
#   define  VDBC_GET1(A, B) ((char) vget_lane_s8(VDBC_ASTM(A),B))
#else
#   define  VDBC_GET1(A, B) ((char) vget_lane_u8(VDBC_ASTM(A),B))
#endif
    return  vget_lane_u64(VDBC_ASDU(v), 0)>>(k*8);
}


INLINE(uint16_t,VDHU_GET1) (Vdhu v, Rc(0,3) k)
{
#define     VDHU_GET1(A, B) vget_lane_u16(A,(3&B))
    return  vget_lane_u64(vreinterpret_u64_u16(v),0)>>(k*16);
}

INLINE( int16_t,VDHI_GET1) (Vdhi v, Rc(0,3) k)
{
#define     VDHI_GET1(A, B) vget_lane_s16(A,(3&B))
    uint64x1_t t = vreinterpret_u64_s16(v);
    t = vshl_u64(t, vdup_n_s64(0-16*k));
    return  vget_lane_s16(vreinterpret_s16_u64(t), 0);
}

INLINE( flt16_t,VDHF_GET1) (Vdhf v, Rc(0,3) k)
{
#define     VDHF_GET1(A, B) vget_lane_f16(A,(3&B))
    uint64x1_t t = vreinterpret_u64_f16(v);
    t = vshl_u64(t, vdup_n_s64(0-16*k));
    return  vget_lane_f16(vreinterpret_f16_u64(t), 0);
}


INLINE(uint32_t,VDWU_GET1) (Vdwu v, Rc(0,1) k)
{
#define     VDWU_GET1(A, B) vget_lane_u32(A,(1&B))
    return  vget_lane_u64(vreinterpret_u64_u32(v),0)>>(k*32);
}

INLINE( int32_t,VDWI_GET1) (Vdwi v, Rc(0,1) k)
{
#define     VDWI_GET1(A, B) vget_lane_s32(A,(1&B))
    uint64x1_t t = vreinterpret_u64_s32(v);
    t = vshl_u64(t, vdup_n_s64(0-32*k));
    return  vget_lane_s32(vreinterpret_s32_u64(t), 0);
}

INLINE(   float,VDWF_GET1) (Vdwf v, Rc(0,1) k)
{
#define     VDWF_GET1(A, B) vget_lane_f32(A,(1&B))
    uint64x1_t t = vreinterpret_u64_f32(v);
    t = vshl_u64(t, vdup_n_s64(0-32*k));
    return vget_lane_s32(vreinterpret_f32_u64(t), 0);
}


INLINE(_Bool, VQYU_GET1) (Vqyu v, Rc(0, 127) k)
{
#define     QYU_GET1(V, K) \
(\
    (_Bool) \
    (\
        (K > 63)\
        ?   (vgetq_lane_u64(V, 1)>>(K-64))&1\
        :   (vgetq_lane_u64(V, 0)>>(K&63))&1\
    )\
)

#define     VQYU_GET1(V, K)   QYU_GET1(VQYU_ASDU(V),K)
    uint64x2_t  m = VQYU_ASDU(v);
    uint64x1_t  b;
    int64x1_t   n = vdup_n_s64(k);
    if (k > 63)
    {
        b = vget_high_u64(m);
        n = vsub_s64(n, vdup_n_s64(64));
    }
    else 
    {
        b = vget_low_u64(m);
    }
    n = vneg_s64(n);
    b = vshl_u64(b, n);
    return  1&vget_lane_u64(b, 0);
}

INLINE( uint8_t,VQBU_GET1) (Vqbu v, Rc(0,15) k)
{
#define     VQBU_GET1(A, B) vgetq_lane_u8(A,(15&B))
    return (VDBU_GET1)
    (
        k > 7
        ?   VQBU_GETR(v)
        :   VQBU_GETL(v),
        k
    );
}

INLINE(  int8_t,VQBI_GET1) (Vqbi v, Rc(0,15) k)
{
#define     VQBI_GET1(A, B) vgetq_lane_s8(A,(15&B))
    return (VDBI_GET1)
    (
        k > 7
        ?   VQBI_GETR(v)
        :   VQBI_GETL(v),
        k
    );
}

INLINE(    char,VQBC_GET1) (Vqbc v, Rc(0,15) k)
{
#if CHAR_MIN
#   define  VQBC_GET1(A, B) ((char) vgetq_lane_s8(VQBC_ASTM(A),(15&B)))
    int8x16_t   q = VQBC_ASTM(v);
    int8x8_t    d = k > 7
        ?   vget_high_s8(q)
        :   vget_low_s8(q);
    uint64x1_t  t = vreinterpret_u64_s8(d);
    t = vshl_u64(t, vdup_n_s64(-8*k));
    d = vreinterpret_s8_u64(t);
    return  vget_lane_s8(d, V8_K0);
#else
#   define  VQBC_GET1(A, B) ((char) vgetq_lane_u8(VQBC_ASTM(A),(15&B)))
    uint8x16_t  q = VQBC_ASTM(v);
    uint8x8_t   d = k > 7
        ?   vget_high_u8(q)
        :   vget_low_u8(q);
    uint64x1_t  t = vreinterpret_u64_u8(d);
    t = vshl_u64(t, vdup_n_s64(-8*k));
    d = vreinterpret_u8_u64(t);
    return  vget_lane_s8(d, V8_K0);

#endif
}

INLINE(uint16_t,VQHU_GET1) (Vqhu v, Rc(0,7) k)
{
#define     VQHU_GET1(A, B) vgetq_lane_u16(A, (7&B))
    return (VDHU_GET1)
    (
        k > 3
        ?   VQHU_GETR(v)
        :   VQHU_GETL(v),
        k
    );
}

INLINE( int16_t,VQHI_GET1) (Vqhi v, Rc(0,7) k)
{
#define     VQHI_GET1(A, B) vgetq_lane_s16(A, (7&B))
    return (VDHU_GET1)
    (
        k > 3
        ?   VQHI_GETR(v)
        :   VQHI_GETL(v),
        k
    );
}

INLINE( flt16_t,VQHF_GET1) (Vqhf v, Rc(0,7) k)
{
#define     VQHF_GET1(A, B) vgetq_lane_f16(A,(7&B))
    return (VDHF_GET1)
    (
        k > 3
        ?   VQHF_GETR(v)
        :   VQHF_GETL(v),
        k
    );
}

INLINE(uint32_t,VQWU_GET1) (Vqwu v, Rc(0,3) k)
{
#define     VQWU_GET1(A, B) vgetq_lane_u32(A, (3&B))
    return (VDWU_GET1)
    (
        k > 1
        ?   VQWU_GETR(v)
        :   VQWU_GETL(v),
        k
    );
}


INLINE( int32_t,VQWI_GET1) (Vqwi v, Rc(0,3) k)
{
#define     VQWI_GET1(A, B) vgetq_lane_s32(A, (3&B))
    return (VDWI_GET1)
    (
        k > 1
        ?   VQWI_GETR(v)
        :   VQWI_GETL(v),
        k
    );
}

INLINE(  float,VQWF_GET1) (Vqwf v, Rc(0,3) k)
{
#define     VQWF_GET1(A, B) vgetq_lane_f32(A, (3&B))
    return (VDWF_GET1)
    (
        k > 1
        ?   VQWF_GETR(v)
        :   VQWF_GETL(v),
        k
    );
}

INLINE(uint64_t,VQDU_GET1) (Vqdu v, Rc(0, 1) k)
{
#define     VQDU_GET1(A, B) vgetq_lane_u64(A,(1&B))
    return  k
    ?   vgetq_lane_u64(v, 1)
    :   vgetq_lane_u64(v, 0);
}

INLINE( int64_t,VQDI_GET1) (Vqdi v, Rc(0,1) k)
{
#define     VQDI_GET1(A, B) vgetq_lane_s64(A,(1&B))
    return  k
    ?   vgetq_lane_s64(v, 1)
    :   vgetq_lane_s64(v, 0);
}

INLINE( int64_t,VQDF_GET1) (Vqdf v, Rc(0,1) k)
{
#define     VQDF_GET1(A, B) vgetq_lane_f64(A,(1&B))
    return  k
    ?   vgetq_lane_f64(v, 1)
    :   vgetq_lane_f64(v, 0);
}

#if _LEAVE_ARM_GET1
}
#endif

#if _ENTER_ARM_BFC1
{
#endif

INLINE( uchar, UCHAR_BFC1) 
(
    uchar                   a, 
    Rc(0, UCHAR_WIDTH-1)    b, 
    Rc(1, UCHAR_WIDTH)      c
)
{
#define     UCHAR_BFC1(A, B, C) \
((uchar) (A&~((UCHAR_MAX>>(UCHAR_WIDTH-C))<<B)))

    return  UCHAR_BFC1(a, b, c);
}

INLINE( schar, SCHAR_BFC1)
(
    schar                   a, 
    Rc(0, SCHAR_WIDTH-1)    b, 
    Rc(1, SCHAR_WIDTH)      c
)
{
#define     SCHAR_BFC1(A, B, C) \
((schar) (A&~((UCHAR_MAX>>(SCHAR_WIDTH-C))<<B)))

    return  SCHAR_BFC1(a, b, c);
}

INLINE(  char,  CHAR_BFC1)
(
    char                   a, 
    Rc(0, CHAR_WIDTH-1)    b, 
    Rc(1, CHAR_WIDTH)      c
)
{
#define     CHAR_BFC1(A, B, C) \
((char) (A&~((UCHAR_MAX>>(CHAR_WIDTH-C))<<B)))

    return  CHAR_BFC1(a, b, c);
}


INLINE(ushort, USHRT_BFC1) 
(
    ushort                  a, 
    Rc(0, USHRT_WIDTH-1)    b, 
    Rc(1, USHRT_WIDTH)      c
)
{
#define     USHRT_BFC1(A, B, C) \
((ushort) (A&~((USHRT_MAX>>(USHRT_WIDTH-C))<<B)))

    return  USHRT_BFC1(a, b, c);
}

INLINE( short,  SHRT_BFC1)
(
    short                   a, 
    Rc(0, SHRT_WIDTH-1)     b, 
    Rc(1, SHRT_WIDTH)       c
)
{
#define     SHRT_BFC1(A, B, C) \
((short) (A&~((USHRT_MAX>>(SHRT_WIDTH-C))<<B)))

    return  SHRT_BFC1(a, b, c);
}


INLINE(  uint,  UINT_BFC1) 
(
    uint                   a, 
    Rc(0, UINT_WIDTH-1)    b, 
    Rc(1, UINT_WIDTH)      c
)
{
#define     UINT_BFC1(A, B, C) \
((uint) (A&~((UINT_MAX>>(UINT_WIDTH-C))<<B)))

    return  UINT_BFC1(a, b, c);
}

INLINE(   int,   INT_BFC1)
(
    int                    a, 
    Rc(0, INT_WIDTH-1)     b, 
    Rc(1, INT_WIDTH)       c
)
{
#define     INT_BFC1(A, B, C) \
((int) (A&~((UINT_MAX>>(INT_WIDTH-C))<<B)))

    return  INT_BFC1(a, b, c);
}


INLINE( ulong, ULONG_BFC1) 
(
    ulong                   a, 
    Rc(0, ULONG_WIDTH-1)    b, 
    Rc(1, ULONG_WIDTH)      c
)
{
#define     ULONG_BFC1(A, B, C) \
((ulong) (A&~((ULONG_MAX>>(ULONG_WIDTH-C))<<B)))

    return  ULONG_BFC1(a, b, c);
}

INLINE(  long,  LONG_BFC1)
(
    long                    a, 
    Rc(0, LONG_WIDTH-1)     b, 
    Rc(1, LONG_WIDTH)       c
)
{
#define     LONG_BFC1(A, B, C) \
((long) (A&~((ULONG_MAX>>(LONG_WIDTH-C))<<B)))

    return  LONG_BFC1(a, b, c);
}


INLINE( ullong, ULLONG_BFC1) 
(
    ullong                  a, 
    Rc(0, ULLONG_WIDTH-1)   b, 
    Rc(1, ULLONG_WIDTH)     c
)
{
#define     ULLONG_BFC1(A, B, C) \
((ullong) (A&~((ULLONG_MAX>>(ULLONG_WIDTH-C))<<B)))

    return  ULLONG_BFC1(a, b, c);
}

INLINE(  llong,  LLONG_BFC1)
(
    llong                   a, 
    Rc(0, LLONG_WIDTH-1)    b, 
    Rc(1, LLONG_WIDTH)      c
)
{
#define     LLONG_BFC1(A, B, C) \
((llong) (A&~((ULLONG_MAX>>(LLONG_WIDTH-C))<<B)))

    return  LLONG_BFC1(a, b, c);
}


/*  TODO: limit bfc1_yu to single bit clear
*/
INLINE(float,WYU_BFC1)
(
    float       src,
    Rc(0, 31)   off,
    Rc(1, 32)   len
)
{
#define WYU_BFC1(SRC, OFF, LEN)                     \
(                                                   \
    (LEN >= 32)                                     \
    ?   0.0f                                        \
    :   vget_lane_f32(                              \
            vreinterpret_f32_u32(                   \
                vbic_u32(                           \
                    vreinterpret_u32_f32(           \
                        vdup_n_f32(SRC)             \
                    ),                              \
                    vshl_n_u32(                     \
                        vshr_n_u32(                 \
                            vdup_n_u32(UINT32_MAX), \
                            (32-LEN*(LEN<32))       \
                        ),                          \
                        (31&OFF)                    \
                    )                               \
                )                                   \
            ),                                      \
            0                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(src);
    uint32x2_t  a = vreinterpret_u32_f32(m);
    uint32x2_t  b = vdup_n_u32(UINT32_MAX);
    int32x2_t   c = vdup_n_s32((32-len));
    b = vshl_u32(b, vneg_s32(c));
    b = vshl_u32(b, vdup_n_s32(off));
    a = vbic_u32(a, b);
    m = vreinterpret_f32_u32(a);
    return vget_lane_f32(m, 0);
}

INLINE(Vwyu,VWYU_BFC1)
(
    Vwyu        src, 
    Rc(0, 31)   off,
    Rc(1, 32)   len
)
{
#define     VWYU_BFC1(A, B, C) WYU_ASTV(WYU_BFC1(VWYU_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWYU_ASTM(src), off, len);
    return WYU_ASTV(r);
}


INLINE(Vwbu,VWBU_BFC1)
(
    Vwbu        src, 
    Rc(0, 31)   off,
    Rc(1, 8)    len
)
{
#define     VWBU_BFC1(A, B, C) WBU_ASTV(WYU_BFC1(VWBU_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWBU_ASTM(src), off, len);
    return  WBU_ASTV(r);
}

INLINE(Vwbi,VWBI_BFC1)
(
    Vwbi        src, 
    Rc(0, 31)   off,
    Rc(1, 8)    len
)
{
#define     VWBI_BFC1(A, B, C) WBI_ASTV(WYU_BFC1(VWBI_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWBI_ASTM(src), off, len);
    return  WBI_ASTV(r);
}

INLINE(Vwbc,VWBC_BFC1)
(
    Vwbc        src, 
    Rc(0, 31)   off,
    Rc(1, 8)    len
)
{
#define     VWBC_BFC1(A, B, C) WBC_ASTV(WYU_BFC1(VWBC_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWBC_ASTM(src), off, len);
    return  WBC_ASTV(r);
}


INLINE(Vwhu,VWHU_BFC1)
(
    Vwhu        src, 
    Rc(0, 31)   off,
    Rc(1, 16)   len
)
{
#define     VWHU_BFC1(A, B, C) WHU_ASTV(WYU_BFC1(VWHU_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWHU_ASTM(src), off, len);
    return  WHU_ASTV(r);
}

INLINE(Vwhi,VWHI_BFC1)
(
    Vwhi        src, 
    Rc(0, 31)   off,
    Rc(1, 16)   len
)
{
#define     VWHI_BFC1(A, B, C) WHI_ASTV(WYU_BFC1(VWHI_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWHI_ASTM(src), off, len);
    return  WHI_ASTV(r);
}


INLINE(Vwwu,VWWU_BFC1)
(
    Vwwu        src, 
    Rc(0, 31)   off,
    Rc(1, 32)   len
)
{
#define     VWWU_BFC1(A, B, C) WWU_ASTV(WYU_BFC1(VWWU_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWWU_ASTM(src), off, len);
    return  WWU_ASTV(r);
}

INLINE(Vwwi,VWWI_BFC1)
(
    Vwwi        src, 
    Rc(0, 31)   off,
    Rc(1, 32)   len
)
{
#define     VWWI_BFC1(A, B, C) WWI_ASTV(WYU_BFC1(VWWI_ASTM(A),B,C))
    float r = (WYU_BFC1)(VWWI_ASTM(src), off, len);
    return  WWI_ASTV(r);
}


INLINE(uint64x1_t, DDU_BFC1)
(
    uint64x1_t  src,
    Rc(0, 63)   off,
    Rc(1, 64)   len
)
{
#if 0
            vshl_n_u64(                             \
                vshr_n_u64(                         \
                    vdup_n_u64(UINT64_MAX),         \
                     (64-LEN*(LEN<64))              \
                ),                                  \
                OFF                                 \
            )                                       
#endif
#define DDU_BFC1(SRC, OFF, LEN)                     \
(                                                   \
    (LEN >= 64)                                     \
    ?   vdup_n_u64(0)                               \
    :   vbic_u64(                                   \
            SRC,                                    \
            vdup_n_u64(                             \
                ((UINT64_MAX>>(64-LEN))<<OFF)       \
            )                                       \
        )                                           \
)

    uint64x1_t  a = src;
    uint64x1_t  b = vdup_n_u64(UINT64_MAX);
    int64x1_t   c = vdup_n_s64((64-len));
    b = vshl_u64(b, vneg_s64(c));
    b = vshl_u64(b, vdup_n_s64(off));
    return  vbic_u64(a, b);
}


INLINE(Vdyu,VDYU_BFC1)
(
    Vdyu        src,
    Rc(0, 63)   off,
    Rc(1, 64)   len
)
{
#define     VDYU_BFC1(A, B, C) VDDU_ASYU(DDU_BFC1(VDYU_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDYU_ASDU(src), off, len);
    return  VDDU_ASYU(r);
}


INLINE(Vdbu,VDBU_BFC1)
(
    Vdbu        src,
    Rc(0, 63)   off,
    Rc(1,  8)   len
)
{
#define     VDBU_BFC1(A, B, C) VDDU_ASBU(DDU_BFC1(VDBU_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDBU_ASDU(src), off, len);
    return  VDDU_ASBU(r);
}

INLINE(Vdbi,VDBI_BFC1)
(
    Vdbi        src,
    Rc(0, 63)   off,
    Rc(1,  8)   len
)
{
#define     VDBI_BFC1(A, B, C) VDDU_ASBI(DDU_BFC1(VDBI_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDBI_ASDU(src), off, len);
    return  VDDU_ASBI(r);
}

INLINE(Vdbc,VDBC_BFC1)
(
    Vdbc        src,
    Rc(0, 63)   off,
    Rc(1,  8)   len
)
{
#define     VDBC_BFC1(A, B, C) VDDU_ASBC(DDU_BFC1(VDBC_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDBC_ASDU(src), off, len);
    return  VDDU_ASBC(r);
}


INLINE(Vdhu,VDHU_BFC1)
(
    Vdhu        src,
    Rc(0, 63)   off,
    Rc(1, 16)   len
)
{
#define     VDHU_BFC1(A, B, C) VDDU_ASHU(DDU_BFC1(VDHU_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDHU_ASDU(src), off, len);
    return  VDDU_ASHU(r);
}

INLINE(Vdhi,VDHI_BFC1)
(
    Vdhi        src,
    Rc(0, 63)   off,
    Rc(1, 16)   len
)
{
#define     VDHI_BFC1(A, B, C) VDDU_ASHI(DDU_BFC1(VDHI_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDHI_ASDU(src), off, len);
    return  VDDU_ASHI(r);
}


INLINE(Vdwu,VDWU_BFC1)
(
    Vdwu        src,
    Rc(0, 63)   off,
    Rc(1, 32)   len
)
{
#define     VDWU_BFC1(A, B, C) VDDU_ASWU(DDU_BFC1(VDWU_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDWU_ASDU(src), off, len);
    return  VDDU_ASWU(r);
}

INLINE(Vdwi,VDWI_BFC1)
(
    Vdwi        src,
    Rc(0, 63)   off,
    Rc(1, 32)   len
)
{
#define     VDWI_BFC1(A, B, C) VDDU_ASWI(DDU_BFC1(VDWI_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDWI_ASDU(src), off, len);
    return  VDDU_ASWI(r);
}


INLINE(Vddu,VDDU_BFC1)
(
    Vddu        src,
    Rc(0, 63)   off,
    Rc(1, 64)   len
)
{
#define     VDDU_BFC1(A, B, C) DDU_BFC1(A,B,C)
    return  (DDU_BFC1)(src, off, len);
}

INLINE(Vddi,VDDI_BFC1)
(
    Vddi        src,
    Rc(0, 63)   off,
    Rc(1, 64)   len
)
{
#define     VDDI_BFC1(A, B, C) VDDU_ASDI(DDU_BFC1(VDDI_ASDU(A),B,C))
    uint64x1_t  r = (DDU_BFC1)(VDDI_ASDU(src), off, len);
    return  VDDU_ASDI(r);
}


#if _LEAVE_ARM_BFC1
}
#endif

#if _ENTER_ARM_BFG1
{
#endif

INLINE( uchar, UCHAR_BFG1) 
(
    uchar                   a, 
    Rc(0, UCHAR_WIDTH-1)    b, 
    Rc(1, UCHAR_WIDTH)      c
)
{
#define     UCHAR_BFG1(A, B, C) \
((uchar)((((unsigned) A)>>B)&(UCHAR_MAX>>(UCHAR_WIDTH-C))))

    return (a>>b)&(UCHAR_MAX>>c);
}

INLINE( schar, SCHAR_BFG1)
(
    schar                   a, 
    Rc(0, SCHAR_WIDTH-1)    b, 
    Rc(1, SCHAR_WIDTH)      c
)
{
#define     MY_BFG1BI(SRC, OFF, LEN)    \
__extension__({                         \
    union {                             \
        signed I;                       \
        struct {signed:OFF,_:LEN&7;};   \
    } __bfg = {.I=SRC};                 \
    __bfg._;                            \
})

#define     SCHAR_BFG1(A, B, C) ((schar) MY_BFG1BI(A,B,C))
    if (a&(1<<b))   
        return ~(((~a)>>b)&(UCHAR_MAX>>(UCHAR_WIDTH-c)));
    else           
        return      (a>>b)&(UCHAR_MAX>>(UCHAR_WIDTH-c));
}

INLINE(  char,  CHAR_BFG1)
(
    char                   a, 
    Rc(0, CHAR_WIDTH-1)    b, 
    Rc(1, CHAR_WIDTH)      c
)
{
#if CHAR_MIN
#   define  CHAR_BFG1(A, B, C) ((char) MY_BFG1BI(A,B,C))
    return (SCHAR_BFG1)(a, b, c);
#else
#   define  CHAR_BFG1(A, B, C) \
((char)((((unsigned) A)>>B)&(UCHAR_MAX>>(8-C))))

    return (UCHAR_BFG1)(a, b, c);
#endif
}


INLINE(ushort, USHRT_BFG1) 
(
    ushort                  a, 
    Rc(0, USHRT_WIDTH-1)    b, 
    Rc(1, USHRT_WIDTH)      c
)
{
#define     USHRT_BFG1(A, B, C) \
((ushort)((((unsigned) A)>>B)&(USHRT_MAX>>(USHRT_WIDTH-C))))

    return (a>>b)&(USHRT_MAX>>c);
}

INLINE( short,  SHRT_BFG1)
(
    short                   a, 
    Rc(0, SHRT_WIDTH-1)     b, 
    Rc(1, SHRT_WIDTH)       c
)
{
#define     MY_BFG1HI(SRC, OFF, LEN)    \
__extension__({                         \
    union {                             \
        signed I;                       \
        struct {signed:OFF,_:LEN;};     \
    } __bfg = {.I=SRC};                 \
    __bfg._;                            \
})

#define     SHRT_BFG1(A, B, C) ((short) MY_BFG1HI(A,B,C))
    if (a&(1<<b))   
        return ~(((~a)>>b)&(USHRT_MAX>>(USHRT_WIDTH-c)));
    else            
        return      (a>>b)&(USHRT_MAX>>(USHRT_WIDTH-c));
}


INLINE(  uint,  UINT_BFG1) 
(
    uint                   a, 
    Rc(0, UINT_WIDTH-1)    b, 
    Rc(1, UINT_WIDTH)      c
)
{
#define     UINT_BFG1(A, B, C) \
((((unsigned) A)>>B)&(UINT_MAX>>(UINT_WIDTH-C)))

    return (a>>b)&(UINT_MAX>>c);
}

INLINE(   int,   INT_BFG1)
(
    int                    a, 
    Rc(0, INT_WIDTH-1)     b, 
    Rc(1, INT_WIDTH)       c
)
{
#define     MY_BFG1WI(SRC, OFF, LEN)    \
__extension__({                         \
    union {                             \
        signed I;                       \
        struct {signed:OFF,_:LEN&31;};  \
    } __bfg = {.I=SRC};                 \
    __bfg._;                            \
})

#define     INT_BFG1(A, B, C) ((int) MY_BFG1WI(A,B,C))
    if (a&(1<<b))   
        return ~(((~a)>>b)&(UINT_MAX>>(UINT_WIDTH-c)));
    else            
        return      (a>>b)&(UINT_MAX>>(UINT_WIDTH-c));
}


INLINE( ulong, ULONG_BFG1) 
(
    ulong                   a, 
    Rc(0, ULONG_WIDTH-1)    b, 
    Rc(1, ULONG_WIDTH)      c
)
{
#define     ULONG_BFG1(A, B, C) \
((((ulong) A)>>B)&(ULONG_MAX>>(ULONG_WIDTH-C)))

    return (a>>b)&(ULONG_MAX>>c);
}

INLINE(  long,  LONG_BFG1)
(
    long                   a, 
    Rc(0, LONG_WIDTH-1)     b, 
    Rc(1, LONG_WIDTH)       c
)
{
#define     MY_BFG1LI(SRC, OFF, LEN)    \
__extension__({                         \
    union {                             \
        long I;                         \
        struct {                        \
            long                        \
            : OFF,                      \
            _:LEN&(LONG_WIDTH-1);       \
        };                              \
    } __bfg = {SRC};                    \
    __bfg._;                            \
})

#define     LONG_BFG1(A, B, C) MY_BFG1LI(A,B,C)
    if (a&(1<<b))   
        return ~(((~a)>>b)&(ULONG_MAX>>(ULONG_WIDTH-c)));
    else            
        return      (a>>b)&(ULONG_MAX>>(ULONG_WIDTH-c));
}


INLINE(ullong,ULLONG_BFG1) 
(
    ullong                  a, 
    Rc(0, ULLONG_WIDTH-1)   b, 
    Rc(1, ULLONG_WIDTH)     c
)
{
#define     ULLONG_BFG1(A, B, C) \
((((ullong) A)>>B)&(ULLONG_MAX>>(ULLONG_WIDTH-C)))

    return (a>>b)&(ULLONG_MAX>>c);
}

INLINE( llong, LLONG_BFG1)
(
    llong                    a, 
    Rc(0, LLONG_WIDTH-1)     b, 
    Rc(1, LLONG_WIDTH)       c
)
{
#define     MY_BFG1LL(SRC, OFF, LEN)    \
__extension__({                         \
    union {                             \
        long long I;                    \
        struct {                        \
            long long                   \
            : OFF,                      \
            _:LEN&(LLONG_WIDTH-1);      \
        };                              \
    } __bfg = {SRC};                    \
    __bfg._;                            \
})

#define     LLONG_BFG1(A, B, C) MY_BFG1LL(A,B,C)
    if (a&(1<<b))   
        return ~(((~a)>>b)&(ULLONG_MAX>>(ULLONG_WIDTH-c)));
    else            
        return      (a>>b)&(ULLONG_MAX>>(ULLONG_WIDTH-c));
}


INLINE(Vwyu,VWYU_BFG1)
(
    Vwyu        src, 
    Rc(0, 31)   off,
    Rc(1, 32)   len
)
{
#define     WYU_BFG1(SRC, OFF, LEN)     \
__extension__({                         \
    union {                             \
        float   V;                      \
        struct {                        \
            uint32_t                    \
                    : OFF,              \
                _   : LEN;              \
        };                              \
    } __bfg = {SRC};                    \
    __bfg._;                            \
})

#define     WYI_BFG1(SRC, OFF, LEN)     \
__extension__({                         \
    union {                             \
        float   V;                      \
        struct {                        \
            int32_t                     \
                    : OFF,              \
                _   : LEN;              \
        };                              \
    } __bfg = {SRC};                    \
    __bfg._;                            \
})

    float       m = VWYU_ASTM(src);
    uint32_t    r = FLT_ASWU(m);
    r = (r>>off)&(UINT32_MAX>>(32-len));
    m = UINT_ASWF(r);
    return  WYU_ASTV(m);
}


INLINE(uint8_t,VWBU_BFG1)
(
    Vwbu        src, 
    Rc(0, 31)   off,
    Rc(1, 8)    len
)
{
#define     VWBU_BFG1(A, B, C) ((uint8_t) WYU_BFG1(VWBU_ASTM(A),B,C))
    float       m = VWBU_ASTM(src);
    uint32_t    r = FLT_ASTU(m);
    return  (r>>off)&(UINT8_MAX>>(8-len));
}

INLINE(int8_t,VWBI_BFG1)
(
    Vwbi        src, 
    Rc(0, 31)   off,
    Rc(1, 8)    len
)
{
#define     VWBI_BFG1(A, B, C) ((int8_t) WYI_BFG1(VWBI_ASTM(A),B,C))
    float       m = VWBI_ASTM(src);
    uint32_t    r = (FLT_ASTU(m)>>off)&(UINT8_MAX>>(8-len));
    if (r>>(len-1))
        return ~0u-~r;
    else            
        return      r;
}

INLINE(char,VWBC_BFG1)
(
    Vwbc        src, 
    Rc(0, 31)   off,
    Rc(1, 8)    len
)
{
#define     VWBC_BFG1(A, B, C) ((char) WYU_BFG1(VWBC_ASTM(A),B,C))
#if CHAR_MIN
    return (VWBI_BFG1)(VWBC_ASBI(src), off, len);
#else
    return (VWBU_BFG1)(VWBC_ASBU(src), off, len);
#endif
}


INLINE(uint16_t,VWHU_BFG1)
(
    Vwhu        src, 
    Rc(0, 31)   off,
    Rc(1, 16)   len
)
{
#define     VWHU_BFG1(A, B, C) ((uint16_t) WYU_BFG1(VWHU_ASTM(A),B,C))
    float       m = VWHU_ASTM(src);
    uint32_t    r = FLT_ASTU(m);
    return  (r>>off)&(UINT16_MAX>>(16-len));
}

INLINE(int16_t,VWHI_BFG1)
(
    Vwhi        src, 
    Rc(0, 31)   off,
    Rc(1, 16)    len
)
{
#define     VWHI_BFG1(A, B, C) ((int16_t) WYI_BFG1(VWHI_ASTM(A),B,C))
    float       m = VWHI_ASTM(src);
    uint32_t    r = (FLT_ASTU(m)>>off)&(UINT16_MAX>>(16-len));
    if (r>>(len-1))
        return ~0u-~r;
    else            
        return      r;
}


INLINE(uint32_t,VWWU_BFG1)
(
    Vwwu        src, 
    Rc(0, 31)   off,
    Rc(1, 32)   len
)
{
#define     VWWU_BFG1(A, B, C) ((uint32_t) WYU_BFG1(VWWU_ASTM(A),B,C))
    float       m = VWWU_ASTM(src);
    uint32_t    r = FLT_ASTU(m);
    return  (r>>off)&(UINT32_MAX>>(32-len));
}

INLINE(int32_t,VWWI_BFG1)
(
    Vwwi        src, 
    Rc(0, 31)   off,
    Rc(1, 32)    len
)
{
#define     VWWI_BFG1(A, B, C) ((int32_t) WYI_BFG1(VWWI_ASTM(A),B,C))
    float       m = VWWI_ASTM(src);
    uint32_t    r = (FLT_ASTU(m)>>off)&(UINT32_MAX>>(32-len));
    if (r>>(len-1))
        return ~0u-~r;
    else            
        return      r;
}


INLINE(uint64x1_t,DDU_BFG1) 
(
    uint64x1_t src, 
    Rc(0, 63)  off,
    Rc(1, 64)  len
)
{
#define     DDU_BFG1(SRC, OFF, LEN)     \
(                                       \
    (LEN>63) ? SRC :                    \
    OFF                                 \
    ?   vand_u64(                       \
            vshr_n_u64(                 \
                SRC,                    \
                (OFF+(OFF==0))          \
            ),                          \
            vshr_n_u64(                 \
                vdup_n_u64(UINT64_MAX), \
                ((64-LEN)+(OFF==0))     \
            )                           \
        )                               \
    :   vand_u64(                       \
            SRC,                        \
            vshr_n_u64(                 \
                vdup_n_u64(UINT64_MAX), \
                (64-(LEN*(LEN<64)))     \
            )                           \
        )                               \
)
    int64x1_t   shr = vdup_n_s64(off);
    uint64x1_t  rhs;
    src = vshl_u64(src, vneg_s64(shr));
    shr = vdup_n_s64(-64+len);
    rhs = vdup_n_u64(UINT64_MAX);
    rhs = vshl_u64(rhs, shr);
    return  vand_u64(shr, rhs);
}


INLINE(Vdyu,VDYU_BFG1) (Vdyu a, Rc(0, 63) b, Rc(1, 64) c)
{
#define     VDYU_BFG1(A, B, C) VDDU_ASYU(DDU_BFG1(VDYU_ASDU(A),B,C))
    uint64x1_t m = VDYU_ASDU(a);
    return  VDDU_ASYU( ((DDU_BFG1)(m, b, c)) );
}


INLINE(uint8_t,VDBU_BFG1) 
(
    Vdbu        src,
    Rc(0, 63)   off, 
    Rc(1, 8)    len
)
{
#define     VDBU_BFG1(A, B, C)      \
(                                   \
    (uint8_t)                       \
    vget_lane_u64(                  \
        DDU_BFG1(VDBU_ASDU(A),B,C)  \
        0                           \
    )                               \
)
    uint64x1_t  v = VDBU_ASDU(src);
    v = (DDU_BFG1)(v, off, len);
    return  vget_lane_u64(v, 0);
}

INLINE(int8_t,VDBI_BFG1) 
(
    Vdbi        src,
    Rc(0, 63)   off, 
    Rc(1, 8)    len
)
{

#define     VDBI_BFG1(SRC, OFF, LEN)        \
(                                           \
    (int8_t)                                \
    __extension__({                         \
        union {                             \
            int8x8_t V;                     \
            struct {                        \
                int64_t                     \
                : OFF,                      \
                _:LEN;                      \
            };                              \
        } __bfg = {SRC};                    \
        __bfg._;                            \
    })                                      \
)

    uint64x1_t  v = vreinterpret_u64_s8(src);
    uint64_t    m = vget_lane_u64(v, 0)>>off;
    uint64_t    r = m&(UINT8_MAX>>(8-len));
    if (r&(UINT64_C(1)<<(len-1)))
        return ~UINT64_C(0)-~r;
    else            
        return      r;
}

INLINE(char,VDBC_BFG1) (Vdbc a, Rc(0, 63) b, Rc(1, 8) c)
{
    uint64x1_t  v = VDBC_ASDU(a);

#if CHAR_MIN

#   define  VDBC_BFG1(SRC, OFF, LEN)        \
(                                           \
    (char)                                  \
    __extension__({                         \
        union {                             \
            int8x8_t V;                     \
            struct {                        \
                int64_t                     \
                : OFF,                      \
                _:LEN;                      \
            };                              \
        } __bfg = {VDBC_ASBI(SRC)};         \
        __bfg._;                            \
    })                                      \
)

    uint64_t    r = vget_lane_u64(v, 0);
    if (r&(UINT64_C(1)<<b))
        return ~(((~r)>>b)&(UINT8_MAX>>(8-c)));
    else            
        return      (r>>b)&(UINT8_MAX>>(8-c));

#else

#   define  VDBC_BFG1(SRC, OFF, LEN)        \
(                                           \
    (char)                                  \
    __extension__({                         \
        union {                             \
            Vdbc V;                         \
            struct {                        \
                uint64_t                    \
                : OFF,                      \
                _:LEN;                      \
            };                              \
        } __bfg = {SRC};                    \
        __bfg._;                            \
    })                                      \
)

    v = (DDU_BFG1)(v, b, c);
    return  vget_lane_u64(v, 0);

#endif

}


INLINE(uint16_t,VDHU_BFG1) (Vdhu a, Rc(0, 63) b, Rc(1, 16) c)
{
#define     VDHU_BFG1(A, B, C)      \
(                                   \
    (uint16_t)                      \
    vget_lane_u64(                  \
        DDU_BFG1(VDHU_ASDU(A),B,C)  \
        0                           \
    )                               \
)
    uint64x1_t  v = VDHU_ASDU(a);
    v = (DDU_BFG1)(v, b, c);
    return  vget_lane_u64(v, 0);
}

INLINE(int16_t,VDHI_BFG1) 
(
    Vdhi        src,
    Rc(0, 63)   off, 
    Rc(1, 16)   len
)
{

#define     VDHI_BFG1(SRC, OFF, LEN)        \
(                                           \
    (int16_t)                               \
    __extension__({                         \
        union {                             \
            int16x4_t V;                    \
            struct {                        \
                int64_t                     \
                : OFF,                      \
                _:LEN;                      \
            };                              \
        } __bfg = {SRC};                    \
        __bfg._;                            \
    })                                      \
)
    uint64x1_t  v = vreinterpret_u64_s16(src);
    uint64_t    m = vget_lane_u64(v, 0)>>off;
    uint64_t    r = m&(UINT16_MAX>>(16-len));
    if (r&(UINT64_C(1)<<(len-1)))
        return ~UINT64_C(0)-~r;
    else            
        return      r;
}


INLINE(uint32_t,VDWU_BFG1) (Vdwu a, Rc(0, 63) b, Rc(1, 32) c)
{
#define     VDWU_BFG1(A, B, C)      \
(                                   \
    (uint32_t)                      \
    vget_lane_u64(                  \
        DDU_BFG1(VDWU_ASDU(A),B,C)  \
        0                           \
    )                               \
)
    uint64x1_t  v = VDWU_ASDU(a);
    v = (DDU_BFG1)(v, b, c);
    return  vget_lane_u64(v, 0);
}

INLINE(int32_t,VDWI_BFG1) 
(
    Vdwi        src,
    Rc(0, 63)   off, 
    Rc(1, 32)   len
)
{

#define     VDWI_BFG1(SRC, OFF, LEN)        \
(                                           \
    (int32_t)                               \
    __extension__({                         \
        union {                             \
            int32x2_t V;                    \
            struct {                        \
                int64_t                     \
                    : OFF,                  \
                    _:LEN,                  \
                    :0;                     \
            };                              \
        } __bfg = {.V=SRC};                 \
        __bfg._;                            \
    })                                      \
)
    uint64x1_t  v = vreinterpret_u64_s32(src);
    uint64_t    m = vget_lane_u64(v, 0)>>off;
    uint64_t    r = m&(UINT32_MAX>>(32-len));
    if (r&(UINT64_C(1)<<(len-1)))
        return ~UINT64_C(0)-~r;
    else            
        return      r;
}


INLINE(uint64_t,VDDU_BFG1) (Vddu a, Rc(0, 63) b, Rc(1, 64) c)
{
#define     VDDU_BFG1(A, B, C)      \
(                                   \
    vget_lane_u64(                  \
        DDU_BFG1(VDDU_ASDU(A),B,C)  \
        0                           \
    )                               \
)
    a = (DDU_BFG1)(a, b, c);
    return  vget_lane_u64(a, 0);
}

INLINE(int64_t,VDDI_BFG1) 
(
    Vddi        src,
    Rc(0, 63)   off, 
    Rc(1, 64)   len
)
{

#define     VDDI_BFG1(SRC, OFF, LEN)    \
__extension__({                         \
    union {                             \
        int64x1_t V;                    \
        struct {                        \
            int64_t                     \
            : OFF,                      \
            _:LEN;                      \
        };                              \
    } __bfg = {SRC};                    \
    __bfg._;                            \
})

    uint64x1_t  v = vreinterpret_u64_s64(src);
    uint64_t    m = vget_lane_u64(v, 0)>>off;
    uint64_t    r = m&(UINT64_MAX>>(64-len));
    if (r&(UINT64_C(1)<<(len-1)))
        return ~UINT64_C(0)-~r;
    else            
        return      r;
}


INLINE(uint64x2_t,QDU_BFG1) 
(
    uint64x2_t  src, 
    Rc(0, 127)  off,
    Rc(1, 128)  len
)
{
#define     QDU_BFG1(SRC, OFF, LEN)     \
__extension__({                         \
    union {                             \
        uint64x2_t V;                   \
        struct {                        \
            QUAD_UTYPE                  \
                : OFF,                  \
                _:LEN&127,              \
                : 0;                    \
        };                              \
    } __a = {SRC};                      \
    union {                             \
        QUAD_UTYPE  U;                  \
        uint64x2_t  V;                  \
    } __b = {__a._};                    \
    __b.V;                              \
})
    union {
        uint64x2_t  V;
        QUAD_UTYPE  U;
    } r = {src};
    r.U >>= off;
    r.U &= (~((QUAD_UTYPE) 0))>>(128-len);
    return  r.V;
}

 
 
INLINE(Vqyu,VQYU_BFG1) 
(
    Vqyu        a,
    Rc(0, 127)  b,
    Rc(1, 128)  c
)
{
#define     VQYU_BFG1(A, B, C) VQDU_ASYU(QDU_BFG1(VQYU_ASDU(A),B,C))
    uint64x2_t m = VQYU_ASDU(a);
    return  VQDU_ASYU( ((QDU_BFG1)(m, b, c)) );
}


INLINE(uint8_t,VQBU_BFG1) 
(
    Vqbu        a,
    Rc(0, 127)  b,
    Rc(1, 8)    c
)
{
#define     QBU_BFG1(SRC, OFF, LEN) \
__extension__({                     \
    union {                         \
        Vqbu V;                     \
        struct {                    \
            unsigned __int128       \
            : OFF,                  \
            _:LEN;                  \
        };                          \
    } __bfg = {SRC};                \
    __bfg._;                        \
})
#define     VQBU_BFG1(A, B, C) ((uint8_t) QBU_BFG1(A,B,C))

    union {Vqbu V; unsigned __int128 U;} t = {a};
    t.U >>= b;
    t.U &= UINT8_MAX>>(8-c);
    return t.U;
}

INLINE(int8_t,VQBI_BFG1)
(
    Vqbi        a,
    Rc(0, 127)  b,
    Rc(1, 8)    c
)
{
#define QBI_BFG1(SRC, OFF, LEN)     \
__extension__({                     \
    union {                         \
        Vqbu V;                     \
        struct {                    \
            signed __int128         \
            : OFF,                  \
            _:LEN;                  \
        };                          \
    } __bfg = {SRC};                \
    __bfg._;                        \
})

#define     VQBI_BFG1(A, B, C)   ((int8_t) QBI_BFG1(A,B,C))

    union {Vqbi V; QUAD_UTYPE U;} t = {a};
    uint64_t r = t.U>>b;
    if (r&(UINT64_C(1)<<(c-1)))
        return ~(~r&(UINT8_MAX>>(8-c)));
    else            
        return    r&(UINT8_MAX>>(8-c));
}

INLINE(char,VQBC_BFG1) 
(
    Vqbc        a,
    Rc(0, 127)  b,
    Rc(1, 8)    c
)
{
#if CHAR_MIN
#   define  VQBC_BFG1(A, B, C) ((char) QBI_BFG1(VQBC_ASBI(A),B,C))
    union {Vqbc V; unsigned __int128 U;} t = {a};
    uint64_t r = t.U>>b;
    if (r&(UINT64_C(1)<<(c-1)))
        return ~(~r&(UINT8_MAX>>(8-c)));
    else            
        return    r&(UINT8_MAX>>(8-c));

#else
#   define  VQBC_BFG1(A, B, C) ((char) QBU_BFG1(VQBC_ASBU(A),B,C))
    union {Vqbc V; unsigned __int128 U;} t = {a};
    t.U >>= b;
    t.U &= UINT8_MAX>>(8-c);
    return t.U;

#endif
}


INLINE(uint16_t,VQHU_BFG1)
(
    Vqhu        src,
    Rc(0, 127)  off,
    Rc(1, 16)   len
)
{

#   define  VQHU_BFG1(SRC, OFF, LEN)    \
(                                       \
    (uint16_t)                          \
    __extension__({                     \
        union {                         \
            Vqhu V;                     \
            struct {                    \
                unsigned __int128       \
                : OFF,                  \
                _:LEN;                  \
            };                          \
        } __bfg = {SRC};                \
        __bfg._;                        \
    })                                  \
)

    union {Vqhu V; QUAD_UTYPE Q; uint64_t D;} t = {.V=src};
    t.Q >>= off;
    return t.D&(UINT16_MAX>>(16-len));
}

INLINE(int16_t,VQHI_BFG1)
(
    Vqhi        src,
    Rc(0, 127)  off,
    Rc(1, 16)   len
)
{

#   define  VQHI_BFG1(SRC, OFF, LEN)    \
(                                       \
    (int16_t)                           \
    __extension__({                     \
        union {                         \
            Vqhi V;                     \
            struct {                    \
                signed __int128         \
                : OFF,                  \
                _:LEN;                  \
            };                          \
        } __bfg = {.V=SRC};             \
        __bfg._;                        \
    })                                  \
)

    union {Vqhi V; QUAD_UTYPE Q; uint64_t D;} t = {.V=src};
    t.Q >>= off;
    if (t.D&(UINT64_C(1)<<(len-1)))
        return ~(~t.D&(UINT16_MAX>>(16-len)));
    else            
        return    t.D&(UINT16_MAX>>(16-len));
}


INLINE(uint32_t,VQWU_BFG1)
(
    Vqwu        src,
    Rc(0, 127)  off,
    Rc(1, 32)   len
)
{

#   define  VQWU_BFG1(SRC, OFF, LEN)    \
(                                       \
    (uint32_t)                          \
    __extension__({                     \
        union {                         \
            Vqwu V;                     \
            struct {                    \
                unsigned __int128       \
                : OFF,                  \
                _:LEN;                  \
            };                          \
        } __bfg = {SRC};                \
        __bfg._;                        \
    })                                  \
)

    union {Vqwu V; QUAD_UTYPE Q; uint64_t D;} t = {.V=src};
    t.Q >>= off;
    return t.D&(UINT32_MAX>>(32-len));
}

INLINE(int32_t,VQWI_BFG1)
(
    Vqwi        src,
    Rc(0, 127)  off,
    Rc(1, 32)   len
)
{

#   define  VQWI_BFG1(SRC, OFF, LEN)    \
(                                       \
    (int32_t)                           \
    __extension__({                     \
        union {                         \
            Vqwi V;                     \
            struct {                    \
                signed __int128         \
                : OFF,                  \
                _:LEN;                  \
            };                          \
        } __bfg = {.V=SRC};             \
        __bfg._;                        \
    })                                  \
)

    union {Vqwi V; QUAD_UTYPE Q; uint64_t D;} t = {.V=src};
    t.Q >>= off;
    if (t.D&(UINT64_C(1)<<(len-1)))
        return ~(~t.D&(UINT32_MAX>>(32-len)));
    else            
        return    t.D&(UINT32_MAX>>(32-len));
}


INLINE(uint64_t,VQDU_BFG1)
(
    Vqdu        src,
    Rc(0, 127)  off,
    Rc(1, 64)   len
)
{

#   define  VQDU_BFG1(SRC, OFF, LEN)    \
(                                       \
    (uint64_t)                          \
    __extension__({                     \
        union {                         \
            Vqdu V;                     \
            struct {                    \
                unsigned __int128       \
                : OFF,                  \
                _:LEN;                  \
            };                          \
        } __bfg = {SRC};                \
        __bfg._;                        \
    })                                  \
)

    union {Vqdu V; QUAD_UTYPE Q; uint64_t D;} t = {.V=src};
    t.Q >>= off;
    return t.D&(UINT64_MAX>>(64-len));
}

INLINE(int64_t,VQDI_BFG1)
(
    Vqdi        src,
    Rc(0, 127)  off,
    Rc(1, 64)   len
)
{

#   define  VQDI_BFG1(SRC, OFF, LEN)    \
(                                       \
    (int64_t)                           \
    __extension__({                     \
        union {                         \
            Vqdi V;                     \
            struct {                    \
                signed __int128         \
                : OFF,                  \
                _:LEN;                  \
            };                          \
        } __bfg = {.V=SRC};             \
        __bfg._;                        \
    })                                  \
)

    union {Vqdi V; QUAD_UTYPE Q; uint64_t D;} t = {.V=src};
    t.Q >>= off;
    if (t.D&(UINT64_C(1)<<(len-1)))
        return ~(~t.D&(UINT64_MAX>>(64-len)));
    else            
        return    t.D&(UINT64_MAX>>(64-len));
}

#if _LEAVE_ARM_BFG1
}
#endif

#if _ENTER_ARM_BFS1
{
#endif

#define MY_BFS1M(T, DST, OFF, MSK, SRC) \
(((T) DST)&(~(((T) MSK)<<OFF)))|((((T) SRC)&((T) MSK))<<OFF)

INLINE(uchar,UCHAR_BFS1)
(
    uchar                   dst, 
    Rc(0, UCHAR_WIDTH-1)    off,
    Rc(1, UCHAR_WIDTH)      len,
    uchar                   src
)
{
#define UCHAR_BFS1(DST, OFF, LEN, SRC)      \
(                                           \
    (uchar)                                 \
    MY_BFS1M(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (UCHAR_MAX>>(UCHAR_WIDTH-LEN)),     \
        SRC                                 \
    )                                       \
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = UCHAR_MAX>>(UCHAR_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}

INLINE(schar,SCHAR_BFS1)
(
    schar                   dst, 
    Rc(0, SCHAR_WIDTH-1)    off,
    Rc(1, SCHAR_WIDTH)      len,
    schar                   src
)
{
#define SCHAR_BFS1(DST, OFF, LEN, SRC)  \
(                                           \
    (schar)                                 \
    MY_BFS1M(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (UCHAR_MAX>>(SCHAR_WIDTH-LEN)),     \
        SRC                                 \
    )                                       \
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = UCHAR_MAX>>(SCHAR_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    s <<= off;  // shift SRC to OFF
    m <<= off;  // shift mask to OFF 
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}

INLINE(char,CHAR_BFS1)
(
    char                    dst, 
    Rc(0, CHAR_WIDTH-1)     off,
    Rc(1, CHAR_WIDTH)       len,
    char                    src
)
{
#define CHAR_BFS1(DST, OFF, LEN, SRC)       \
(                                           \
    (char)                                  \
    MY_BFS1M(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (UCHAR_MAX>>(CHAR_WIDTH-LEN)),      \
        SRC                                 \
    )                                       \
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = UCHAR_MAX>>(CHAR_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}


INLINE(ushort,USHRT_BFS1)
(
    ushort                  dst, 
    Rc(0, USHRT_WIDTH-1)    off,
    Rc(1, USHRT_WIDTH)      len,
    ushort                  src
)
{
#define     USHRT_BFS1(DST, OFF, LEN, SRC)  \
(                                           \
    (ushort)                                \
    MY_BFS1M(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (USHRT_MAX>>(USHRT_WIDTH-LEN)),     \
        SRC                                 \
    )                                       \
)
    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = USHRT_MAX>>(USHRT_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}

INLINE(short,SHRT_BFS1)
(
    short                   dst, 
    Rc(0, SHRT_WIDTH-1)     off,
    Rc(1, SHRT_WIDTH)       len,
    short                   src
)
{
#define     SHRT_BFS1(DST, OFF, LEN, SRC)   \
(                                           \
    (short)                                 \
    MY_BFS1M(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (USHRT_MAX>>(SHRT_WIDTH-LEN)),      \
        SRC                                 \
    )                                       \
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = USHRT_MAX>>(USHRT_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}


INLINE(uint,UINT_BFS1)
(
    uint                    dst, 
    Rc(0, UINT_WIDTH-1)     off,
    Rc(1, UINT_WIDTH)       len,
    uint                    src
)
{
#define     UINT_BFS1(DST, OFF, LEN, SRC)  \
(                                           \
    (uint)                                  \
    MY_BFS1M(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (UINT_MAX>>(UINT_WIDTH-LEN)),     \
        SRC                                 \
    )                                       \
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = UINT_MAX>>(UINT_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}

INLINE(int,INT_BFS1)
(
    int                     dst, 
    Rc(0, INT_WIDTH-1)      off,
    Rc(1, INT_WIDTH)        len,
    int                     src
)
{
#define     INT_BFS1(DST, OFF, LEN, SRC)    \
(                                           \
    (int)                                 \
    MY_BFS1M(                               \
        unsigned,                           \
        DST,                                \
        OFF,                                \
        (UINT_MAX>>(UINT_WIDTH-LEN)),     \
        SRC                                 \
    )                                       \
)

    unsigned  d = dst;
    unsigned  s = src;
    unsigned  m = UINT_MAX>>(UINT_WIDTH-len);
    s &= m;     // keep lower LEN bits of SRC
    m <<= off;  // shift mask to OFF 
    s <<= off;  // shift SRC to OFF
    d &= ~m;    // clear DST[OFF:OFF+LEN]
    d |= s;     // insert SRC
    return d;
}


INLINE(ulong,ULONG_BFS1)
(
    ulong                   dst, 
    Rc(0, ULONG_WIDTH-1)    off,
    Rc(1, ULONG_WIDTH)      len,
    ulong                   src
)
{
#define     ULONG_BFS1(DST, OFF, LEN, SRC)  \
(                                           \
    (ulong)                                 \
    MY_BFS1M(                               \
        ulong,                              \
        DST,                                \
        OFF,                                \
        (ULONG_MAX>>(ULONG_WIDTH-LEN)),     \
        SRC                                 \
    )                                       \
)

    ulong  d = dst;
    ulong  s = src;
    ulong  m = ULONG_MAX>>(ULONG_WIDTH-len);
    s &= m;
    m <<= off;
    s <<= off;
    d &= ~m;
    d |= s;
    return d;
}

INLINE(long,LONG_BFS1)
(
    long                    dst, 
    Rc(0, LONG_WIDTH-1)     off,
    Rc(1, LONG_WIDTH)       len,
    long                    src
)
{
#define     LONG_BFS1(DST, OFF, LEN, SRC)   \
(                                           \
    (ulong)                                 \
    MY_BFS1M(                               \
        ulong,                              \
        DST,                                \
        OFF,                                \
        (ULONG_MAX>>(LONG_WIDTH-LEN)),      \
        SRC                                 \
    )                                       \
)

    ulong  d = dst;
    ulong  s = src;
    ulong  m = ULONG_MAX>>(ULONG_WIDTH-len);
    s &= m;
    m <<= off;
    s <<= off;
    d &= ~m;
    d |= s;
    return d;
}


INLINE(ullong,ULLONG_BFS1)
(
    ullong                  dst, 
    Rc(0, ULLONG_WIDTH-1)   off,
    Rc(1, ULLONG_WIDTH)     len,
    ullong                  src
)
{
#define ULLONG_BFS1(DST, OFF, LEN, SRC)     \
(                                           \
    (ullong)                                \
    MY_BFS1M(                               \
        ullong,                             \
        DST,                                \
        OFF,                                \
        (ULLONG_MAX>>(ULLONG_WIDTH-LEN)),   \
        SRC                                 \
    )                                       \
)

    ullong  d = dst;
    ullong  s = src;
    ullong  m = ULLONG_MAX>>(ULLONG_WIDTH-len);
    s &= m;
    m <<= off;
    s <<= off;
    d &= ~m;
    d |= s;
    return d;
}

INLINE(llong,LLONG_BFS1)
(
    llong                   dst, 
    Rc(0, LLONG_WIDTH-1)    off,
    Rc(1, LLONG_WIDTH)      len,
    llong                   src
)
{
#define     LLONG_BFS1(DST, OFF, LEN, SRC)  \
(                                           \
    (llong)                                 \
    MY_BFS1M(                               \
        ullong,                             \
        DST,                                \
        OFF,                                \
        (ULLONG_MAX>>(LLONG_WIDTH-LEN)),    \
        SRC                                 \
    )                                       \
)


    ullong  d = dst;
    ullong  s = src;
    ullong  m = ULLONG_MAX>>(ULLONG_WIDTH-len);
    s &= m;
    m <<= off;
    s <<= off;
    d &= ~m;
    d |= s;
    return d;
}


INLINE(Vdbu,VDBU_BFS1)
(
    Vdbu        dst, 
    Rc(0, 63)   off,
    Rc(1, 8)    len,
    uint8_t     src
)
{
    uint64x1_t d = vreinterpret_u64_u8(dst);
    uint64x1_t s = vdup_n_u64(src);
    uint64x1_t m = vdup_n_u64(UINT8_MAX);
    int64x1_t  n = vdup_n_u64(8-len);
    m = vshl_u64(m, vneg_s64(n));
    s = vand_u64(m, s);
    n = vdup_n_s64(off);
    m = vshl_u64(m, n);
    s = vshl_u64(s, n);
    d = vbic_u64(d, m);
    d = vorr_u64(d, s);
    return vreinterpret_u8_u64(d);
}


#if _LEAVE_ARM_BFS1
}
#endif

#if _ENTER_ARM_ABSU
{
#endif

INLINE(uchar,SCHAR_ABSU)  (schar a)
{
    int b = a>>(SCHAR_WIDTH-1);
    return b^a+b;
}

INLINE(uchar, CHAR_ABSU)   (char a)
{
#if CHAR_MIN
    return  SCHAR_ASBU(a);
#else
    return  a;
#endif
}

INLINE(ushort,SHRT_ABSU)  (short a)
{
    int b = a>>(SHRT_WIDTH-1);
    return b^(a+b);
}

INLINE(  uint, INT_ABSU)    (int a)
{
    int b = a>>(INT_WIDTH-1);
    return b^(a+b);
}

INLINE( ulong, LONG_ABSU)  (long a)
{
    long b = a>>(LONG_WIDTH-1);
    return b^(a+b);
}

INLINE(ullong,LLONG_ABSU) (llong a)
{
    llong b = a>>(LLONG_WIDTH-1);
    return b^(a+b);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE, absuqi) (QUAD_ITYPE a)
{
    QUAD_ITYPE b = a>>127;
    return b^(a+b);
}

#endif

INLINE(uint16_t,FLT16_ABSU) (flt16_t a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvth_u16_f16(vabsh_f16(a));
#else
    uint32_t    z = vcvts_u32_f32((a <= -0.0f ? -a : a));
    return  vqmovns_u32(z);
#endif
}

INLINE(uint32_t,FLT_ABSU) (float a)
{
    return  vcvts_u32_f32((a <= -0.0f ? -a : a));
}

INLINE(uint64_t,DBL_ABSU) (double a)
{
    return  vcvtd_u64_f64((a <= -0.0 ? -a : a));
}

INLINE(Vwbu,VWBI_ABSU) (Vwbi a)
{
    float m = VWBI_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    int8x8_t    i = vreinterpret_s8_f32(f);
    int8x8_t    b = vshr_n_s8(i, 7);
    i = veor_s8(b, vadd_s8(b, i));
    f = vreinterpret_f32_s8(i);
    m = vget_lane_f32(f, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbu,VWBC_ABSU) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ABSU(VWBC_ASBI(a));
#else
    return  VWBC_ASBU(a);
#endif
}


INLINE(Vwhu,VWHI_ABSU) (Vwhi a)
{
    float m = VWHI_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    int16x4_t   i = vreinterpret_s16_f32(f);
    int16x4_t   b = vshr_n_s16(i, 15);
    i = veor_s16(b, vadd_s16(b, i));
    f = vreinterpret_f32_s16(i);
    m = vget_lane_f32(f, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhu,VWHF_ABSU) (Vwhf a)
{
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    f = vabs_f16(f);
    uint16x4_t  z = vcvt_u16_f16(f);
    v = vreinterpret_f32_u16(z);
    m = vget_lane_f32(v, 0);
    return WHU_ASTV(m);
#else
    float32x4_t r = vcvt_f32_f16(f);
    r = vabsq_f32(r);
    v = vget_low_f32(r);
    return VDWF_CVHZ(v);
#endif

}


INLINE(Vwwu,VWWI_ABSU) (Vwwi a)
{
    int64_t     z = vabsd_s64(VWWI_ASTV(a));
    return  UINT_ASTV(z);
}

INLINE(Vwwu,VWWF_ABSU) (Vwwf a)
{
    float f = VWWF_ASTM(a);
    f = f <= -0.0f ? -f : f;
    uint32_t z = vcvts_u32_f32(f);
    return WWU_ASTV(z);
}


INLINE(Vdbu,VDBI_ABSU) (Vdbi a)
{
    Vdbi b = vshr_n_s8(a, 7);
    return  vreinterpret_u8_s8(veor_s8(b, vadd_s8(a, b)));
}

INLINE(Vdbu,VDBC_ABSU) (Vdbc a)
{
#if CHAR_MIN
    return  VDBI_ABSU(VDBC_ASBI(a));
#else
    return  VDBC_ASBU(a);
#endif
}


INLINE(Vdhu,VDHI_ABSU) (Vdhi a)
{
    Vdhi b = vshr_n_s16(a, 15);
    return  vreinterpret_u16_s16(veor_s16(b, vadd_s16(a, b)));
}

INLINE(Vdhu,VDHF_ABSU) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvt_u16_f16(vabs_f16(a));
#else
    float32x4_t f = vcvt_f32_f16(a);
    f = vabsq_f32(f);
    return  VQWF_CVHZ(f);
#endif

}


INLINE(Vdwu,VDWI_ABSU) (Vdwi a)
{
    Vdwi b = vshr_n_s32(a, 31);
    return  vreinterpret_u32_s32(veor_s32(b, vadd_s32(a, b)));
}

INLINE(Vdwu,VDWF_ABSU) (Vdwf a)
{
    return  vcvt_u32_f32(vabs_f32(a));
}


INLINE(Vddu,VDDI_ABSU) (Vddi a)
{
    Vddi b = vshr_n_s64(a, 63);
    return  vreinterpret_u64_s64(veor_s64(b, vadd_s64(a, b)));
}

INLINE(Vddu,VDDF_ABSU) (Vddf a)
{
    return  vcvt_u64_f64(vabs_f64(a));
}


INLINE(Vqbu,VQBI_ABSU) (Vqbi a)
{
    Vqbi b = vshrq_n_s8(a, 7);
    return  vreinterpretq_u8_s8(veorq_s8(b, vaddq_s8(a, b)));
}

INLINE(Vqbu,VQBC_ABSU) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ABSU(VQBC_ASBI(a));
#else
    return  VQBC_ASBU(a);
#endif
}


INLINE(Vqhu,VQHI_ABSU) (Vqhi a)
{
    Vqhi b = vshrq_n_s16(a, 15);
    return  vreinterpretq_u16_s16(veorq_s16(b, vaddq_s16(a, b)));
}

INLINE(Vqhu,VQHF_ABSU) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtq_u16_f16(vabsq_f16(a));
#else
    return  vcombine_u16(
        VDHF_ABSU(vget_low_f16(a)),
        VDHF_ABSU(vget_high_f16(a))
    );
#endif

}


INLINE(Vqwu,VQWI_ABSU) (Vqwi a)
{
    Vqwi b = vshrq_n_s32(a, 31);
    return  vreinterpretq_u32_s32(veorq_s32(b, vaddq_s32(a, b)));
}

INLINE(Vqwu,VQWF_ABSU) (Vqwf a)
{
    return  vcvtq_u32_f32(vabsq_f32(a));
}


INLINE(Vqdu,VQDI_ABSU) (Vqdi a)
{
    Vqdi b = vshrq_n_s64(a, 63);
    return  vreinterpretq_u64_s64(veorq_s64(b, vaddq_s64(a, b)));
}

INLINE(Vqdu,VQDF_ABSU) (Vqdf a)
{
    return  vcvtq_u64_f64(vabsq_f64(a));
}


#if _LEAVE_ARM_ABSU
}
#endif

#if _ENTER_ARM_ABSS
{
#endif

INLINE(  schar,SCHAR_ABSS)   (schar x) {return vqabsb_s8(x);}
INLINE(   char, CHAR_ABSS)    (char x)
{
#if CHAR_MIN
    return  vqabsb_s8(x);
#else
    return  x;
#endif
}

INLINE(  short, SHRT_ABSS)   (short x) {return vqabsh_s16(x);}
INLINE(    int,  INT_ABSS)     (int x) {return vqabss_s32(x);}

INLINE(   long, LONG_ABSS)    (long x)
{
#if DWRD_NLONG == 2
    return vqabss_s32(x);
#else
    return vqabsd_s64(x);
#endif
}

INLINE(  llong,LLONG_ABSS)   (llong x)
{
#if QUAD_NLONG == 2
    return vqabsd_s64(x);
#else

#endif
}


INLINE(Vwbi,VWBI_ABSS) (Vwbi x)
{
    float32x2_t m = vdup_n_f32(VWBI_ASTM(x));
    int8x8_t    v = vreinterpret_s8_f32(m);
    v = vqabs_s8(v);
    m = vreinterpret_f32_s8(v);
    return  WBI_ASTV(vget_lane_f32(m,V2_K0));
}

INLINE(Vwbc,VWBC_ABSS) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ABSS(VWBC_ASBI(x)));
#else
    return  x;
#endif
}


INLINE(Vwhi,VWHI_ABSS) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   v = vreinterpret_s16_f32(m);
    v = vqabs_s16(v);
    m = vreinterpret_f32_s16(v);
    return  WHI_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vwwi,VWWI_ABSS) (Vwwi x)
{
    return  INT32_ASTV(vqabss_s32(VWWI_ASTV(x)));
}


INLINE(Vdbi,VDBI_ABSS) (Vdbi x) {return vqabs_s8(x);}
INLINE(Vdbc,VDBC_ABSS) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_ASBC(vqabs_s8(VDBC_ASBI(x)));
#else
    return  x;
#endif
}

INLINE(Vdhi,VDHI_ABSS) (Vdhi x) {return vqabs_s16(x);}
INLINE(Vdwi,VDWI_ABSS) (Vdwi x) {return vqabs_s32(x);}
INLINE(Vddi,VDDI_ABSS) (Vddi x) {return vqabs_s64(x);}

INLINE(Vqbi,VQBI_ABSS) (Vqbi x) {return vqabsq_s8(x);}
INLINE(Vqbc,VQBC_ABSS) (Vqbc x)
{
#if CHAR_MIN
    return  VQBI_ASBC(vqabsq_s8(VQBC_ASBI(x)));
#else
    return  x;
#endif
}

INLINE(Vqhi,VQHI_ABSS) (Vqhi x) {return vqabsq_s16(x);}
INLINE(Vqwi,VQWI_ABSS) (Vqwi x) {return vqabsq_s32(x);}
INLINE(Vqdi,VQDI_ABSS) (Vqdi x) {return vqabsq_s64(x);}

#if _LEAVE_ARM_ABSS
}
#endif

#if _ENTER_ARM_ABSL
{
#endif

INLINE(schar, SCHAR_ABSL) (schar a) {return vabsd_s64(a);}
INLINE(schar,  CHAR_ABSL)  (char a) {return vabsd_s64(a);}
INLINE(short,  SHRT_ABSL) (short a) {return vabsd_s64(a);}
INLINE(  int,   INT_ABSL)   (int a) {return vabsd_s64(a);}
INLINE( long,  LONG_ABSL)  (long a) {return vabsd_s64(a);}
INLINE(llong, LLONG_ABSL) (llong a) {return vabsd_s64(a);}

#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE, abslqi) (QUAD_ITYPE a)
{
    return a < 0 ? -a : a;
}

#endif

INLINE(Vwbi,VWBI_ABSL) (Vwbi a)
{
    float       m = VWBI_ASTM(a);
    float32x2_t d = vdup_n_f32(m);
    int8x8_t    v = vreinterpret_s8_f32(d);
    v = vabs_s8(v);
    d = vreinterpret_f32_s8(v);
    m = vget_lane_f32(d, 0);
    return  WBI_ASTV(m);
}

INLINE(Vwbi,VWBC_ABSL) (Vwbc a) 
{
#if CHAR_MIN
    return  VWBI_ABSL(VWBC_ASBI(a));
#else
    return  VWBC_ASBI(a);
#endif
}

INLINE(Vwhi,VWHI_ABSL) (Vwhi a)
{
    float       m = VWHI_ASTM(a);
    float32x2_t d = vdup_n_f32(m);
    int16x4_t   v = vreinterpret_s16_f32(d);
    v = vabs_s16(v);
    d = vreinterpret_f32_s16(v);
    m = vget_lane_f32(d, 0);
    return  WHI_ASTV(m);
}

INLINE(Vwwi,VWWI_ABSL) (Vwwi a)
{
    float       m = VWWI_ASTM(a);
    float32x2_t d = vdup_n_f32(m);
    int16x4_t   v = vreinterpret_s32_f32(d);
    v = vabs_s32(v);
    d = vreinterpret_f32_s32(v);
    m = vget_lane_f32(d, 0);
    return  WWI_ASTV(m);
}

INLINE(Vdbi,VDBI_ABSL) (Vdbi a) {return vabs_s8(a);}
INLINE(Vdbi,VDBC_ABSL) (Vdbc a) {return vabs_s8(VDBC_ASBI(a));}
INLINE(Vdhi,VDHI_ABSL) (Vdhi a) {return vabs_s16(a);}
INLINE(Vdwi,VDWI_ABSL) (Vdwi a) {return vabs_s32(a);}
INLINE(Vddi,VDDI_ABSL) (Vddi a) {return vabs_s64(a);}

INLINE(Vqbi,VQBI_ABSL) (Vqbi a) {return vabsq_s8(a);}
INLINE(Vqbi,VQBC_ABSL) (Vqbc a) {return vabsq_s8(VQBC_ASBI(a));}
INLINE(Vqhi,VQHI_ABSL) (Vqhi a) {return vabsq_s16(a);}
INLINE(Vqwi,VQWI_ABSL) (Vqwi a) {return vabsq_s32(a);}
INLINE(Vqdi,VQDI_ABSL) (Vqdi a) {return vabsq_s64(a);}

#if _LEAVE_ARM_ABSL
}
#endif

#if _ENTER_ARM_ABSH
{
#endif

INLINE(flt16_t,SCHAR_ABSH)  (schar a) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabsh_f16(a);
#else
    float32x2_t f = vdup_n_f32(a);
    f = vabs_f32(f);
    return  vget_lane_f32(f, 0);
#endif
}

INLINE(flt16_t, CHAR_ABSH)   (char a)
{
#if CHAR_MIN
    return  SCHAR_ASBH(a);
#else
    return  a;
#endif
}


INLINE(flt16_t,SHRT_ABSH)  (short a)
{
    float32x2_t f = vdup_n_f32(a);
    f = vabs_f32(f);
    return  vget_lane_f32(f, 0);
}

INLINE(flt16_t, INT_ABSH)    (int a)
{
    float64x1_t f = vdup_n_f64(a);
    f = vabs_f64(f);
    return  vget_lane_f64(f, 0);
}

INLINE(flt16_t, LONG_ABSH)  (long a)
{
    float64x1_t f = vdup_n_f64(a);
    f = vabs_f64(f);
    return  vget_lane_f64(f, 0);
}

INLINE(flt16_t,LLONG_ABSH) (llong a)
{
    float64x1_t f = vdup_n_f64(a);
    f = vabs_f64(f);
    return  vget_lane_f64(f, 0);
}

#if QUAD_NLLONG == 2

INLINE(flt16_t, abshqi) (QUAD_ITYPE a)
{
    float64x1_t f = vdup_n_f64(((int64_t) a));
    f = vabs_f64(f);
    return  vget_lane_f64(f, 0);
}

#endif

INLINE(flt16_t,FLT16_ABSH) (flt16_t a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabsh_f16(a);
#else
    float32x2_t v = vdup_n_f32(a);
    v = vabs_f32(v);
    return vget_lane_f32(v, 0);
#endif
}

INLINE(flt16_t,FLT_ABSH) (float a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabsh_f16(a);
#else
    float32x2_t v = vdup_n_f32(a);
    v = vabs_f32(v);
    return vget_lane_f32(v, 0);
#endif
}

INLINE(flt16_t,DBL_ABSH) (double a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabsh_f16(a);
#else
    float64x1_t v = vdup_n_f64(a);
    v = vabs_f64(v);
    return  vget_lane_f64(v, 0);
#endif
}

INLINE(Vdhf,VWBI_ABSH) (Vwbi a)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t m = VWBI_CVHF(a);
    return  vabs_f16(m);
#else
    float32x4_t m = VWBI_CVWF(a);
    m = vabsq_f32(m);
    return  vcvt_f16_f32(m);
#endif
}

INLINE(Vdhf,VWBC_ABSH) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ABSH(VWBC_ASBI(a));
#else
    return  VWBC_CVHF(a);
#endif
}


INLINE(Vwhf,VWHI_ABSH) (Vwhi a)
{
    float       m = VWHI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int16x4_t   z = vreinterpret_s16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t c = vcvt_f16_s16(z);
    c = vabs_f16(c);
#else
    int32x2_t   u = vget_low_s32(vmovl_s16(z));
    float32x2_t f = vcvt_f32_s32(u);
    f = vabs_f32(f);
    float32x4_t q = vcombine_f32(f, f);
    float16x4_t c = vcvt_f16_f32(q);
#endif
    v = vreinterpret_f32_f16(c);
    m = vget_lane_f32(v, 0);
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_ABSH) (Vwhf a)
{
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t z = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t c = vabs_f16(c);
#else
    float32x4_t f = vcvt_f32_f16(z);
    f = vabsq_f32(f);
    float16x4_t c = vcvt_f16_f32(f);
#endif

    v = vreinterpret_f32_f16(c);
    m = vget_lane_f32(v, 0);
    return  WHF_ASTV(m);
}


INLINE(Vqhf,VDBI_ABSH) (Vdbi a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vabsq_f16(VDBI_CVHF(a));
#else
    int16x8_t   m = vmovl_s8(a);
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(vget_low_s16(m)));
    float32x4_t r = vcvtq_f32_s32(vmovl_s16(vget_high_s16(m)));
    return  vcombine_f16(
        vcvt_f16_f32(vabsq_f32(l)),
        vcvt_f16_f32(vabsq_f32(r))
    );
#endif
}

INLINE(Vqhf,VDBC_ABSH) (Vdbc a)
{
#if CHAR_MIN
    return  VDBI_ABSH(VDBC_ASBI(a));
#else
    return  VDBC_CVHF(a);
#endif
}


INLINE(Vdhf,VDHI_ABSH) (Vdhi a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabs_f16(vcvt_f16_s16(a));
#else
    int32x4_t   z = vmovl_s16(a);
    float32x4_t f = vcvtq_f32_s32(z);
    f = vabsq_f32(f);
    return  vcvt_f16_f32(f);
#endif
}

INLINE(Vdhf,VDHF_ABSH) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabs_f16(a);
#else
    float32x4_t f = vcvt_f32_f16(a);
    f = vabsq_f32(f);
    return  vcvt_f16_f32(f);
#endif
}

INLINE(Vwhf,VDWI_ABSH) (Vdwi a)
{
    float32x2_t     w = vcvt_f32_s32(a);
    w = vabs_f32(w);
    return  VDWF_CVHF(w);
}

INLINE(Vwhf,VDWF_ABSH) (Vdwf a)
{
    a = vabs_f32(a);
    return  VDWF_CVHF(a);
}

INLINE(Vqhf,VQHI_ABSH) (Vqhi a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vabsq_f16(vcvtq_f16_s16(a));
#else
    return vcombine_f16(
        VDHI_ABSH(vget_low_s16(a)),
        VDHI_ABSH(vget_high_s16(a))
    );
#endif
}

INLINE(Vqhf,VQHF_ABSH) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vabsq_f16(a);
#else
    return vcombine_f16(
        VDHF_ABSH(vget_low_f16(a)),
        VDHF_ABSH(vget_high_f16(a))
    );
#endif
}


INLINE(Vdhf,VQWI_ABSH) (Vqwi a)
{
    float32x4_t f = vcvtq_f32_s32(a);
    f = vabsq_f32(f);
    return vcvt_f16_f32(f);
}

INLINE(Vdhf,VQWF_ABSH) (Vqwf a)
{
    a = vabsq_f32(a);
    return vcvt_f16_f32(a);
}

INLINE(Vwhf,VQDI_ABSH) (Vqdi a)
{
    float64x2_t f = vcvtq_f64_s64(a);
    f = vabsq_f64(f);
    return VQDF_CVHF(f);
}

INLINE(Vwhf,VQDF_ABSH) (Vqdf a)
{
    a = vabsq_f64(a);
    return VQDF_CVHF(a);
}

#if _LEAVE_ARM_ABSH
}
#endif

#if _ENTER_ARM_ABSW
{
#endif

INLINE(float,SCHAR_ABSW)  (schar a) 
{
    return a < 0 ? 0.0f-a : a;
}

INLINE(float, CHAR_ABSW)   (char a)
{
#if CHAR_MIN
    return  SCHAR_ASBH(a);
#else
    return  a;
#endif
}


INLINE(float,SHRT_ABSW)  (short a)
{
    return  a < 0 ? 0.0f-a : a;
}

INLINE(float, INT_ABSW)    (int a)
{
    return  a < 0 ? 0.0f-a : a;
}

INLINE(float, LONG_ABSW)  (long a)
{
    return  a < 0l ? 0.0f-a : a;
}

INLINE(float,LLONG_ABSW) (llong a)
{
    return  a < 0ll ? 0.0f-a : a;
}

#if QUAD_NLLONG == 2

INLINE(float, abswqi) (QUAD_ITYPE a)
{
    return  a < 0ll ? 0.0f-a : a;
}

#endif

INLINE(float,FLT16_ABSW) (flt16_t a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabsh_f16(a);
#else
    return a <= -0.0f16 ? 0.0f-a : a;
#endif
}

INLINE(float,FLT_ABSW) (float a)
{
    return a <= -0.0f ? -a : a;
}

INLINE(float,DBL_ABSW) (double a)
{
    float64x1_t v = vdup_n_f64(a);
    v = vabs_f64(v);
    return  vget_lane_f64(v, 0);
}


INLINE(Vqwf,VWBI_ABSW) (Vwbi a)
{
    return vabsq_f32(VWBI_CVWF(a));
}

INLINE(Vqwf,VWBC_ABSW) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ABSW(VWBC_ASBI(a));
#else
    return  VWBC_CVWF(a);
#endif
}


INLINE(Vdwf,VWHI_ABSW) (Vwhi a)
{
    return  vabs_f32(VWHI_CVWF(a));
}

INLINE(Vdwf,VWHF_ABSW) (Vwhf a)
{
    return  vabs_f32(VWHF_CVWF(a));
}


INLINE(Vwwf,VWWI_ABSW) (Vwwi a)
{
    float   m = VWWI_ASTM(a);
    int     r = FLT_ASTI(m);
    m = r < 0 ? 0.0f-r : r;
    return  WWF_ASTV(m);
}

INLINE(Vwwf,VWWF_ABSW) (Vwwf a)
{
    float   m = VWWF_ASTM(a);
    m = m <= -0.0f ? -m : m;
    return  WWF_ASTV(m);
}


INLINE(Vqwf,VDHI_ABSW) (Vdhi a)
{
    return  vabsq_f32(VDHI_CVWF(a));
}

INLINE(Vqwf,VDHF_ABSW) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    a = vabs_f16(a);
    return  vcvt_f32_f16(a);
#else
    return vabsq_f32(vcvt_f32_f16(a));
#endif

}


INLINE(Vdwf,VDWI_ABSW) (Vdwi a)
{
    return vabs_f32(vcvt_f32_s32(a));
}

INLINE(Vdwf,VDWF_ABSW) (Vdwf a)
{
    return vabs_f32(a);
}


INLINE(Vwwf,VDDI_ABSW) (Vddi a)
{
    float64x1_t f = vcvt_f64_s64(a);
    f = vabs_f64(f);
    float m = vget_lane_f64(f, 0);
    return  WWF_ASTV(m);
}

INLINE(Vwwf,VDDF_ABSW) (Vddf a)
{
    a = vabs_f64(a);
    float m = vget_lane_f64(a, 0);
    return  WWF_ASTV(m);
}


INLINE(Vqwf,VQWI_ABSW) (Vqwi a)
{
    return vabsq_f32(vcvtq_f32_s32(a));
}

INLINE(Vqwf,VQWF_ABSW) (Vqwf a)
{
    return vabsq_f32(a);
}


INLINE(Vdwf,VQDI_ABSW) (Vqdi a)
{
    float64x2_t q = vcvtq_f64_s64(a);
    float32x2_t f = vcvt_f32_f64(q);
    return vabs_f32(f);
}

INLINE(Vdwf,VQDF_ABSW) (Vqdf a)
{
    float32x2_t f = vcvt_f32_f64(a);
    return vabs_f32(f);
}

#if _LEAVE_ARM_ABSW
}
#endif

#if _ENTER_ARM_ABSD
{
#endif

INLINE(double,SCHAR_ABSD)  (schar a) 
{
    return a < 0 ? 0.0-a : a;
}

INLINE(double, CHAR_ABSD)   (char a)
{
#if CHAR_MIN
    return  SCHAR_ASBH(a);
#else
    return  a;
#endif
}


INLINE(double,SHRT_ABSD)  (short a)
{
    return  a < 0 ? 0.0-a : a;
}

INLINE(double, INT_ABSD)    (int a)
{
    return  a < 0 ? 0.0-a : a;
}

INLINE(double, LONG_ABSD)  (long a)
{
    return  a < 0l ? 0.0-a : a;
}

INLINE(double,LLONG_ABSD) (llong a)
{
    return  a < 0ll ? 0.0-a : a;
}

#if QUAD_NLLONG == 2

INLINE(double, absdqi) (QUAD_ITYPE a)
{
    return  a < 0ll ? 0.0-a : a;
}

#endif

INLINE(double,FLT16_ABSD) (flt16_t a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabsh_f16(a);
#else
    return a <= -0.0f16 ? 0.0-a : a;
#endif
}

INLINE(double,FLT_ABSD) (float a)
{
    return a <= -0.0f ? -a : a;
}

INLINE(double,DBL_ABSD) (double a)
{
    float64x1_t v = vdup_n_f64(a);
    v = vabs_f64(v);
    return  vget_lane_f64(v, 0);
}



INLINE(Vqdf,VWHI_ABSD) (Vwhi a)
{
    return  vabsq_f64(VWHI_CVDF(a));
}

INLINE(Vqdf,VWHF_ABSD) (Vwhf a)
{
    return  vabsq_f64(VWHF_CVDF(a));
}


INLINE(Vddf,VWWI_ABSD) (Vwwi a)
{
    float       m = VWWI_ASTM(a);
    int64x1_t   z = vdup_n_s64(FLT_ASTI(m));
    float64x1_t f = vcvt_f64_s64(z);
    return  vabs_f64(f);
}

INLINE(Vddf,VWWF_ABSD) (Vwwf a)
{
    float   m = VWWF_ASTM(a);
    float64x1_t f = vdup_n_f32(m);
    return  vabs_f64(f);
}



INLINE(Vqdf,VDWI_ABSD) (Vdwi a)
{
    return vabsq_f64(VDWI_CVDF(a));
}

INLINE(Vqdf,VDWF_ABSD) (Vdwf a)
{
    return vabsq_f64(vcvt_f64_f32(a));
}


INLINE(Vddf,VDDI_ABSD) (Vddi a)
{
    float64x1_t f = vcvt_f64_s64(a);
    return vabs_f64(f);
}

INLINE(Vddf,VDDF_ABSD) (Vddf a)
{
    return vabs_f64(a);
}



INLINE(Vqdf,VQDI_ABSD) (Vqdi a)
{
    float64x2_t q = vcvtq_f64_s64(a);
    return  vabsq_f64(q);
}

INLINE(Vqdf,VQDF_ABSD) (Vqdf a)
{
    return  vabsq_f64(a);
}

#if _LEAVE_ARM_ABSD
}
#endif

#if _ENTER_ARM_NEGL
{
#endif

INLINE(  uchar, UCHAR_NEGL)   (uchar x) {return -x;}
INLINE(  schar, SCHAR_NEGL)   (schar x) {return -x;}
INLINE(   char,  CHAR_NEGL)    (char x) {return -x;}
INLINE( ushort, USHRT_NEGL)  (ushort x) {return -x;}
INLINE(  short,  SHRT_NEGL)   (short x) {return -x;}
INLINE(   uint,  UINT_NEGL)    (uint x) {return -x;}
INLINE(    int,   INT_NEGL)     (int x) {return -x;}
INLINE(  ulong, ULONG_NEGL)   (ulong x) {return -x;}
INLINE(   long,  LONG_NEGL)    (long x) {return -x;}
INLINE( ullong,ULLONG_NEGL)  (ullong x) {return -x;}
INLINE(  llong, LLONG_NEGL)   (llong x) {return -x;}
INLINE(flt16_t, FLT16_NEGL) (flt16_t x) {return -x;}
INLINE(  float,   FLT_NEGL)   (float x) {return -x;}
INLINE( double,   DBL_NEGL)  (double x) {return -x;}

#define     WBI_NEGL(X)     \
vget_lane_f32(vneg_s8(VDWF_ASBI(vdup_n_f32(X))),  V2_K0)


#define     WHI_NEGL(X)     \
vget_lane_f32(vneg_s16(VDWF_ASHI(vdup_n_f32(X))), V2_K0)

#define     WHF_NEGL(X) (0.0f)


#define     WWI_NEGL(X)     \
vget_lane_f32(vneg_s32(VDWF_ASWI(vdup_n_f32(X))), V2_K0)

#define     WWF_NEGL(X) (-X)


INLINE(Vwbu,VWBU_NEGL) (Vwbu x)
{
    return  WBU_ASTV(WBI_NEGL(VWBU_ASTM(x)));
}

INLINE(Vwbi,VWBI_NEGL) (Vwbi x)
{
    return  WBI_ASTV(WBI_NEGL(VWBI_ASTM(x)));
}

INLINE(Vwbc,VWBC_NEGL) (Vwbc x)
{
    return  WBC_ASTV(WBI_NEGL(VWBC_ASTM(x)));
}


INLINE(Vwhu,VWHU_NEGL) (Vwhu x)
{
    return  WHU_ASTV(WHI_NEGL(VWHU_ASTM(x)));
}

INLINE(Vwhi,VWHI_NEGL) (Vwhi x)
{
    return  WHI_ASTV(WHI_NEGL(VWHI_ASTM(x)));
}

INLINE(Vwhf,VWHF_NEGL) (Vwhf x)
{
    return  WHF_ASTV(0.0f);
}


INLINE(Vwwu,VWWU_NEGL) (Vwwu x)
{
    return  WWU_ASTV(WWI_NEGL(VWWU_ASTM(x)));
}

INLINE(Vwwi,VWWI_NEGL) (Vwwi x)
{
    return  WWI_ASTV(WWI_NEGL(VWWI_ASTM(x)));
}

INLINE(Vwwf,VWWF_NEGL) (Vwwf x)
{
    return  WWF_ASTV(WWF_NEGL(VWWF_ASTM(x)));
}


INLINE(Vdbu,VDBU_NEGL) (Vdbu x)
{
    return  vreinterpret_u8_s8(
        vneg_s8(
            vreinterpret_s8_u8(x)
        )
    );
}

INLINE(Vdbi,VDBI_NEGL) (Vdbi x) {return vneg_s8(x);}

INLINE(Vdbc,VDBC_NEGL) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_NEGL(VDBC_ASBI(x)));
#else
    return  VDBU_ASBC(VDBU_NEGL(VDBC_ASBU(x)));
#endif
}

INLINE(Vdhu,VDHU_NEGL) (Vdhu x)
{
    return  vreinterpret_u16_s16(
        vneg_s16(
            vreinterpret_s16_u16(x)
        )
    );
}

INLINE(Vdhi,VDHI_NEGL) (Vdhi x) {return vneg_s16(x);}
INLINE(Vdhf,VDHF_NEGL) (Vdhf x) {return VDHF_VOID;}


INLINE(Vdwu,VDWU_NEGL) (Vdwu x)
{
    return  vreinterpret_u32_s32(
        vneg_s32(
            vreinterpret_s32_u32(x)
        )
    );
}

INLINE(Vdwi,VDWI_NEGL) (Vdwi x) {return vneg_s32(x);}
INLINE(Vdwf,VDWF_NEGL) (Vdwf x) {return vneg_f32(x);}


INLINE(Vddu,VDDU_NEGL) (Vddu x)
{
    return  vreinterpret_u64_s64(
        vneg_s64(
            vreinterpret_s64_u64(x)
        )
    );
}

INLINE(Vddi,VDDI_NEGL) (Vddi x) {return vneg_s64(x);}
INLINE(Vddf,VDDF_NEGL) (Vddf x) {return vneg_f64(x);}


INLINE(Vqbu,VQBU_NEGL) (Vqbu x)
{
    return  vreinterpretq_u8_s8(
        vnegq_s8(
            vreinterpretq_s8_u8(x)
        )
    );
}

INLINE(Vqbi,VQBI_NEGL) (Vqbi x) {return vnegq_s8(x);}

INLINE(Vqbc,VQBC_NEGL) (Vqbc x)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_NEGL(VQBC_ASBI(x)));
#else
    return  VQBU_ASBC(VQBU_NEGL(VQBC_ASBU(x)));
#endif
}

INLINE(Vqhu,VQHU_NEGL) (Vqhu x)
{
    return  vreinterpretq_u16_s16(
        vnegq_s16(
            vreinterpretq_s16_u16(x)
        )
    );
}

INLINE(Vqhi,VQHI_NEGL) (Vqhi x) {return vnegq_s16(x);}
INLINE(Vqhf,VQHF_NEGL) (Vqhf x) {return VQHF_VOID;}


INLINE(Vqwu,VQWU_NEGL) (Vqwu x)
{
    return  vreinterpretq_u32_s32(
        vnegq_s32(
            vreinterpretq_s32_u32(x)
        )
    );
}

INLINE(Vqwi,VQWI_NEGL) (Vqwi x) {return vnegq_s32(x);}
INLINE(Vqwf,VQWF_NEGL) (Vqwf x) {return vnegq_f32(x);}


INLINE(Vqdu,VQDU_NEGL) (Vqdu x)
{
    return  vreinterpretq_u64_s64(
        vnegq_s64(
            vreinterpretq_s64_u64(x)
        )
    );
}

INLINE(Vqdi,VQDI_NEGL) (Vqdi x) {return vnegq_s64(x);}
INLINE(Vqdf,VQDF_NEGL) (Vqdf x) {return vnegq_f64(x);}



#if _LEAVE_ARM_NEGL
}
#endif

#if _ENTER_ARM_NEGS
{
#endif

INLINE(schar, UCHAR_NEGS)  (uchar a)
{
    return  a > SCHAR_MAX ? SCHAR_MIN : ((schar)(-a));
}

INLINE(schar, SCHAR_NEGS)  (schar a) {return vqnegb_s8(a);}

INLINE(schar,  CHAR_NEGS)   (char a) 
{
#if CHAR_MIN
    return  vqnegb_s8(a);
#else
    return  a > SCHAR_MAX ? SCHAR_MIN : ((schar)(-a));
#endif
}

INLINE(short, USHRT_NEGS) (ushort a)
{
    return a > SHRT_MAX ? SHRT_MIN : ((short)(-a));
}

INLINE(short,  SHRT_NEGS)  (short a) {return vqnegh_s16(a);}

INLINE(  int,  UINT_NEGS)   (uint a)
{
    return a > INT_MAX ? INT_MIN : ((int)(-a));
}

INLINE(  int,   INT_NEGS)    (int a) {return vqnegs_s32(a);}

INLINE( long, ULONG_NEGS)  (ulong a)
{
    return a > LONG_MAX ? LONG_MIN : ((long)(-a));
}

INLINE( long,  LONG_NEGS)   (long a) 
{
#if DWRD_NLONG == 2
    return  vqnegs_s32(a);
#else
    return  vqnegd_s64(a);
#endif
}


INLINE(llong,ULLONG_NEGS) (ullong a)
{
    return a > LLONG_MAX ? LLONG_MIN : ((llong)(-a));
}

INLINE(llong, LLONG_NEGS)  (llong a) 
{
#if QUAD_NLLONG == 2
    return  vqnegd_s64(a);
#else

#endif

}


#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE,negsqu) (QUAD_UTYPE a)
{
    if (a>>127)
    {
        return (QUAD_ITYPE) ((QUAD_UTYPE) 1<<127);
    }
    return  -(QUAD_ITYPE) a;
}

INLINE(QUAD_ITYPE,negsqi) (QUAD_ITYPE a) 
{
    union {
        QUAD_ITYPE I;
        struct {uint64_t Lo, Hi;};
    } v = {a};
    if ((v.Hi>>63) && (!v.Lo && !(v.Hi&0x7fffffffffffffffull)))
    {
        v.Lo = UINT64_MAX;
        v.Hi =  INT64_MAX;
    }
    else 
    {
        v.I = -v.I;
    }
    return v.I;
}

#endif

#define     WBI_NEGS(X)     \
vget_lane_f32(vqneg_s8(vreinterpret_s8_f32(vdup_n_f32(X))),0)

#if CHAR_MIN

#   define  WBC_NEGS(X) \
vget_lane_f32(vqneg_s8(vreinterpret_s8_f32(vdup_n_f32(X))),0)

#   define  DBC_NEGS(X) vqneg_s8(X)
#   define  QBC_NEGS(X) vqnegq_s8(X)

#else

#   define  WBC_NEGS(X) 
#   define  DBC_NEGS(X) vdup_n_u8(0)
#   define  QBC_NEGS(X) vdupq_n_u8(0)

#endif

#define     WHI_NEGS(X)     \
vget_lane_f32(vqneg_s16(vreinterpret_s16_f32(vdup_n_f32(X))),0)


#define     WWI_NEGS(X)     \
vget_lane_f32(vqneg_s32(vreinterpret_s32_f32(vdup_n_f32(X))),0)


INLINE(Vwbi,VWBU_NEGS) (Vwbu a) 
{
    float       f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  u = vmovl_u8(d);
    int16x8_t   i = vreinterpretq_s16_u16(u);
    int8x8_t    b = vqmovn_s16(vnegq_s16(i));
    m = vreinterpret_f32_s8(b);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbi,VWBI_NEGS) (Vwbi a) {return WBI_ASTV(WBI_NEGS(VWBI_ASTM(a)));}

INLINE(Vwbi,VWBC_NEGS) (Vwbc a) 
{
#if CHAR_MIN
    return  VWBI_NEGS(VWBC_ASBI(a));
#else
    return  VWBU_NEGS(VWBC_ASBU(a));
#endif
}


INLINE(Vwhi,VWHU_NEGS) (Vwhu a) 
{
    float       f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  d = vreinterpret_u16_f32(m);
    uint32x4_t  u = vmovl_u16(d);
    int32x4_t   i = vreinterpretq_s32_u32(u);
    int16x4_t   b = vqmovn_s32(vnegq_s32(i));
    m = vreinterpret_f32_s16(b);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHI_NEGS) (Vwhi a) {return WHI_ASTV(WHI_NEGS(VWHI_ASTM(a)));}


INLINE(Vwwi,VWWU_NEGS) (Vwwu a) 
{
    float       f = VWWU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  d = vreinterpret_u32_f32(m);
    uint64x2_t  u = vmovl_u32(d);
    int64x2_t   i = vreinterpretq_s64_u64(u);
    int32x2_t   b = vqmovn_s64(vnegq_s64(i));
    m = vreinterpret_f32_s32(b);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWI_NEGS) (Vwwi a) {return WWI_ASTV(WWI_NEGS(VWWI_ASTM(a)));}


INLINE(Vdbi,VDBU_NEGS) (Vdbu a)
{
    uint16x8_t  u = vmovl_u8(a);
    int16x8_t   i = vreinterpretq_s16_u16(u);
    i = vnegq_s16(i);
    return vqmovn_s16(i);
}

INLINE(Vdbi,VDBI_NEGS) (Vdbi a) {return vqneg_s8(a);}

INLINE(Vdbi,VDBC_NEGS) (Vdbc a) 
{
#if CHAR_MIN
    return  VDBI_NEGS(VDBC_ASBI(a));
#else
    return  VDBU_NEGS(VDBC_ASBU(a));
#endif
}


INLINE(Vdhi,VDHU_NEGS) (Vdhu a)
{
    uint32x4_t  u = vmovl_u16(a);
    int32x4_t   i = vreinterpretq_s32_u32(u);
    i = vnegq_s32(i);
    return  vqmovn_s32(i);
}

INLINE(Vdhi,VDHI_NEGS) (Vdhi a) {return vqneg_s16(a);}


INLINE(Vdwi,VDWU_NEGS) (Vdwu a)
{
    uint64x2_t  u = vmovl_u32(a);
    int64x2_t   i = vreinterpretq_s64_u64(u);
    i = vnegq_s64(i);
    return  vqmovn_s64(i);
}

INLINE(Vdwi,VDWI_NEGS) (Vdwi a) {return vqneg_s32(a);}


INLINE(Vddi,VDDU_NEGS) (Vddu a) 
{
    uint64_t b = vget_lane_u64(a, 0);
    return  (b>>63) 
    ?   vdup_n_s64(INT64_MIN)
    :   vneg_s64(vreinterpret_s64_u64(a));
}

INLINE(Vddi,VDDI_NEGS) (Vddi a) {return vqneg_s64(a);}



INLINE(Vqbi,VQBU_NEGS) (Vqbu a) 
{
    return vcombine_s8(
        VDBU_NEGS(vget_low_u8(a)),
        VDBU_NEGS(vget_high_u8(a))
    );
}

INLINE(Vqbi,VQBI_NEGS) (Vqbi a) {return vqnegq_s8(a);}

INLINE(Vqbi,VQBC_NEGS) (Vqbc a) 
{
#if CHAR_MIN
    return  VQBI_NEGS(VQBC_ASBI(a));
#else
    return  VQBU_NEGS(VQBC_ASBU(a));
#endif
}


INLINE(Vqhi,VQHU_NEGS) (Vqhu a) 
{
    return vcombine_s16(
        VDHU_NEGS(vget_low_u16(a)),
        VDHU_NEGS(vget_high_u16(a))
    );
}

INLINE(Vqhi,VQHI_NEGS) (Vqhi a) {return vqnegq_s16(a);}

INLINE(Vqwi,VQWU_NEGS) (Vqwu a) 
{
    return vcombine_s32(
        VDWU_NEGS(vget_low_u32(a)),
        VDWU_NEGS(vget_high_u32(a))
    );
}

INLINE(Vqwi,VQWI_NEGS) (Vqwi a) {return vqnegq_s32(a);}


INLINE(Vqdi,VQDU_NEGS) (Vqdu a) 
{
    return vcombine_s64(
        VDDU_NEGS(vget_low_u64(a)),
        VDDU_NEGS(vget_high_u64(a))
    );
}

INLINE(Vqdi,VQDI_NEGS) (Vqdi a) {return vqnegq_s64(a);}

#if _LEAVE_ARM_NEGS
}
#endif

#if _ENTER_ARM_NEGH
{
#endif

INLINE(flt16_t,  BOOL_NEGH)  (_Bool x) {return 0.0f16-x;}

INLINE(flt16_t, UCHAR_NEGH)  (uchar x) {return 0.0f16-x;}
INLINE(flt16_t, SCHAR_NEGH)  (schar x) {return 0.0f16-x;}
INLINE(flt16_t,  CHAR_NEGH)   (char x) {return 0.0f16-x;}

INLINE(flt16_t, USHRT_NEGH) (ushort x) {return 0.0f16-x;}
INLINE(flt16_t,  SHRT_NEGH)  (short x) {return 0.0f16-x;}

INLINE(flt16_t,  UINT_NEGH)   (uint x) {return 0.0f16-x;}
INLINE(flt16_t,   INT_NEGH)    (int x) {return 0.0f16-x;}

INLINE(flt16_t, ULONG_NEGH)  (ulong x) {return 0.0f16-x;}
INLINE(flt16_t,  LONG_NEGH)   (long x) {return 0.0f16-x;}

INLINE(flt16_t,ULLONG_NEGH) (ullong x) {return 0.0f16-x;}
INLINE(flt16_t, LLONG_NEGH)  (llong x) {return 0.0f16-x;}

#if QUAD_NLLONG == 2
INLINE(flt16_t,neghqu) (QUAD_UTYPE x) {return 0.0f16-x;}
INLINE(flt16_t,neghqi) (QUAD_ITYPE x) {return 0.0f16-x;}
INLINE(flt16_t,neghqf) (QUAD_FTYPE x) {return -x;}
#else
INLINE(flt16_t,LDBL_NEGH) (long double x) {return -x;}
#endif

INLINE(flt16_t, FLT16_NEGH) (flt16_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegh_f16(x);
#else
    return  0.0f-x;
#endif
}

INLINE(flt16_t, FLT_NEGH) (float x) {return  -x;}

INLINE(flt16_t, DBL_NEGH) (double x) {return  -x;}


INLINE(Vdhf,VWBU_NEGH) (Vwbu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vneg_f16(VWBU_CVHF(x));
#else
    float32x4_t f = VWBU_CVWF(x);
    f = vnegq_f32(f);
    return  vcvt_f16_f32(f);
#endif
}

INLINE(Vdhf,VWBI_NEGH) (Vwbi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vneg_f16(VWBI_CVHF(x));
#else
    float32x4_t f = VWBI_CVWF(x);
    f = vnegq_f32(f);
    return  vcvt_f16_f32(f);
#endif
}

INLINE(Vdhf,VWBC_NEGH) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_NEGH(VWBC_ASBI(x));
#else
    return  VWBU_NEGH(VWBC_ASBU(x));
#endif
}


INLINE(Vwhf,VWHU_NEGH) (Vwhu x)
{
    float       m = VWHU_ASTM(x);
    float32x2_t f = vdup_n_f32(m);
    uint16x4_t  z = vreinterpret_u16_f32(f);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t r = vcvt_f16_u16(z);
    r = vneg_f16(r);
#else
    float32x4_t y = vcvtq_f32_u32(vmovl_u16(z));
    y = vnegq_f32(y);
    float16x4_t r = vcvt_f16_f32(y);
#endif
    f = vreinterpret_f32_f16(r);
    m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHI_NEGH) (Vwhi x)
{
    float           m = VWHI_ASTM(x);
    float32x2_t     f = vdup_n_f32(m);
    int16x4_t       z = vreinterpret_s16_f32(f);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t     r = vcvt_f16_s16(z);
    r = vneg_f16(r);
#else
    float32x4_t     y = vcvtq_f32_s32(vmovl_s16(z));
    y = vnegq_f32(y);
    float16x4_t     r = vcvt_f16_f32(y);
#endif
    f = vreinterpret_f32_f16(r);
    m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_NEGH) (Vwhf x)
{
    float           m = VWHF_ASTM(x);
    float32x2_t     f = vdup_n_f32(m);
    float16x4_t     z = vreinterpret_f16_f32(f);
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t     r = vneg_f16(z);
#else
    float32x4_t     y = vcvt_f32_f16(z);
    y = vnegq_f32(y);
    float16x4_t     r = vcvt_f16_f32(y);
#endif
    f = vreinterpret_f32_f16(r);
    m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}


INLINE(Vqhf,VDBU_NEGH) (Vdbu x)
{
    uint16x8_t  m = vmovl_u8(x);
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegq_f16(vcvtq_f16_u16(m));
#else
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(vget_low_u16(m)));
    float32x4_t r = vcvtq_f32_u32(vmovl_u16(vget_high_u16(m)));
    return  vcombine_f16(
        vcvt_f16_f32(vnegq_f32(l)),
        vcvt_f16_f32(vnegq_f32(r))
    );
#endif

}

INLINE(Vqhf,VDBI_NEGH) (Vdbi x)
{
    int16x8_t   m = vmovl_s8(x);
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegq_f16(vcvtq_f16_s16(m));
#else
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(vget_low_s16(m)));
    float32x4_t r = vcvtq_f32_s32(vmovl_s16(vget_high_s16(m)));
    return  vcombine_f16(
        vcvt_f16_f32(vnegq_f32(l)),
        vcvt_f16_f32(vnegq_f32(r))
    );
#endif

}

INLINE(Vqhf,VDBC_NEGH) (Vdbc x)
{
#if CHAR_MIN
    return  VDBI_NEGH(VDBC_ASBI(x));
#else
    return  VDBU_NEGH(VDBC_ASBU(x));
#endif
}


INLINE(Vdhf,VDHU_NEGH) (Vdhu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vneg_f16(vcvt_f16_u16(x));
#else
    uint32x4_t  z = vmovl_u16(x);
    float32x4_t f = vcvtq_f32_u32(z);
    f = vnegq_f32(f);
    return  vcvt_f16_f32(f);
#endif

}

INLINE(Vdhf,VDHI_NEGH) (Vdhi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vneg_f16(vcvt_f16_s16(x));
#else
    int32x4_t  z = vmovl_s16(x);
    float32x4_t f = vcvtq_f32_s32(z);
    f = vnegq_f32(f);
    return  vcvt_f16_f32(f);
#endif

}

INLINE(Vdhf,VDHF_NEGH) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vneg_f16(x);
#else
    float32x4_t f = vcvtq_f32_s32(vcvt_f32_f16(x));
    f = vnegq_f32(f);
    return  vcvt_f16_f32(f);
#endif

}


INLINE(Vwhf,VDWU_NEGH) (Vdwu x)
{
    float32x2_t     f = vcvt_f32_u32(x);
    f = vneg_f32(f);
    float32x4_t     c = vcombine_f32(f, f);
    float16x4_t     r = vcvt_f16_f32(c);
    f = vreinterpret_f16_f32(f);
    float m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VDWI_NEGH) (Vdwi x)
{
    float32x2_t     f = vcvt_f32_s32(x);
    f = vneg_f32(f);
    float32x4_t     c = vcombine_f32(f, f);
    float16x4_t     r = vcvt_f16_f32(c);
    f = vreinterpret_f16_f32(f);
    float m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VDWF_NEGH) (Vdwf x)
{
    float32x2_t f = vneg_f32(x);
    float32x4_t     c = vcombine_f32(f, f);
    float16x4_t     r = vcvt_f16_f32(c);
    f = vreinterpret_f16_f32(f);
    float m = vget_lane_f32(f, 0);
    return  WHF_ASTV(m);
}


INLINE(Vqhf,VQHU_NEGH) (Vqhu x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegq_f16(vcvtq_f16_u16(x));
#else
    uint32x4_t  lz = vmovl_u16(vget_low_u16(x));
    uint32x4_t  rz = vmovl_u16(vget_high_u16(x));
    float32x4_t lf = vcvtq_f32_u32(lz);
    float32x4_t rf = vcvtq_f32_u32(rz);
    lf = vnegq_f32(lf);
    rf = vnegq_f32(rf);
    return  vcombine_f16(
        vcvt_f16_f32(lf),
        vcvt_f16_f32(rf)
    );
#endif
}

INLINE(Vqhf,VQHI_NEGH) (Vqhi x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegq_f16(vcvtq_f16_s16(x));
#else
    int32x4_t   lz = vmovl_s16(vget_low_s16(x));
    int32x4_t   rz = vmovl_s16(vget_high_s16(x));
    float32x4_t lf = vcvtq_f32_s32(lz);
    float32x4_t rf = vcvtq_f32_s32(rz);
    lf = vnegq_f32(lf);
    rf = vnegq_f32(rf);
    return  vcombine_f16(
        vcvt_f16_f32(lf),
        vcvt_f16_f32(rf)
    );
#endif
}

INLINE(Vqhf,VQHF_NEGH) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegq_f16(x);
#else
    float16x4_t     lr = vget_low_f16(x);
    float16x4_t     rr = vget_high_f16(x);
    float32x4_t     lf = vcvt_f32_f16(lr);
    float32x4_t     rf = vcvt_f32_f16(rr);
    lf = vnegq_f32(lf);
    rf = vnegq_f32(rf);
    return  vcombine_f16(
        vcvt_f16_f32(lf),
        vcvt_f16_f32(rf)
    );
#endif
}


INLINE(Vdhf,VQWU_NEGH) (Vqwu x)
{
    float32x4_t f = vcvtq_f32_u32(x);
    f = vnegq_f32(f);
    return vcvt_f16_f32(f);
}

INLINE(Vdhf,VQWI_NEGH) (Vqwi x)
{
    float32x4_t f = vcvtq_f32_s32(x);
    f = vnegq_f32(f);
    return vcvt_f16_f32(f);
}

INLINE(Vdhf,VQWF_NEGH) (Vqwf x)
{
    x = vnegq_f32(x);
    return  vcvt_f16_f32(x);
}


INLINE(Vwhf,VQDU_NEGH) (Vqdu x)
{
    float64x2_t d = vcvtq_f64_u64(x);
    d = vnegq_f64(d);
    float32x2_t w = vcvt_f32_f64(d);
    float16x4_t h = vcvt_f16_f32(vcombine_f32(w, w));
    w = vreinterpret_f32_f16(h);
    float m = vget_lane_f32(w, 0);
    return WHF_ASTV(m);
}

INLINE(Vwhf,VQDI_NEGH) (Vqdi x)
{
    float64x2_t d = vcvtq_f64_s64(x);
    d = vnegq_f64(d);
    float32x2_t w = vcvt_f32_f64(d);
    float16x4_t h = vcvt_f16_f32(vcombine_f32(w, w));
    w = vreinterpret_f32_f16(h);
    float m = vget_lane_f32(w, 0);
    return WHF_ASTV(m);
}

INLINE(Vwhf,VQDF_NEGH) (Vqdf x)
{
    x = vnegq_f64(x);
    float32x2_t w = vcvt_f32_f64(x);
    float16x4_t h = vcvt_f16_f32(vcombine_f32(w, w));
    w = vreinterpret_f32_f16(h);
    float m = vget_lane_f32(w, 0);
    return  WHF_ASTV(m);
}


#if _LEAVE_ARM_NEGH
}
#endif

#if _ENTER_ARM_NEGW
{
#endif

INLINE(float,  BOOL_NEGW)  (_Bool x) {return 0.0f-x;}

INLINE(float, UCHAR_NEGW)  (uchar x) {return 0.0f-x;}
INLINE(float, SCHAR_NEGW)  (schar x) {return 0.0f-x;}
INLINE(float,  CHAR_NEGW)   (char x) {return 0.0f-x;}

INLINE(float, USHRT_NEGW) (ushort x) {return 0.0f-x;}
INLINE(float,  SHRT_NEGW)  (short x) {return 0.0f-x;}

INLINE(float,  UINT_NEGW)   (uint x) {return 0.0f-x;}
INLINE(float,   INT_NEGW)    (int x) {return 0.0f-x;}

INLINE(float, ULONG_NEGW)  (ulong x) {return 0.0f-x;}
INLINE(float,  LONG_NEGW)   (long x) {return 0.0f-x;}

INLINE(float,ULLONG_NEGW) (ullong x) {return 0.0f-x;}
INLINE(float, LLONG_NEGW)  (llong x) {return 0.0f-x;}

#if QUAD_NLLONG == 2
INLINE(float,negwqu) (QUAD_UTYPE x) {return 0.0f-x;}
INLINE(float,negwqi) (QUAD_ITYPE x) {return 0.0f-x;}
INLINE(float,negwqf) (QUAD_FTYPE x) {return -x;}
#else
INLINE(float,LDBL_NEGW) (long double x) {return -x;}
#endif

INLINE(float, FLT16_NEGW) (flt16_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegh_f16(x);
#else
    return  0.0f-x;
#endif
}

INLINE(float, FLT_NEGW) (float x) {return  -x;}

INLINE(float, DBL_NEGW) (double x) {return  -x;}


INLINE(Vqwf,VWBU_NEGW) (Vwbu x)
{
    return  vnegq_f32(VWBU_CVWF(x));
}

INLINE(Vqwf,VWBI_NEGW) (Vwbi x)
{
    return  vnegq_f32(VWBI_CVWF(x));
}

INLINE(Vqwf,VWBC_NEGW) (Vwbc x)
{
#if CHAR_MIN
    return  VWBI_NEGW(VWBC_ASBI(x));
#else
    return  VWBU_NEGW(VWBC_ASBU(x));
#endif
}


INLINE(Vdwf,VWHU_NEGW) (Vwhu x)
{
    return  vneg_f32(VWHU_CVWF(x));
}

INLINE(Vdwf,VWHI_NEGW) (Vwhi x)
{
    return  vneg_f32(VWHI_CVWF(x));
}

INLINE(Vdwf,VWHF_NEGW) (Vwhf x)
{
    return  vneg_f32(VWHF_CVWF(x));
}


INLINE(Vwwf,VWWU_NEGW) (Vwwu x)
{
    float m = VWWU_ASTM(x);
    m = FLT_ASWU(m);
    return WWF_ASTV((-m));
}

INLINE(Vwwf,VWWI_NEGW) (Vwwi x)
{
    float m = VWWI_ASTM(x);
    m = FLT_ASWI(m);
    return WWF_ASTV((-m));
}

INLINE(Vwwf,VWWF_NEGW) (Vwwf x)
{
    return  WWF_ASTV((-VWWF_ASTM(x)));
}



INLINE(Vqwf,VDHU_NEGW) (Vdhu x)
{
    return  vnegq_f32(VDHU_CVWF(x));
}

INLINE(Vqwf,VDHI_NEGW) (Vdhi x)
{
    return  vnegq_f32(VDHI_CVWF(x));
}

INLINE(Vqwf,VDHF_NEGW) (Vdhf x)
{
    return  vnegq_f32(VDHF_CVWF(x));
}


INLINE(Vdwf,VDWU_NEGW) (Vdwu x)
{
    return  vneg_f32(vcvt_f32_u32(x));
}

INLINE(Vdwf,VDWI_NEGW) (Vdwi x)
{
    return  vneg_f32(vcvt_f32_s32(x));
}

INLINE(Vdwf,VDWF_NEGW) (Vdwf x)
{
    return  vneg_f32(x);
}


INLINE(Vwwf,VDDU_NEGW) (Vddu x)
{
    return  WWF_ASTV((0.0f-vget_lane_u64(x, 0)));
}

INLINE(Vwwf,VDDI_NEGW) (Vddi x)
{
    return  WWF_ASTV((0.0f-vget_lane_u64(x, 0)));
}

INLINE(Vwwf,VDDF_NEGW) (Vddf x)
{
    x = vneg_f64(x);
    return  WWF_ASTV((vget_lane_f64(x, 0)));
}



INLINE(Vqwf,VQWU_NEGW) (Vqwu x)
{
    return  vnegq_f32(vcvtq_f32_u32(x));
}

INLINE(Vqwf,VQWI_NEGW) (Vqwi x)
{
    return  vnegq_f32(vcvtq_f32_s32(x));
}

INLINE(Vqwf,VQWF_NEGW) (Vqwf x)
{
    return  vnegq_f32(x);
}


INLINE(Vdwf,VQDU_NEGW) (Vqdu x)
{
    float64x2_t f = vcvtq_f64_u64(x);
    float32x2_t m = vcvt_f32_f64(f);
    return  vneg_f32(m);
}

INLINE(Vdwf,VQDI_NEGW) (Vqdi x)
{
    float64x2_t f = vcvtq_f64_s64(x);
    float32x2_t m = vcvt_f32_f64(f);
    return  vneg_f32(m);
}

INLINE(Vdwf,VQDF_NEGW) (Vqdf x)
{
    float32x2_t m = vcvt_f32_f64(x);
    return  vneg_f32(m);
}

#if _LEAVE_ARM_NEGW
}
#endif

#if _ENTER_ARM_NEGD
{
#endif

INLINE(double,  BOOL_NEGD)  (_Bool x) {return 0.0-x;}

INLINE(double, UCHAR_NEGD)  (uchar x) {return 0.0-x;}
INLINE(double, SCHAR_NEGD)  (schar x) {return 0.0-x;}
INLINE(double,  CHAR_NEGD)   (char x) {return 0.0-x;}

INLINE(double, USHRT_NEGD) (ushort x) {return 0.0-x;}
INLINE(double,  SHRT_NEGD)  (short x) {return 0.0-x;}

INLINE(double,  UINT_NEGD)   (uint x) {return 0.0-x;}
INLINE(double,   INT_NEGD)    (int x) {return 0.0-x;}

INLINE(double, ULONG_NEGD)  (ulong x) {return 0.0-x;}
INLINE(double,  LONG_NEGD)   (long x) {return 0.0-x;}

INLINE(double,ULLONG_NEGD) (ullong x) {return 0.0-x;}
INLINE(double, LLONG_NEGD)  (llong x) {return 0.0-x;}

#if QUAD_NLLONG == 2
INLINE(double,negdqu) (QUAD_UTYPE x) {return 0.0-x;}
INLINE(double,negdqi) (QUAD_ITYPE x) {return 0.0-x;}
INLINE(double,negdqf) (QUAD_FTYPE x) {return -x;}
#else
INLINE(double,LDBL_NEGD) (long double x) {return -x;}
#endif

INLINE(double, FLT16_NEGD) (flt16_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vnegh_f16(x);
#else
    return  0.0-x;
#endif
}

INLINE(double, FLT_NEGD) (float x) {return  -x;}

INLINE(double, DBL_NEGD) (double x) {return -x;}



INLINE(Vqdf,VWHU_NEGD) (Vwhu x)
{
    return  vnegq_f64(VWHU_CVDF(x));
}

INLINE(Vqdf,VWHI_NEGD) (Vwhi x)
{
    return  vnegq_f64(VWHI_CVDF(x));
}

INLINE(Vqdf,VWHF_NEGD) (Vwhf x)
{
    return  vnegq_f64(VWHF_CVDF(x));
}


INLINE(Vddf,VWWU_NEGD) (Vwwu x)
{
    return  vneg_f64(vdup_n_f64(FLT_ASWU(VWWU_ASTM(x))));
}

INLINE(Vddf,VWWI_NEGD) (Vwwi x)
{
    return  vneg_f64(vdup_n_f64(FLT_ASWI(VWWI_ASTM(x))));
}

INLINE(Vddf,VWWF_NEGD) (Vwwf x)
{
    return  vneg_f64(vdup_n_f64(VWWF_ASTM(x)));
}



INLINE(Vqdf,VDWU_NEGD) (Vdwu x)
{
    return  vnegq_f64(vcvtq_f64_u64(vmovl_u32(x)));
}

INLINE(Vqdf,VDWI_NEGD) (Vdwi x)
{
    return  vnegq_f64(vcvtq_f64_s64(vmovl_s32(x)));
}

INLINE(Vqdf,VDWF_NEGD) (Vdwf x)
{
    return  vcvt_f64_f32(vneg_f32(x));
}


INLINE(Vddf,VDDU_NEGD) (Vddu x)
{
    return  vneg_f64(vcvt_f64_u64(x));
}

INLINE(Vddf,VDDI_NEGD) (Vddi x)
{
    return  vneg_f64(vcvt_f64_s64(x));
}

INLINE(Vddf,VDDF_NEGD) (Vddf x)
{
    return  vneg_f64(x);
}



INLINE(Vqdf,VQDU_NEGD) (Vqdu x)
{
    return vnegq_f64(vcvtq_f64_u64(x));
}

INLINE(Vqwf,VQDI_NEGD) (Vqdi x)
{
    return vnegq_f64(vcvtq_f64_s64(x));
}

INLINE(Vqdf,VQDF_NEGD) (Vqdf x)
{
    return  vnegq_f64(x);
}

#if _LEAVE_ARM_NEGD
}
#endif

#if _ENTER_ARM_INVS
{
#endif

INLINE(            _Bool,  BOOL_INVS)  (_Bool x) {return ~x;}
INLINE(          uint8_t, UCHAR_INVS)  (uchar x) {return ~x;}
INLINE(           int8_t, SCHAR_INVS)  (schar x) {return ~x;}
INLINE(             char,  CHAR_INVS)   (char x) {return ~x;}
INLINE(         uint16_t, USHRT_INVS) (ushort x) {return ~x;}
INLINE(          int16_t,  SHRT_INVS)  (short x) {return ~x;}
INLINE(         uint32_t,  UINT_INVS)    (int x) {return ~x;}
INLINE(          int32_t,   INT_INVS)   (uint x) {return ~x;}
INLINE( ULONG_STG(UTYPE), ULONG_INVS)  (ulong x) {return ~x;}
INLINE(  LONG_STG(ITYPE),  LONG_INVS)   (long x) {return ~x;}
INLINE(ULLONG_STG(UTYPE),ULLONG_INVS) (ullong x) {return ~x;}
INLINE( LLONG_STG(ITYPE), LLONG_INVS)  (llong x) {return ~x;}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,invsqu) (QUAD_UTYPE x) {return ~x;}
INLINE(QUAD_ITYPE,invsqi) (QUAD_ITYPE x) {return ~x;}
#endif

#define   BOOL_INVS(X)  ((_Bool) (!X))
#define  UCHAR_INVS(X)  ((unsigned char) ~(UCHAR_MAX&X))
#define  SCHAR_INVS(X)  ((signed char) ~(UCHAR_MAX&X))
#define   CHAR_INVS(X)  ((char) ~(UCHAR_MAX&X))
#define  USHRT_INVS(X)  ((unsigned short) ~(USHRT_MAX&X))
#define   SHRT_INVS(X)  ((short) ~(USHRT_MAX&X))
#define   UINT_INVS(X)  ((unsigned int) ~(UINT_MAX&X))
#define    INT_INVS(X)  ((int) ~(UINT_MAX&X))
#define  ULONG_INVS(X)  ((unsigned long) ~(ULONG_MAX&X))
#define   LONG_INVS(X)  ((long) ~(ULONG_MAX&X))
#define ULLONG_INVS(X)  ((unsigned long long) ~(ULLONG_MAX&X))
#define  LLONG_INVS(X)  ((long long) ~(ULLONG_MAX&X))

#define     DYU_INVS(M) DBU_ASYU(vmvn_u8(DYU_ASBU(M)))
#define     DBU_INVS             vmvn_u8
#define     DBI_INVS             vmvn_s8
#if CHAR_MIN
#   define  DBC_INVS             vmvn_s8
#else
#   define  DBC_INVS             vmvn_u8
#endif

#define     DHU_INVS             vmvn_u16
#define     DHI_INVS             vmvn_s16
#define     DWU_INVS             vmvn_u32
#define     DWI_INVS             vmvn_s32
#define     DDU_INVS(M) DWU_ASDU(vmvn_u32(DDU_ASWU(M)))
#define     DDI_INVS(M) DWI_ASDI(vmvn_s32(DDI_ASWI(M)))

#define     QYU_INVS(M) QBU_ASYU(vmvnq_u8(QYU_ASBU(M)))
#define     QBU_INVS             vmvnq_u8
#define     QBI_INVS             vmvnq_s8
#if CHAR_MIN
#   define  QBC_INVS             vmvnq_s8
#else
#   define  QBC_INVS             vmvnq_u8
#endif

#define     QHU_INVS             vmvnq_u16
#define     QHI_INVS             vmvnq_s16
#define     QWU_INVS             vmvnq_u32
#define     QWI_INVS             vmvnq_s32
#define     QDU_INVS(M) QWU_ASDU(vmvnq_u32(QDU_ASWU(M)))
#define     QDI_INVS(M) QWI_ASDI(vmvnq_s32(QDI_ASWI(M)))

INLINE(Vwyu,VWYU_INVS) (Vwyu v)
{
    float32x2_t f = vdup_n_f32(VWYU_ASTM(v));
    uint8x8_t   u = vmvn_u8(vreinterpret_u8_f32(f));
    return  WYU_ASTV(vget_lane_f32(vreinterpret_f32_u8(u), V2_K0));
}


INLINE(Vwbu,VWBU_INVS) (Vwbu v)
{
#define     VWBU_INVS   VWBU_INVS
    float32x2_t f = vdup_n_f32(VWBU_ASTM(v));
    uint8x8_t   u = vmvn_u8(vreinterpret_u8_f32(f));
    return  WBU_ASTV(vget_lane_f32(vreinterpret_f32_u8(u), V2_K0));
}

INLINE(Vwbi,VWBI_INVS) (Vwbi v)
{
    float32x2_t f = vdup_n_f32(VWBI_ASTM(v));
    uint8x8_t   u = vmvn_u8(vreinterpret_u8_f32(f));
    return  WBI_ASTV(vget_lane_f32(vreinterpret_f32_u8(u), V2_K0));
}

INLINE(Vwbc,VWBC_INVS) (Vwbc v)
{
    float32x2_t f = vdup_n_f32(VWBC_ASTM(v));
    uint8x8_t   u = vmvn_u8(vreinterpret_u8_f32(f));
    return  WBC_ASTV(vget_lane_f32(vreinterpret_f32_u8(u), V2_K0));
}


INLINE(Vwhu,VWHU_INVS) (Vwhu v)
{
    float32x2_t f = vdup_n_f32(VWHU_ASTM(v));
    uint8x8_t   u = vmvn_u8(vreinterpret_u8_f32(f));
    return  WHU_ASTV(vget_lane_f32(vreinterpret_f32_u8(u), V2_K0));
}

INLINE(Vwhi,VWHI_INVS) (Vwhi v)
{
    float32x2_t f = vdup_n_f32(VWHI_ASTM(v));
    uint8x8_t   u = vmvn_u8(vreinterpret_u8_f32(f));
    return  WHI_ASTV(vget_lane_f32(vreinterpret_f32_u8(u), V2_K0));
}


INLINE(Vwwu,VWWU_INVS) (Vwwu v)
{
    float32x2_t f = vdup_n_f32(VWWU_ASTM(v));
    uint8x8_t   u = vmvn_u8(vreinterpret_u8_f32(f));
    return  WWU_ASTV(vget_lane_f32(vreinterpret_f32_u8(u), V2_K0));
}

INLINE(Vwwi,VWWI_INVS) (Vwwi v)
{
    float32x2_t f = vdup_n_f32(VWWI_ASTM(v));
    uint8x8_t   u = vmvn_u8(vreinterpret_u8_f32(f));
    return  WWI_ASTV(vget_lane_f32(vreinterpret_f32_u8(u), V2_K0));
}


INLINE(Vdyu,VDYU_INVS) (Vdyu v) {return DYU_ASTV(DYU_INVS(VDYU_ASTM(v)));}
INLINE(Vdbu,VDBU_INVS) (Vdbu v) {return DBU_INVS(v);}
INLINE(Vdbi,VDBI_INVS) (Vdbi v) {return DBI_INVS(v);}
INLINE(Vdbc,VDBC_INVS) (Vdbc v) {return DBC_ASTV(DBC_INVS(VDBC_ASTM(v)));}
INLINE(Vdhu,VDHU_INVS) (Vdhu v) {return DHU_INVS(v);}
INLINE(Vdhi,VDHI_INVS) (Vdhi v) {return DHI_INVS(v);}
INLINE(Vdwu,VDWU_INVS) (Vdwu v) {return DWU_INVS(v);}
INLINE(Vdwi,VDWI_INVS) (Vdwi v) {return DWI_INVS(v);}
INLINE(Vddu,VDDU_INVS) (Vddu v) {return DDU_INVS(v);}
INLINE(Vddi,VDDI_INVS) (Vddi v) {return DDI_INVS(v);}

INLINE(Vqyu,VQYU_INVS) (Vqyu v) {return QYU_ASTV(QYU_INVS(VQYU_ASTM(v)));}
INLINE(Vqbu,VQBU_INVS) (Vqbu v) {return QBU_INVS(v);}
INLINE(Vqbi,VQBI_INVS) (Vqbi v) {return QBI_INVS(v);}
INLINE(Vqbc,VQBC_INVS) (Vqbc v) {return QBC_ASTV(QBC_INVS(VQBC_ASTM(v)));}
INLINE(Vqhu,VQHU_INVS) (Vqhu v) {return QHU_INVS(v);}
INLINE(Vqhi,VQHI_INVS) (Vqhi v) {return QHI_INVS(v);}
INLINE(Vqwu,VQWU_INVS) (Vqwu v) {return QWU_INVS(v);}
INLINE(Vqwi,VQWI_INVS) (Vqwi v) {return QWI_INVS(v);}
INLINE(Vqdu,VQDU_INVS) (Vqdu v) {return QDU_INVS(v);}
INLINE(Vqdi,VQDI_INVS) (Vqdi v) {return QDI_INVS(v);}


#if _LEAVE_ARM_INVS
}
#endif

#if _ENTER_ARM_ANDS
{
#endif

INLINE(            _Bool,  BOOL_ANDS)  (_Bool a,  _Bool b) {return a&b;}
INLINE(          uint8_t, UCHAR_ANDS)  (uchar a,  uchar b) {return a&b;}
INLINE(           int8_t, SCHAR_ANDS)  (schar a,  schar b) {return a&b;}
INLINE(             char,  CHAR_ANDS)   (char a,   char b) {return a&b;}
INLINE(         uint16_t, USHRT_ANDS) (ushort a, ushort b) {return a&b;}
INLINE(          int16_t,  SHRT_ANDS)  (short a,  short b) {return a&b;}
INLINE(         uint32_t,  UINT_ANDS)   (uint a,   uint b) {return a&b;}
INLINE(          int32_t,   INT_ANDS)    (int a,    int b) {return a&b;}
INLINE( ULONG_STG(UTYPE), ULONG_ANDS)  (ulong a,  ulong b) {return a&b;}
INLINE(  LONG_STG(ITYPE),  LONG_ANDS)   (long a,   long b) {return a&b;}
INLINE(ULLONG_STG(UTYPE),ULLONG_ANDS) (ullong a, ullong b) {return a&b;}
INLINE( LLONG_STG(ITYPE), LLONG_ANDS)  (llong a,  llong b) {return a&b;}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,andsqu) (QUAD_UTYPE a, QUAD_UTYPE b) {return a&b;}
INLINE(QUAD_ITYPE,andsqi) (QUAD_ITYPE a, QUAD_ITYPE b) {return a&b;}
#endif

INLINE(Vwyu,VWYU_ANDS) (Vwyu a, Vwyu b)
{
    return  WYU_ASTV(FLT_ANDS(VWYU_ASTM(a), VWYU_ASTM(b)));
}

INLINE(Vwbu,VWBU_ANDS) (Vwbu a, Vwbu b)
{
    return  WBU_ASTV(FLT_ANDS(VWBU_ASTM(a), VWBU_ASTM(b)));
}

INLINE(Vwbi,VWBI_ANDS) (Vwbi a, Vwbi b)
{
    return  WBI_ASTV(FLT_ANDS(VWBI_ASTM(a), VWBI_ASTM(b)));
}

INLINE(Vwbc,VWBC_ANDS) (Vwbc a, Vwbc b)
{
    return  WBC_ASTV(FLT_ANDS(VWBC_ASTM(a), VWBC_ASTM(b)));
}

INLINE(Vwhu,VWHU_ANDS) (Vwhu a, Vwhu b)
{
    return  WHU_ASTV(FLT_ANDS(VWHU_ASTM(a), VWHU_ASTM(b)));
}

INLINE(Vwhi,VWHI_ANDS) (Vwhi a, Vwhi b)
{
    return  WHI_ASTV(FLT_ANDS(VWHI_ASTM(a), VWHI_ASTM(b)));
}


INLINE(Vwwu,VWWU_ANDS) (Vwwu a, Vwwu b)
{
    return  WWU_ASTV(FLT_ANDS(VWWU_ASTM(a), VWWU_ASTM(b)));
}

INLINE(Vwwi,VWWI_ANDS) (Vwwi a, Vwwi b)
{
    return  WWI_ASTV(FLT_ANDS(VWWI_ASTM(a), VWWI_ASTM(b)));
}

INLINE(Vdyu,VDYU_ANDS) (Vdyu a, Vdyu b)
{
    return  VDDU_ASYU(vand_u64(VDYU_ASDU(a), VDYU_ASDU(b)));
}

INLINE(Vdbu,VDBU_ANDS) (Vdbu a, Vdbu b) {return  vand_u8(a, b);}
INLINE(Vdbi,VDBI_ANDS) (Vdbi a, Vdbi b) {return  vand_s8(a, b);}
INLINE(Vdbc,VDBC_ANDS) (Vdbc a, Vdbc b)
{
    return  VDDU_ASBC(vand_u64(VDBC_ASDU(a), VDBC_ASDU(b)));
}

INLINE(Vdhu,VDHU_ANDS) (Vdhu a, Vdhu b) {return  vand_u16(a, b);}
INLINE(Vdhi,VDHI_ANDS) (Vdhi a, Vdhi b) {return  vand_s16(a, b);}
INLINE(Vdwu,VDWU_ANDS) (Vdwu a, Vdwu b) {return  vand_u32(a, b);}
INLINE(Vdwi,VDWI_ANDS) (Vdwi a, Vdwi b) {return  vand_s32(a, b);}
INLINE(Vddu,VDDU_ANDS) (Vddu a, Vddu b) {return  vand_u64(a, b);}
INLINE(Vddi,VDDI_ANDS) (Vddi a, Vddi b) {return  vand_s64(a, b);}


INLINE(Vqyu,VQYU_ANDS) (Vqyu a, Vqyu b)
{
    return  VQDU_ASYU(vandq_u64(VQYU_ASDU(a), VQYU_ASDU(b)));
}

INLINE(Vqbu,VQBU_ANDS) (Vqbu a, Vqbu b) {return  vandq_u8(a, b);}
INLINE(Vqbi,VQBI_ANDS) (Vqbi a, Vqbi b) {return  vandq_s8(a, b);}
INLINE(Vqbc,VQBC_ANDS) (Vqbc a, Vqbc b)
{
    return  VQDU_ASBC(vandq_u64(VQBC_ASDU(a), VQBC_ASDU(b)));
}

INLINE(Vqhu,VQHU_ANDS) (Vqhu a, Vqhu b) {return  vandq_u16(a, b);}
INLINE(Vqhi,VQHI_ANDS) (Vqhi a, Vqhi b) {return  vandq_s16(a, b);}
INLINE(Vqwu,VQWU_ANDS) (Vqwu a, Vqwu b) {return  vandq_u32(a, b);}
INLINE(Vqwi,VQWI_ANDS) (Vqwi a, Vqwi b) {return  vandq_s32(a, b);}
INLINE(Vqdu,VQDU_ANDS) (Vqdu a, Vqdu b) {return  vandq_u64(a, b);}
INLINE(Vqdi,VQDI_ANDS) (Vqdi a, Vqdi b) {return  vandq_s64(a, b);}


#if _LEAVE_ARM_ANDS
}
#endif

#if _ENTER_ARM_ANDN
{
#endif

INLINE(            _Bool,  BOOL_ANDN)  (_Bool a,  _Bool b) {return a&~b;}
INLINE(          uint8_t, UCHAR_ANDN)  (uchar a,  uchar b) {return a&~b;}
INLINE(           int8_t, SCHAR_ANDN)  (schar a,  schar b) {return a&~b;}
INLINE(             char,  CHAR_ANDN)   (char a,   char b) {return a&~b;}
INLINE(         uint16_t, USHRT_ANDN) (ushort a, ushort b) {return a&~b;}
INLINE(          int16_t,  SHRT_ANDN)  (short a,  short b) {return a&~b;}
INLINE(         uint32_t,  UINT_ANDN)   (uint a,   uint b) {return a&~b;}
INLINE(          int32_t,   INT_ANDN)    (int a,    int b) {return a&~b;}
INLINE( ULONG_STG(UTYPE), ULONG_ANDN)  (ulong a,  ulong b) {return a&~b;}
INLINE(  LONG_STG(ITYPE),  LONG_ANDN)   (long a,   long b) {return a&~b;}
INLINE(ULLONG_STG(UTYPE),ULLONG_ANDN) (ullong a, ullong b) {return a&~b;}
INLINE( LLONG_STG(ITYPE), LLONG_ANDN)  (llong a,  llong b) {return a&~b;}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,andnqu) (QUAD_UTYPE a, QUAD_UTYPE b) {return a&~b;}
INLINE(QUAD_ITYPE,andnqi) (QUAD_ITYPE a, QUAD_ITYPE b) {return a&~b;}
#endif

INLINE(Vwyu,VWYU_ANDN) (Vwyu a, Vwyu b)
{
    return  WYU_ASTV(FLT_ANDN(VWYU_ASTM(a), VWYU_ASTM(b)));
}

INLINE(Vwbu,VWBU_ANDN) (Vwbu a, Vwbu b)
{
    return  WBU_ASTV(FLT_ANDN(VWBU_ASTM(a), VWBU_ASTM(b)));
}

INLINE(Vwbi,VWBI_ANDN) (Vwbi a, Vwbi b)
{
    return  WBI_ASTV(FLT_ANDN(VWBI_ASTM(a), VWBI_ASTM(b)));
}

INLINE(Vwbc,VWBC_ANDN) (Vwbc a, Vwbc b)
{
    return  WBC_ASTV(FLT_ANDN(VWBC_ASTM(a), VWBC_ASTM(b)));
}

INLINE(Vwhu,VWHU_ANDN) (Vwhu a, Vwhu b)
{
    return  WHU_ASTV(FLT_ANDN(VWHU_ASTM(a), VWHU_ASTM(b)));
}

INLINE(Vwhi,VWHI_ANDN) (Vwhi a, Vwhi b)
{
    return  WHI_ASTV(FLT_ANDN(VWHI_ASTM(a), VWHI_ASTM(b)));
}


INLINE(Vwwu,VWWU_ANDN) (Vwwu a, Vwwu b)
{
    return  WWU_ASTV(FLT_ANDN(VWWU_ASTM(a), VWWU_ASTM(b)));
}

INLINE(Vwwi,VWWI_ANDN) (Vwwi a, Vwwi b)
{
    return  WWI_ASTV(FLT_ANDN(VWWI_ASTM(a), VWWI_ASTM(b)));
}


INLINE(Vdyu,VDYU_ANDN) (Vdyu a, Vdyu b)
{
    return  VDWU_ASYU(vand_u32(VDYU_ASWU(a), vmvn_u32(VDYU_ASWU(b))));
}

INLINE(Vdbu,VDBU_ANDN) (Vdbu a, Vdbu b) {return  vand_u8(a, vmvn_u8(b));}
INLINE(Vdbi,VDBI_ANDN) (Vdbi a, Vdbi b) {return  vand_s8(a, vmvn_s8(b));}
INLINE(Vdbc,VDBC_ANDN) (Vdbc a, Vdbc b)
{
    return  VDWU_ASBC(vand_u32(VDBC_ASWU(a), vmvn_u32(VDBC_ASWU(b))));
}

INLINE(Vdhu,VDHU_ANDN) (Vdhu a, Vdhu b) {return  vand_u16(a, vmvn_u16(b));}
INLINE(Vdhi,VDHI_ANDN) (Vdhi a, Vdhi b) {return  vand_s16(a, vmvn_s16(b));}
INLINE(Vdwu,VDWU_ANDN) (Vdwu a, Vdwu b) {return  vand_u32(a, vmvn_u32(b));}
INLINE(Vdwi,VDWI_ANDN) (Vdwi a, Vdwi b) {return  vand_s32(a, vmvn_s32(b));}
INLINE(Vddu,VDDU_ANDN) (Vddu a, Vddu b)
{
    return  VDWU_ASDU(vand_u32(VDDU_ASWU(a), vmvn_u32(VDDU_ASWU(b))));
}

INLINE(Vddi,VDDI_ANDN) (Vddi a, Vddi b)
{
    return  VDWU_ASDI(vand_u32(VDDI_ASWU(a), vmvn_u32(VDDI_ASWU(b))));
}



INLINE(Vqyu,VQYU_ANDN) (Vqyu a, Vqyu b)
{
    return  VQWU_ASYU(vandq_u32(VQYU_ASWU(a), vmvnq_u32(VQYU_ASWU(b))));
}

INLINE(Vqbu,VQBU_ANDN) (Vqbu a, Vqbu b) {return  vandq_u8(a, vmvnq_u8(b));}
INLINE(Vqbi,VQBI_ANDN) (Vqbi a, Vqbi b) {return  vandq_s8(a, vmvnq_s8(b));}
INLINE(Vqbc,VQBC_ANDN) (Vqbc a, Vqbc b)
{
    return  VQWU_ASBC(vandq_u32(VQBC_ASWU(a), vmvnq_u32(VQBC_ASWU(b))));
}

INLINE(Vqhu,VQHU_ANDN) (Vqhu a, Vqhu b) {return  vandq_u16(a, vmvnq_u16(b));}
INLINE(Vqhi,VQHI_ANDN) (Vqhi a, Vqhi b) {return  vandq_s16(a, vmvnq_s16(b));}
INLINE(Vqwu,VQWU_ANDN) (Vqwu a, Vqwu b) {return  vandq_u32(a, vmvnq_u32(b));}
INLINE(Vqwi,VQWI_ANDN) (Vqwi a, Vqwi b) {return  vandq_s32(a, vmvnq_s32(b));}
INLINE(Vqdu,VQDU_ANDN) (Vqdu a, Vqdu b)
{
    return  VQWU_ASDU(vandq_u32(VQDU_ASWU(a), vmvnq_u32(VQDU_ASWU(b))));
}
INLINE(Vqdi,VQDI_ANDN) (Vqdi a, Vqdi b)
{
    return  VQWU_ASDI(vandq_u32(VQDI_ASWU(a), vmvnq_u32(VQDI_ASWU(b))));
}


#if _LEAVE_ARM_ANDN
}
#endif

#if _ENTER_ARM_ANDV
{
#endif

INLINE(_Bool,VWYU_ANDV) (Vwyu a)
{
    uint32_t v = FLT_ASTU(VWYU_ASTM(a));
    return v == UINT32_MAX;
}


INLINE(uint8_t,VWBU_ANDV) (Vwbu a)
{
    float   m = VWBU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint32x2_t  x = vreinterpret_u32_f32(f);
    uint32x2_t  y = vshr_n_u32(x, 16);
    x = vand_u32(x, y);
    y = vshr_n_u32(x, 8);
    x = vand_u32(x, y);
    return  vget_lane_u32(x, 0);
}

INLINE(int8_t, VWBI_ANDV) (Vwbi a)
{
    return  VWBU_ANDV(VWBI_ASBU(a));
}

INLINE(char, VWBC_ANDV) (Vwbc a)
{
    return  VWBU_ANDV(VWBC_ASBU(a));
}


INLINE(uint16_t,VWHU_ANDV) (Vwhu a)
{
    float       m = VWHU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint16x4_t  x = vreinterpret_u16_f32(f);
    return  vget_lane_u16(x, 0)&vget_lane_u16(x, 1);
}

INLINE( int16_t,VWHI_ANDV) (Vwhi a)
{
    return  VWHU_ANDV(VWHI_ASTU(a));
}


INLINE(_Bool,VDYU_ANDV) (Vdyu a)
{
    uint64x1_t l = VDYU_ASDU(a);
    l = vceq_u64(l, vdup_n_u64(UINT64_MAX));
    return vget_lane_u64(l, 0);
}

INLINE( uint8_t,VDBU_ANDV) (Vdbu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    r = l&r;
    l = r>>16;
    r = l&r;
    l = r>>8;
    return l&r;
}

INLINE(  int8_t,VDBI_ANDV) (Vdbi a)
{
    return  VDBU_ANDV(VDBI_ASTU(a));
}

INLINE(   char,VDBC_ANDV) (Vdbc a)
{
    return  VDBU_ANDV(VDBC_ASTU(a));
}


INLINE(uint16_t,VDHU_ANDV) (Vdhu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    l = l&r;
    r = l>>16;
    return l&r;
}

INLINE( int16_t,VDHI_ANDV) (Vdhi a)
{
    return  VDHU_ANDV(VDHI_ASTU(a));
}


INLINE(uint32_t,VDWU_ANDV) (Vdwu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    return  l&r;
}

INLINE( int32_t,VDWI_ANDV) (Vdwi a)
{
    return  VDWU_ANDV(VDWI_ASTU(a));
}


INLINE(   _Bool,VQYU_ANDV) (Vqyu a)
{
    uint64x2_t q = VQYU_ASDU(a);
    q = vceqq_u64(q, vdupq_n_u64(UINT64_MAX));
    uint64x1_t l = vget_low_u64(q);
    uint64x1_t r = vget_high_u64(q);
    l = vand_u64(l, r);
    return vget_lane_u64(l, 0);
}


INLINE( uint8_t,VQBU_ANDV) (Vqbu a)
{
    uint64x2_t q = vreinterpretq_u64_u8(a);
    uint64x1_t d = vand_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDBU_ANDV(VDDU_ASBU(d));
}

INLINE(  int8_t,VQBI_ANDV) (Vqbi a)
{
    return VQBU_ANDV(VQBI_ASTU(a));
}

INLINE(    char,VQBC_ANDV) (Vqbc a)
{
    return VQBU_ANDV(VQBC_ASTU(a));
}


INLINE(uint16_t,VQHU_ANDV) (Vqhu a)
{
    uint64x2_t q = vreinterpretq_u64_u16(a);
    uint64x1_t d = vand_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDHU_ANDV(VDDU_ASHU(d));
}

INLINE( int16_t,VQHI_ANDV) (Vqhi a)
{
    return VQHU_ANDV(VQHI_ASTU(a));
}


INLINE(uint32_t,VQWU_ANDV) (Vqwu a)
{
    uint64x2_t q = vreinterpretq_u64_u32(a);
    uint64x1_t d = vand_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDWU_ANDV(VDDU_ASWU(d));
}

INLINE( int32_t,VQWI_ANDV) (Vqwi a)
{
    return VQWU_ANDV(VQWI_ASTU(a));
}


INLINE(uint64_t,VQDU_ANDV) (Vqdu a)
{
    uint64x1_t d = vand_u64(
        vget_low_u64(a),
        vget_high_u64(a)
    );
    return  vget_lane_u64(d, 0);
}

INLINE( int64_t,VQDI_ANDV) (Vqdi a)
{
    int64x1_t d = vand_s64(
        vget_low_s64(a),
        vget_high_s64(a)
    );
    return  vget_lane_s64(d, 0);
}


#if _LEAVE_ARM_ANDV
}
#endif

#if _ENTER_ARM_ORRS
{
#endif

INLINE(            _Bool,  BOOL_ORRS)  (_Bool a,  _Bool b) {return a|b;}
INLINE(          uint8_t, UCHAR_ORRS)  (uchar a,  uchar b) {return a|b;}
INLINE(           int8_t, SCHAR_ORRS)  (schar a,  schar b) {return a|b;}
INLINE(             char,  CHAR_ORRS)   (char a,   char b) {return a|b;}
INLINE(         uint16_t, USHRT_ORRS) (ushort a, ushort b) {return a|b;}
INLINE(          int16_t,  SHRT_ORRS)  (short a,  short b) {return a|b;}
INLINE(         uint32_t,  UINT_ORRS)   (uint a,   uint b) {return a|b;}
INLINE(          int32_t,   INT_ORRS)    (int a,    int b) {return a|b;}
INLINE( ULONG_STG(UTYPE), ULONG_ORRS)  (ulong a,  ulong b) {return a|b;}
INLINE(  LONG_STG(ITYPE),  LONG_ORRS)   (long a,   long b) {return a|b;}
INLINE(ULLONG_STG(UTYPE),ULLONG_ORRS) (ullong a, ullong b) {return a|b;}
INLINE( LLONG_STG(ITYPE), LLONG_ORRS)  (llong a,  llong b) {return a|b;}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,orrsqu) (QUAD_UTYPE a, QUAD_UTYPE b) {return a|b;}
INLINE(QUAD_ITYPE,orrsqi) (QUAD_ITYPE a, QUAD_ITYPE b) {return a|b;}
#endif

INLINE(Vwyu,VWYU_ORRS) (Vwyu a, Vwyu b)
{
    return  WYU_ASTV(FLT_ORRS(VWYU_ASTM(a), VWYU_ASTM(b)));
}

INLINE(Vwbu,VWBU_ORRS) (Vwbu a, Vwbu b)
{
    return  WBU_ASTV(FLT_ORRS(VWBU_ASTM(a), VWBU_ASTM(b)));
}

INLINE(Vwbi,VWBI_ORRS) (Vwbi a, Vwbi b)
{
    return  WBI_ASTV(FLT_ORRS(VWBI_ASTM(a), VWBI_ASTM(b)));
}

INLINE(Vwbc,VWBC_ORRS) (Vwbc a, Vwbc b)
{
    return  WBC_ASTV(FLT_ORRS(VWBC_ASTM(a), VWBC_ASTM(b)));
}

INLINE(Vwhu,VWHU_ORRS) (Vwhu a, Vwhu b)
{
    return  WHU_ASTV(FLT_ORRS(VWHU_ASTM(a), VWHU_ASTM(b)));
}

INLINE(Vwhi,VWHI_ORRS) (Vwhi a, Vwhi b)
{
    return  WHI_ASTV(FLT_ORRS(VWHI_ASTM(a), VWHI_ASTM(b)));
}


INLINE(Vwwu,VWWU_ORRS) (Vwwu a, Vwwu b)
{
    return  WWU_ASTV(FLT_ORRS(VWWU_ASTM(a), VWWU_ASTM(b)));
}

INLINE(Vwwi,VWWI_ORRS) (Vwwi a, Vwwi b)
{
    return  WWI_ASTV(FLT_ORRS(VWWI_ASTM(a), VWWI_ASTM(b)));
}

INLINE(Vdyu,VDYU_ORRS) (Vdyu a, Vdyu b)
{
    return  VDDU_ASYU(vorr_u64(VDYU_ASDU(a), VDYU_ASDU(b)));
}

INLINE(Vdbu,VDBU_ORRS) (Vdbu a, Vdbu b) {return  vorr_u8(a, b);}
INLINE(Vdbi,VDBI_ORRS) (Vdbi a, Vdbi b) {return  vorr_s8(a, b);}
INLINE(Vdbc,VDBC_ORRS) (Vdbc a, Vdbc b)
{
    return  VDDU_ASBC(vorr_u64(VDBC_ASDU(a), VDBC_ASDU(b)));
}

INLINE(Vdhu,VDHU_ORRS) (Vdhu a, Vdhu b) {return  vorr_u16(a, b);}
INLINE(Vdhi,VDHI_ORRS) (Vdhi a, Vdhi b) {return  vorr_s16(a, b);}
INLINE(Vdwu,VDWU_ORRS) (Vdwu a, Vdwu b) {return  vorr_u32(a, b);}
INLINE(Vdwi,VDWI_ORRS) (Vdwi a, Vdwi b) {return  vorr_s32(a, b);}
INLINE(Vddu,VDDU_ORRS) (Vddu a, Vddu b) {return  vorr_u64(a, b);}
INLINE(Vddi,VDDI_ORRS) (Vddi a, Vddi b) {return  vorr_s64(a, b);}


INLINE(Vqyu,VQYU_ORRS) (Vqyu a, Vqyu b)
{
    return  VQDU_ASYU(vorrq_u64(VQYU_ASDU(a), VQYU_ASDU(b)));
}

INLINE(Vqbu,VQBU_ORRS) (Vqbu a, Vqbu b) {return  vorrq_u8(a, b);}
INLINE(Vqbi,VQBI_ORRS) (Vqbi a, Vqbi b) {return  vorrq_s8(a, b);}
INLINE(Vqbc,VQBC_ORRS) (Vqbc a, Vqbc b)
{
    return  VQDU_ASBC(vorrq_u64(VQBC_ASDU(a), VQBC_ASDU(b)));
}

INLINE(Vqhu,VQHU_ORRS) (Vqhu a, Vqhu b) {return  vorrq_u16(a, b);}
INLINE(Vqhi,VQHI_ORRS) (Vqhi a, Vqhi b) {return  vorrq_s16(a, b);}
INLINE(Vqwu,VQWU_ORRS) (Vqwu a, Vqwu b) {return  vorrq_u32(a, b);}
INLINE(Vqwi,VQWI_ORRS) (Vqwi a, Vqwi b) {return  vorrq_s32(a, b);}
INLINE(Vqdu,VQDU_ORRS) (Vqdu a, Vqdu b) {return  vorrq_u64(a, b);}
INLINE(Vqdi,VQDI_ORRS) (Vqdi a, Vqdi b) {return  vorrq_s64(a, b);}


#if _LEAVE_ARM_ORRS
}
#endif

#if _ENTER_ARM_ORRN
{
#endif

INLINE(            _Bool,  BOOL_ORRN)  (_Bool a,  _Bool b) {return a|~b;}
INLINE(          uint8_t, UCHAR_ORRN)  (uchar a,  uchar b) {return a|~b;}
INLINE(           int8_t, SCHAR_ORRN)  (schar a,  schar b) {return a|~b;}
INLINE(             char,  CHAR_ORRN)   (char a,   char b) {return a|~b;}
INLINE(         uint16_t, USHRT_ORRN) (ushort a, ushort b) {return a|~b;}
INLINE(          int16_t,  SHRT_ORRN)  (short a,  short b) {return a|~b;}
INLINE(         uint32_t,  UINT_ORRN)   (uint a,   uint b) {return a|~b;}
INLINE(          int32_t,   INT_ORRN)    (int a,    int b) {return a|~b;}
INLINE( ULONG_STG(UTYPE), ULONG_ORRN)  (ulong a,  ulong b) {return a|~b;}
INLINE(  LONG_STG(ITYPE),  LONG_ORRN)   (long a,   long b) {return a|~b;}
INLINE(ULLONG_STG(UTYPE),ULLONG_ORRN) (ullong a, ullong b) {return a|~b;}
INLINE( LLONG_STG(ITYPE), LLONG_ORRN)  (llong a,  llong b) {return a|~b;}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,orrnqu) (QUAD_UTYPE a, QUAD_UTYPE b) {return a|~b;}
INLINE(QUAD_ITYPE,orrnqi) (QUAD_ITYPE a, QUAD_ITYPE b) {return a|~b;}
#endif

INLINE(Vwyu,VWYU_ORRN) (Vwyu a, Vwyu b)
{
    return  WYU_ASTV(FLT_ORRN(VWYU_ASTM(a), VWYU_ASTM(b)));
}

INLINE(Vwbu,VWBU_ORRN) (Vwbu a, Vwbu b)
{
    return  WBU_ASTV(FLT_ORRN(VWBU_ASTM(a), VWBU_ASTM(b)));
}

INLINE(Vwbi,VWBI_ORRN) (Vwbi a, Vwbi b)
{
    return  WBI_ASTV(FLT_ORRN(VWBI_ASTM(a), VWBI_ASTM(b)));
}

INLINE(Vwbc,VWBC_ORRN) (Vwbc a, Vwbc b)
{
    return  WBC_ASTV(FLT_ORRN(VWBC_ASTM(a), VWBC_ASTM(b)));
}

INLINE(Vwhu,VWHU_ORRN) (Vwhu a, Vwhu b)
{
    return  WHU_ASTV(FLT_ORRN(VWHU_ASTM(a), VWHU_ASTM(b)));
}

INLINE(Vwhi,VWHI_ORRN) (Vwhi a, Vwhi b)
{
    return  WHI_ASTV(FLT_ORRN(VWHI_ASTM(a), VWHI_ASTM(b)));
}


INLINE(Vwwu,VWWU_ORRN) (Vwwu a, Vwwu b)
{
    return  WWU_ASTV(FLT_ORRN(VWWU_ASTM(a), VWWU_ASTM(b)));
}

INLINE(Vwwi,VWWI_ORRN) (Vwwi a, Vwwi b)
{
    return  WWI_ASTV(FLT_ORRN(VWWI_ASTM(a), VWWI_ASTM(b)));
}

INLINE(Vdyu,VDYU_ORRN) (Vdyu a, Vdyu b)
{
    return  VDDU_ASYU(vorn_u64(VDYU_ASDU(a), VDYU_ASDU(b)));
}

INLINE(Vdbu,VDBU_ORRN) (Vdbu a, Vdbu b) {return  vorn_u8(a, b);}
INLINE(Vdbi,VDBI_ORRN) (Vdbi a, Vdbi b) {return  vorn_s8(a, b);}
INLINE(Vdbc,VDBC_ORRN) (Vdbc a, Vdbc b)
{
    return  VDDU_ASBC(vorn_u64(VDBC_ASDU(a), VDBC_ASDU(b)));
}

INLINE(Vdhu,VDHU_ORRN) (Vdhu a, Vdhu b) {return  vorn_u16(a, b);}
INLINE(Vdhi,VDHI_ORRN) (Vdhi a, Vdhi b) {return  vorn_s16(a, b);}
INLINE(Vdwu,VDWU_ORRN) (Vdwu a, Vdwu b) {return  vorn_u32(a, b);}
INLINE(Vdwi,VDWI_ORRN) (Vdwi a, Vdwi b) {return  vorn_s32(a, b);}
INLINE(Vddu,VDDU_ORRN) (Vddu a, Vddu b) {return  vorn_u64(a, b);}
INLINE(Vddi,VDDI_ORRN) (Vddi a, Vddi b) {return  vorn_s64(a, b);}


INLINE(Vqyu,VQYU_ORRN) (Vqyu a, Vqyu b)
{
    return  VQDU_ASYU(vornq_u64(VQYU_ASDU(a), VQYU_ASDU(b)));
}

INLINE(Vqbu,VQBU_ORRN) (Vqbu a, Vqbu b) {return  vornq_u8(a, b);}
INLINE(Vqbi,VQBI_ORRN) (Vqbi a, Vqbi b) {return  vornq_s8(a, b);}
INLINE(Vqbc,VQBC_ORRN) (Vqbc a, Vqbc b)
{
    return  VQDU_ASBC(vornq_u64(VQBC_ASDU(a), VQBC_ASDU(b)));
}

INLINE(Vqhu,VQHU_ORRN) (Vqhu a, Vqhu b) {return  vornq_u16(a, b);}
INLINE(Vqhi,VQHI_ORRN) (Vqhi a, Vqhi b) {return  vornq_s16(a, b);}
INLINE(Vqwu,VQWU_ORRN) (Vqwu a, Vqwu b) {return  vornq_u32(a, b);}
INLINE(Vqwi,VQWI_ORRN) (Vqwi a, Vqwi b) {return  vornq_s32(a, b);}
INLINE(Vqdu,VQDU_ORRN) (Vqdu a, Vqdu b) {return  vornq_u64(a, b);}
INLINE(Vqdi,VQDI_ORRN) (Vqdi a, Vqdi b) {return  vornq_s64(a, b);}


#if _LEAVE_ARM_ORRN
}
#endif

#if _ENTER_ARM_ORRV
{
#endif

INLINE(_Bool,VWYU_ORRV) (Vwyu a)
{
    float f = VWYU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    uint64_t    w = vget_lane_u32(v, 0);
    return vtstd_u64(w, UINT64_MAX);
}

INLINE( uint8_t,VWBU_ORRV) (Vwbu a)
{
    float f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    uint32_t    l = vget_lane_u32(v, 0);
    return ((l>>24)|(l>>16)|(l>>8)|l)&UINT8_MAX;
}

INLINE(int8_t, VWBI_ORRV) (Vwbi a)
{
    return  VWBU_ORRV(VWBI_ASTU(a));
}

INLINE(char, VWBC_ORRV) (Vwbc a)
{
    return  VWBU_ORRV(VWBC_ASTU(a));
}


INLINE( uint16_t,VWHU_ORRV) (Vwhu a)
{
    float f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    uint32_t    l = vget_lane_u32(v, 0);
    return  ((l>>16)|l)&UINT16_MAX;
}

INLINE(int16_t, VWHI_ORRV) (Vwhi a)
{
    return  VWHU_ORRV(VWHI_ASTU(a));
}


INLINE(_Bool,VDYU_ORRV) (Vdyu a)
{
    uint64x1_t l = VDYU_ASDU(a);
    l = vtst_u64(l, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(l, 0);
}

INLINE( uint8_t,VDBU_ORRV) (Vdbu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    r = l|r;
    l = r>>16;
    r = l|r;
    l = r>>8;
    return l|r;
}

INLINE(  int8_t,VDBI_ORRV) (Vdbi a)
{
    return  VDBU_ORRV(VDBI_ASTU(a));
}

INLINE(   char,VDBC_ORRV) (Vdbc a)
{
    return  VDBU_ORRV(VDBC_ASTU(a));
}


INLINE(uint16_t,VDHU_ORRV) (Vdhu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    l = l|r;
    r = l>>16;
    return l|r;
}

INLINE( int16_t,VDHI_ORRV) (Vdhi a)
{
    return  VDHU_ORRV(VDHI_ASTU(a));
}


INLINE(uint32_t,VDWU_ORRV) (Vdwu a)
{
    uint32_t    l = vget_lane_u32(a, 0);
    uint32_t    r = vget_lane_u32(a, 1);
    return  l|r;
}

INLINE( int32_t,VDWI_ORRV) (Vdwi a)
{
    return  VDWU_ORRV(VDWI_ASTU(a));
}


INLINE(   _Bool,VQYU_ORRV) (Vqyu a)
{
    uint64x2_t q = VQYU_ASDU(a);
    q = vtstq_u64(q, vdupq_n_u64(UINT64_MAX));
    uint64x1_t l = vget_low_u64(q);
    uint64x1_t r = vget_high_u64(q);
    l = vorr_u64(l, r);
    return  vget_lane_u64(l, 0);
}


INLINE( uint8_t,VQBU_ORRV) (Vqbu a)
{
    uint64x2_t q = vreinterpretq_u64_u8(a);
    uint64x1_t d = vorr_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDBU_ORRV(VDDU_ASBU(d));
}

INLINE(  int8_t,VQBI_ORRV) (Vqbi a)
{
    return VQBU_ORRV(VQBI_ASTU(a));
}

INLINE(    char,VQBC_ORRV) (Vqbc a)
{
    return VQBU_ORRV(VQBC_ASTU(a));
}


INLINE(uint16_t,VQHU_ORRV) (Vqhu a)
{
    uint64x2_t q = vreinterpretq_u64_u16(a);
    uint64x1_t d = vand_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDHU_ORRV(VDDU_ASHU(d));
}

INLINE( int16_t,VQHI_ORRV) (Vqhi a)
{
    return VQHU_ORRV(VQHI_ASTU(a));
}


INLINE(uint32_t,VQWU_ORRV) (Vqwu a)
{
    uint64x2_t q = vreinterpretq_u64_u32(a);
    uint64x1_t d = vand_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDWU_ORRV(VDDU_ASWU(d));
}

INLINE( int32_t,VQWI_ORRV) (Vqwi a)
{
    return VQWU_ORRV(VQWI_ASTU(a));
}


INLINE(uint64_t,VQDU_ORRV) (Vqdu a)
{
    uint64x1_t d = vand_u64(
        vget_low_u64(a),
        vget_high_u64(a)
    );
    return  vget_lane_u64(d, 0);
}

INLINE( int64_t,VQDI_ORRV) (Vqdi a)
{
    int64x1_t d = vand_s64(
        vget_low_s64(a),
        vget_high_s64(a)
    );
    return  vget_lane_s64(d, 0);
}

#if _LEAVE_ARM_ORRV
}
#endif

#if _ENTER_ARM_XORS
{
#endif

INLINE(            _Bool,  BOOL_XORS)  (_Bool a,  _Bool b) {return a^b;}
INLINE(          uint8_t, UCHAR_XORS)  (uchar a,  uchar b) {return a^b;}
INLINE(           int8_t, SCHAR_XORS)  (schar a,  schar b) {return a^b;}
INLINE(             char,  CHAR_XORS)   (char a,   char b) {return a^b;}
INLINE(         uint16_t, USHRT_XORS) (ushort a, ushort b) {return a^b;}
INLINE(          int16_t,  SHRT_XORS)  (short a,  short b) {return a^b;}
INLINE(         uint32_t,  UINT_XORS)   (uint a,   uint b) {return a^b;}
INLINE(          int32_t,   INT_XORS)    (int a,    int b) {return a^b;}
INLINE( ULONG_STG(UTYPE), ULONG_XORS)  (ulong a,  ulong b) {return a^b;}
INLINE(  LONG_STG(ITYPE),  LONG_XORS)   (long a,   long b) {return a^b;}
INLINE(ULLONG_STG(UTYPE),ULLONG_XORS) (ullong a, ullong b) {return a^b;}
INLINE( LLONG_STG(ITYPE), LLONG_XORS)  (llong a,  llong b) {return a^b;}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,xorsqu) (QUAD_UTYPE a, QUAD_UTYPE b) {return a^b;}
INLINE(QUAD_ITYPE,xorsqi) (QUAD_ITYPE a, QUAD_ITYPE b) {return a^b;}
#endif


INLINE(Vwyu,VWYU_XORS) (Vwyu a, Vwyu b)
{
    return  WYU_ASTV(FLT_XORS(VWYU_ASTM(a), VWYU_ASTM(b)));
}

INLINE(Vwbu,VWBU_XORS) (Vwbu a, Vwbu b)
{
    return  WBU_ASTV(FLT_XORS(VWBU_ASTM(a), VWBU_ASTM(b)));
}

INLINE(Vwbi,VWBI_XORS) (Vwbi a, Vwbi b)
{
    return  WBI_ASTV(FLT_XORS(VWBI_ASTM(a), VWBI_ASTM(b)));
}

INLINE(Vwbc,VWBC_XORS) (Vwbc a, Vwbc b)
{
    return  WBC_ASTV(FLT_XORS(VWBC_ASTM(a), VWBC_ASTM(b)));
}

INLINE(Vwhu,VWHU_XORS) (Vwhu a, Vwhu b)
{
    return  WHU_ASTV(FLT_XORS(VWHU_ASTM(a), VWHU_ASTM(b)));
}

INLINE(Vwhi,VWHI_XORS) (Vwhi a, Vwhi b)
{
    return  WHI_ASTV(FLT_XORS(VWHI_ASTM(a), VWHI_ASTM(b)));
}


INLINE(Vwwu,VWWU_XORS) (Vwwu a, Vwwu b)
{
    return  WWU_ASTV(FLT_XORS(VWWU_ASTM(a), VWWU_ASTM(b)));
}

INLINE(Vwwi,VWWI_XORS) (Vwwi a, Vwwi b)
{
    return  WWI_ASTV(FLT_XORS(VWWI_ASTM(a), VWWI_ASTM(b)));
}

INLINE(Vdyu,VDYU_XORS) (Vdyu a, Vdyu b)
{
    return  VDDU_ASYU(veor_u64(VDYU_ASDU(a), VDYU_ASDU(b)));
}

INLINE(Vdbu,VDBU_XORS) (Vdbu a, Vdbu b) {return  veor_u8(a, b);}
INLINE(Vdbi,VDBI_XORS) (Vdbi a, Vdbi b) {return  veor_s8(a, b);}
INLINE(Vdbc,VDBC_XORS) (Vdbc a, Vdbc b)
{
    return  VDDU_ASBC(veor_u64(VDBC_ASDU(a), VDBC_ASDU(b)));
}

INLINE(Vdhu,VDHU_XORS) (Vdhu a, Vdhu b) {return  veor_u16(a, b);}
INLINE(Vdhi,VDHI_XORS) (Vdhi a, Vdhi b) {return  veor_s16(a, b);}
INLINE(Vdwu,VDWU_XORS) (Vdwu a, Vdwu b) {return  veor_u32(a, b);}
INLINE(Vdwi,VDWI_XORS) (Vdwi a, Vdwi b) {return  veor_s32(a, b);}
INLINE(Vddu,VDDU_XORS) (Vddu a, Vddu b) {return  veor_u64(a, b);}
INLINE(Vddi,VDDI_XORS) (Vddi a, Vddi b) {return  veor_s64(a, b);}


INLINE(Vqyu,VQYU_XORS) (Vqyu a, Vqyu b)
{
    return  VQDU_ASYU(veorq_u64(VQYU_ASDU(a), VQYU_ASDU(b)));
}

INLINE(Vqbu,VQBU_XORS) (Vqbu a, Vqbu b) {return  veorq_u8(a, b);}
INLINE(Vqbi,VQBI_XORS) (Vqbi a, Vqbi b) {return  veorq_s8(a, b);}
INLINE(Vqbc,VQBC_XORS) (Vqbc a, Vqbc b)
{
    return  VQDU_ASBC(veorq_u64(VQBC_ASDU(a), VQBC_ASDU(b)));
}

INLINE(Vqhu,VQHU_XORS) (Vqhu a, Vqhu b) {return  veorq_u16(a, b);}
INLINE(Vqhi,VQHI_XORS) (Vqhi a, Vqhi b) {return  veorq_s16(a, b);}
INLINE(Vqwu,VQWU_XORS) (Vqwu a, Vqwu b) {return  veorq_u32(a, b);}
INLINE(Vqwi,VQWI_XORS) (Vqwi a, Vqwi b) {return  veorq_s32(a, b);}
INLINE(Vqdu,VQDU_XORS) (Vqdu a, Vqdu b) {return  veorq_u64(a, b);}
INLINE(Vqdi,VQDI_XORS) (Vqdi a, Vqdi b) {return  veorq_s64(a, b);}

#if _LEAVE_ARM_XORS
}
#endif

#if _ENTER_ARM_XORN
{
#endif

INLINE(            _Bool,  BOOL_XORN)  (_Bool a,  _Bool b) {return a^~b;}
INLINE(          uint8_t, UCHAR_XORN)  (uchar a,  uchar b) {return a^~b;}
INLINE(           int8_t, SCHAR_XORN)  (schar a,  schar b) {return a^~b;}
INLINE(             char,  CHAR_XORN)   (char a,   char b) {return a^~b;}
INLINE(         uint16_t, USHRT_XORN) (ushort a, ushort b) {return a^~b;}
INLINE(          int16_t,  SHRT_XORN)  (short a,  short b) {return a^~b;}
INLINE(         uint32_t,  UINT_XORN)   (uint a,   uint b) {return a^~b;}
INLINE(          int32_t,   INT_XORN)    (int a,    int b) {return a^~b;}
INLINE( ULONG_STG(UTYPE), ULONG_XORN)  (ulong a,  ulong b) {return a^~b;}
INLINE(  LONG_STG(ITYPE),  LONG_XORN)   (long a,   long b) {return a^~b;}
INLINE(ULLONG_STG(UTYPE),ULLONG_XORN) (ullong a, ullong b) {return a^~b;}
INLINE( LLONG_STG(ITYPE), LLONG_XORN)  (llong a,  llong b) {return a^~b;}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,xornqu) (QUAD_UTYPE a, QUAD_UTYPE b) {return a^~b;}
INLINE(QUAD_ITYPE,xornqi) (QUAD_ITYPE a, QUAD_ITYPE b) {return a^~b;}
#endif


INLINE(Vwyu,VWYU_XORN) (Vwyu a, Vwyu b)
{
    return  WYU_ASTV(FLT_XORN(VWYU_ASTM(a), VWYU_ASTM(b)));
}

INLINE(Vwbu,VWBU_XORN) (Vwbu a, Vwbu b)
{
    return  WBU_ASTV(FLT_XORN(VWBU_ASTM(a), VWBU_ASTM(b)));
}

INLINE(Vwbi,VWBI_XORN) (Vwbi a, Vwbi b)
{
    return  WBI_ASTV(FLT_XORN(VWBI_ASTM(a), VWBI_ASTM(b)));
}

INLINE(Vwbc,VWBC_XORN) (Vwbc a, Vwbc b)
{
    return  WBC_ASTV(FLT_XORN(VWBC_ASTM(a), VWBC_ASTM(b)));
}


INLINE(Vwhu,VWHU_XORN) (Vwhu a, Vwhu b)
{
    return  WHU_ASTV(FLT_XORN(VWHU_ASTM(a), VWHU_ASTM(b)));
}

INLINE(Vwhi,VWHI_XORN) (Vwhi a, Vwhi b)
{
    return  WHI_ASTV(FLT_XORN(VWHI_ASTM(a), VWHI_ASTM(b)));
}


INLINE(Vwwu,VWWU_XORN) (Vwwu a, Vwwu b)
{
    return  WWU_ASTV(FLT_XORN(VWWU_ASTM(a), VWWU_ASTM(b)));
}

INLINE(Vwwi,VWWI_XORN) (Vwwi a, Vwwi b)
{
    return  WWI_ASTV(FLT_XORN(VWWI_ASTM(a), VWWI_ASTM(b)));
}


INLINE(Vdyu,VDYU_XORN) (Vdyu a, Vdyu b)
{
    uint64x1_t d = VDYU_ASDU(b);
    uint32x2_t w = vreinterpret_u32_u64(d);
    w = vmvn_u32(w);
    d = vreinterpret_u64_u32(w);
    d = veor_u64(VDYU_ASDU(a), d);
    return  VDDU_ASYU(d);
}

INLINE(Vdbu,VDBU_XORN) (Vdbu a, Vdbu b) {return  veor_u8(a, vmvn_u8(b));}           
INLINE(Vdbi,VDBI_XORN) (Vdbi a, Vdbi b) {return  veor_s8(a, vmvn_s8(b));}
INLINE(Vdbc,VDBC_XORN) (Vdbc a, Vdbc b)
{
    uint8x8_t c = VDBC_ASBU(b);
    c = vmvn_u8(c);
    c = veor_u8(VDBC_ASWU(a), c);
    return  VDBU_ASBC(c);
}

INLINE(Vdhu,VDHU_XORN) (Vdhu a, Vdhu b) {return  veor_u16(a, vmvn_u16(b));}
INLINE(Vdhi,VDHI_XORN) (Vdhi a, Vdhi b) {return  veor_s16(a, vmvn_s16(b));}

INLINE(Vdwu,VDWU_XORN) (Vdwu a, Vdwu b) {return  veor_u32(a, vmvn_u32(b));}
INLINE(Vdwi,VDWI_XORN) (Vdwi a, Vdwi b) {return  veor_s32(a, vmvn_s32(b));}

INLINE(Vddu,VDDU_XORN) (Vddu a, Vddu b) 
{
    uint8x8_t c = vreinterpret_u8_u64(b);
    c = vmvn_u8(c);
    b = vreinterpret_u64_u8(c);
    return  veor_u64(a, b);
}

INLINE(Vddi,VDDI_XORN) (Vddi a, Vddi b) 
{
    uint8x8_t c = vreinterpret_u8_s64(b);
    c = vmvn_u8(c);
    b = vreinterpret_s64_u8(c);
    return  veor_s64(a, b);
}


INLINE(Vqyu,VQYU_XORN) (Vqyu a, Vqyu b)
{
    uint64x2_t d = VQYU_ASDU(b);
    uint8x16_t q = vreinterpretq_u8_u64(d);
    q = vmvnq_u8(q);
    d = vreinterpretq_u64_u8(q);
    d = veorq_u64(VQYU_ASDU(a), d);
    return  VQDU_ASYU(d);
}

INLINE(Vqbu,VQBU_XORN) (Vqbu a, Vqbu b) {return  veorq_u8(a, b);}
INLINE(Vqbi,VQBI_XORN) (Vqbi a, Vqbi b) {return  veorq_s8(a, b);}
INLINE(Vqbc,VQBC_XORN) (Vqbc a, Vqbc b)
{
    return  VQDU_ASBC(veorq_u64(VQBC_ASDU(a), VQBC_ASDU(b)));
}

INLINE(Vqhu,VQHU_XORN) (Vqhu a, Vqhu b) {return  veorq_u16(a, b);}
INLINE(Vqhi,VQHI_XORN) (Vqhi a, Vqhi b) {return  veorq_s16(a, b);}
INLINE(Vqwu,VQWU_XORN) (Vqwu a, Vqwu b) {return  veorq_u32(a, b);}
INLINE(Vqwi,VQWI_XORN) (Vqwi a, Vqwi b) {return  veorq_s32(a, b);}
INLINE(Vqdu,VQDU_XORN) (Vqdu a, Vqdu b) 
{
    uint8x16_t c = vreinterpretq_u8_u64(b);
    c = vmvnq_u8(c);
    b = vreinterpretq_u64_u8(c);
    return  veorq_u64(a, b);
}

INLINE(Vqdi,VQDI_XORN) (Vqdi a, Vqdi b) 
{
    uint8x16_t c = vreinterpretq_u8_s64(b);
    c = vmvnq_u8(c);
    b = vreinterpretq_s64_u8(c);
    return  veorq_s64(a, b);
}

#if _LEAVE_ARM_XORN
}
#endif

#if _ENTER_ARM_XORV
{
#endif

INLINE(_Bool,VWYU_XORV) (Vwyu a)
{
    uint32_t v = VWWU_ASTV(VWYU_ASWU(a));
    v = v^(v>>16);
    v = v^(v>>8);
    v = v^(v>>4);
    v = v^(v>>2);
    return 1&(v^(v>>1));
}


INLINE( uint8_t,VWBU_XORV) (Vwbu a)
{
    uint32_t  v = VWWU_ASTV(VWBU_ASWU(a));
    v = v^(v>>16);
    v = v^(v>>8);
    return 0xff&v;
}

INLINE(  int8_t,VWBI_XORV) (Vwbi a)
{
    return VWBU_XORV(VWBI_ASTU(a));
}

INLINE(uint16_t,VWHU_XORV) (Vwhu a)
{
    uint32_t  v = VWWU_ASTV(VWHU_ASWU(a));
    v = v^(v>>16);
    return 0xffff&v;
}

INLINE( int16_t,VWHI_XORV) (Vwhi a)
{
    return VWHU_XORV(VWHI_ASTU(a));
}

INLINE(_Bool,VDYU_XORV) (Vdyu a)
{
    uint32x2_t l = VDYU_ASWU(a);
    uint32_t   x = vget_lane_u32(l, 0);
    uint32_t   y = vget_lane_u32(l, 1);
    x = x^y;
    x = (x>>16)^x;
    x = (x>>8)^x;
    x = (x>>4)^x;
    x = (x>>2)^x;
    return 1&((x>>1)^x);
}

INLINE( uint8_t,VDBU_XORV) (Vdbu a)
{
    uint32x2_t  v = vreinterpret_u32_u8(a);
    uint32_t    x = vget_lane_u32(v, 0)^vget_lane_u32(v, 1);
    x ^= (x>>16);
    return 0xff&(x^(x>>8));
}

INLINE(  int8_t,VDBI_XORV) (Vdbi a)
{
    return  VDBU_XORV(VDBI_ASTU(a));
}

INLINE(   char,VDBC_XORV) (Vdbc a)
{
    return  VDBU_XORV(VDBC_ASTU(a));
}

INLINE(uint16_t,VDHU_XORV) (Vdhu a)
{
    uint32x2_t  v = vreinterpret_u32_u16(a);
    uint32_t    x = vget_lane_u32(v, 0)^vget_lane_u32(v, 1);
    return 0xffff&((x>>16)^x);
}

INLINE( int16_t,VDHI_XORV) (Vdhi a)
{
    return  VDHU_XORV(VDHI_ASTU(a));
}

INLINE(uint32_t,VDWU_XORV) (Vdwu a)
{
    return vget_lane_u32(a, 0)^vget_lane_u32(a, 1);
}

INLINE( int32_t,VDWI_XORV) (Vdwi a)
{
    return vget_lane_s32(a, 0)^vget_lane_s32(a, 1);
}


INLINE(   _Bool,VQYU_XORV) (Vqyu a)
{
    uint64x2_t v = VQYU_ASDU(a);
    uint64_t   x = vgetq_lane_u64(v, 0)^vgetq_lane_u64(v, 1);
    x ^= x>>32;
    x ^= x>>16;
    x ^= x>>8;
    x ^= x>>4;
    x ^= x>>2;
    x ^= x>>1;
    return x&1;
}


INLINE( uint8_t,VQBU_XORV) (Vqbu a)
{
    uint64x2_t q = vreinterpretq_u64_u8(a);
    uint64x1_t d = veor_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDBU_XORV(VDDU_ASBU(d));
}

INLINE(  int8_t,VQBI_XORV) (Vqbi a)
{
    return VQBU_XORV(VQBI_ASTU(a));
}

INLINE(    char,VQBC_XORV) (Vqbc a)
{
    return VQBU_XORV(VQBC_ASTU(a));
}


INLINE(uint16_t,VQHU_XORV) (Vqhu a)
{
    uint64x2_t q = vreinterpretq_u64_u16(a);
    uint64x1_t d = veor_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDHU_XORV(VDDU_ASHU(d));
}

INLINE( int16_t,VQHI_XORV) (Vqhi a)
{
    return VQHU_XORV(VQHI_ASTU(a));
}


INLINE(uint32_t,VQWU_XORV) (Vqwu a)
{
    uint64x2_t q = vreinterpretq_u64_u32(a);
    uint64x1_t d = veor_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    return  VDWU_XORV(VDDU_ASWU(d));
}

INLINE( int32_t,VQWI_XORV) (Vqwi a)
{
    return VQWU_XORV(VQWI_ASTU(a));
}


INLINE(uint64_t,VQDU_XORV) (Vqdu a)
{
    uint64x1_t d = veor_u64(
        vget_low_u64(a),
        vget_high_u64(a)
    );
    return  vget_lane_u64(d, 0);
}

INLINE( int64_t,VQDI_XORV) (Vqdi a)
{
    int64x1_t d = veor_s64(
        vget_low_s64(a),
        vget_high_s64(a)
    );
    return  vget_lane_s64(d, 0);
}


#if _LEAVE_ARM_XORV
}
#endif

#if _ENTER_ARM_SHLL
{
#endif

INLINE( _Bool,  BOOL_SHLL)  (_Bool a, Rc(0,  1) b)
{
#define     BOOL_SHLL(A, B) ((_Bool) (A&&!(1&B)))
    return  a && !b;
}

INLINE( uchar, UCHAR_SHLL)  (uchar a, Rc(0,  UCHAR_WIDTH) b)
{
#define     UCHAR_SHLL(A, B) ((uchar) (((unsigned) A)<<B))
    return  a<<b;
}

INLINE( schar, SCHAR_SHLL)  (schar a, Rc(0,  SCHAR_WIDTH) b)
{
#define     SCHAR_SHLL(A, B) ((schar) (((unsigned) A)<<B))
    return  (unsigned) a<<b;
}

INLINE(  char,  CHAR_SHLL)   (char a, Rc(0,   CHAR_WIDTH) b)
{
#define     CHAR_SHLL(A, B) ((char) (((unsigned) A)<<B))
    return  (unsigned) a<<b;
}


INLINE(ushort, USHRT_SHLL) (ushort a, Rc(0,  USHRT_WIDTH) b)
{
#define     USHRT_SHLL(A, B) ((ushort) (((unsigned) A)<<B))
    return  a<<b;
}

INLINE( short,  SHRT_SHLL)  (short a, Rc(0,   SHRT_WIDTH) b)
{
#define     SHRT_SHLL(A, B) ((short) (((unsigned) A)<<B))
    return  (unsigned) a<<b;
}


INLINE(  uint,  UINT_SHLL)   (uint a, Rc(0,   UINT_WIDTH) b)
{
#define     UINT_SHLL(A, B) (((unsigned) A)<<B)
    return  a<<b;
}

INLINE(   int,   INT_SHLL)    (int a, Rc(0,    INT_WIDTH) b)
{
#define     INT_SHLL(A, B) ((int) UINT_SHLL(A,B))
    return  (uint) a<<b;
}


INLINE( ulong, ULONG_SHLL)  (ulong a, Rc(0,  ULONG_WIDTH) b)
{
#define     ULONG_SHLL(A, B) (((ulong) A)<<B)
    return  a<<b;
}

INLINE(  long,  LONG_SHLL)   (long a, Rc(0,   LONG_WIDTH) b)
{
#define     LONG_SHLL(A, B) ((long) ULONG_SHLL(A,B))
    return  (ulong) a<<b;
}

INLINE(ullong,ULLONG_SHLL) (ullong a, Rc(0, ULLONG_WIDTH) b)
{
#define     ULLONG_SHLL(A, B) (((ullong) A)<<B)
    return  a<<b;
}

INLINE( llong, LLONG_SHLL)  (llong a, Rc(0,  LLONG_WIDTH) b)
{
#define     LLONG_SHLL(A, B) ((llong) ULLONG_SHLL(A,B))
    return  (ullong) a<<b;
}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,shllqu) (QUAD_UTYPE a, Rc(0, 128) b) {return a<<b;}
INLINE(QUAD_ITYPE,shllqi) (QUAD_ITYPE a, Rc(0, 128) b) 
{
    return  shllqu(a, b);
}
#endif

INLINE(float,WBU_SHLL) (float a, Rc(0, 8) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vshl_u8(
                vreinterpret_u8_f32(
                    vdup_n_f32(a)
                ),
                vdup_n_s8(b)
            )
        ),
        0
    );
}

INLINE(float,WHU_SHLL) (float a, Rc(0, 16) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshl_u16(
                vreinterpret_u16_f32(
                    vdup_n_f32(a)
                ),
                vdup_n_s16(b)
            )
        ),
        0
    );
}

INLINE(float,WWU_SHLL) (float a, Rc(0, 32) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshl_u32(
                vreinterpret_u32_f32(
                    vdup_n_f32(a)
                ),
                vdup_n_s32(b)
            )
        ),
        0
    );
}

#define     DBU_SHLL(A, B) vshl_u8( A,vdup_n_s8( B))
#define     DHU_SHLL(A, B) vshl_u16(A,vdup_n_s16(B))
#define     DWU_SHLL(A, B) vshl_u32(A,vdup_n_s32(B))
#define     DDU_SHLL(A, B) vshl_u64(A,vdup_n_s64(B))

#define     QBU_SHLL(A, B) vshlq_u8( A,vdupq_n_s8( B))
#define     QHU_SHLL(A, B) vshlq_u16(A,vdupq_n_s16(B))
#define     QWU_SHLL(A, B) vshlq_u32(A,vdupq_n_s32(B))
#define     QDU_SHLL(A, B) vshlq_u64(A,vdupq_n_s64(B))


INLINE(Vwyu,VWYU_SHLL) (Vwyu a, Rc(0, 1) b)
{
    float       f = VWYU_ASTM(a);
    float32x2_t v = vdup_n_f32(f);
    uint32x2_t  l = vreinterpret_u32_f32(v);
    uint32x2_t  r = vdup_n_u32(0u-(b==0));
    l = vand_u32(l, r);
    v = vreinterpret_f32_u32(l);
    f = vget_lane_f32(v, 0);
    return  WYU_ASTV(f);
}

INLINE(Vwbu,VWBU_SHLL) (Vwbu a, Rc(0,  8) b)
{
#define     VWBU_SHLL(A, B)     \
(                               \
    (7 < B)                     \
    ?   WBU_ASTV(0.0f)          \
    :   WBU_ASTV(               \
            vget_lane_f32(      \
                VDBU_ASWF(      \
                    vshl_n_u8(  \
                        VDWF_ASBU(vdup_n_f32(VWBU_ASTM(A))),\
                        (7&B)   \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WBU_ASTV(WBU_SHLL(VWBU_ASTM(a), b));
}

INLINE(Vwbi,VWBI_SHLL) (Vwbi a, Rc(0,  8) b)
{
#define     VWBI_SHLL(A, B)     \
(                               \
    (7 < B)                     \
    ?   WBI_ASTV(0.0f)          \
    :   WBI_ASTV(               \
            vget_lane_f32(      \
                VDBU_ASWF(      \
                    vshl_n_u8(  \
                        VDWF_ASBU(\
                            vdup_n_f32(VWBI_ASTM(A))\
                        ),      \
                        (7&B)   \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WBI_ASTV(WBU_SHLL(VWBI_ASTM(a), b));
}

INLINE(Vwbc,VWBC_SHLL) (Vwbc a, Rc(0,  8) b)
{
#define     VWBC_SHLL(A, B)     \
(                               \
    (7 < B)                     \
    ?   WBC_ASTV(0.0f)          \
    :   WBC_ASTV(               \
            vget_lane_f32(      \
                VDBU_ASWF(      \
                    vshl_n_u8(  \
                        VDWF_ASBU(\
                            vdup_n_f32(VWBC_ASTM(A))\
                        ),      \
                        (7&B)   \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WBC_ASTV(WBU_SHLL(VWBC_ASTM(a), b));
}


INLINE(Vwhu,VWHU_SHLL) (Vwhu a, Rc(0, 16) b)
{
#define     VWHU_SHLL(A, B)     \
(                               \
    (15 < B)                    \
    ?   WHU_ASTV(0.0f)          \
    :   WHU_ASTV(               \
            vget_lane_f32(      \
                VDHU_ASWF(      \
                    vshl_n_u16( \
                        VDWF_ASHU(\
                            vdup_n_f32(VWHU_ASTM(A))\
                        ),      \
                        (15&B)   \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WHU_ASTV(WHU_SHLL(VWHU_ASTM(a), b));
}

INLINE(Vwhi,VWHI_SHLL) (Vwhi a, Rc(0, 16) b)
{
#define     VWHI_SHLL(A, B)     \
(                               \
    (15 < B)                    \
    ?   WHI_ASTV(0.0f)          \
    :   WHI_ASTV(               \
            vget_lane_f32(      \
                VDHU_ASWF(      \
                    vshl_n_u16( \
                        VDWF_ASHU(\
                            vdup_n_f32(VWHI_ASTM(A))\
                        ),      \
                        (15&B)  \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WHI_ASTV(WHU_SHLL(VWHI_ASTM(a), b));
}


INLINE(Vwwu,VWWU_SHLL) (Vwwu a, Rc(0, 32) b)
{
#define     VWWU_SHLL(A, B)     \
(                               \
    (31 < B)                    \
    ?   WWU_ASTV(0.0f)          \
    :   WWU_ASTV(               \
            vget_lane_f32(      \
                VDWU_ASWF(      \
                    vshl_n_u32( \
                        VDWF_ASWU(\
                            vdup_n_f32(VWWU_ASTM(A))\
                        ),      \
                        (31&B)  \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WWU_ASTV(WWU_SHLL(VWWU_ASTM(a), b));
}

INLINE(Vwwi,VWWI_SHLL) (Vwwi a, Rc(0, 32) b)
{
#define     VWWI_SHLL(A, B)     \
(                               \
    (31 < B)                    \
    ?   WWI_ASTV(0.0f)          \
    :   WWI_ASTV(               \
            vget_lane_f32(      \
                VDWU_ASWF(      \
                    vshl_n_u32( \
                        VDWF_ASWU(\
                            vdup_n_f32(VWWI_ASTM(A))\
                        ),      \
                        (31&B)  \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WWI_ASTV(WWU_SHLL(VWWI_ASTM(a), b));
}


INLINE(Vdyu,VDYU_SHLL) (Vdyu a, Rc(0, 1) b)
{
    return  VDDU_ASYU(
        vand_u64(
            VDYU_ASDU(a),
            vdup_n_u64(0ull-(b==0))
        )
    );
        
}


INLINE(Vdbu,VDBU_SHLL) (Vdbu a, Rc(0, 8) b)
{
#define     VDBU_SHLL(A, B) \
(                           \
    (7 < B)                 \
    ?   vdup_n_u8(0)        \
    :   vshl_n_u8(A, (7&B)) \
)
    return  DBU_SHLL(a, b);
}

INLINE(Vdbi,VDBI_SHLL) (Vdbi a, Rc(0, 8) b)
{
#define     VDBI_SHLL(A, B)             \
(                                       \
    (7 < B)                             \
    ?   vdup_n_s8(0)                    \
    :   vreinterpret_s8_u8(             \
            vshl_n_u8(                  \
                vreinterpret_u8_s8(A),  \
                (7&B)                   \
            )                           \
        )                               \
)
    return  VDBU_ASBI(DBU_SHLL(VDBI_ASBU(a), b));
}

INLINE(Vdbc,VDBC_SHLL) (Vdbc a, Rc(0, 8) b)
{
#define     VDBC_SHLL(A, B)     \
(                               \
    (7 < B)                     \
    ?   VDBU_ASBC(vdup_n_u8(0)) \
    :   VDBU_ASBC(              \
            vshl_n_u8(          \
                VDBC_ASBU(A),   \
                (7&B)           \
            )                   \
        )                       \
)
    return  VDBU_ASBC(DBU_SHLL(VDBC_ASBU(a), b));
}


INLINE(Vdhu,VDHU_SHLL) (Vdhu a, Rc(0, 16) b)
{
#define     VDHU_SHLL(A, B)     \
(                               \
    (15 < B)                    \
    ?   vdup_n_u16(0)           \
    :   vshl_n_u16(A, (15&B))   \
)
    return  DHU_SHLL(a, b);
}

INLINE(Vdhi,VDHI_SHLL) (Vdhi a, Rc(0, 16) b)
{
#define     VDHI_SHLL(A, B)     \
(                               \
    (15 < B)                    \
    ?   vdup_n_s16(0)           \
    :   vreinterpret_s16_u16(   \
            vshl_n_u16(         \
                VDHI_ASHU(A),   \
                (15&B)          \
            )                   \
        )                       \
)
    return  VDHU_ASHI(DHU_SHLL(VDHI_ASHU(a), b));
}


INLINE(Vdwu,VDWU_SHLL) (Vdwu a, Rc(0, 32) b)
{
#define     VDWU_SHLL(A, B)     \
(                               \
    (31 < B)                    \
    ?   vdup_n_u32(0)           \
    :   vshl_n_u32(A, (31&B))   \
)
    return  DWU_SHLL(a, b);
}

INLINE(Vdwi,VDWI_SHLL) (Vdwi a, Rc(0, 32) b)
{
#define     VDWI_SHLL(A, B)     \
(                               \
    (31 < B)                    \
    ?   vdup_n_s32(0)           \
    :   vreinterpret_s32_u32(   \
            vshl_n_u32(         \
                VDWI_ASWU(A),   \
                (31&B)          \
            )                   \
        )                       \
)
    return  VDWU_ASWI(DWU_SHLL(VDWI_ASWU(a), b));
}


INLINE(Vddu,VDDU_SHLL) (Vddu a, Rc(0, 64) b)
{
#define     VDDU_SHLL(A, B)     \
(                               \
    (63 < B)                    \
    ?   vdup_n_u64(0)           \
    :   vshl_n_u64(A, (63&B))   \
)
    return  DDU_SHLL(a, b);
}

INLINE(Vddi,VDDI_SHLL) (Vddi a, Rc(0, 64) b)
{
#define     VDDI_SHLL(A, B)     \
(                               \
    (63 < B)                    \
    ?   vdup_n_s32(0)           \
    :   vreinterpret_s64_u64(   \
            vshl_n_u64(         \
                VDDI_ASDU(A),   \
                (63&B)          \
            )                   \
        )                       \
)
    return  VDDU_ASDI(DDU_SHLL(VDDI_ASDU(a), b));
}


INLINE(Vqyu,VQYU_SHLL) (Vqyu a, Rc(0, 1) b)
{
    return  VQDU_ASYU(
        vandq_u64(
            VQYU_ASDU(a),
            vdupq_n_u64(0ull-(b==0))
        )
    );
        
}

INLINE(Vqbu,VQBU_SHLL) (Vqbu a, Rc(0, 8) b)
{
#define     VQBU_SHLL(A, B) \
(                           \
    (7 < B)                 \
    ?   vdupq_n_u8(0)       \
    :   vshlq_n_u8(A,(7&B)) \
)
    return  QBU_SHLL(a, b);
}

INLINE(Vqbi,VQBI_SHLL) (Vqbi a, Rc(0, 8) b)
{
#define     VQBI_SHLL(A, B)             \
(                                       \
    (7 < B)                             \
    ?   vdupq_n_s8(0)                   \
    :   vreinterpretq_s8_u8(            \
            vshlq_n_u8(                 \
                vreinterpretq_u8_s8(A), \
                (7&B)                   \
            )                           \
        )                               \
)
    return  VQBU_ASBI(QBU_SHLL(VQBI_ASBU(a), b));
}

INLINE(Vqbc,VQBC_SHLL) (Vqbc a, Rc(0, 8) b)
{
#define     VQBC_SHLL(A, B)     \
(                               \
    (15 < B)                    \
    ?   VQBU_ASBC(vdupq_n_u8(0))\
    :   VQBU_ASBC(              \
            vshlq_n_u8(         \
                VQBC_ASBU(A),   \
                (7&B)           \
            )                   \
        )                       \
)
    return  VQBU_ASBC(QBU_SHLL(VQBC_ASBU(a), b));
}


INLINE(Vqhu,VQHU_SHLL) (Vqhu a, Rc(0, 16) b)
{
#define     VQHU_SHLL(A, B)     \
(                               \
    (15 < B)                    \
    ?   vdupq_n_u16(0)          \
    :   vshlq_n_u16(A, (15&B))  \
)
    return  QHU_SHLL(a, b);
}

INLINE(Vqhi,VQHI_SHLL) (Vqhi a, Rc(0, 16) b)
{
#define     VQHI_SHLL(A, B)     \
(                               \
    (15 < B)                    \
    ?   vdupq_n_s16(0)          \
    :   vreinterpretq_s16_u16(  \
            vshlq_n_u16(        \
                VQHI_ASHU(A),   \
                (15&B)          \
            )                   \
        )                       \
)
    return  VQHU_ASHI(QHU_SHLL(VQHI_ASHU(a), b));
}


INLINE(Vqwu,VQWU_SHLL) (Vqwu a, Rc(0, 32) b)
{
#define     VQWU_SHLL(A, B)     \
(                               \
    (31 < B)                    \
    ?   vdupq_n_u32(0)          \
    :   vshlq_n_u32(A, (31&B))  \
)
    return  QWU_SHLL(a, b);
}

INLINE(Vqwi,VQWI_SHLL) (Vqwi a, Rc(0, 32) b)
{
#define     VQWI_SHLL(A, B)     \
(                               \
    (31 < B)                    \
    ?   vdupq_n_s32(0)          \
    :   vreinterpretq_s32_u32(  \
            vshlq_n_u32(        \
                VQWI_ASWU(A),   \
                (31&B)          \
            )                   \
        )                       \
)
    return  VQWU_ASWI(QWU_SHLL(VQWI_ASWU(a), b));
}


INLINE(Vqdu,VQDU_SHLL) (Vqdu a, Rc(0, 64) b)
{
#define     VQDU_SHLL(A, B)     \
(                               \
    (63 < B)                    \
    ?   vdupq_n_u64(0)          \
    :   vshlq_n_u64(A, (63&B))  \
)
    return  QDU_SHLL(a, b);
}

INLINE(Vqdi,VQDI_SHLL) (Vqdi a, Rc(0, 64) b)
{
#define     VQDI_SHLL(A, B)     \
(                               \
    (63 < B)                    \
    ?   vdupq_n_s32(0)          \
    :   vreinterpretq_s64_u64(  \
            vshlq_n_u64(        \
                VQDI_ASDU(A),   \
                (63&B)          \
            )                   \
        )                       \
)
    return  VQDU_ASDI(QDU_SHLL(VQDI_ASDU(a), b));
}

#if _LEAVE_ARM_SHLL
}
#endif

#if _ENTER_ARM_SHLS
{
#endif

INLINE( _Bool,  BOOL_SHLS)  (_Bool a, Rc(0, 1) b) {return a;}

INLINE( uchar, UCHAR_SHLS)  (uchar a, Rc(0,  UCHAR_WIDTH) b)
{
#define     UCHAR_SHLS(A, B) vqshlb_n_u8(A,((UCHAR_WIDTH-1)&B))
    return  vqshlb_u8(a, b);
}

INLINE( schar, SCHAR_SHLS)  (schar a, Rc(0,  SCHAR_WIDTH) b)
{
#define     SCHAR_SHLS(A, B) vqshlb_n_s8(A,((SCHAR_WIDTH-1)&B))
    return  vqshlb_s8(a, b);
}

INLINE(  char,  CHAR_SHLS)   (char a, Rc(0,   CHAR_WIDTH) b)
{
#if CHAR_MIN
#   define  CHAR_SHLS(A, B) ((char)vqshlb_n_s8(A,((CHAR_WIDTH-1)&B)))
    return  vqshlb_s8(a, b);
#else
#   define  CHAR_SHLS(A, B) ((char)vqshlb_n_u8(A,((CHAR_WIDTH-1)&B)))
    return  vqshlb_u8(a, b);
#endif
}

INLINE(ushort, USHRT_SHLS) (ushort a, Rc(0,  USHRT_WIDTH) b)
{
#define     USHRT_SHLS(A, B) vqshlh_n_s16(A,((USHRT_WIDTH-1)&B))
    return  vqshlh_u16(a, b);
}

INLINE( short,  SHRT_SHLS)  (short a, Rc(0,   SHRT_WIDTH) b)
{
#define     SHRT_SHLS(A, B) vqshlh_n_s16(A,((SHRT_WIDTH-1)&B))
    return  vqshlh_s16(a, b);
}

INLINE(  uint,  UINT_SHLS)   (uint a, Rc(0,   UINT_WIDTH) b)
{
#define     UINT_SHLS(A, B) vqshls_n_u32(A,((UINT_WIDTH-1)&B))
    return  vqshls_u32(a, b);
}

INLINE(   int,   INT_SHLS)    (int a, Rc(0,    INT_WIDTH) b)
{
#define     INT_SHLS(A, B) vqshls_n_s32(A,((INT_WIDTH-1)&B))
    return  vqshls_s32(a, b);
}

INLINE( ulong, ULONG_SHLS)  (ulong a, Rc(0,  ULONG_WIDTH) b)
{
#if DWRD_NLONG == 2
#   define  ULONG_SHLS(A, B) ((ulong) vqshls_n_u32(A, ((ULONG_WIDTH-1)&B) ))
    return  vqshls_u32(a, b);
#else
#   define  ULONG_SHLS(A, B) vqshld_n_u64(A, ((ULONG_WIDTH-1)&B) )
    return  vqshld_u64(a, b);
#endif
}

INLINE( long,   LONG_SHLS)   (long a, Rc(0,   LONG_WIDTH) b)
{
#if DWRD_NLONG == 2
#   define  LONG_SHLS(A, B) ((long) vqshls_n_s32(A, ((LONG_WIDTH-1)&B) ))
    return  vqshls_u32(a, b);
#else
#   define  LONG_SHLS(A, B) vqshld_n_s64(A, ((LONG_WIDTH-1)&B) )
    return  vqshld_s64(a, b);
#endif
}

INLINE(ullong,ULLONG_SHLS) (ullong a, Rc(0, ULLONG_WIDTH) b)
{
#if QUAD_NLLONG == 2
#   define  ULLONG_SHLS(A, B) \
(\
    (B >= 64)\
    ?   0xffffffffffffffffULL\
    :   ((ullong) vqshld_n_u64(A, ((ULLONG_WIDTH-1)&B) ))\
)
    return  vqshld_u64(a, b);
#else

#endif

}

INLINE( llong, LLONG_SHLS)  (llong a, Rc(0,  LLONG_WIDTH) b)
{
#if QUAD_NLLONG == 2
#   define  LLONG_SHLS(A, B)        \
(                                   \
    (B >= 64)                       \
    ?   (a < 0)                     \
        ?   -0x8000000000000000LL   \
        :   +0x7fffffffffffffffLL   \
    :   ((llong) vqshld_n_u64(A, ((LLONG_WIDTH-1)&B) ))\
)
    return  vqshld_s64(a, b);
#else

#endif

}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,shlsqu) (QUAD_UTYPE a, Rc(0, 128) b) 
{
    unsigned z;
    z = (a > UINT64_MAX) 
    ?   __clzll(a>>64)
    :   __clzll(a)+64;
    return  (z < b) ? (~(QUAD_UTYPE){0}) : (a<<b);
}

INLINE(QUAD_ITYPE,shlsqi) (QUAD_ITYPE a, Rc(0, 128) b) 
{
    unsigned    z;
    QUAD_TYPE   r = {.I=a};
    if (a < 0)
    {
        z = __clzll(~r.Hi.U);
        if (z == 64)
            z += __clzll(~r.Lo.U);
        if (b >= z) 
        {
            r.Hi.I = ~INT64_MAX;
            r.Lo.U = UINT64_MAX;
        }
        else 
        {
            r.I <<= b;
        }
    }
    else
    {
        z = __clzll(r.Hi.U);
        if (z == 64)
            z += __clzll(r.Lo.U);
        if (b >= z)
        {
            r.Hi.U =  INT64_MAX;
            r.Lo.U = UINT64_MAX;
        }
        else 
        {
            r.I <<= b;
        }
    }
    return  r.I;
}

#endif

INLINE(float,WBU_SHLS) (float a, Rc(0, 8) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vqshl_u8(
                vreinterpret_u8_f32(vdup_n_f32(a)),
                vdup_n_s8(b)
            )
        ),
        V2_K0
    );
}

INLINE(float,WBI_SHLS) (float a, Rc(0, 8) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_s8(
            vqshl_s8(
                vreinterpret_s8_f32(vdup_n_f32(a)),
                vdup_n_s8(b)
            )
        ),
        V2_K0
    );
}

INLINE(float,WHU_SHLS) (float a, Rc(0, 16) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vqshl_u16(
                vreinterpret_u16_f32(vdup_n_f32(a)),
                vdup_n_s16(b)
            )
        ),
        V2_K0
    );
}

INLINE(float,WHI_SHLS) (float a, Rc(0, 16) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_s16(
            vqshl_s16(
                vreinterpret_s16_f32(vdup_n_f32(a)),
                vdup_n_s16(b)
            )
        ),
        V2_K0
    );
}

INLINE(float,WWU_SHLS) (float a, Rc(0, 32) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vqshl_u32(
                vreinterpret_u32_f32(vdup_n_f32(a)),
                vdup_n_s32(b)
            )
        ),
        V2_K0
    );
}

INLINE(float,WWI_SHLS) (float a, Rc(0, 32) b)
{
    return  vget_lane_f32(
        vreinterpret_f32_s32(
            vqshl_s32(
                vreinterpret_s32_f32(vdup_n_f32(a)),
                vdup_n_s32(b)
            )
        ),
        V2_K0
    );
}

#define     DBU_SHLS(A, B) vqshl_u8( A,vdup_n_s8( B))
#define     DBI_SHLS(A, B) vqshl_s8( A,vdup_n_s8( B))
#define     DHU_SHLS(A, B) vqshl_u16(A,vdup_n_s16(B))
#define     DHI_SHLS(A, B) vqshl_s16(A,vdup_n_s16(B))
#define     DWU_SHLS(A, B) vqshl_u32(A,vdup_n_s32(B))
#define     DWI_SHLS(A, B) vqshl_s32(A,vdup_n_s32(B))
#define     DDU_SHLS(A, B) vqshl_u64(A,vdup_n_s64(B))
#define     DDI_SHLS(A, B) vqshl_s64(A,vdup_n_s64(B))

#define     QBU_SHLS(A, B) vqshlq_u8( A,vdupq_n_s8( B))
#define     QBI_SHLS(A, B) vqshlq_s8( A,vdupq_n_s8( B))
#define     QHU_SHLS(A, B) vqshlq_u16(A,vdupq_n_s16(B))
#define     QHI_SHLS(A, B) vqshlq_s16(A,vdupq_n_s16(B))
#define     QWU_SHLS(A, B) vqshlq_u32(A,vdupq_n_s32(B))
#define     QWI_SHLS(A, B) vqshlq_s32(A,vdupq_n_s32(B))
#define     QDU_SHLS(A, B) vqshlq_u64(A,vdupq_n_s64(B))
#define     QDI_SHLS(A, B) vqshlq_s64(A,vdupq_n_s64(B))

INLINE(Vwbu,VWBU_SHLS) (Vwbu a, Rc(0, 8) b)
{
#define     VWBU_SHLS(A, B)     \
(                               \
    (B > 7)                     \
    ?   VWWU_ASBU(UINT_ASTV(0xffffffffu))\
    :   WBU_ASTV(               \
            vget_lane_f32(      \
                VDBU_ASWF(      \
                    vqshl_n_u8( \
                        VDWF_ASBU(vdup_n_f32(VWBU_ASTM(A))),\
                        (7&B)   \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WBU_ASTV(WBU_SHLS(VWBU_ASTM(a), b));
}

INLINE(Vwbi,VWBI_SHLS) (Vwbi a, Rc(0, 8) b)
{
    return  WBI_ASTV(WBU_SHLS(VWBI_ASTM(a), b));
}

INLINE(Vwbc,VWBC_SHLS) (Vwbc a, Rc(0, 8) b)
{
#if CHAR_MIN
#   define  VWBC_SHLS(A, B) VWBI_ASBC(VWBI_SHLS(VWBC_ASBI(A), B))
    return  WBC_ASTV(WBI_SHLS(VWBC_ASTM(a), b));
#else
#   define  VWBC_SHLS(A, B) VWBU_ASBC(VWBU_SHLS(VWBC_ASBU(A), B))
    return  WBC_ASTV(WBU_SHLS(VWBC_ASTM(a), b));
#endif
}


INLINE(Vwhu,VWHU_SHLS) (Vwhu a, Rc(0, 16) b)
{
#define     VWHU_SHLS(A, B)     \
(                               \
    (B > 15)                    \
    ?   VWWU_ASHU(UINT_ASTV(0xffffffffu))\
    :   WHU_ASTV(               \
            vget_lane_f32(      \
                VDHU_ASWF(      \
                    vqshl_n_u16( \
                        VDWF_ASHU(\
                            vdup_n_f32(VWHU_ASTM(A))\
                        ),      \
                        (15&B)   \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WHU_ASTV(WHU_SHLS(VWHU_ASTM(a), b));
}

INLINE(Vwhi,VWHI_SHLS) (Vwhi a, Rc(0, 16) b)
{
    return  WHI_ASTV(WHI_SHLS(VWHI_ASTM(a), b));
}


INLINE(Vwwu,VWWU_SHLS) (Vwwu a, Rc(0, 32) b)
{
#define     VWWU_SHLS(A, B)     \
(                               \
    (B > 31)                    \
    ?   WWU_ASTV(0.0f)          \
    :   WWU_ASTV(               \
            vget_lane_f32(      \
                VDWU_ASWF(      \
                    vqshl_n_u32(\
                        VDWF_ASWU(\
                            vdup_n_f32(VWWU_ASTM(A))\
                        ),      \
                        (31&B)  \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WWU_ASTV(WWU_SHLS(VWWU_ASTM(a), b));
}

INLINE(Vwwi,VWWI_SHLS) (Vwwi a, Rc(0, 32) b)
{
#define     VWWI_SHLS(A, B)     \
(                               \
    (B > 31)                    \
    ?   WWI_ASTV(0.0f)          \
    :   WWI_ASTV(               \
            vget_lane_f32(      \
                VDWU_ASWF(      \
                    vqshl_n_s32(\
                        VDWF_ASWI(\
                            vdup_n_f32(VWWI_ASTM(A))\
                        ),      \
                        (31&B)   \
                    )           \
                ),              \
                V2_K0           \
            )                   \
        )                       \
)
    return  WWI_ASTV(WWU_SHLS(VWWI_ASTM(a), b));
}

INLINE(Vdyu,VDYU_SHLS) (Vdyu a, Rc(0, 1) b) {return a;}

INLINE(Vdbu,VDBU_SHLS) (Vdbu a, Rc(0, 8) b)
{
#define     VDBU_SHLS(A, B)     \
(                               \
    (B >= 8)                    \
    ?   vqshl_u8(A, B)          \
    :   vqshl_n_u8(A, (7&B))    \
)
    return  DBU_SHLS(a, b);
}

INLINE(Vdbi,VDBI_SHLS) (Vdbi a, Rc(0, 8) b)
{
#define     VDBI_SHLS(A, B)     \
(                               \
    (B >= 8)                    \
    ?   vqshl_s8(A, B)          \
    :   vqshl_n_s8(A, (7&B))    \
)
    return  DBI_SHLS(a, b);
}

INLINE(Vdbc,VDBC_SHLS) (Vdbc a, Rc(0, 8) b)
{
#if CHAR_MIN
#   define  VDBC_SHLS(A, B) VDBI_ASBC(VDBI_SHLS(VDBC_ASBI(A),B))
    return  VDBI_ASBC(DBI_SHLS(VDBC_ASTM(a), b));
#else
#   define  VDBC_SHLS(A, B) VDBU_ASBC(VDBU_SHLS(VDBC_ASBU(A),B))
    return  VDBU_ASBC(DBU_SHLS(VDBC_ASTM(a), b));
#endif
}


INLINE(Vdhu,VDHU_SHLS) (Vdhu a, Rc(0, 16) b)
{
#define     VDHU_SHLS(A, B)     \
(                               \
    (B >= 16)                   \
    ?   vqshl_u16(A, B)         \
    :   vqshl_n_u16(A, (15&B))  \
)

    return  DHU_SHLS(a, b);
}

INLINE(Vdhi,VDHI_SHLS) (Vdhi a, Rc(0, 16) b)
{
#define     VDHI_SHLS(A, B)     \
(                               \
    (B >= 16)                   \
    ?   vqshl_s16(A, B)         \
    :   vqshl_n_s16(A, (15&B))  \
)

    return  DHI_SHLS(a, b);
}


INLINE(Vdwu,VDWU_SHLS) (Vdwu a, Rc(0, 32) b)
{
#define     VDWU_SHLS(A, B)     \
(                               \
    (B >= 32)                   \
    ?   vqshl_u32(A, B)         \
    :   vqshl_n_u32(A, (31&B))  \
)

    return  DWU_SHLS(a, b);
}

INLINE(Vdwi,VDWI_SHLS) (Vdwi a, Rc(0, 32) b)
{
#define     VDWI_SHLS(A, B)     \
(                               \
    (B >= 32)                   \
    ?   vqshl_s32(A, B)         \
    :   vqshl_n_s32(A, (31&B))  \
)

    return  DWI_SHLS(a, b);
}

INLINE(Vddu,VDDU_SHLS) (Vddu a, Rc(0, 64) b)
{
#define     VDDU_SHLS(A, B)     \
(                               \
    (B >= 64)                   \
    ?   vqshl_u64(A, B)         \
    :   vqshl_n_u64(A, (63&B))  \
)
    return  DDU_SHLS(a, b);
}

INLINE(Vddi,VDDI_SHLS) (Vddi a, Rc(0, 64) b)
{
#define     VDDI_SHLS(A, B)     \
(                               \
    (B >= 64)                   \
    ?   vqshl_s64(A, B)         \
    :   vqshl_n_u64(A, (63&B))  \
)
    return  DDI_SHLS(a, b);
}

// TODO: update arm's shlsqz to same method as shlsdz
INLINE(Vqbu,VQBU_SHLS) (Vqbu a, Rc(0, 8) b)
{
#define     VQBU_SHLS(A, B) vqshlq_n_u8(A, (7&B))
    return  QBU_SHLS(a, b);
}

INLINE(Vqbi,VQBI_SHLS) (Vqbi a, Rc(0, 8) b)
{
#define     VQBI_SHLS(A, B) vqshlq_n_s8(A, (7&B))
    return  QBI_SHLS(a, b);
}

INLINE(Vqbc,VQBC_SHLS) (Vqbc a, Rc(0, CHAR_WIDTH-1) b)
{
#if CHAR_MIN
#   define  VQBC_SHLS(A, B) VQBI_ASBC(VQBI_SHLS(VQBC_ASBI(A),B))
    return  VQBI_ASBC(QBI_SHLS(VQBC_ASTM(a), b));
#else
#   define  VQBC_SHLS(A, B) VQBU_ASBC(VQBU_SHLS(VQBC_ASBU(A),B))
    return  VQBU_ASBC(QBU_SHLS(VQBC_ASTM(a), b));
#endif
}


INLINE(Vqhu,VQHU_SHLS) (Vqhu a, Rc(0, 16) b)
{
#define     VQHU_SHLS(A, B) vqshlq_n_u16(A, (15&B))
    return  QHU_SHLS(a, b);
}

INLINE(Vqhi,VQHI_SHLS) (Vqhi a, Rc(0, 16) b)
{
#define     VQHI_SHLS(A, B) vqshlq_n_s16(A, (15&B))
    return  QHI_SHLS(a, b);
}


INLINE(Vqwu,VQWU_SHLS) (Vqwu a, Rc(0, 32) b)
{
#define     VQWU_SHLS(A, B) vqshlq_n_u32(A, (31&B))
    return  QWU_SHLS(a, b);
}

INLINE(Vqwi,VQWI_SHLS) (Vqwi a, Rc(0, 32) b)
{
#define     VQWI_SHLS(A, B) vqshlq_n_s32(A, (31&B))
    return  QWI_SHLS(a, b);
}


INLINE(Vqdu,VQDU_SHLS) (Vqdu a, Rc(0, 64) b)
{
#define     VQDU_SHLS(A, B) vqshlq_n_u64(A, (63&B))
    return  QDU_SHLS(a, b);
}

INLINE(Vqdi,VQDI_SHLS) (Vqdi a, Rc(0, 64) b)
{
#define     VQDI_SHLS(A, B) vqshlq_n_s64(A, (63&B))
    return  QDI_SHLS(a, b);
}

#if _LEAVE_ARM_SHLS
}
#endif

#if _ENTER_ARM_SHL2
{
#endif

INLINE(uint16_t, UCHAR_SHL2)  (uchar a, Rc(0,  UCHAR_WIDTH) b)
{
#define     UCHAR_SHL2(A, B) ((uint16_t) (((unsigned) A)<<B))
    return  (unsigned) a<<b;
}

INLINE( int16_t, SCHAR_SHL2)  (schar a, Rc(0,  SCHAR_WIDTH) b)
{
#define     SCHAR_SHL2(A, B) ((int16_t) ((signed) A<<B))
    return  (signed) a<<b;
}

#if CHAR_MIN

INLINE( int16_t,  CHAR_SHL2)   (char a, Rc(0,   CHAR_WIDTH) b)
{
#   define  CHAR_SHL2(A, B) ((int16_t) (((signed) A)<<B))
    return  (signed) a<<b;
}
#else

INLINE( int16_t,  CHAR_SHL2)   (char a, Rc(0,   CHAR_WIDTH) b)
{
#   define  CHAR_SHL2(A, B) ((uint16_t) (((unsigned) A)<<B))
    return  (unsigned) a<<b;
}

#endif

INLINE(uint32_t, USHRT_SHL2) (ushort a, Rc(0,  USHRT_WIDTH) b)
{
#define     USHRT_SHL2(A, B) (((uint32_t) A)<<B)
    return  (uint32_t) a<<b;
}

INLINE( int32_t,  SHRT_SHL2)  (short a, Rc(0,   SHRT_WIDTH) b)
{
#define     SHRT_SHL2(A, B) (((int32_t) A)<<B)
    return  (int32_t) a<<b;
}


INLINE(uint64_t,  UINT_SHL2)   (uint a, Rc(0,   UINT_WIDTH) b)
{
#define     UINT_SHL2(A, B) (((uint64_t) A)<<B)
    return  (uint64_t) a<<b;
}

INLINE( int64_t,   INT_SHL2)    (int a, Rc(0,    INT_WIDTH) b)
{
#define     INT_SHL2(A, B) (((int64_t)A)<<B)
    return  (int64_t) a<<b;
}

#if DWRD_NLONG == 2

INLINE(uint64_t, ULONG_SHL2)   (ulong a, Rc(0, ULONG_WIDTH) b)
{
#define     ULONG_SHL2(A, B) (((uint64_t) A)<<B)
    return  (uint64_t) a<<b;
}

INLINE( int64_t,  LONG_SHL2)    (long a, Rc(0,  LONG_WIDTH) b)
{
#define     LONG_SHL2(A, B) (((int64_t)A)<<B)
    return  (int64_t) a<<b;
}

#endif

/*
    NOTE: vshll_n_u8(v, n) has two entries in arm's NEON
    docs; one for when 0 <= n <= 7 and another for n == 8.
    Some implementations allow specifying 9..15, implicitly
    converting the call to:
        r = vshll_n_u8(v, 8);
        r = vshlq_n_u16(r,n-8);
    However, we're not going to use this.
*/

#define WBU_SHL2(A, B)              \
(                                   \
    (B > 7)                         \
    ?   vzip1_u8(                   \
            vdup_n_u8(0),           \
            vreinterpret_u8_f32(    \
                vdup_n_f32(A)       \
            )                       \
        )                           \
    :   vshl_n_u16(                 \
            vzip1_u8(               \
                vreinterpret_u8_f32(\
                    vdup_n_f32(A)   \
                )                   \
            ),                      \
            (7&B)                   \
        )                           \
)

#define WBI_SHL2(A, B)              \
(                                   \
    (B > 7)                         \
    ?   vget_low_s16(               \
            vshll_n_s8(             \
                vreinterpret_s8_f32(\
                    vdup_n_f32(A)   \
                ),                  \
                8                   \
            )                       \
        )                           \
    :   vget_low_s16(               \
            vshll_n_s8(             \
                vreinterpret_s8_f32(\
                    vdup_n_f32(A)   \
                ),                  \
                (7&B)               \
            )                       \
        )                           \
)


#define WHU_SHL2(A, B)                      \
(                                           \
    (B > 15)                                \
    ?   vreinterpret_u32_u16(               \
            vzip1_u16(                      \
                vdup_n_u16(0),              \
                vreinterpret_u16_f32(       \
                    vdup_n_f32(A)           \
                )                           \
            )                               \
        )                                   \
    :   vshl_n_u32(                         \
            vreinterpret_u32_u16(           \
                vzip1_u16(                  \
                    vreinterpret_u16_f32(   \
                        vdup_n_f32(A)       \
                    ),                      \
                    vdup_n_u16(0)           \
                )                           \
            ),                              \
            (15&B)                          \
        )                                   \
)

#define WHI_SHL2(A, B)                      \
(                                           \
    (B > 15)                                \
    ?   vget_low_s32(                       \
            vmovl_s16(                      \
                vreinterpret_s16_f32(       \
                    vdup_n_f32(A)           \
                )                           \
            )                               \
        )                                   \
    :   vget_low_s32(                       \
            vshll_n_s16(                    \
                vreinterpret_s16_f32(       \
                    vdup_n_f32(A)           \
                ),                          \
                (15&B)                      \
            )                               \
        )                                   \
)

#define WWU_SHL2(A, B)                      \
(                                           \
    (B > 31)                                \
    ?   vreinterpret_u64_u32(               \
            vzip1_u32(                      \
                vdup_n_u32(0),              \
                vreinterpret_u32_f32(       \
                    vdup_n_f32(A)           \
                )                           \
            )                               \
        )                                   \
    :   vshl_n_u64(                         \
            vreinterpret_u64_u32(           \
                vzip1_u32(                  \
                    vreinterpret_u32_f32(   \
                        vdup_n_f32(A)       \
                    ),                      \
                    vdup_n_u32(0)           \
                )                           \
            ),                              \
            (31&B)                          \
        )                                   \
)

#define WWI_SHL2(A, B)                      \
(                                           \
    (B > 31)                                \
    ?   vget_low_s64(                       \
            vmovl_s32(                      \
                vreinterpret_s32_f32(       \
                    vdup_n_f32(A)           \
                )                           \
            )                               \
        )                                   \
    :   vget_low_s64(                       \
            vshll_n_s32(                    \
                vreinterpret_s32_f32(       \
                    vdup_n_f32(A)           \
                ),                          \
                (31&B)                      \
            )                               \
        )                                   \
)
#if 0
#define     VWBU_SHL2(A, B) \
(                           \
    (B > 7)                 \
    ?   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u8_f32(vdup_n_f32(VWBU_ASTM(A))),\
                8           \
            )               \
        )                   \
    :   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u8_f32(vdup_n_f32(VWBU_ASTM(A))),\
                7&B         \
            )               \
        )                   \
)
#define     VWBI_SHL2(A, B) \
(                           \
    (B > 7)                 \
    ?   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s8_f32(vdup_n_f32(VWBI_ASTM(A))),\
                8           \
            )               \
        )                   \
    :   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s8_f32(vdup_n_f32(VWBI_ASTM(A))),\
                7&B         \
            )               \
        )                   \
)
#define     VWBC_SHL2(A, B) \
(                           \
    (B > 7)                 \
    ?   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s8_f32(vdup_n_f32(VWBC_ASTM(A))),\
                8           \
            )               \
        )                   \
    :   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s8_f32(vdup_n_f32(VWBC_ASTM(A))),\
                7&B         \
            )               \
        )                   \
)
#define     VWBC_SHL2(A, B) \
(                           \
    (B > 7)                 \
    ?   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u8_f32(vdup_n_f32(VWBC_ASTM(A))),\
                8           \
            )               \
        )                   \
    :   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u8_f32(vdup_n_f32(VWBC_ASTM(A))),\
                7&B         \
            )               \
        )                   \
)
#define     VWHU_SHL2(A, B) \
(                           \
    (B > 15)                \
    ?   vget_low_u32(       \
            vshll_n_u16(    \
                vreinterpret_u16_f32(vdup_n_f32(VWHU_ASTM(A))),\
                16          \
            )               \
        )                   \
    :   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u16_f32(vdup_n_f32(VWHU_ASTM(A))),\
                15&B        \
            )               \
        )                   \
)

#define     VWWI_SHL2(A, B) \
(                           \
    (B > 31)                \
    ?   vget_low_s32(       \
            vshll_n_s32(    \
                vreinterpret_s32_f32(vdup_n_f32(VWWI_ASTM(A))),\
                32          \
            )               \
        )                   \
    :   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s32_f32(vdup_n_f32(VWWI_ASTM(A))),\
                31&B        \
            )               \
        )                   \
)
#define     VWWU_SHL2(A, B) \
(                           \
    (B > 31)                \
    ?   vget_low_u32(       \
            vshll_n_u32(    \
                vreinterpret_u32_f32(vdup_n_f32(VWWU_ASTM(A))),\
                32          \
            )               \
        )                   \
    :   vget_low_u16(       \
            vshll_n_u8(     \
                vreinterpret_u32_f32(vdup_n_f32(VWWU_ASTM(A))),\
                31&B        \
            )               \
        )                   \
)
#define     VWHI_SHL2(A, B) \
(                           \
    (B > 15)                \
    ?   vget_low_s32(       \
            vshll_n_s16(    \
                vreinterpret_s16_f32(vdup_n_f32(VWHI_ASTM(A))),\
                16          \
            )               \
        )                   \
    :   vget_low_s16(       \
            vshll_n_s8(     \
                vreinterpret_s16_f32(vdup_n_f32(VWHI_ASTM(A))),\
                15&B        \
            )               \
        )                   \
)
#endif

INLINE(Vdhu,VWBU_SHL2) (Vwbu a, Rc(0,  8) b)
{
#define     VWBU_SHL2(A, B) WBU_SHL2(VWBU_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWBU_ASTM(a));
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  q = vmovl_u8(d);
    q = vshlq_u16(q, vdupq_n_s16(b));
    return  vget_low_u16(q);
}

INLINE(Vdhi,VWBI_SHL2) (Vwbi a, Rc(0,  8) b)
{
#define     VWBI_SHL2(A, B) WBI_SHL2(VWBI_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWBI_ASTM(a));
    int8x8_t    d = vreinterpret_s8_f32(m);
    int16x8_t   q = vmovl_s8(d);
    q = vshlq_s16(q, vdupq_n_s16(b));
    return  vget_low_s16(q);
}

#if CHAR_MIN

INLINE(Vdhi,VWBC_SHL2) (Vwbc a, Rc(0,  8) b)
{

#define     VWBC_SHL2(A, B) WBI_SHL2(VWBC_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWBC_ASTM(a));
    int8x8_t    d = vreinterpret_s8_f32(m);
    int16x8_t   q = vmovl_s8(d);
    q = vshlq_s16(q, vdupq_n_s16(b));
    return  vget_low_s16(q);
}

#else

INLINE(Vdhu,VWBC_SHL2) (Vwbc a, Rc(0,  8) b)
{
#define     VWBC_SHL2(A, B) WBU_SHL2(VWBC_ASTM(A),B)

    float32x2_t m = vdup_n_f32(VWBC_ASTM(a));
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  q = vmovl_u8(d);
    q = vshlq_u16(q, vdupq_n_s16(b));
    return  vget_low_u16(q);
}

#endif


INLINE(Vdwu,VWHU_SHL2) (Vwhu a, Rc(0, 16) b)
{
#define     VWHU_SHL2(A, B) WHU_SHL2(VWHU_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWHU_ASTM(a));
    uint16x4_t  d = vreinterpret_u16_f32(m);
    uint32x4_t  q = vmovl_u16(d);
    q = vshlq_u32(q, vdupq_n_s32(b));
    return  vget_low_u32(q);
}

INLINE(Vdwi,VWHI_SHL2) (Vwhi a, Rc(0, 16) b)
{
#define     VWHI_SHL2(A, B) WHI_SHL2(VWHI_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWHI_ASTM(a));
    uint16x4_t  d = vreinterpret_s16_f32(m);
    uint32x4_t  q = vmovl_s16(d);
    q = vshlq_s32(q, vdupq_n_s32(b));
    return  vget_low_s32(q);
}


INLINE(Vddu,VWWU_SHL2) (Vwwu a, Rc(0, 32) b)
{
#define     VWWU_SHL2(A, B) WWU_SHL2(VWWU_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWWU_ASTM(a));
    uint32x2_t  d = vreinterpret_u32_f32(m);
    uint64x2_t  q = vmovl_u32(d);
    q = vshlq_u64(q, vdupq_n_s64(b));
    return  vget_low_u64(q);
}

INLINE(Vddi,VWWI_SHL2) (Vwwi a, Rc(0, 32) b)
{
#define     VWWI_SHL2(A, B) WWI_SHL2(VWWI_ASTM(A),B)
    float32x2_t m = vdup_n_f32(VWWI_ASTM(a));
    uint32x2_t  d = vreinterpret_s32_f32(m);
    uint64x2_t  q = vmovl_s32(d);
    q = vshlq_s64(q, vdupq_n_s64(b));
    return  vget_low_s64(q);
}


INLINE(Vqhu,VDBU_SHL2) (Vdbu a, Rc(0,  8) b)
{
#define     VDBU_SHL2(A, B)     \
(                               \
    (B > 7)                     \
    ?   vshll_n_u8(A, (8))      \
    :   vshll_n_u8(A, (7&B))    \
)
    return  vshlq_u16(vmovl_u8(a), vdupq_n_s16(b));
}

INLINE(Vqhi,VDBI_SHL2) (Vdbi a, Rc(0,  8) b)
{
#define     VDBI_SHL2(A, B)     \
(                               \
    (B > 7)                     \
    ?   vshll_n_s8(A, (8))      \
    :   vshll_n_s8(A, (7&B))    \
)
    return  vshlq_s16(vmovl_s8(a), vdupq_n_s16(b));
}

#if CHAR_MIN

INLINE(Vqhi,VDBC_SHL2) (Vdbc a, Rc(0,  8) b)
{
#define     VDBC_SHL2(A, B)             \
(                                       \
    (B > 7)                             \
    ?   vshll_n_s8(VDBC_ASBI(A),(8))    \
    :   vshll_n_s8(VDBC_ASBI(A),(7&B))  \
)
    int16x8_t   v = vmovl_s8(VDBC_ASTM(a));
    return  vshlq_s16(v, vdupq_n_s16(b));
}

#else

INLINE(Vqhu,VDBC_SHL2) (Vdbc a, Rc(0,  8) b)
{
#define     VDBC_SHL2(A, B)             \
(                                       \
    (B > 7)                             \
    ?   vshll_n_u8(VDBC_ASBU(A),(8))    \
    :   vshll_n_u8(VDBC_ASBU(A),(7&B))  \
)
    uint16x8_t  v = vmovl_u8(VDBC_ASTM(a));
    return  vshlq_u16(v, vdupq_n_s16(b));
}

#endif


INLINE(Vqwu,VDHU_SHL2) (Vdhu a, Rc(0, 16) b)
{
#define     VDHU_SHL2(A, B)     \
(                               \
    (B > 15)                    \
    ?   vshll_n_u16(A,(16))     \
    :   vshll_n_u16(A,(15&B))   \
)
    return  vshlq_u32(vmovl_u16(a), vdupq_n_s32(b));
}

INLINE(Vqwi,VDHI_SHL2) (Vdhi a, Rc(0, 16) b)
{
#define     VDHI_SHL2(A, B)     \
(                               \
    (B > 15)                    \
    ?   vshll_n_s16(A,(16))     \
    :   vshll_n_s16(A,(15&B))   \
)
    return  vshlq_s32(vmovl_s16(a), vdupq_n_s32(b));
}


INLINE(Vqdu,VDWU_SHL2) (Vdwu a, Rc(0, 32) b)
{
#define     VDWU_SHL2(A, B)     \
(                               \
    (B > 31)                    \
    ?   vshll_n_u32(A,(32))     \
    :   vshll_n_u32(A,(31&B))   \
)
    return  vshlq_u64(vmovl_u32(a), vdupq_n_s64(b));
}

INLINE(Vqdi,VDWI_SHL2) (Vdwi a, Rc(0, 32) b)
{
#define     VDWI_SHL2(A, B)     \
(                               \
    (B > 63)                    \
    ?   vshll_n_s64(A,(64))     \
    :   vshll_n_u64(A,(63&B))   \
)
    return  vshlq_s64(vmovl_s32(a), vdupq_n_s64(b));
}


#if _LEAVE_ARM_SHL2
}
#endif

#if _ENTER_ARM_SHLR
{
#endif

INLINE( _Bool,  BOOL_SHLR)  (_Bool a, Rc(0,  1) b)
{
#define     BOOL_SHLR(A, B) ((_Bool) (A&&B))
    return  a && b;
}


INLINE( uchar, UCHAR_SHLR)  (uchar a, Rc(0,  UCHAR_WIDTH) b)
{
#define     UCHAR_SHLR(A, B) ((uchar)((A&UCHAR_MAX)>>(UCHAR_WIDTH-B)))
    return  a>>(UCHAR_WIDTH-b);
}

INLINE( schar, SCHAR_SHLR)  (schar a, Rc(0,  SCHAR_WIDTH) b)
{
#define     SCHAR_SHLR(A, B) ((schar)(((int) A)>>(SCHAR_WIDTH-B)))
    return  (schar) (((int) a)>>(SCHAR_WIDTH-b));
}

INLINE(  char,  CHAR_SHLR)   (char a, Rc(0,   CHAR_WIDTH) b)
{
#if CHAR_MIN
#   define  CHAR_SHLR(A, B) ((char)(((int) A)>>(CHAR_WIDTH-B)))
    return  (char) (((int) a)>>(CHAR_WIDTH-b));
#else
#   define  CHAR_SHLR(A, B) ((char)((A&UCHAR_MAX)>>(CHAR_WIDTH-B)))
    return  a>>(CHAR_WIDTH-b);
#endif
}


INLINE(ushort, USHRT_SHLR)  (ushort a, Rc(0,  USHRT_WIDTH) b)
{
#define     USHRT_SHLR(A, B) ((ushort)((A&USHRT_MAX)>>(USHRT_WIDTH-B)))
    return  a>>(USHRT_WIDTH-b);
}

INLINE( short,  SHRT_SHLR)  (short a, Rc(0,   SHRT_WIDTH) b)
{
#define     SHRT_SHLR(A, B) ((short)(((int) A)>>(SHRT_WIDTH-B)))
    return  a>>(SHRT_WIDTH-b);
}


INLINE(  uint,  UINT_SHLR)   (uint a, Rc(0,   UINT_WIDTH) b)
{
#define     UINT_SHLR(A, B) ((uint)((A&UINT_MAX)>>(UINT_WIDTH-B)))
    return  a>>(UINT_WIDTH-b);
}

INLINE(   int,   INT_SHLR)    (int a, Rc(0,    INT_WIDTH) b)
{
#define     INT_SHLR(A, B) (((int) A)>>(INT_WIDTH-B))
    return  a>>(INT_WIDTH-b);
}


INLINE( ulong, ULONG_SHLR)  (ulong a, Rc(0,  ULONG_WIDTH) b)
{
#define ULONG_SHLR(A, B) ((ulong)((A&ULONG_MAX)>>(ULONG_WIDTH-B)))
    return  a>>(ULONG_WIDTH-b);
}

INLINE(  long,  LONG_SHLR)   (long a, Rc(0,   LONG_WIDTH) b)
{
#define     LONG_SHLR(A, B) (((long) A)>>(LONG_WIDTH-B))
    return  a>>(LONG_WIDTH-b);
}


INLINE(ullong,ULLONG_SHLR) (ullong a, Rc(0, ULLONG_WIDTH) b)
{
#define ULLONG_SHLR(A, B) ((ullong)((A&ULLONG_MAX)>>(ULLONG_WIDTH-B)))
    return  a>>(ULLONG_WIDTH-b);
}

INLINE( llong, LLONG_SHLR)  (llong a, Rc(0,  LLONG_WIDTH) b)
{
#define     LLONG_SHLR(A, B) (((llong) A)>>(LLONG_WIDTH-B))
    return  a>>(LLONG_WIDTH-b);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,shlrqu) (QUAD_UTYPE a, Rc(0, 128) b) 
{
    return  a>>(128-b);
}

INLINE(QUAD_ITYPE,shlrqi) (QUAD_ITYPE a, Rc(0, 128) b) 
{
    return  a>>(128-b);
}

#endif


INLINE(Vwyu,VWYU_SHLR) (Vwyu a, Rc(0, 1) b)
{
#define     VWYU_SHLR(A, B) ((B==1)?A:((Vwyu){0}))
    return  (b==1) ? a : ((Vwyu){0});
}


INLINE(Vwbu,VWBU_SHLR) (Vwbu a, Rc(0, 8) b)
{
#define     VWBU_SHLR(A, B)                         \
(                                                   \
    (B > 7)                                         \
    ?   A                                           \
    :   WBU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vshr_n_u8(                      \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBU_ASTM(A))\
                        ),                          \
                        (8-(B*(B<8)))               \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWBU_ASTM(a));
    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  q = vmovl_u8(d);
    q = vshlq_u16(q, vdupq_n_s16(b));
    uint8x16_t  r = vreinterpretq_u8_u16(q);
    d = vget_low_u8(r);
    d = vuzp2_u8(d, d);
    m = vreinterpret_f32_u8(d);
    return  WBU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwbi,VWBI_SHLR) (Vwbi a, Rc(0, 8) b)
{
#define     VWBI_SHLR(A, B)                         \
(                                                   \
    (B > 7)                                         \
    ?   A                                           \
    :   WBI_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_s8(                \
                    vshr_n_s8(                      \
                        vreinterpret_s8_f32(        \
                            vdup_n_f32(VWBI_ASTM(A))\
                        ),                          \
                        (8-(B*(B<8)))               \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWBI_ASTM(a));
    int8x8_t    d = vreinterpret_s8_f32(m);
    int16x8_t   q = vmovl_s8(d);
    q = vshlq_s16(q, vdupq_n_s16(b));
    uint8x16_t  r = vreinterpretq_s8_s16(q);
    d = vget_low_s8(r);
    d = vuzp2_s8(d, d);
    m = vreinterpret_f32_s8(d);
    return  WBI_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwbc,VWBC_SHLR) (Vwbc a, Rc(0, 8) b)
{
    float32x2_t m = vdup_n_f32(VWBC_ASTM(a));
#if CHAR_MIN
#   define  VWBC_SHLR(A, B)                         \
(                                                   \
    (B > 7)                                         \
    ?   A                                           \
    :   WBC_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_s8(                \
                    vshr_n_s8(                      \
                        vreinterpret_s8_f32(        \
                            vdup_n_f32(VWBC_ASTM(A))\
                        ),                          \
                        (8-(B*(B<8)))               \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)


    int8x8_t    d = vreinterpret_s8_f32(m);
    int16x8_t   q = vmovl_s8(d);
    q = vshlq_s16(q, vdupq_n_s16(b));
    uint8x16_t  r = vreinterpretq_s8_s16(q);
    d = vget_low_s8(r);
    d = vuzp2_s8(d, d);
    m = vreinterpret_f32_s8(d);

#else

#define     VWBC_SHLR(A, B)                         \
(                                                   \
    (B > 7)                                         \
    ?   A                                           \
    :   WBU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vshr_n_u8(                      \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBC_ASTM(A))\
                        ),                          \
                        (8-(B*(B<8)))               \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    uint8x8_t   d = vreinterpret_u8_f32(m);
    uint16x8_t  q = vmovl_u8(d);
    q = vshlq_u16(q, vdupq_n_s16(b));
    uint8x16_t  r = vreinterpretq_u8_u16(q);
    d = vget_low_u8(r);
    d = vuzp2_u8(d, d);
    m = vreinterpret_f32_u8(d);

#endif

    return  WBC_ASTV(vget_lane_f32(m, 0));
    
}


INLINE(Vwhu,VWHU_SHLR) (Vwhu a, Rc(0, 16) b)
{
#define     VWHU_SHLR(A, B)                         \
(                                                   \
    (B > 15)                                        \
    ?   A                                           \
    :   WHU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u16(               \
                    vshr_n_u16(                     \
                        vreinterpret_u16_f32(       \
                            vdup_n_f32(VWHU_ASTM(A))\
                        ),                          \
                        (16-(B*(B<16)))             \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWHU_ASTM(a));
    uint16x4_t  d = vreinterpret_u16_f32(m);
    uint32x4_t  q = vmovl_u16(d);
    q = vshlq_u32(q, vdupq_n_s32(b));
    uint16x8_t  r = vreinterpretq_u16_u32(q);
    d = vget_low_u16(r);
    d = vuzp2_u16(d, d);
    m = vreinterpret_f32_u16(d);
    return  WHU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwhi,VWHI_SHLR) (Vwhi a, Rc(0, 16) b)
{
#define     VWHI_SHLR(A, B)                         \
(                                                   \
    (B > 15)                                        \
    ?   A                                           \
    :   WHI_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_s16(               \
                    vshr_n_s16(                     \
                        vreinterpret_s16_f32(       \
                            vdup_n_f32(VWHI_ASTM(A))\
                        ),                          \
                        (16-(B*(B<16)))             \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWHI_ASTM(a));
    int16x4_t   d = vreinterpret_s16_f32(m);
    int32x4_t   q = vmovl_s16(d);
    q = vshlq_s32(q, vdupq_n_s32(b));
    int16x8_t   r = vreinterpretq_s16_s32(q);
    d = vget_low_s16(r);
    d = vuzp2_s16(d, d);
    m = vreinterpret_f32_s16(d);
    return  WHI_ASTV(vget_lane_f32(m, 0));
}


INLINE(Vwwu,VWWU_SHLR) (Vwwu a, Rc(0, 32) b)
{
#define     VWWU_SHLR(A, B)                         \
(                                                   \
    (B > 31)                                        \
    ?   A                                           \
    :   WWU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u32(               \
                    vshr_n_u32(                     \
                        vreinterpret_u32_f32(       \
                            vdup_n_f32(VWWU_ASTM(A))\
                        ),                          \
                        (32-(B*(B<32)))             \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWWU_ASTM(a));
    uint32x2_t  d = vreinterpret_u32_f32(m);
    uint64x2_t  q = vmovl_u32(d);
    q = vshlq_u64(q, vdupq_n_s64(b));
    uint32x4_t  r = vreinterpretq_u32_u64(q);
    d = vget_low_u32(r);
    d = vuzp2_u32(d, d);
    m = vreinterpret_f32_u32(d);
    return  WWU_ASTV(vget_lane_f32(m, 0));
}

INLINE(Vwwi,VWWI_SHLR) (Vwwi a, Rc(0, 32) b)
{
#define     VWWI_SHLR(A, B)                         \
(                                                   \
    (B > 31)                                        \
    ?   A                                           \
    :   WWI_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_s32(               \
                    vshr_n_s32(                     \
                        vreinterpret_s32_f32(       \
                            vdup_n_f32(VWWI_ASTM(A))\
                        ),                          \
                        (32-(B*(B<32)))             \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float32x2_t m = vdup_n_f32(VWWI_ASTM(a));
    int32x2_t   d = vreinterpret_s32_f32(m);
    int64x2_t   q = vmovl_s32(d);
    q = vshlq_s64(q, vdupq_n_s64(b));
    uint32x4_t  r = vreinterpretq_s32_s64(q);
    d = vget_low_s32(r);
    d = vuzp2_s32(d, d);
    m = vreinterpret_f32_s32(d);
    return  WWI_ASTV(vget_lane_f32(m, 0));
}



INLINE(Vdyu,VDYU_SHLR) (Vdyu a, Rc(0, 1) b)
{
#define     VDYU_SHLR(A, B) ((B==1)?A:((Vdyu){0}))
    return  (b==1) ? a : ((Vdyu){0});
}


INLINE(Vdbu,VDBU_SHLR) (Vdbu a, Rc(0, 8) b)
{
#define     VDBU_SHLR(A, B) \
(                           \
    (B > 7)                 \
    ?   A                   \
    :   vshr_n_u8(          \
            A,              \
            (8-B*(B<8))     \
        )                   \
)
    uint16x8_t  q = vmovl_u8(a);
    q = vshlq_u16(q, vdupq_n_s16(b));
    uint8x16_t  v = vreinterpretq_u8_u16(q);
    return  vqtbl1_u8(
        v,
        vcreate_u8(0x0f0d0b0907050301ULL)
    );
}

INLINE(Vdbi,VDBI_SHLR) (Vdbi a, Rc(0, 8) b)
{
#define     VDBI_SHLR(A, B) \
(                           \
    (B > 7)                 \
    ?   A                   \
    :   vshr_n_s8(          \
            A,              \
            (8-B*(B<8))     \
        )                   \
)
    int16x8_t   q = vmovl_s8(a);
    q = vshlq_s16(q, vdupq_n_s16(b));
    int8x16_t   v = vreinterpretq_s8_s16(q);
    return  vqtbl1_s8(
        v,
        vcreate_u8(0x0f0d0b0907050301ULL)
    );
}

INLINE(Vdbc,VDBC_SHLR) (Vdbc a, Rc(0, 8) b)
{
#if CHAR_MIN
#   define  VDBC_SHLR(A, B)     \
(                               \
    (B > 7)                     \
    ?   A                       \
    :   VDBI_ASBC(              \
            vshr_n_s8(          \
                VDBC_ASBI(A),   \
                (8-B*(B<8))     \
            )                   \
        )                       \
)
    int16x8_t   q = vmovl_s8(VDBC_ASBI(a));
    q = vshlq_s16(q, vdupq_n_s16(b));
    int8x16_t   v = vreinterpretq_s8_s16(q);
    return  VDBI_ASBC(
        vqtbl1_s8(
            v,
            vcreate_u8(0x0f0d0b0907050301ULL)
        )
    );
#else
#   define  VDBC_SHLR(A, B)     \
(                               \
    (B > 7)                     \
    ?   A                       \
    :   VDBU_ASBC(              \
            vshr_n_u8(          \
                VDBC_ASBU(A),   \
                (8-B*(B<8))     \
            )                   \
        )                       \
)
    uint16x8_t  q = vmovl_u8(VDBC_ASBU(a));
    q = vshlq_u16(q, vdupq_n_s16(b));
    uint8x16_t  v = vreinterpretq_u8_u16(q);
    return  VDBU_ASBC(
        vqtbl1_u8(
            v,
            vcreate_u8(0x0f0d0b0907050301ULL)
        )
    );

#endif

}



INLINE(Vdhu,VDHU_SHLR) (Vdhu a, Rc(0, 16) b)
{
#define     VDHU_SHLR(A, B) \
(                           \
    (B > 15)                \
    ?   A                   \
    :   vshr_n_u16(         \
            A,              \
            (16-B*(B<16))   \
        )                   \
)
    uint32x4_t  q = vmovl_u16(a);
    q = vshlq_u32(q, vdupq_n_s32(b));
    uint8x16_t  v = vreinterpretq_u8_u32(q);
    return  vreinterpret_u16_u8(
        vqtbl1_u8(
            v,
            vcreate_u8(0x0f0e0b0a07060302ULL)
        )
    );
}

INLINE(Vdhi,VDHI_SHLR) (Vdhi a, Rc(0, 16) b)
{
#define     VDHI_SHLR(A, B) \
(                           \
    (B > 15)                \
    ?   A                   \
    :   vshr_n_s16(         \
            A,              \
            (16-B*(B<16))   \
        )                   \
)
    int32x4_t   q = vmovl_s16(a);
    q = vshlq_s32(q, vdupq_n_s32(b));
    uint8x16_t  v = vreinterpretq_u8_s32(q);
    return  vreinterpret_s16_u8(
        vqtbl1_u8(
            v,
            vcreate_u8(0x0f0e0b0a07060302ULL)
        )
    );
}


INLINE(Vdwu,VDWU_SHLR) (Vdwu a, Rc(0, 32) b)
{
#define     VDWU_SHLR(A, B) \
(                           \
    (B > 31)                \
    ?   A                   \
    :   vshr_n_u32(         \
            A,              \
            (32-B*(B<32))   \
        )                   \
)
    uint64x2_t  q = vmovl_u32(a);
    q = vshlq_u64(q, vdupq_n_s64(b));
    uint8x16_t  v = vreinterpretq_u8_u64(q);
    return  vreinterpret_u32_u8(
        vqtbl1_u8(
            v,
            vcreate_u8(0x0f0e0d0c07060504ULL)
        )
    );
}

INLINE(Vdwi,VDWI_SHLR) (Vdwi a, Rc(0, 32) b)
{
#define     VDWI_SHLR(A, B) \
(                           \
    (B > 31)                \
    ?   A                   \
    :   vshr_n_s32(         \
            A,              \
            (32-B*(B<32))   \
        )                   \
)
    int64x2_t   q = vmovl_s32(a);
    q = vshlq_s64(q, vdupq_n_s64(b));
    uint8x16_t  v = vreinterpretq_u8_s64(q);
    return  vreinterpret_s32_u8(
        vqtbl1_u8(
            v,
            vcreate_u8(0x0f0e0d0c07060504ULL)
        )
    );
}


INLINE(Vddu,VDDU_SHLR) (Vddu a, Rc(0, 64) b)
{
#define     VDDU_SHLR(A, B) \
(                           \
    (B > 63)                \
    ?   A                   \
    :   vshr_n_u64(         \
            A,              \
            (64-B*(B<64))   \
        )                   \
)
    return  vshl_u64(a, vdup_n_s64(b-64));
}

INLINE(Vddi,VDDI_SHLR) (Vddi a, Rc(0, 64) b)
{
#define     VDDI_SHLR(A, B) \
(                           \
    (B > 63)                \
    ?   A                   \
    :   vshr_n_s64(         \
            A,              \
            (64-B*(B<64))   \
        )                   \
)
    return  vshl_s64(a, vdup_n_s64(b-64));
}


INLINE(Vqyu,VQYU_SHLR) (Vqyu a, Rc(0, 1) b)
{
#define     VQYU_SHLR(A, B) ((B==1)?A:((Vqyu){0}))
    return  (b==1) ? a : ((Vqyu){0});
}

INLINE(Vqbu,VQBU_SHLR) (Vqbu a, Rc(0, 8) b)
{
#define     VQBU_SHLR(A, B) \
(                           \
    (B > 7)                 \
    ?   A                   \
    :   vshrq_n_u8(         \
            A,              \
            (8-B*(B<8))     \
        )                   \
)
    return  vshlq_u8(a, vdupq_n_s8(b-8));
}

INLINE(Vqbi,VQBI_SHLR) (Vqbi a, Rc(0, 8) b)
{
#define     VQBI_SHLR(A, B) \
(                           \
    (B > 7)                 \
    ?   A                   \
    :   vshrq_n_s8(         \
            A,              \
            (8-B*(B<8))     \
        )                   \
)
    return  vshlq_s8(a, vdupq_n_s8(b-8));
}

INLINE(Vqbc,VQBC_SHLR) (Vqbc a, Rc(0, 8) b)
{
#if CHAR_MIN
#   define  VQBC_SHLR(A, B)     \
(                               \
    (B > 7)                     \
    ?   A                       \
    :   VQBI_ASBC(              \
            vshrq_n_s8(         \
                VQBC_ASBI(A),   \
                (8-B*(B<8))     \
            )                   \
        )                       \
)
    return  VQBI_ASBC(
        vshlq_s8(
            VQBC_ASBI(a), 
            vdupq_n_s8(b-8)
        )
    );
#else
#   define  VQBC_SHLR(A, B)     \
(                               \
    (B > 7)                     \
    ?   A                       \
    :   VQBU_ASBC(              \
            vshrq_n_u8(         \
                VQBC_ASBU(A),   \
                (8-B*(B<8))     \
            )                   \
        )                       \
)
    return  VQBU_ASBC(
        vshlq_u8(
            VQBC_ASBU(a), 
            vdupq_n_u8(b-8)
        )
    );
#endif

}


INLINE(Vqhu,VQHU_SHLR) (Vqhu a, Rc(0, 16) b)
{
#define     VQHU_SHLR(A, B) \
(                           \
    (B > 15)                \
    ?   A                   \
    :   vshrq_n_u16(        \
            A,              \
            (16-B*(B<16))   \
        )                   \
)
    return  vshlq_u16(a, vdupq_n_s16(b-16));
}

INLINE(Vqhi,VQHI_SHLR) (Vqhi a, Rc(0, 16) b)
{
#define     VQHI_SHLR(A, B) \
(                           \
    (B > 15)                \
    ?   A                   \
    :   vshrq_n_s16(        \
            A,              \
            (16-B*(B<16))   \
        )                   \
)
    return  vshlq_s16(a, vdupq_n_s16(b-16));
}


INLINE(Vqwu,VQWU_SHLR) (Vqwu a, Rc(0, 32) b)
{
#define     VQWU_SHLR(A, B) \
(                           \
    (B > 31)                \
    ?   A                   \
    :   vshrq_n_u32(        \
            A,              \
            (32-B*(B<32))   \
        )                   \
)
    return  vshlq_u32(a, vdupq_n_s32(b-32));
}

INLINE(Vqwi,VQWI_SHLR) (Vqwi a, Rc(0, 32) b)
{
#define     VQWI_SHLR(A, B) \
(                           \
    (B > 31)                \
    ?   A                   \
    :   vshrq_n_s32(        \
            A,              \
            (32-B*(B<32))   \
        )                   \
)
    return  vshlq_s32(a, vdupq_n_s32(b-32));
}


INLINE(Vqdu,VQDU_SHLR) (Vqdu a, Rc(0, 64) b)
{
#define     VQDU_SHLR(A, B) \
(                           \
    (B > 63)                \
    ?   A                   \
    :   vshrq_n_u64(        \
            A,              \
            (64-B*(B<64))   \
        )                   \
)
    return  vshlq_u64(a, vdupq_n_s64(b-64));
}

INLINE(Vqdi,VQDI_SHLR) (Vqdi a, Rc(0, 64) b)
{
#define     VQDI_SHLR(A, B) \
(                           \
    (B > 63)                \
    ?   A                   \
    :   vshrq_n_s64(        \
            A,              \
            (64-B*(B<64))   \
        )                   \
)
    return  vshlq_s64(a, vdupq_n_s64(b-64));
}

#if _LEAVE_ARM_SHLR
}
#endif

#if _ENTER_ARM_SHRS
{
#endif

INLINE( _Bool,  BOOL_SHRS)  (_Bool a, Rc(0,  1) b)
{
#define     BOOL_SHRS(A, B)  ((_Bool)(A&&!B))
    return  a && !b;
}

INLINE( uchar, UCHAR_SHRS)  (uchar a, Rc(0,  UCHAR_WIDTH) b)
{
#define     UCHAR_SHRS(A, B)    ((uchar)(((unsigned) A)>>B))
    return  a>>b;
}

INLINE( schar, SCHAR_SHRS)  (schar a, Rc(0,  SCHAR_WIDTH) b)
{
#define     SCHAR_SHRS(A, B)    vqshlb_s8(A,(0-B))
    return  SCHAR_SHRS(a, b);
}

INLINE(  char,  CHAR_SHRS)   (char a, Rc(0,   CHAR_WIDTH) b)
{
#if CHAR_MIN
#   define  CHAR_SHRS(A, B) ((char) vqshlb_s8(A,(0-B)))
#else
#   define  CHAR_SHRS(A, B) ((char)(((unsigned) A)>>B))
#endif
    return  CHAR_SHRS(a, b);
}

INLINE(ushort, USHRT_SHRS) (ushort a, Rc(0,  USHRT_WIDTH) b)
{
#define     USHRT_SHRS(A, B)    ((ushort)(((unsigned) A)>>B))
    return  a>>b;
}

INLINE( short,  SHRT_SHRS)  (short a, Rc(0,   SHRT_WIDTH) b)
{
#define     SHRT_SHRS(A, B)    vqshlh_s16(A,(0-B))
    return  SHRT_SHRS(a, b);
}

INLINE(  uint,  UINT_SHRS)   (uint a, Rc(0,   UINT_WIDTH) b)
{
#define     UINT_SHRS(A, B)    (((unsigned) A)>>B)
    return  a>>b;
}

INLINE(   int,   INT_SHRS)    (int a, Rc(0,    INT_WIDTH) b)
{
#define     INT_SHRS(A, B)    vqshls_s32(A,(0-B))
    return  INT_SHRS(a, b);
}

INLINE( ulong, ULONG_SHRS)  (ulong a, Rc(0,  ULONG_WIDTH) b)
{
#define     ULONG_SHRS(A, B) (((unsigned long) A)>>B)
    return  a>>b;
}

INLINE(  long,  LONG_SHRS)   (long a, Rc(0,   LONG_WIDTH) b)
{
#if DWRD_NLONG == 2
#   define  LONG_SHRS(A, B)  ((long) vqshls_s32(A,(0-B)))
#else
#   define  LONG_SHRS(A, B)  vqshld_s64(A, (0-B))
#endif
    return  LONG_SHRS(a, b);
}

INLINE(ullong,ULLONG_SHRS) (ullong a, Rc(0, ULLONG_WIDTH) b)
{
#define     ULLONG_SHRS(A, B)    (((ullong) A)>>B)
    return  a>>b;
}

INLINE( llong, LLONG_SHRS)  (llong a, Rc(0,  LLONG_WIDTH) b)
{
#if QUAD_NLONG == 2
#   define  LLONG_SHRS(A, B)  ((llong) vqshld_s64(A, (0-B)))
#else
#   error "does vqshlq_s128 exist yet?"
#endif
    return  LLONG_SHRS(a, b);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE, shrsqu) (QUAD_UTYPE a, Rc(0, 128) b)
{
#define         shrsqu(A, B) (((QUAD_UTYPE) A)>>B)
    return  a>>b;
}

INLINE(QUAD_ITYPE, shrsqi) (QUAD_ITYPE a, Rc(0, 128) b)
{
#define         shrsqi(A, B) (((QUAD_ITYPE) A)>>B)
    return  a>>b;
}

#endif

INLINE(Vwyu,VWYU_SHRS) (Vwyu a, Rc(0, 1) b)
{
    float       f = VWYU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  l = vreinterpret_u32_f32(m);
    uint32x2_t  r = vdup_n_u32(0u-(b==0));
    l = vand_u32(l, r);
    m = vreinterpret_u32_f32(l);
    f = vget_lane_f32(m, 0);
    return WYU_ASTV(f);
}


INLINE(Vwbu,VWBU_SHRS) (Vwbu a, Rc(0, 8) b)
{
#define     WBU_SHRS(A, B)      \
vget_lane_f32(                  \
    VDBU_ASWF(                  \
        vshl_u8(                \
            VDWF_ASBU(          \
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s8(0-B)      \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     VWBU_SHRS(A, B) WBU_ASTV(WBU_SHRS(VWBU_ASTM(A), B))
    return  VWBU_SHRS(a, b);
}

INLINE(Vwbi,VWBI_SHRS) (Vwbi a, Rc(0, 8) b)
{
#define     WBI_SHRS(A, B)      \
vget_lane_f32(                  \
    vreinterpret_f32_s8(        \
        vshl_s8(                \
            vreinterpret_s8_f32(\
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s8(0-B)      \
        )                       \
    ),                          \
    V2_K0                       \
)
#define     VWBI_SHRS(A, B) WBI_ASTV(WBI_SHRS(VWBI_ASTM(A), B))
    return  VWBI_SHRS(a, b);
}

INLINE(Vwbc,VWBC_SHRS) (Vwbc a, Rc(0, CHAR_WIDTH) b)
{
#if CHAR_MIN
#   define  VWBC_SHRS(A, B) WBC_ASTV(WBI_SHRS(VWBC_ASTM(A), B))
#else
#   define  VWBC_SHRS(A, B) WBC_ASTV(WBU_SHRS(VWBC_ASTM(A), B))
#endif
    return  VWBC_SHRS(a, b);
}


INLINE(Vwhu,VWHU_SHRS) (Vwhu a, Rc(0, 16) b)
{
#define     WHU_SHRS(A, B)      \
vget_lane_f32(                  \
    vreinterpret_f32_u16(       \
        vshl_u16(               \
            VDWF_ASHU(          \
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s16(0-B)     \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     VWHU_SHRS(A, B) WHU_ASTV(WHU_SHRS(VWHU_ASTM(A), B))
    return  VWHU_SHRS(a, b);
}

INLINE(Vwhi,VWHI_SHRS) (Vwhi a, Rc(0, 16) b)
{
#define     WHI_SHRS(A, B)      \
vget_lane_f32(                  \
    vreinterpret_f32_s16(       \
        vshl_s16(               \
            VDWF_ASHI(          \
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s16(0-B)     \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     VWHI_SHRS(A, B) WHI_ASTV(WHI_SHRS(VWHI_ASTM(A), B))
    return  VWHI_SHRS(a, b);
}


INLINE(Vwwu,VWWU_SHRS) (Vwwu a, Rc(0, 32) b)
{
#define     WWU_SHRS(A, B)      \
vget_lane_f32(                  \
    vreinterpret_f32_u32(       \
        vshl_u32(               \
            VDWF_ASWU(          \
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s32(0-B)     \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     VWWU_SHRS(A, B) WWU_ASTV(WWU_SHRS(VWWU_ASTM(A), B))
    return  VWWU_SHRS(a, b);
}

INLINE(Vwwi,VWWI_SHRS) (Vwwi a, Rc(0, 32) b)
{
#define     WWI_SHRS(A, B)      \
vget_lane_f32(                  \
    vreinterpret_f32_s32(       \
        vshl_s32(               \
            VDWF_ASWI(          \
                vdup_n_f32(A)   \
            ),                  \
            vdup_n_s32(0-B)     \
        )                       \
    ),                          \
    V2_K0                       \
)

#define     VWWI_SHRS(A, B) WWI_ASTV(WWI_SHRS(VWWI_ASTM(A), B))
    return  VWWI_SHRS(a, b);
}


INLINE(Vdyu,VDYU_SHRS) (Vdyu a, Rc(0, 1) b)
{
    uint64x1_t l = VDYU_ASDU(a);
    uint64x1_t r = vdup_n_u64(UINT64_C(0)-(b==0));
    l = vand_u64(l, r);
    return  DYU_ASTV(l);
}


INLINE(Vdbu,VDBU_SHRS) (Vdbu a, Rc(0, 8) b)
{
#define     VDBU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   vshr_n_u8(A, 16)                \
    :   vshr_n_u8(A,((B&7)+(B==0)))     \
)
    return  vshl_u8(a, vneg_s8(vdup_n_s8(b)));
}

INLINE(Vdbi,VDBI_SHRS) (Vdbi a, Rc(0, 8) b)
{
#define     VDBI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   vshr_n_s8(A, 8)                 \
    :   vshr_n_s8(A,((B&7)+(B==0)))     \
)
    return  vshl_s8(a, vneg_s8(vdup_n_s8(b)));

}

INLINE(Vdbc,VDBC_SHRS) (Vdbc a, Rc(0, CHAR_WIDTH) b)
{
#if CHAR_MIN
#   define  VDBC_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   VDBI_ASBC(vshr_n_s8(VDBC_ASBI(A), 8))               \
    :   VDBI_ASBC(vshr_n_s8(VDBC_ASBI(A),((B&7)+(B==0))))   \
)

    return  VDBI_ASBC(vshl_s8(VDBC_ASBI(a), vneg_s8(vdup_n_s8(b))));
    
#else
#   define  VDBC_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   VDBU_ASBC(vshr_n_u8(VDBC_ASBU(A), 8))               \
    :   VDBU_ASBC(vshr_n_u8(VDBC_ASBU(A),((B&7)+(B==0))))   \
)

    return  VDBU_ASBC(vshl_u8(VDBC_ASBU(a), vneg_s8(vdup_n_s8(b))));

#endif

}


INLINE(Vdhu,VDHU_SHRS) (Vdhu a, Rc(0, 16) b)
{
#define     VDHU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==16)                  \
    ?   vshr_n_u16(A, 16)               \
    :   vshr_n_u16(A,((B&15)+(B==0)))   \
)
    return  vshl_u16(a, vneg_s16(vdup_n_s16(b)));
}

INLINE(Vdhi,VDHI_SHRS) (Vdhi a, Rc(0, 16) b)
{
#define     VDHI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==16)                  \
    ?   vshr_n_s16(A, 16)               \
    :   vshr_n_s16(A,((B&15)+(B==0)))   \
)
    return  vshl_s16(a, vneg_s16(vdup_n_s16(b)));

}


INLINE(Vdwu,VDWU_SHRS) (Vdwu a, Rc(0, 32) b)
{
#define     VDWU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==32)                  \
    ?   vshr_n_u32(A,32)                \
    :   vshr_n_u32(A,((B&31)+(B==0)))   \
)
    return  vshl_u32(a, vneg_s32(vdup_n_s32(b)));
}

INLINE(Vdwi,VDWI_SHRS) (Vdwi a, Rc(0, 32) b)
{
#define     VDWI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==32)                  \
    ?   vshr_n_s32(A,32)                \
    :   vshr_n_s32(A,((B&31)+(B==0)))   \
)
    return  vshl_s32(a, vneg_s32(vdup_n_s32(b)));

}


INLINE(Vddu,VDDU_SHRS) (Vddu a, Rc(0, 64) b)
{
#define     VDDU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==64)                  \
    ?   vshr_n_u64(A,64)                \
    :   vshr_n_u64(A,((B&63)+(B==0)))   \
)
    return  vshl_u64(a, vneg_s64(vdup_n_s64(b)));
}

INLINE(Vddi,VDDI_SHRS) (Vddi a, Rc(0, 64) b)
{
#define     VDDI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==64)                  \
    ?   vshr_n_s64(A,64)                \
    :   vshr_n_s64(A,((B&63)+(B==0)))   \
)
    return  vshl_s64(a, vneg_s64(vdup_n_s64(b)));

}


INLINE(Vqyu,VQYU_SHRS) (Vqyu a, Rc(0, 1) b)
{
    uint64x2_t l = VQYU_ASDU(a);
    uint64x2_t r = vdupq_n_u64(UINT64_C(0)-(b==0));
    l = vandq_u64(l, r);
    return  QYU_ASTV(l);
}


INLINE(Vqbu,VQBU_SHRS) (Vqbu a, Rc(0, 8) b)
{
#define     VQBU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   vshrq_n_u8(A, 16)               \
    :   vshrq_n_u8(A,((B&7)+(B==0)))    \
)
    return  vshlq_u8(a, vnegq_s8(vdupq_n_s8(b)));
}

INLINE(Vqbi,VQBI_SHRS) (Vqbi a, Rc(0, 8) b)
{
#define     VQBI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   vshrq_n_s8(A, 8)                \
    :   vshrq_n_s8(A,((B&7)+(B==0)))    \
)
    return  vshlq_s8(a, vnegq_s8(vdupq_n_s8(b)));

}

INLINE(Vqbc,VQBC_SHRS) (Vqbc a, Rc(0, 8) b)
{
#if CHAR_MIN
#   define  VQBC_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   VQBI_ASBC(vshrq_n_s8(VQBC_ASBI(A), 8))               \
    :   VQBI_ASBC(vshrq_n_s8(VQBC_ASBI(A),((B&7)+(B==0))))   \
)

    return  VQBI_ASBC(vshlq_s8(VQBC_ASBI(a), vnegq_s8(vdupq_n_s8(b))));
    
#else
#   define  VQBC_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==8)                   \
    ?   VQBU_ASBC(vshrq_n_u8(VQBC_ASBU(A), 8))               \
    :   VQBU_ASBC(vshrq_n_u8(VQBC_ASBU(A),((B&7)+(B==0))))   \
)

    return  VQBU_ASBC(vshlq_u8(VQBC_ASBU(a), vnegq_s8(vdupq_n_s8(b))));

#endif

}


INLINE(Vqhu,VQHU_SHRS) (Vqhu a, Rc(0, 16) b)
{
#define     VQHU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==16)                  \
    ?   vshrq_n_u16(A, 16)              \
    :   vshrq_n_u16(A,((B&15)+(B==0)))  \
)
    return  vshlq_u16(a, vnegq_s16(vdupq_n_s16(b)));
}

INLINE(Vqhi,VQHI_SHRS) (Vqhi a, Rc(0, 16) b)
{
#define     VQHI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==16)                  \
    ?   vshrq_n_s16(A, 16)              \
    :   vshrq_n_s16(A,((B&15)+(B==0)))  \
)
    return  vshlq_s16(a, vnegq_s16(vdupq_n_s16(b)));

}


INLINE(Vqwu,VQWU_SHRS) (Vqwu a, Rc(0, 32) b)
{
#define     VQWU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==32)                  \
    ?   vshrq_n_u32(A,32)               \
    :   vshrq_n_u32(A,((B&31)+(B==0)))  \
)
    return  vshlq_u32(a, vnegq_s32(vdupq_n_s32(b)));
}

INLINE(Vqwi,VQWI_SHRS) (Vqwi a, Rc(0, 32) b)
{
#define     VQWI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==32)                  \
    ?   vshrq_n_s32(A,32)               \
    :   vshrq_n_s32(A,((B&31)+(B==0)))  \
)
    return  vshlq_s32(a, vnegq_s32(vdupq_n_s32(b)));

}


INLINE(Vqdu,VQDU_SHRS) (Vqdu a, Rc(0, 64) b)
{
#define     VQDU_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==64)                  \
    ?   vshrq_n_u64(A,64)               \
    :   vshrq_n_u64(A,((B&63)+(B==0)))  \
)
    return  vshlq_u64(a, vnegq_s64(vdupq_n_s64(b)));
}

INLINE(Vqdi,VQDI_SHRS) (Vqdi a, Rc(0, 64) b)
{
#define     VQDI_SHRS(A, B)             \
(                                       \
    (!B) ? A : (B==64)                  \
    ?   vshrq_n_s64(A,64)               \
    :   vshrq_n_s64(A,((B&63)+(B==0)))  \
)
    return  vshlq_s64(a, vnegq_s64(vdupq_n_s64(b)));

}

#if _LEAVE_ARM_SHRS
}
#endif

#if _ENTER_ARM_SILL
{
#endif

INLINE( _Bool,  BOOL_SILL)   (_Bool a,  _Bool b, Rc(0,  1) c)
{
#define     BOOL_SILL(A, B, C)  ((_Bool)(C ? B : A))
    return  c ? b : a;
}

INLINE( uchar, UCHAR_SILL)  (uchar a,  uchar b, Rc(0,  UCHAR_WIDTH) c)
{
#define     UCHAR_SILL(A, B, C)    \
((uchar)((((unsigned) A)<<C)|(B&(UCHAR_MAX>>(UCHAR_WIDTH-C)))))

    return  (a<<c)|(b&(UCHAR_MAX>>(UCHAR_WIDTH-c)));
}

INLINE(  char,  CHAR_SILL)   (char a,   char b, Rc(0,   CHAR_WIDTH) c)
{
#   define  CHAR_SILL(A, B) \
((char)((((unsigned) A)<<C)|(B&(UCHAR_MAX>>(CHAR_WIDTH-C)))))
    return  ((unsigned) a<<c)|(b&(UCHAR_MAX>>(CHAR_WIDTH-c)));
}

INLINE(ushort, USHRT_SILL) (ushort a, ushort b, Rc(0,  USHRT_WIDTH) c)
{
#define     USHRT_SILL(A, B, C)    \
((ushort)((((unsigned) A)<<C)|(B&(USHRT_MAX>>(USHRT_WIDTH-C)))))

    return  (a<<c)|(b&(USHRT_MAX>>(USHRT_WIDTH-c)));
}

INLINE(  uint,  UINT_SILL)   (uint a,   uint b, Rc(0,   UINT_WIDTH) c)
{
#define     UINT_SILL(A, B, C)    \
((unsigned)((((unsigned) A)<<C)|(B&(UINT_MAX>>(UINT_WIDTH-C)))))

    return  (a<<c)|(b&(UINT_MAX>>(UINT_WIDTH-c)));
}

INLINE( ulong, ULONG_SILL)  (ulong a,  ulong b, Rc(0,  ULONG_WIDTH) c)
{
#define     ULONG_SILL(A, B, C)    \
((ulong)((((ulong) A)<<C)|(B&(ULONG_MAX>>(ULONG_WIDTH-C)))))

    return  (a<<c)|(b&(ULONG_MAX>>(ULONG_WIDTH-c)));
}

INLINE(ullong,ULLONG_SILL) (ullong a, ullong b, Rc(0, ULLONG_WIDTH) c)            
{
#define     ULLONG_SILL(A, B, C)    \
((ullong)((((ullong) A)<<C)|(B&(ULLONG_MAX>>(ULLONG_WIDTH-C)))))

    return  (a<<c)|(b&(ULLONG_MAX>>(ULLONG_WIDTH-c)));
}


INLINE(Vwyu,VWYU_SILL) (Vwyu a, Vwyu b, Rc(0, 1) c)
{
    return c == 0 ? a : b;
}


INLINE(Vwbu,VWBU_SILL) (Vwbu a, Vwbu b, Rc(0, 8) c)
{
#define     VWBU_SILL(A, B, C)                      \
(                                                   \
    (C > 7)                                         \
    ?   B                                           \
    :   WBU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vsli_n_u8(                      \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBU_ASTM(B))\
                        ),                          \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(VWBU_ASTM(A))\
                        ),                          \
                        (7&C)                       \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float       s0 = VWBU_ASTM(a);
    uint8x8_t   v0 = vreinterpret_u8_f32(vdup_n_f32(s0));

    float       s1 = VWBU_ASTM(b);
    uint8x8_t   v1 = vreinterpret_u8_f32(vdup_n_f32(s1));

    uint8x8_t   v2 = vdup_n_u8(UINT8_MAX);
    v2 = vshl_u8(v2, vdup_n_s8(c-8));
    v1 = vand_u8(v1, v2);
    v0 = vshl_u8(v0, vdup_n_s8(c));
    v0 = vorr_u8(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u8(v0), 0);
    return  WBU_ASTV(s0);
}


INLINE(Vwhu,VWHU_SILL) (Vwhu a, Vwhu b, Rc(0, 16) c)
{
#define     VWHU_SILL(A, B, C)                          \
(                                                       \
    (C > 15)                                            \
    ?   B                                               \
    :   WHU_ASTV(                                       \
            vget_lane_f32(                              \
                vreinterpret_f32_u16(                   \
                    vsli_n_u16(                         \
                        vreinterpret_u16_f32(           \
                            vdup_n_f32(VWHU_ASTM(B))    \
                        ),                              \
                        vreinterpret_u16_f32(           \
                            vdup_n_f32(VWHU_ASTM(A))    \
                        ),                              \
                        (15&C)                          \
                    )                                   \
                ),                                      \
                0                                       \
            )                                           \
        )                                               \
)

    float       s0 = VWHU_ASTM(a);
    uint16x4_t  v0 = vreinterpret_u16_f32(vdup_n_f32(s0));

    float       s1 = VWHU_ASTM(b);
    uint16x4_t  v1 = vreinterpret_u16_f32(vdup_n_f32(s1));

    uint16x4_t  v2 = vdup_n_u16(UINT16_MAX);
    v2 = vshl_u16(v2, vdup_n_s16(c-16));
    v1 = vand_u16(v1, v2);
    v0 = vshl_u16(v0, vdup_n_s16(c));
    v0 = vorr_u16(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u16(v0), 0);
    return  WHU_ASTV(s0);
}


INLINE(Vwwu,VWWU_SILL) (Vwwu a, Vwwu b, Rc(0, 32) c)
{
#define     VWWU_SILL(A, B, C)                          \
(                                                       \
    (C > 31)                                            \
    ?   B                                               \
    :   WWU_ASTV(                                       \
            vget_lane_f32(                              \
                vreinterpret_f32_u32(                   \
                    vsli_n_u32(                         \
                        vreinterpret_u32_f32(           \
                            vdup_n_f32(VWWU_ASTM(B))    \
                        ),                              \
                        vreinterpret_u32_f32(           \
                            vdup_n_f32(VWWU_ASTM(A))    \
                        ),                              \
                        (31&C)                          \
                    )                                   \
                ),                                      \
                0                                       \
            )                                           \
        )                                               \
)

    float       s0 = VWWU_ASTM(a);
    uint32x2_t  v0 = vreinterpret_u32_f32(vdup_n_f32(s0));

    float       s1 = VWWU_ASTM(b);
    uint32x2_t  v1 = vreinterpret_u32_f32(vdup_n_f32(s1));

    uint32x2_t  v2 = vdup_n_u32(UINT32_MAX);
    v2 = vshl_u32(v2, vdup_n_s32(c-16));
    v1 = vand_u32(v1, v2);
    v0 = vshl_u32(v0, vdup_n_s32(c));
    v0 = vorr_u32(v0, v1);
    s0 = vget_lane_f32(vreinterpret_f32_u32(v0), 0);
    return  WWU_ASTV(s0);
}



INLINE(Vdyu,VDYU_SILL) (Vdyu a, Vdyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vdbu,VDBU_SILL) (Vdbu a, Vdbu b, Rc(0, 8) c)
{
#define     VDBU_SILL(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsli_n_u8(B,A,(7&C))    \
)
    uint8x8_t   m = vdup_n_u8(UINT8_MAX);
    m = vshl_u8(m, vdup_n_s8(c-8));
    b = vand_u8(b, m);
    a = vshl_u8(a, vdup_n_s8(c));
    return  vorr_u8(a, b);
}

INLINE(Vdbc,VDBC_SILL) (Vdbc a, Vdbc b, Rc(0, 8) c)
{
#define     VDBC_SILL(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   VDBU_ASBC(              \
            vsli_n_u8(          \
                VDBC_ASBU(B),   \
                VDBC_ASBU(A),   \
                (7&C)           \
            )                   \
        )                       \
)
    return  VDBU_ASBC((VDBU_SILL)(VDBC_ASBU(a), VDBC_ASBU(b), c));
}

INLINE(Vdhu,VDHU_SILL) (Vdhu a, Vdhu b, Rc(0, 16) c)
{
#define     VDHU_SILL(A, B, C)  \
(                               \
    (C > 15)                    \
    ?   B                       \
    :   vsli_n_u16(B,A,(15&C))  \
)
    uint16x4_t   m = vdup_n_u16(UINT16_MAX);
    m = vshl_u16(m, vdup_n_s16(c-16));
    b = vand_u16(b, m);
    a = vshl_u16(a, vdup_n_s16(c));
    return  vorr_u16(a, b);
}

INLINE(Vdwu,VDWU_SILL) (Vdwu a, Vdwu b, Rc(0, 32) c)
{
#define     VDWU_SILL(A, B, C)  \
(                               \
    (C > 31)                    \
    ?   B                       \
    :   vsli_n_u32(B,A,(31&C))  \
)
    uint32x2_t   m = vdup_n_u32(UINT32_MAX);
    m = vshl_u32(m, vdup_n_s32(c-32));
    b = vand_u32(b, m);
    a = vshl_u32(a, vdup_n_s32(c));
    return  vorr_u32(a, b);
}

INLINE(Vddu,VDDU_SILL) (Vddu a, Vddu b, Rc(0, 64) c)
{
#define     VDDU_SILL(A, B, C)  \
(                               \
    (C > 63)                    \
    ?   B                       \
    :   vsli_n_u64(B,A,(63&C))  \
)
    uint64x1_t   m = vdup_n_u64(UINT64_MAX);
    m = vshl_u64(m, vdup_n_s64(c-64));
    b = vand_u64(b, m);
    a = vshl_u64(a, vdup_n_s64(c));
    return  vorr_u64(a, b);
}


INLINE(Vqyu,VQYU_SILL) (Vqyu a, Vqyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vqbu,VQBU_SILL) (Vqbu a, Vqbu b, Rc(0, 8) c)
{
#define     VQBU_SILL(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsliq_n_u8(B,A,(7&C))    \
)
    uint8x16_t  m = vdupq_n_u8(UINT8_MAX);
    m = vshlq_u8(m, vdupq_n_s8(c-8));
    b = vandq_u8(b, m);
    a = vshlq_u8(a, vdupq_n_s8(c));
    return  vorrq_u8(a, b);
}

INLINE(Vqbc,VQBC_SILL) (Vqbc a, Vqbc b, Rc(0, 8) c)
{
#define     VQBC_SILL(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   VQBU_ASBC(              \
            vsliq_n_u8(         \
                VQBC_ASBU(B),   \
                VQBC_ASBU(A),   \
                (7&C)           \
            )                   \
        )                       \
)
    return  VQBU_ASBC((VQBU_SILL)(VQBC_ASBU(a), VQBC_ASBU(b), c));
}

INLINE(Vqhu,VQHU_SILL) (Vqhu a, Vqhu b, Rc(0, 16) c)
{
#define     VQHU_SILL(A, B, C)  \
(                               \
    (C > 15)                    \
    ?   B                       \
    :   vsliq_n_u16(B,A,(15&C)) \
)
    uint16x8_t   m = vdupq_n_u16(UINT16_MAX);
    m = vshlq_u16(m, vdupq_n_s16(c-16));
    b = vandq_u16(b, m);
    a = vshlq_u16(a, vdupq_n_s16(c));
    return  vorrq_u16(a, b);
}

INLINE(Vqwu,VQWU_SILL) (Vqwu a, Vqwu b, Rc(0, 32) c)
{
#define     VQWU_SILL(A, B, C)  \
(                               \
    (C > 31)                    \
    ?   B                       \
    :   vsliq_n_u32(B,A,(31&C)) \
)
    uint32x4_t   m = vdupq_n_u32(UINT32_MAX);
    m = vshlq_u32(m, vdupq_n_s32(c-32));
    b = vandq_u32(b, m);
    a = vshlq_u32(a, vdupq_n_s32(c));
    return  vorrq_u32(a, b);
}

INLINE(Vqdu,VQDU_SILL) (Vqdu a, Vqdu b, Rc(0, 64) c)
{
#define     VQDU_SILL(A, B, C)  \
(                               \
    (C > 63)                    \
    ?   B                       \
    :   vsliq_n_u64(B,A,(63&C)) \
)
    uint64x2_t   m = vdupq_n_u64(UINT64_MAX);
    m = vshlq_u64(m, vdupq_n_s64(c-64));
    b = vandq_u64(b, m);
    a = vshlq_u64(a, vdupq_n_s64(c));
    return  vorrq_u64(a, b);
}

#if _LEAVE_ARM_SILL
}
#endif

#if _ENTER_ARM_SILR
{
#endif

INLINE( _Bool,  BOOL_SILR)  (_Bool a,  _Bool b, Rc(0,  1) c)
{
#define     BOOL_SILR(A, B, C)  ((_Bool)(C ? B : A))
    return  c ? b : a;
}

INLINE( uchar, UCHAR_SILR)  (uchar a,  uchar b, Rc(0,  UCHAR_WIDTH) c)
{
#define     UCHAR_SILR(A, B, C)    \
((uchar)((((unsigned) A)<<C)|(((unsigned) B)>>(UCHAR_WIDTH-C))))

    return  UCHAR_SILR(a, b, c);
}

INLINE(  char,  CHAR_SILR)   (char a,   char b, Rc(0,   CHAR_WIDTH) c)
{
#define     CHAR_SILR(A, B, C)    \
((char)((((unsigned) A)<<C)|(((unsigned) B)>>(CHAR_WIDTH-C))))

    return  CHAR_SILR(a, b, c);
}

INLINE(ushort, USHRT_SILR) (ushort a, ushort b, Rc(0,  USHRT_WIDTH) c)
{
#define     USHRT_SILR(A, B, C)    \
((ushort)((((unsigned) A)<<C)|(((unsigned) B)>>(USHRT_WIDTH-C))))

    return  USHRT_SILR(a, b, c);
}

INLINE(  uint,  UINT_SILR)   (uint a,   uint b, Rc(0,   UINT_WIDTH) c)
{
#define     UINT_SILR(A, B, C)    \
((((unsigned) A)<<C)|(((unsigned) B)>>(UINT_WIDTH-C)))

    return  UINT_SILR(a, b, c);
}

INLINE( ulong, ULONG_SILR)  (ulong a,  ulong b, Rc(0,  ULONG_WIDTH) c)
{
#define     ULONG_SILR(A, B, C)    \
((((ulong) A)<<C)|(((ulong) B)>>(ULONG_WIDTH-C)))

    return  ULONG_SILR(a, b, c);
}

INLINE(ullong,ULLONG_SILR) (ullong a, ullong b, Rc(0, ULLONG_WIDTH) c)            
{
#define     ULLONG_SILR(A, B, C)    \
((((ullong) A)<<C)|(((ullong) B)>>(ULLONG_WIDTH-C)))

    return  UINT_SILR(a, b, c);
}


INLINE(Vdyu,VDYU_SILR) (Vdyu a, Vdyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vdbu,VDBU_SILR) (Vdbu a, Vdbu b, Rc(0, 8) c)
{
#define     VDBU_SILR(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsli_n_u8(              \
            vshr_n_u8(          \
                B,              \
                8-(7&C)         \
            ),                  \
            A,                  \
            (7&C)               \
        )                       \
)
    a = vshl_u8(a, vdup_n_s8(c));
    b = vshl_u8(b, vdup_n_s8((8-c)-8));
    return  vorr_u8(a, b);
}

INLINE(Vdbc,VDBC_SILR) (Vdbc a, Vdbc b, Rc(0, 8) c)
{
#define     VDBC_SILR(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsli_n_u8(              \
            vshr_n_u8(          \
                VDBC_ASBU(B),   \
                8-(7&C)         \
            ),                  \
            VDBC_ASBU(A),       \
            (7&C)               \
        )                       \
)
    return  VDBU_ASBC(((VDBU_SILR)(VDBC_ASBU(a), VDBC_ASBU(b), c)));
}

INLINE(Vdhu,VDHU_SILR) (Vdhu a, Vdhu b, Rc(0, 16) c)
{
#define     VDHU_SILR(A, B, C)  \
(                               \
    (C > 15)                    \
    ?   B                       \
    :   vsli_n_u16(             \
            vshr_n_u16(         \
                B,              \
                16-(15&C)       \
            ),                  \
            A,                  \
            (15&C)              \
        )                       \
)
    a = vshl_u16(a, vdup_n_s16(c));
    b = vshl_u16(b, vdup_n_s16((16-c)-16));
    return  vorr_u16(a, b);
}

INLINE(Vdwu,VDWU_SILR) (Vdwu a, Vdwu b, Rc(0, 32) c)
{
#define     VDWU_SILR(A, B, C)  \
(                               \
    (C > 31)                    \
    ?   B                       \
    :   vsli_n_u32(             \
            vshr_n_u32(         \
                B,              \
                32-(31&C)       \
            ),                  \
            A,                  \
            (31&C)              \
        )                       \
)
    a = vshl_u32(a, vdup_n_s32(c));
    b = vshl_u32(b, vdup_n_s32((32-c)-32));
    return  vorr_u32(a, b);
}

INLINE(Vddu,VDDU_SILR) (Vddu a, Vddu b, Rc(0, 64) c)
{
#define     VDDU_SILR(A, B, C)  \
(                               \
    (C > 63)                    \
    ?   B                       \
    :   vsli_n_u64(             \
            vshr_n_u64(         \
                B,              \
                64-(63&C)       \
            ),                  \
            A,                  \
            (63&C)              \
        )                       \
)
    a = vshl_u64(a, vdup_n_s64(c));
    b = vshl_u64(b, vdup_n_s64((64-c)-64));
    return  vorr_u64(a, b);
}


INLINE(Vqyu,VQYU_SILR) (Vqyu a, Vqyu b, Rc(0, 1) c)
{
    return  c == 0 ? a : b;
}

INLINE(Vqbu,VQBU_SILR) (Vqbu a, Vqbu b, Rc(0, 8) c)
{
#define     VQBU_SILR(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsliq_n_u8(             \
            vshrq_n_u8(         \
                B,              \
                8-(7&C)         \
            ),                  \
            A,                  \
            (7&C)               \
        )                       \
)
    a = vshlq_u8(a, vdupq_n_s8(c));
    b = vshlq_u8(b, vdupq_n_s8((8-c)-8));
    return  vorrq_u8(a, b);
}

INLINE(Vqbc,VQBC_SILR) (Vqbc a, Vqbc b, Rc(0, 8) c)
{
#define     VQBC_SILR(A, B, C)  \
(                               \
    (C > 7)                     \
    ?   B                       \
    :   vsliq_n_u8(             \
            vshrq_n_u8(         \
                VQBC_ASBU(B),   \
                8-(7&C)         \
            ),                  \
            VQBC_ASBU(A),       \
            (7&C)               \
        )                       \
)
    return  VQBU_ASBC(((VQBU_SILR)(VQBC_ASBU(a), VQBC_ASBU(b), c)));
}

INLINE(Vqhu,VQHU_SILR) (Vqhu a, Vqhu b, Rc(0, 16) c)
{
#define     VQHU_SILR(A, B, C)  \
(                               \
    (C > 15)                    \
    ?   B                       \
    :   vsliq_n_u16(            \
            vshrq_n_u16(        \
                B,              \
                16-(15&C)       \
            ),                  \
            A,                  \
            (15&C)              \
        )                       \
)
    a = vshlq_u16(a, vdupq_n_s16(c));
    b = vshlq_u16(b, vdupq_n_s16((16-c)-16));
    return  vorrq_u16(a, b);
}

INLINE(Vqwu,VQWU_SILR) (Vqwu a, Vqwu b, Rc(0, 32) c)
{
#define     VQWU_SILR(A, B, C)  \
(                               \
    (C > 31)                    \
    ?   B                       \
    :   vsliq_n_u32(            \
            vshrq_n_u32(        \
                B,              \
                32-(31&C)       \
            ),                  \
            A,                  \
            (31&C)              \
        )                       \
)
    a = vshlq_u32(a, vdupq_n_s32(c));
    b = vshlq_u32(b, vdupq_n_s32((32-c)-32));
    return  vorrq_u32(a, b);
}

INLINE(Vqdu,VQDU_SILR) (Vqdu a, Vqdu b, Rc(0, 64) c)
{
#define     VQDU_SILR(A, B, C)  \
(                               \
    (C > 63)                    \
    ?   B                       \
    :   vsliq_n_u64(            \
            vshrq_n_u64(        \
                B,              \
                64-(63&C)       \
            ),                  \
            A,                  \
            (63&C)              \
        )                       \
)
    a = vshlq_u64(a, vdupq_n_s64(c));
    b = vshlq_u64(b, vdupq_n_s64((64-c)-64));
    return  vorrq_u64(a, b);
}

#if _LEAVE_ARM_SILR
}
#endif

#if _ENTER_ARM_SPRL
{
#endif


INLINE(Vdyu,VDYU_SPRL) (Vdyu l, Vdyu r, Rc(0, 64) n)
{
#define     DYU_SPRL(L, R, N)   \
(                               \
    (N > 63) ? R :              \
    (N == 0) ? L :              \
    vorr_u64(                   \
        vshr_n_u64(             \
            L,                  \
            (63&(N+(N==0))),    \
        ),                      \
        vshl_n_u64(             \
            R,                  \
            (63&(64-N))         \
        )                       \
    )                           \
)

#define     VDYU_SPRL(L, R, N)  \
DYU_ASTV(DYU_SPRL(VDYU_ASDU(L),VDYU_ASDU(L),N))

    uint64x1_t  a = VDYU_ASDU(l);
    uint64x1_t  b = VDYU_ASDU(r);
    int64x1_t   x = vdup_n_s64(n);
    int64x1_t   y = vdup_n_s64(64);
    x = vneg_s64(x);
    y = vadd_s64(x, y);
    a = vshl_u64(a, x);
    b = vshl_u64(b, y);
    a = vorr_u64(a, b);
    return  VDDU_ASYU(a);
}


INLINE(Vdbu,VDBU_SPRL) (Vdbu l, Vdbu r, Rc(0, 8) n)
{
#define     VDBU_SPRL(L, R, N)  \
(                               \
    (N==8)                      \
    ?   R                       \
    :   vext_u8(L,R,(7&N))      \
)

    return  vqtbl1_u8(
        vcombine_u8(l, r),
        vadd_u8(
            vcreate_u8(0x0706050403020100ULL),
            vdup_n_u8(n)
        )
    );
}

INLINE(Vdbi,VDBI_SPRL) (Vdbi l, Vdbi r, Rc(0, 8) n)
{
#define     VDBI_SPRL(L, R, N)  \
(                               \
    (N==8)                      \
    ?   R                       \
    :   vext_s8(L,R,(7&N))      \
)

    return  vqtbl1_s8(
        vcombine_s8(l, r),
        vadd_u8(
            vcreate_u8(0x0706050403020100ULL),
            vdup_n_u8(n)
        )
    );
}

INLINE(Vdbc,VDBC_SPRL) (Vdbc l, Vdbc r, Rc(0, 8) n)
{
#   define  VDBC_SPRL(L, R, N) \
VDBU_ASBC(VDBU_SPRL(VDBC_ASBU(L),VDBC_ASBU(R),N))

    return VDBU_ASBC(
        (VDBU_SPRL)(VDBC_ASBU(l), VDBC_ASBU(r), n)
    );
}


INLINE(Vdhu,VDHU_SPRL) (Vdhu l, Vdhu r, Rc(0, 4) n)
{
#define     VDHU_SPRL(L, R, N)  \
(                               \
    (N==4)                      \
    ?   R                       \
    :   vext_u16(L,R,(3&N))     \
)

    uint16x8_t c = vcombine_u16(l, r);
    return  vreinterpret_u16_u8(
        vqtbl1_u8(
            vreinterpretq_u8_u16(c),
            vadd_u8(
                vcreate_u8(0x0706050403020100ULL),
                vdup_n_u8(n<<1)
            )
        )
    );
}

INLINE(Vdhi,VDHI_SPRL) (Vdhi l, Vdhi r, Rc(0, 4) n)
{
#define     VDHI_SPRL(L, R, N)  \
(                               \
    (N==4)                      \
    ?   R                       \
    :   vext_s16(L,R,(3&N))     \
)

    int16x8_t c = vcombine_s16(l, r);
    return  vreinterpret_s16_u8(
        vqtbl1_u8(
            vreinterpretq_u8_s16(c),
            vadd_u8(
                vcreate_u8(0x0706050403020100ULL),
                vdup_n_u8(n<<1)
            )
        )
    );
}

INLINE(Vdhf,VDHF_SPRL) (Vdhf l, Vdhf r, Rc(0, 4) n)
{
#define     VDHF_SPRL(L, R, N) \
VDHU_ASHF(VDHU_SPRL(VDHF_ASHU(L),VDHF_ASHU(R),N))

    return  VDHU_ASHF((VDHU_SPRL)(VDHF_ASHU(l), VDHF_ASHU(r), n));
}


INLINE(Vdwu,VDWU_SPRL) (Vdwu l, Vdwu r, Rc(0, 2) n)
{
#define     VDWU_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vext_u32(L,R,(1&N))     \
)

    uint32x4_t c = vcombine_u32(l, r);
    return  vreinterpret_u32_u8(
        vqtbl1_u8(
            vreinterpretq_u8_u32(c),
            vadd_u8(
                vcreate_u8(0x0706050403020100ULL),
                vdup_n_u8(n<<2)
            )
        )
    );
}

INLINE(Vdwi,VDWI_SPRL) (Vdwi l, Vdwi r, Rc(0, 2) n)
{
#define     VDWI_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vext_s32(L,R,(1&N))     \
)

    int32x4_t c = vcombine_s32(l, r);
    return  vreinterpret_s32_u8(
        vqtbl1_u8(
            vreinterpretq_u8_s32(c),
            vadd_u8(
                vcreate_u8(0x0706050403020100ULL),
                vdup_n_u8(n<<2)
            )
        )
    );
}

INLINE(Vdwf,VDWF_SPRL) (Vdwf l, Vdwf r, Rc(0, 2) n)
{
#define     VDWF_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vext_f32(L,R,(1&N))     \
)

    float32x4_t c = vcombine_f32(l, r);
    return  vreinterpret_f32_u8(
        vqtbl1_u8(
            vreinterpretq_u8_f32(c),
            vadd_u8(
                vcreate_u8(0x0706050403020100ULL),
                vdup_n_u8(n<<2)
            )
        )
    );
}


INLINE(Vqyu,VQYU_SPRL) (Vqyu l, Vqyu r, Rc(0, 128) n)
{
    
    uint64x1_t  a, m, z, x, y;
    uint64x2_t  p = VQYU_ASDU(l);
    uint64x2_t  q = VQYU_ASDU(r);
    int v;
    if (v < 64)
    {
        a = vget_low_u64(p);
        m = vget_high_u64(p);
        z = vget_low_u64(q);
        v = n;
    }
    else 
    {
        x = vget_high_u64(p);
        y = vget_low_u64(q);
        z = vget_high_u64(q);
        v = n-64;
    }
    int s = 64-v;
    x = (VDDU_SHRS)(a, v);
    y = (VDDU_SHLL)(m, s);
    a = vorr_u64(x, y);
    x = (VDDU_SHRS)(m, v);
    y = (VDDU_SHLL)(z, s);
    z = vorr_u64(x, y);
    p = vcombine_u64(a, z);
    return VQDU_ASYU(p);

}

INLINE(Vqbu,VQBU_SPRL) (Vqbu l, Vqbu r, Rc(0, 16) n)
{
#define     VQBU_SPRL(L, R, N)  \
(                               \
    (N==16)                     \
    ?   R                       \
    :   vextq_u8(L,R,(15&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    t = vaddq_u8(t, vdupq_n_u8(n));
    return  vqtbl2q_u8(
        ((uint8x16x2_t){l,r}),
        t
    );
}

INLINE(Vqbi,VQBI_SPRL) (Vqbi l, Vqbi r, Rc(0, 16) n)
{
#define     VQBI_SPRL(L, R, N)  \
(                               \
    (N==16)                     \
    ?   R                       \
    :   vextq_s8(L,R,(15&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    t = vaddq_u8(t, vdupq_n_u8(n));
    return  vqtbl2q_s8(
        ((int8x16x2_t){l, r}),
        t
    );
}

INLINE(Vqbc,VQBC_SPRL) (Vqbc l, Vqbc r, Rc(0, 16) n)
{
#define     VQBC_SPRL(L, R, N)  \
(                               \
    (N==16)                     \
    ?   R                       \
    :   VQBU_ASBC(              \
            vextq_u8(           \
                VQBC_ASBU(L),   \
                VQBC_ASBU(R),   \
                (15&N)          \
            )                   \
        )                       \
)

    return VQBU_ASBC(((VQBU_SPRL)(VQBC_ASBU(l), VQBC_ASBU(r), n)));

}


INLINE(Vqhu,VQHU_SPRL) (Vqhu l, Vqhu r, Rc(0, 8) n)
{
#define     VQHU_SPRL(L, R, N)  \
(                               \
    (N==8)                      \
    ?   R                       \
    :   vextq_u16(L,R,(7&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_u16(l);
    uint8x16_t b = vreinterpretq_u8_u16(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<1));
    return  vreinterpretq_u16_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}

INLINE(Vqhi,VQHI_SPRL) (Vqhi l, Vqhi r, Rc(0, 8) n)
{
#define     VQHI_SPRL(L, R, N)  \
(                               \
    (N==8)                      \
    ?   R                       \
    :   vextq_s16(L,R,(7&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_s16(l);
    uint8x16_t b = vreinterpretq_u8_s16(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<1));
    return  vreinterpretq_s16_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
}
   
INLINE(Vqhf,VQHF_SPRL) (Vqhf l, Vqhf r, Rc(0, 8) n)
{
#define     VQHF_SPRL(L, R, N)              \
(                                           \
    (N==8)                                  \
    ?   R                                   \
    :   vreinterpretq_f16_u16(              \
            vextq_u16(                      \
                vreinterpretq_u16_f16(L),   \
                vreinterpretq_u16_f16(R),   \
                (7&N)                       \
            )                               \
        )                                   \
)
    return VQHU_ASHF(((VQHF_SPRL)(VQHF_ASHU(l), VQHF_ASHU(r), n)));
}


INLINE(Vqwu,VQWU_SPRL) (Vqwu l, Vqwu r, Rc(0, 4) n)
{
#define     VQWU_SPRL(L, R, N)  \
(                               \
    (N==4)                      \
    ?   R                       \
    :   vextq_u32(L,R,(3&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_u32(l);
    uint8x16_t b = vreinterpretq_u8_u32(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<2));
    return  vreinterpretq_u32_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}

INLINE(Vqwi,VQWI_SPRL) (Vqwi l, Vqwi r, Rc(0, 4) n)
{
#define     VQWI_SPRL(L, R, N)  \
(                               \
    (N==4)                      \
    ?   R                       \
    :   vextq_s32(L,R,(3&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_s32(l);
    uint8x16_t b = vreinterpretq_u8_s32(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<2));
    return  vreinterpretq_s32_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}

INLINE(Vqwf,VQWF_SPRL) (Vqwf l, Vqwf r, Rc(0, 4) n)
{
#define     VQWF_SPRL(L, R, N)  \
(                               \
    (N==4)                      \
    ?   R                       \
    :   vextq_f32(L,R,(3&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_f32(l);
    uint8x16_t b = vreinterpretq_u8_f32(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<2));
    return  vreinterpretq_f32_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}


INLINE(Vqdu,VQDU_SPRL) (Vqdu l, Vqdu r, Rc(0, 2) n)
{
#define     VQDU_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vextq_u64(L,R,(1&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_u64(l);
    uint8x16_t b = vreinterpretq_u8_u64(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<3));
    return  vreinterpretq_u64_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}

INLINE(Vqdi,VQDI_SPRL) (Vqdi l, Vqdi r, Rc(0, 2) n)
{
#define     VQDI_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vextq_s64(L,R,(1&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_s64(l);
    uint8x16_t b = vreinterpretq_u8_s64(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<3));
    return  vreinterpretq_s64_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}


INLINE(Vqdf,VQDF_SPRL) (Vqdf l, Vqdf r, Rc(0, 2) n)
{
#define     VQDF_SPRL(L, R, N)  \
(                               \
    (N==2)                      \
    ?   R                       \
    :   vextq_f64(L,R,(1&N))    \
)
    uint8x16_t      t = vcombine_u8(
        vcreate_u8(0x0706050403020100ULL),
        vcreate_u8(0x0f0e0d0c0b0a0908ULL)
    );
    uint8x16_t a = vreinterpretq_u8_f64(l);
    uint8x16_t b = vreinterpretq_u8_f64(r);
    t = vaddq_u8(t, vdupq_n_u8(n<<3));
    return  vreinterpretq_f64_u8(
        vqtbl2q_u8(
            ((uint8x16x2_t){a,b}),
            t
        )
    );
    
}


#if _LEAVE_ARM_SPRL
}
#endif

#if _ENTER_ARM_SVLL
{
#endif

#define     DBU_SVLL(A,B)   vshl_u8(A,vreinterpret_s8_u8(B))
#define     DBI_SVLL(A,B)   vshl_s8(A,vreinterpret_s8_u8(B))
#if CHAR_MIN
#   define  DBC_SVLL    DBI_SVLL
#else
#   define  DBC_SVLL    DBU_SVLL
#endif

#define     DHU_SVLL(A, B)  vshl_u16(A,vreinterpret_s16_u16(B))
#define     DHI_SVLL(A, B)  vshl_s16(A,vreinterpret_s16_u16(B))
#define     DWU_SVLL(A, B)  vshl_u32(A,vreinterpret_s32_u32(B))
#define     DWI_SVLL(A, B)  vshl_s32(A,vreinterpret_s32_u32(B))
#define     DDU_SVLL(A, B)  vshl_u64(A,vreinterpret_s64_u64(B))
#define     DDI_SVLL(A, B)  vshl_s64(A,vreinterpret_s64_u64(B))

#define     QBU_SVLL(A, B)  vshlq_u8(A,vreinterpretq_s8_u8(B))
#define     QBI_SVLL(A, B)  vshlq_s8(A,vreinterpretq_s8_u8(B))
#if CHAR_MIN
#   define  QBC_SVLL    QBI_SVLL
#else
#   define  QBC_SVLL    QBU_SVLL
#endif

#define     QHU_SVLL(A, B)  vshlq_u16(A,vreinterpretq_s16_u16(B))
#define     QHI_SVLL(A, B)  vshlq_s16(A,vreinterpretq_s16_u16(B))
#define     QWU_SVLL(A, B)  vshlq_u32(A,vreinterpretq_s32_u32(B))
#define     QWI_SVLL(A, B)  vshlq_s32(A,vreinterpretq_s32_u32(B))
#define     QDU_SVLL(A, B)  vshlq_u64(A,vreinterpretq_s64_u64(B))
#define     QDI_SVLL(A, B)  vshlq_s64(A,vreinterpretq_s64_u64(B))

INLINE(Vwyu,VWYU_SVLL) (Vwyu a, Vwyu b)
{
#define     VWYU_SVLL(A, B)                         \
WYU_ASTV(                                           \
    vget_lane_f32(                                  \
        vand_u8(                                    \
            vreinterpret_u8_f32(                    \
                vdup_n_f32(VWYU_ASTM(A))            \
            ),                                      \
            vmvn_u8(                                \
                vreinterpret_u8_f32(                \
                    vdup_n_f32(VWYU_ASTM(A))        \
                )                                   \
            )                                       \
        ),                                          \
        0                                           \
    )                                               \
)

    return  VWYU_SVLL(a, b);
/*
    return a;
*/
}

INLINE(Vwbu,VWBU_SVLL) (Vwbu a, Vwbu b)
{
    float32x2_t l = vdup_n_f32(VWBU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBU_ASTM(b));
    uint8x8_t   v = vshl_u8(
        vreinterpret_u8_f32(l),
        vreinterpret_s8_f32(r)
    );
    l = vreinterpret_f32_u8(v);
    return  WBU_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwbi,VWBI_SVLL) (Vwbi a, Vwbu b)
{
    float32x2_t l = vdup_n_f32(VWBI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBU_ASTM(b));
    int8x8_t    v = vshl_s8(
        vreinterpret_s8_f32(l),
        vreinterpret_s8_f32(r)
    );
    l = vreinterpret_f32_s8(v);
    return  WBI_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwbc,VWBC_SVLL) (Vwbc a, Vwbu b)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_SVLL(VWBC_ASBI(a), b));
#else
    return  VWBU_ASBC(VWBU_SVLL(VWBC_ASBU(a), b));
#endif

}

INLINE(Vwhu,VWHU_SVLL) (Vwhu a, Vwhu b)
{
    float32x2_t l = vdup_n_f32(VWHU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHU_ASTM(b));
    uint16x4_t   v = vshl_u16(
        vreinterpret_u16_f32(l),
        vreinterpret_s16_f32(r)
    );
    l = vreinterpret_f32_u16(v);
    return  WHU_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwhi,VWHI_SVLL) (Vwhi a, Vwhu b)
{
    float32x2_t l = vdup_n_f32(VWHI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHU_ASTM(b));
    int16x4_t   v = vshl_s16(
        vreinterpret_s16_f32(l),
        vreinterpret_s16_f32(r)
    );
    l = vreinterpret_f32_s16(v);
    return  WHI_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwwu,VWWU_SVLL) (Vwwu a, Vwwu b)
{
    float32x2_t l = vdup_n_f32(VWWU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWWU_ASTM(b));
    uint32x2_t  v = vshl_u32(
        vreinterpret_u32_f32(l),
        vreinterpret_s32_f32(r)
    );
    l = vreinterpret_f32_u32(v);
    return  WWU_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwwi,VWWI_SVLL) (Vwwi a, Vwwu b)
{
    float32x2_t l = vdup_n_f32(VWWI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWWU_ASTM(b));
    int32x2_t   v = vshl_s32(
        vreinterpret_s32_f32(l),
        vreinterpret_s32_f32(r)
    );
    l = vreinterpret_f32_s32(v);
    return  WWI_ASTV(vget_lane_f32(l,0));
}


INLINE(Vdyu,VDYU_SVLL) (Vdyu a, Vdyu b) 
{
    return  VDBU_ASYU(
        vand_u8(
            VDYU_ASBU(a),
            vmvn_u8(VDYU_ASBU(b))
        )
    );
}

INLINE(Vdbu,VDBU_SVLL) (Vdbu a, Vdbu b) {return DBU_SVLL(a, b);}
INLINE(Vdbi,VDBI_SVLL) (Vdbi a, Vdbu b) {return DBI_SVLL(a, b);}
INLINE(Vdbc,VDBC_SVLL) (Vdbc a, Vdbu b)
{
    return  DBC_ASTV(DBC_SVLL(VDBC_ASTM(a), b));
}

INLINE(Vdhu,VDHU_SVLL) (Vdhu a, Vdhu b) {return DHU_SVLL(a, b);}
INLINE(Vdhi,VDHI_SVLL) (Vdhi a, Vdhu b) {return DHI_SVLL(a, b);}
INLINE(Vdwu,VDWU_SVLL) (Vdwu a, Vdwu b) {return DWU_SVLL(a, b);}
INLINE(Vdwi,VDWI_SVLL) (Vdwi a, Vdwu b) {return DWI_SVLL(a, b);}
INLINE(Vddu,VDDU_SVLL) (Vddu a, Vddu b) {return DDU_SVLL(a, b);}
INLINE(Vddi,VDDI_SVLL) (Vddi a, Vddu b) {return DDI_SVLL(a, b);}


INLINE(Vqyu,VQYU_SVLL) (Vqyu a, Vqyu b) 
{
    return  VQBU_ASYU(
        vandq_u8(
            VQYU_ASBU(a),
            vmvnq_u8(VQYU_ASBU(b))
        )
    );
}

INLINE(Vqbu,VQBU_SVLL) (Vqbu a, Vqbu b) {return QBU_SVLL(a, b);}
INLINE(Vqbi,VQBI_SVLL) (Vqbi a, Vqbu b) {return QBI_SVLL(a, b);}
INLINE(Vqbc,VQBC_SVLL) (Vqbc a, Vqbu b)
{
    return  QBC_ASTV(QBC_SVLL(VQBC_ASTM(a), b));
}

INLINE(Vqhu,VQHU_SVLL) (Vqhu a, Vqhu b) {return QHU_SVLL(a, b);}
INLINE(Vqhi,VQHI_SVLL) (Vqhi a, Vqhu b) {return QHI_SVLL(a, b);}

INLINE(Vqwu,VQWU_SVLL) (Vqwu a, Vqwu b) {return QWU_SVLL(a, b);}
INLINE(Vqwi,VQWI_SVLL) (Vqwi a, Vqwu b) {return QWI_SVLL(a, b);}

INLINE(Vqdu,VQDU_SVLL) (Vqdu a, Vqdu b) {return QDU_SVLL(a, b);}
INLINE(Vqdi,VQDI_SVLL) (Vqdi a, Vqdu b) {return QDI_SVLL(a, b);}

#if _LEAVE_ARM_SVLL
}
#endif

#if _ENTER_ARM_SVLS
{
#endif

#define     DBU_SVLS(A,B)   vqshl_u8(A, vreinterpret_s8_u8(B))
#define     DBI_SVLS(A,B)   vqshl_s8(A, vreinterpret_s8_u8(B))
#if CHAR_MIN
#   define  DBC_SVLS    DBI_SVLS
#else
#   define  DBC_SVLS    DBU_SVLS
#endif

#define     DHU_SVLS(A, B)  vqshl_u16(A, vreinterpret_s16_u16(B))
#define     DHI_SVLS(A, B)  vqshl_s16(A, vreinterpret_s16_u16(B))
#define     DWU_SVLS(A, B)  vqshl_u32(A, vreinterpret_s32_u32(B))
#define     DWI_SVLS(A, B)  vqshl_s32(A, vreinterpret_s32_u32(B))
#define     DDU_SVLS(A, B)  vqshl_u64(A, vreinterpret_s64_u64(B))
#define     DDI_SVLS(A, B)  vqshl_s64(A, vreinterpret_s64_u64(B))

#define     QBU_SVLS(A, B)  vqshlq_u8(A, vreinterpretq_s8_u8(B))
#define     QBI_SVLS(A, B)  vqshlq_s8(A, vreinterpretq_s8_u8(B))
#if CHAR_MIN
#   define  QBC_SVLS    QBI_SVLS
#else
#   define  QBC_SVLS    QBU_SVLS
#endif

#define     QHU_SVLS(A, B)  vqshlq_u16(A, vreinterpretq_s16_u16(B))
#define     QHI_SVLS(A, B)  vqshlq_s16(A, vreinterpretq_s16_u16(B))
#define     QWU_SVLS(A, B)  vqshlq_u32(A, vreinterpretq_s32_u32(B))
#define     QWI_SVLS(A, B)  vqshlq_s32(A, vreinterpretq_s32_u32(B))
#define     QDU_SVLS(A, B)  vqshlq_u64(A, vreinterpretq_s64_u64(B))
#define     QDI_SVLS(A, B)  vqshlq_s64(A, vreinterpretq_s64_u64(B))


INLINE(Vwyu,VWYU_SVLS) (Vwyu a, Vwyu b) {return a;}

INLINE(Vwbu,VWBU_SVLS) (Vwbu a, Vwbu b)
{
    float32x2_t l = vdup_n_f32(VWBU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBU_ASTM(b));
    uint8x8_t   v = vqshl_u8(
        vreinterpret_u8_f32(l),
        vreinterpret_s8_f32(r)
    );
    l = vreinterpret_f32_u8(v);
    return  WBU_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwbi,VWBI_SVLS) (Vwbi a, Vwbu b)
{
    float32x2_t l = vdup_n_f32(VWBI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWBU_ASTM(b));
    int8x8_t    v = vqshl_s8(
        vreinterpret_s8_f32(l),
        vreinterpret_s8_f32(r)
    );
    l = vreinterpret_f32_s8(v);
    return  WBI_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwbc,VWBC_SVLS) (Vwbc a, Vwbu b)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_SVLS(VWBC_ASBI(a), b));
#else
    return  VWBU_ASBC(VWBU_SVLS(VWBC_ASBU(a), b));
#endif

}

INLINE(Vwhu,VWHU_SVLS) (Vwhu a, Vwhu b)
{
    float32x2_t l = vdup_n_f32(VWHU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHU_ASTM(b));
    uint16x4_t   v = vqshl_u16(
        vreinterpret_u16_f32(l),
        vreinterpret_s16_f32(r)
    );
    l = vreinterpret_f32_u16(v);
    return  WHU_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwhi,VWHI_SVLS) (Vwhi a, Vwhu b)
{
    float32x2_t l = vdup_n_f32(VWHI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWHU_ASTM(b));
    int16x4_t   v = vqshl_s16(
        vreinterpret_s16_f32(l),
        vreinterpret_s16_f32(r)
    );
    l = vreinterpret_f32_s16(v);
    return  WHI_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwwu,VWWU_SVLS) (Vwwu a, Vwwu b)
{
    float32x2_t l = vdup_n_f32(VWWU_ASTM(a));
    float32x2_t r = vdup_n_f32(VWWU_ASTM(b));
    uint32x2_t  v = vqshl_u32(
        vreinterpret_u32_f32(l),
        vreinterpret_s32_f32(r)
    );
    l = vreinterpret_f32_u32(v);
    return  WWU_ASTV(vget_lane_f32(l,0));
}

INLINE(Vwwi,VWWI_SVLS) (Vwwi a, Vwwu b)
{
    float32x2_t l = vdup_n_f32(VWWI_ASTM(a));
    float32x2_t r = vdup_n_f32(VWWU_ASTM(b));
    int32x2_t   v = vqshl_s32(
        vreinterpret_s32_f32(l),
        vreinterpret_s32_f32(r)
    );
    l = vreinterpret_f32_s32(v);
    return  WWI_ASTV(vget_lane_f32(l,0));
}


INLINE(Vdyu,VDYU_SVLS) (Vdyu a, Vdyu b) {return a;}
INLINE(Vdbu,VDBU_SVLS) (Vdbu a, Vdbu b) {return DBU_SVLS(a, b);}
INLINE(Vdbi,VDBI_SVLS) (Vdbi a, Vdbu b) {return DBI_SVLS(a, b);}
INLINE(Vdbc,VDBC_SVLS) (Vdbc a, Vdbu b)
{
    return  DBC_ASTV(DBC_SVLS(VDBC_ASTM(a), b));
}

INLINE(Vdhu,VDHU_SVLS) (Vdhu a, Vdhu b) {return DHU_SVLS(a, b);}
INLINE(Vdhi,VDHI_SVLS) (Vdhi a, Vdhu b) {return DHI_SVLS(a, b);}
INLINE(Vdwu,VDWU_SVLS) (Vdwu a, Vdwu b) {return DWU_SVLS(a, b);}
INLINE(Vdwi,VDWI_SVLS) (Vdwi a, Vdwu b) {return DWI_SVLS(a, b);}
INLINE(Vddu,VDDU_SVLS) (Vddu a, Vddu b) {return DDU_SVLS(a, b);}
INLINE(Vddi,VDDI_SVLS) (Vddi a, Vddu b) {return DDI_SVLS(a, b);}


INLINE(Vqyu,VQYU_SVLS) (Vqyu a, Vqyu b) {return a;}
INLINE(Vqbu,VQBU_SVLS) (Vqbu a, Vqbu b) {return QBU_SVLS(a, b);}
INLINE(Vqbi,VQBI_SVLS) (Vqbi a, Vqbu b) {return QBI_SVLS(a, b);}
INLINE(Vqbc,VQBC_SVLS) (Vqbc a, Vqbu b)
{
    return  QBC_ASTV(QBC_SVLS(VQBC_ASTM(a), b));
}

INLINE(Vqhu,VQHU_SVLS) (Vqhu a, Vqhu b) {return QHU_SVLS(a, b);}
INLINE(Vqhi,VQHI_SVLS) (Vqhi a, Vqhu b) {return QHI_SVLS(a, b);}
INLINE(Vqwu,VQWU_SVLS) (Vqwu a, Vqwu b) {return QWU_SVLS(a, b);}
INLINE(Vqwi,VQWI_SVLS) (Vqwi a, Vqwu b) {return QWI_SVLS(a, b);}
INLINE(Vqdu,VQDU_SVLS) (Vqdu a, Vqdu b) {return QDU_SVLS(a, b);}
INLINE(Vqdi,VQDI_SVLS) (Vqdi a, Vqdu b) {return QDI_SVLS(a, b);}

#if _LEAVE_ARM_SVLS
}
#endif

#if _ENTER_ARM_SVL2
{
#endif

#define     WBU_SVL2(A, B) vshl_u16(WBU_CVHU(A),WBU_CVHI(B))

INLINE(int16x4_t,WBI_SVL2) (float a, float b)
{
    return vget_low_s16(
        vshlq_s16(
            vmovl_s8(vreinterpret_s8_f32(vdup_n_f32(a))),
            vmovl_s8(vreinterpret_s8_f32(vdup_n_f32(b)))
        )
    );
}

#if CHAR_MIN
#   define  WBC_SVL2 WBI_SVL2
#else
#   define  WBC_SVL2 WBU_SVL2
#endif

#define     WHU_SVL2(A, B) vshl_u32(WHU_CVWU(A),WHU_CVWI(B))

INLINE(int32x2_t,WHI_SVL2) (float a, float b)
{
#define     WHI_SVL2(A, B) \
((WHI_SVL2)(_Generic(A,float:A),_Generic(B,float:B)))

    return vget_low_s32(
        vshlq_s32(
            vmovl_s16(vreinterpret_s16_f32(vdup_n_f32(a))),
            vmovl_s16(vreinterpret_s16_f32(vdup_n_f32(b)))
        )
    );
}

#define     WWU_SVL2(A, B) vshl_u64(WWU_CVDU(A),WWU_CVDU(B))

INLINE(int64x1_t,WWI_SVL2) (float a, float b)
{
#define     WWI_SVL2(A, B) \
((WWI_SVL2)(_Generic(A,float:A),_Generic(B,float:B)))

    return vget_low_s64(
        vshlq_s64(
            vmovl_s32(vreinterpret_s32_f32(vdup_n_f32(a))),
            vmovl_s32(vreinterpret_s32_f32(vdup_n_f32(b)))
        )
    );
}

INLINE(Vdhu,VWBU_SVL2) (Vwbu a, Vwbu b)
{
    return  WBU_SVL2(VWBU_ASTM(a), VWBU_ASTM(b));
}

INLINE(Vdhi,VWBI_SVL2) (Vwbi a, Vwbu b)
{
    return  WBI_SVL2(VWBI_ASTM(a), VWBU_ASTM(b));
}

#if CHAR_MIN

INLINE(Vdhi,VWBC_SVL2) (Vwbc a, Vwbu b)
{
    return  WBC_SVL2(VWBC_ASTM(a), VWBU_ASTM(b));
}
#else

INLINE(Vdhu,VWBC_SVL2) (Vwbc a, Vwbu b)
{
    return  WBC_SVL2(VWBC_ASTM(a), VWBU_ASTM(b));
}

#endif


INLINE(Vdwu,VWHU_SVL2) (Vwhu a, Vwhu b)
{
    return  WHU_SVL2(VWHU_ASTM(a), VWHU_ASTM(b));
}

INLINE(Vdwi,VWHI_SVL2) (Vwhi a, Vwhu b)
{
    return  WHI_SVL2(VWHI_ASTM(a), VWHU_ASTM(b));
}


INLINE(Vddu,VWWU_SVL2) (Vwwu a, Vwwu b)
{
    return  WWU_SVL2(VWWU_ASTM(a), VWWU_ASTM(b));
}

INLINE(Vddi,VWWI_SVL2) (Vwwi a, Vwwu b)
{
    return  WWI_SVL2(VWWI_ASTM(a), VWWU_ASTM(b));
}


INLINE(Vqhu,VDBU_SVL2) (Vdbu a, Vdbu b)
{
    return vshlq_u16(VDBU_CVHU(a),VDBU_CVHI(b));
}

INLINE(Vqhi,VDBI_SVL2) (Vdbi a, Vdbu b)
{
    return vshlq_s16(VDBI_CVHI(a),VDBU_CVHI(b));
}
    
#if CHAR_MIN

INLINE(Vqhi,VDBC_SVL2) (Vdbc a, Vdbu b)
{
    return  vshlq_s16(VDBC_CVHI(a), VDBU_CVHU(b));
}
#else

INLINE(Vqhu,VDBC_SVL2) (Vdbc a, Vdbu b)
{
    return  vshlq_u16(VDBC_CVHU(a), VDBU_CVHU(b));
}

#endif


INLINE(Vqwu,VDHU_SVL2) (Vdhu a, Vdhu b)
{
    return vshlq_u32(VDHU_CVWU(a),VDHU_CVWI(b));
}

INLINE(Vqwi,VDHI_SVL2) (Vdhi a, Vdhu b)
{
    return vshlq_s32(VDHI_CVWI(a),VDHU_CVWI(b));
}


INLINE(Vqdu,VDWU_SVL2) (Vdwu a, Vdwu b)
{
    return vshlq_u64(VDWU_CVDU(a),VDWU_CVDI(b));
}

INLINE(Vqdi,VDWI_SVL2) (Vdwi a, Vdwu b)
{
    return vshlq_s32(VDWI_CVDI(a),VDWU_CVDI(b));
}

#if _LEAVE_ARM_SVL2
}
#endif

#if _ENTER_ARM_SVRS
{
#endif

INLINE(Vwbu,VWBU_SVRS) (Vwbu a, Vwbu b)
{
    float32x2_t lm = vdup_n_f32(VWBU_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWBU_ASTM(b));
    uint8x8_t   lv = vreinterpret_u8_f32(lm);
    int8x8_t    rv = vreinterpret_s8_f32(rm);
    lv = vshl_u8(lv, vneg_s8(rv));
    lm = vreinterpret_f32_u8(lv);
    return  WBU_ASTV(vget_lane_f32(lm, V2_K0));
}

INLINE(Vwbi,VWBI_SVRS) (Vwbi a, Vwbu b)
{
    float32x2_t lm = vdup_n_f32(VWBI_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWBU_ASTM(b));
    int8x8_t    lv = vreinterpret_s8_f32(lm);
    int8x8_t    rv = vreinterpret_s8_f32(rm);
    lv = vshl_u8(lv, vneg_s8(rv));
    lm = vreinterpret_f32_s8(lv);
    return  WBI_ASTV(vget_lane_f32(lm, V2_K0));
}

INLINE(Vwbc,VWBC_SVRS) (Vwbc a, Vwbu b)
{
#if CHAR_MIN
    return VWBI_ASBC(VWBI_SVRS(VWBC_ASBI(a), b));
#else
    return VWBU_ASBC(VWBU_SVRS(VWBC_ASBU(a), b));
#endif
}

INLINE(Vwhu,VWHU_SVRS) (Vwhu a, Vwhu b)
{
    float32x2_t lm = vdup_n_f32(VWHU_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWHU_ASTM(b));
    uint16x4_t  lv = vreinterpret_u16_f32(lm);
    int16x4_t   rv = vreinterpret_s16_f32(rm);
    lv = vshl_u16(lv, vneg_s16(rv));
    lm = vreinterpret_f32_u16(lv);
    return  WHU_ASTV(vget_lane_f32(lm, V2_K0));
}

INLINE(Vwhi,VWHI_SVRS) (Vwhi a, Vwhu b)
{
    float32x2_t lm = vdup_n_f32(VWHI_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWHU_ASTM(b));
    int16x4_t   lv = vreinterpret_s16_f32(lm);
    int16x4_t   rv = vreinterpret_s16_f32(rm);
    lv = vshl_s16(lv, vneg_s16(rv));
    lm = vreinterpret_f32_s16(lv);
    return  WHI_ASTV(vget_lane_f32(lm, V2_K0));
}


INLINE(Vwwu,VWWU_SVRS) (Vwwu a, Vwwu b)
{
    float32x2_t lm = vdup_n_f32(VWWU_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWWU_ASTM(b));
    uint32x2_t  lv = vreinterpret_u32_f32(lm);
    int32x2_t   rv = vreinterpret_s32_f32(rm);
    lv = vshl_u32(lv, vneg_s32(rv));
    lm = vreinterpret_f32_u32(lv);
    return  WWU_ASTV(vget_lane_f32(lm, V2_K0));
}

INLINE(Vwwi,VWWI_SVRS) (Vwwi a, Vwwu b)
{
    float32x2_t lm = vdup_n_f32(VWWI_ASTM(a));
    float32x2_t rm = vdup_n_f32(VWWU_ASTM(b));
    int32x2_t   lv = vreinterpret_s32_f32(lm);
    int32x2_t   rv = vreinterpret_s32_f32(rm);
    lv = vshl_s32(lv, vneg_s32(rv));
    lm = vreinterpret_f32_s32(lv);
    return  WWI_ASTV(vget_lane_f32(lm, V2_K0));
}


INLINE(Vdbu,VDBU_SVRS) (Vdbu a, Vdbu b)
{
    return  vshl_u8(a, vneg_s8(vreinterpret_s8_u8(b)));
}

INLINE(Vdbi,VDBI_SVRS) (Vdbi a, Vdbu b)
{
    return  vshl_s8(a, vneg_s8(vreinterpret_s8_u8(b)));
}

INLINE(Vdbc,VDBC_SVRS) (Vdbc a, Vdbu b)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_SVRS(VDBC_ASBI(a), b));
#else
    return  VDBU_ASBC(VDBU_SVRS(VDBC_ASBU(a), b));
#endif
}


INLINE(Vdhu,VDHU_SVRS) (Vdhu a, Vdhu b)
{
    return  vshl_u16(a, vneg_s16(vreinterpret_s16_u16(b)));
}

INLINE(Vdhi,VDHI_SVRS) (Vdhi a, Vdhu b)
{
    return  vshl_s16(a, vneg_s16(vreinterpret_s16_u16(b)));
}


INLINE(Vdwu,VDWU_SVRS) (Vdwu a, Vdwu b)
{
    return  vshl_u32(a, vneg_s32(vreinterpret_s32_u32(b)));
}

INLINE(Vdwi,VDWI_SVRS) (Vdwi a, Vdwu b)
{
    return  vshl_s32(a, vneg_s32(vreinterpret_s32_u32(b)));
}


INLINE(Vddu,VDDU_SVRS) (Vddu a, Vddu b)
{
    return  vshl_u64(a, vneg_s64(vreinterpret_s64_u64(b)));
}

INLINE(Vddi,VDDI_SVRS) (Vddi a, Vddu b)
{
    return  vshl_s64(a, vneg_s64(vreinterpret_s64_u64(b)));
}


INLINE(Vqbu,VQBU_SVRS) (Vqbu a, Vqbu b)
{
    return  vshlq_u8(a, vnegq_s8(vreinterpretq_s8_u8(b)));
}

INLINE(Vqbi,VQBI_SVRS) (Vqbi a, Vqbu b)
{
    return  vshlq_s8(a, vnegq_s8(vreinterpretq_s8_u8(b)));
}

INLINE(Vqbc,VQBC_SVRS) (Vqbc a, Vqbu b)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_SVRS(VQBC_ASBI(a), b));
#else
    return  VQBU_ASBC(VQBU_SVRS(VQBC_ASBU(a), b));
#endif
}


INLINE(Vqhu,VQHU_SVRS) (Vqhu a, Vqhu b)
{
    return  vshlq_u16(a, vnegq_s16(vreinterpretq_s16_u16(b)));
}

INLINE(Vqhi,VQHI_SVRS) (Vqhi a, Vqhu b)
{
    return  vshlq_s16(a, vnegq_s16(vreinterpretq_s16_u16(b)));
}


INLINE(Vqwu,VQWU_SVRS) (Vqwu a, Vqwu b)
{
    return  vshlq_u32(a, vnegq_s32(vreinterpretq_s32_u32(b)));
}

INLINE(Vqwi,VQWI_SVRS) (Vqwi a, Vqwu b)
{
    return  vshlq_s32(a, vnegq_s32(vreinterpretq_s32_u32(b)));
}


INLINE(Vqdu,VQDU_SVRS) (Vqdu a, Vqdu b)
{
    return  vshlq_u64(a, vnegq_s64(vreinterpretq_s64_u64(b)));
}

INLINE(Vqdi,VQDI_SVRS) (Vqdi a, Vqdu b)
{
    return  vshlq_s64(a, vnegq_s64(vreinterpretq_s64_u64(b)));
}

#if _LEAVE_ARM_SVRS
}
#endif

#if _ENTER_ARM_ROTL
{
#endif

#define     SPC_EMULATED_ROTL(_, A, B)  \
(                                       \
    (_(TYPE))                           \
    (                                   \
        (((unsigned) A)<<B)             \
    |   (((unsigned) A)>>(_(WIDTH)-B))  \
    )                                   \
)


INLINE( uchar, UCHAR_ROTL)  (uchar a, Rc(1, UCHAR_WIDTH-1) b)
{
#define     UCHAR_ROTL(A, B) SPC_EMULATED_ROTL(UCHAR_,A,B)
    return  UCHAR_ROTL(a, b);
}

INLINE(  char,  CHAR_ROTL)   (char a, Rc(1,   CHAR_WIDTH-1) b)
{
#define     CHAR_ROTL(A, B) SPC_EMULATED_ROTL(CHAR_,A,B)
    return  CHAR_ROTL(a, b);
}

INLINE(ushort, USHRT_ROTL) (ushort a, Rc(1,  USHRT_WIDTH-1) b)
{
#define     USHRT_ROTL(A, B) SPC_EMULATED_ROTL(USHRT_, A, B)
    return  USHRT_ROTL(a, b);
}

INLINE(  uint,  UINT_ROTL)   (uint a, Rc(1,   UINT_WIDTH-1) b)
{
#define     UINT_ROTL(A, B) SPC_EMULATED_ROTL(UINT_,A,B)
    return  UINT_ROTL(a, b);
}

INLINE( ulong, ULONG_ROTL)  (ulong a, Rc(1,  ULONG_WIDTH-1) b)
{
#define     ULONG_ROTL(A, B) SPC_EMULATED_ROTL(ULONG_,A,B)
    return  ULONG_ROTL(a, b);
}

INLINE(ullong,ULLONG_ROTL) (ullong a, Rc(1, ULLONG_WIDTH-1) b)
{
#define     ULONG_ROTL(A, B) SPC_EMULATED_ROTL(ULONG_,A,B)
    return  ULLONG_ROTL(a, b);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE, rotlqu) (QUAD_UTYPE a, Rc(1, 127) b)
{
    return (a<<b)|(a>>(127-b));
}

#endif

INLINE(Vwbu,VWBU_ROTL) (Vwbu a, Rc(1, 7) b)
{
#define     VWBU_ROTL(A, B)                         \
(                                                   \
    ((B > 7) || (B < 1))                            \
    ?   A                                           \
    :   WBU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vsli_n_u8(                      \
                        vshr_n_u8(                  \
                            vreinterpret_u8_f32(    \
                                vdup_n_f32(         \
                                    VWBU_ASTM(A)    \
                                )                   \
                            ),                      \
                            (8-(7&B))               \
                        ),                          \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(             \
                                VWBU_ASTM(A)        \
                            )                       \
                        ),                          \
                        (7&B)                       \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)
    float       m = VWBU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   z = vreinterpret_u8_f32(v);
    int8x8_t    n = vdup_n_s8(b);
    uint8x8_t   r = vshl_u8(z, n);
    n = vadd_s8(n, vdup_n_s8(-8));
    z = vshl_u8(z, n);
    z = vorr_u8(z, r);
    v = vreinterpret_f32_u8(z);
    m = vget_lane_f32(v, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbc,VWBC_ROTL) (Vwbc a, Rc(1, 7) b)
{
#define     VWBC_ROTL(A, B)                         \
(                                                   \
    ((B > 7) || (B < 1))                            \
    ?   A                                           \
    :   WBC_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u8(                \
                    vsli_n_u8(                      \
                        vshr_n_u8(                  \
                            vreinterpret_u8_f32(    \
                                vdup_n_f32(         \
                                    VWBC_ASTM(A)    \
                                )                   \
                            ),                      \
                            (8-(7&B))               \
                        ),                          \
                        vreinterpret_u8_f32(        \
                            vdup_n_f32(             \
                                VWBC_ASTM(A)        \
                            )                       \
                        ),                          \
                        (7&B)                       \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)
    float       m = VWBC_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   z = vreinterpret_u8_f32(v);
    int8x8_t    n = vdup_n_s8(b);
    uint8x8_t   r = vshl_u8(z, n);
    n = vadd_s8(n, vdup_n_s8(-8));
    z = vshl_u8(z, n);
    z = vorr_u8(z, r);
    v = vreinterpret_f32_u8(z);
    m = vget_lane_f32(v, 0);
    return  WBC_ASTV(m);
}

INLINE(Vwhu,VWHU_ROTL) (Vwhu a, Rc(1, 15) b)
{
#define     VWHU_ROTL(A, B)                         \
(                                                   \
    ((B > 15) || (B < 1))                           \
    ?   A                                           \
    :   WHU_ASTV(                                   \
            vget_lane_f32(                          \
                vreinterpret_f32_u16(               \
                    vsli_n_u16(                     \
                        vshr_n_u16(                 \
                            vreinterpret_u16_f32(   \
                                vdup_n_f32(         \
                                    VWHU_ASTM(A)    \
                                )                   \
                            ),                      \
                            (16-(15&B))             \
                        ),                          \
                        vreinterpret_u16_f32(       \
                            vdup_n_f32(             \
                                VWHU_ASTM(A)        \
                            )                       \
                        ),                          \
                        (15&B)                      \
                    )                               \
                ),                                  \
                0                                   \
            )                                       \
        )                                           \
)

    float       m = VWHU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint16x4_t  z = vreinterpret_u16_f32(v);
    int16x4_t   n = vdup_n_s16(b);
    uint16x4_t  r = vshl_u16(z, n);
    n = vadd_s16(n, vdup_n_s16(-16));
    z = vshl_u16(z, n);
    z = vorr_u16(z, r);
    v = vreinterpret_f32_u16(z);
    m = vget_lane_f32(v, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwwu,VWWU_ROTL) (Vwwu a, Rc(1, 31) b)
{
#define     VWWU_ROTL(A, B)     \
UINT32_ASTV(                    \
    (                           \
        (VWWU_ASTV(A)<<(   B))  \
    |   (VWWU_ASTV(A)>>(32-B))  \
    )                           \
)
    uint32_t m = VWWU_ASTV(a);
    m = (m<<b)|(m>>(32-b));
    return  UINT32_ASTV(m);
}


INLINE(Vdbu,VDBU_ROTL) (Vdbu a, Rc(1, 7) b)
{
#define     VDBU_ROTL(A, B)             \
(                                       \
    ((B > 7) || (B < 1))                \
    ?   A                               \
    :   vsli_n_u8(                      \
            vshr_n_u8(A,(8-(B&7))),     \
            A,                          \
            (B&7)                       \
        )                               \
)
    int8x8_t    n = vdup_n_s8(b);
    uint8x8_t   r = vshl_u8(a, n);
    n = vadd_s8(n, vdup_n_s8(-8));
    a = vshl_u8(a, n);
    return  vorr_u8(a, r);
}

INLINE(Vdbc,VDBC_ROTL) (Vdbc a, Rc(1, 7) b)
{
#define     VDBC_ROTL(A, B) VDBU_ASBC(VDBU_ROTL(VDBC_ASBU(A),B))
    return  VDBU_ASBC( ((VDBU_ROTL)(VDBC_ASBU(a), b)) );
}

INLINE(Vdhu,VDHU_ROTL) (Vdhu a, Rc(1, 15) b)
{
#define    VDHU_ROTL(A, B)              \
(                                       \
    ((B > 15) || (B < 1))               \
    ?   A                               \
    :   vsli_n_u16(                     \
            vshr_n_u16(A,(16-(B&15))),  \
            A,                          \
            (B&15)                      \
        )                               \
)
    int16x4_t   n = vdup_n_s16(b);
    uint16x4_t  r = vshl_u16(a, n);
    n = vadd_s16(n, vdup_n_s16(-16));
    a = vshl_u16(a, n);
    return  vorr_u16(a, r);
}

INLINE(Vdwu,VDWU_ROTL) (Vdwu a, Rc(1, 31) b)
{
#define     VDWU_ROTL(A, B)             \
(                                       \
    ((B > 31) || (B < 1))               \
    ?   A                               \
    :   vsli_n_u32(                     \
            vshr_n_u32(A,(32-(B&31))),  \
            A,                          \
            (B&31)                      \
        )                               \
)
    int32x2_t   n = vdup_n_s32(b);
    uint32x2_t  r = vshl_u32(a, n);
    n = vadd_s32(n, vdup_n_s32(-32));
    a = vshl_u32(a, n);
    return  vorr_u32(a, r);
}

INLINE(Vddu,VDDU_ROTL) (Vddu a, Rc(1, 63) b)
{
#define     VDDU_ROTL(A, B)             \
(                                       \
    ((B > 63) || (B < 1))               \
    ?   A                               \
    :   vsli_n_u64(                     \
            vshr_n_u64(A,(64-(B&63))),  \
            A,                          \
            (B&63)                      \
        )                               \
)
    int64x1_t   n = vdup_n_s64(b);
    uint64x1_t  r = vshl_u64(a, n);
    n = vadd_s64(n, vdup_n_s64(-64));
    a = vshl_u64(a, n);
    return  vorr_u64(a, r);
}



INLINE(Vqbu,VQBU_ROTL) (Vqbu a, Rc(1, 7) b)
{
#define     VQBU_ROTL(A, B)             \
(                                       \
    ((B > 7) || (B < 1))                \
    ?   A                               \
    :   vsliq_n_u8(                     \
            vshrq_n_u8(A,(8-(B&7))),    \
            A,                          \
            (B&7)                       \
        )                               \
)
    int8x16_t   n = vdupq_n_s8(b);
    uint8x16_t  r = vshlq_u8(a, n);
    n = vaddq_s8(n, vdupq_n_s8(-8));
    a = vshlq_u8(a, n);
    return  vorrq_u8(a, r);
}

INLINE(Vqbc,VQBC_ROTL) (Vqbc a, Rc(1, 7) b)
{
#define     VQBC_ROTL(A, B) VQBU_ASBC(VQBU_ROTL(VQBC_ASBU(A),B))
    return  VQBU_ASBC( ((VQBU_ROTL)(VQBC_ASBU(a), b)) );
}

INLINE(Vqhu,VQHU_ROTL) (Vqhu a, Rc(1, 15) b)
{
#define    VQHU_ROTL(A, B)              \
(                                       \
    ((B > 15) || (B < 1))               \
    ?   A                               \
    :   vsliq_n_u16(                    \
            vshrq_n_u16(A,(16-(B&15))), \
            A,                          \
            (B&15)                      \
        )                               \
)
    int16x8_t   n = vdupq_n_s16(b);
    uint16x8_t  r = vshlq_u16(a, n);
    n = vaddq_s16(n, vdupq_n_s16(-16));
    a = vshlq_u16(a, n);
    return  vorrq_u16(a, r);
}

INLINE(Vqwu,VQWU_ROTL) (Vqwu a, Rc(1, 31) b)
{
#define     VQWU_ROTL(A, B)             \
(                                       \
    ((B > 31) || (B < 1))               \
    ?   A                               \
    :   vsliq_n_u32(                    \
            vshrq_n_u32(A,(32-(B&31))), \
            A,                          \
            (B&31)                      \
        )                               \
)
    int32x4_t   n = vdupq_n_s32(b);
    uint32x4_t  r = vshlq_u32(a, n);
    n = vaddq_s32(n, vdupq_n_s32(-32));
    a = vshlq_u32(a, n);
    return  vorrq_u32(a, r);
}

INLINE(Vqdu,VQDU_ROTL) (Vqdu a, Rc(1, 63) b)
{
#define     VQDU_ROTL(A, B)             \
(                                       \
    ((B > 63) || (B < 1))               \
    ?   A                               \
    :   vsliq_n_u64(                    \
            vshrq_n_u64(A,(64-(B&63))), \
            A,                          \
            (B&63)                      \
        )                               \
)
    int64x2_t   n = vdupq_n_s64(b);
    uint64x2_t  r = vshlq_u64(a, n);
    n = vaddq_s64(n, vdupq_n_s64(-64));
    a = vshlq_u64(a, n);
    return  vorrq_u64(a, r);
}

#if _LEAVE_ARM_ROTL
}
#endif

#if _ENTER_ARM_ROTR
{
#endif

INLINE( uchar, UCHAR_ROTR)  (uchar a, Rc(1,  UCHAR_WIDTH-1) b)
{
#define     UCHAR_ROTR(A, B)                \
(                                           \
    (uchar)                                 \
    (                                       \
        ( ((unsigned) A)>>B)                \
    |   ( ((unsigned) A)<<(UCHAR_WIDTH-B))  \
    )                                       \
)

    return  UCHAR_ROTR(a, b);
}

INLINE(  char,  CHAR_ROTR)   (char a, Rc(1,   CHAR_WIDTH-1) b)
{
#define     CHAR_ROTR(A, B)                 \
(                                           \
    (char)                                  \
    (                                       \
        ( ((unsigned) A)>>B)                \
    |   ( ((unsigned) A)<<(CHAR_WIDTH-B))   \
    )                                       \
)

    return  CHAR_ROTR(a, b);
}

INLINE(ushort, USHRT_ROTR) (ushort a, Rc(1,  USHRT_WIDTH-1) b)
{
#define     USHRT_ROTR(A, B)                \
(                                           \
    (ushort)                                \
    (                                       \
        ( ((unsigned) A)>>B)                \
    |   ( ((unsigned) A)<<(USHRT_WIDTH-B))  \
    )                                       \
)

    return  USHRT_ROTR(a, b);
}

INLINE(  uint,  UINT_ROTR)   (uint a, Rc(1,   UINT_WIDTH-1) b)
{
#define     UINT_ROTR(A, B)   __ror(A,B)
    return  UINT_ROTR(a, b);
}

INLINE( ulong, ULONG_ROTR)  (ulong a, Rc(1,  ULONG_WIDTH-1) b)
{
#define     ULONG_ROTR(A, B)   __rorl(A, B)
    return  ULONG_ROTR(a, b);
}

INLINE(ullong,ULLONG_ROTR) (ullong a, Rc(1, ULLONG_WIDTH-1) b)
{
#define     ULLONG_ROTR(A, B)   __rorll(A, B)
    return  ULLONG_ROTR(a, b);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE, rorsqu) (QUAD_UTYPE a, Rc(1, 127) b)
{
    return (a>>b)|(a<<(128-b));
}

#endif

INLINE(Vdbu,VDBU_ROTR) (Vdbu a, Rc(1, 7) b)
{
#define     VDBU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 7))                \
    ?   A                               \
    :   vsri_n_u8(                      \
            vshl_n_u8(A,(8-(B&7))),     \
            A,                          \
            ((B&7)+(B==0))              \
        )                               \
)
    int8x8_t    n = vdup_n_s8(b);
    n = vneg_s8(n);
    return  vorr_u8(
        vshl_u8(a, n),
        vshl_u8(a, vadd_s8(n, vdup_n_s8(8)))
    );
}

INLINE(Vdbc,VDBC_ROTR) (Vdbc a, Rc(1, 7) b)
{
#define     VDBC_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 7))                \
    ?   A                               \
    :   VDBU_ASBC(                      \
            vsri_n_u8(                  \
                vshl_n_u8(              \
                    VDBC_ASBU(A),       \
                    (8-(B&7))           \
                )                       \
                VDBC_ASBU(A),           \
                ((B&7)+(B==0))          \
            )                           \
        )                               \
)
    uint8x8_t   m = VDBC_ASBU(a);
    int8x8_t    n = vdup_n_s8(b);
    n = vneg_s8(n);
    m = vorr_u8(
        vshl_u8(m, n),
        vshl_u8(m, vadd_s8(n, vdup_n_s8(8)))
    );
    return  VDBU_ASBC(m);
}

INLINE(Vdhu,VDHU_ROTR) (Vdhu a, Rc(1, 15) b)
{
#define     VDHU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 15))               \
    ?   A                               \
    :   vsri_n_u16(                     \
            vshl_n_u16(A,(16-(B&15))),  \
            A,                          \
            ((B&15)+(B==0))             \
        )                               \
)
    int16x4_t    n = vdup_n_s16(b);
    n = vneg_s16(n);
    return  vorr_u16(
        vshl_u16(a, n),
        vshl_u16(a, vadd_s16(n, vdup_n_s16(16)))
    );
}

INLINE(Vdwu,VDWU_ROTR) (Vdwu a, Rc(1, 31) b)
{
#define     VDWU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 31))               \
    ?   A                               \
    :   vsri_n_u32(                     \
            vshl_n_u32(A,(32-(B&31))),  \
            A,                          \
            ((B&31)+(B==0))             \
        )                               \
)
    int32x2_t    n = vdup_n_s32(b);
    n = vneg_s32(n);
    return  vorr_u32(
        vshl_u32(a, n),
        vshl_u32(a, vadd_s32(n, vdup_n_s32(32)))
    );
}

INLINE(Vddu,VDDU_ROTR) (Vddu a, Rc(1, 63) b)
{
#define     VDDU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 63))               \
    ?   A                               \
    :   vsri_n_u64(                     \
            vshl_n_u64(A,(64-(B&63))),  \
            A,                          \
            ((B&63)+(B==0))             \
        )                               \
)
    int64x1_t    n = vdup_n_s64(b);
    n = vneg_s64(n);
    return  vorr_u64(
        vshl_u64(a, n),
        vshl_u64(a, vadd_s64(n, vdup_n_s64(64)))
    );
}



INLINE(Vqbu,VQBU_ROTR) (Vqbu a, Rc(1, 7) b)
{
#define     VQBU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 7))                \
    ?   A                               \
    :   vsriq_n_u8(                     \
            vshlq_n_u8(A,(8-(B&7))),    \
            A,                          \
            ((B&7)+(B==0))              \
        )                               \
)
    int8x16_t   n = vdupq_n_s8(b);
    n = vnegq_s8(n);
    return  vorrq_u8(
        vshlq_u8(a, n),
        vshlq_u8(a, vaddq_s8(n, vdupq_n_s8(8)))
    );
}

INLINE(Vqbc,VQBC_ROTR) (Vqbc a, Rc(1, 7) b)
{
#define     VQBC_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 7))                \
    ?   A                               \
    :   VQBU_ASBC(                      \
            vsriq_n_u8(                 \
                vshlq_n_u8(             \
                    VQBC_ASBU(A),       \
                    (8-(B&7))           \
                )                       \
                VQBC_ASBU(A),           \
                ((B&7)+(B==0))          \
            )                           \
        )                               \
)
    uint8x16_t  m = VQBC_ASBU(a);
    int8x16_t   n = vdupq_n_s8(b);
    n = vnegq_s8(n);
    m = vorrq_u8(
        vshlq_u8(m, n),
        vshlq_u8(m, vaddq_s8(n, vdupq_n_s8(8)))
    );
    return  VQBU_ASBC(m);
}

INLINE(Vqhu,VQHU_ROTR) (Vqhu a, Rc(1, 15) b)
{
#define     VQHU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 15))               \
    ?   A                               \
    :   vsriq_n_u16(                    \
            vshlq_n_u16(A,(16-(B&15))), \
            A,                          \
            ((B&15)+(B==0))             \
        )                               \
)
    int16x8_t    n = vdupq_n_s16(b);
    n = vnegq_s16(n);
    return  vorrq_u16(
        vshlq_u16(a, n),
        vshlq_u16(a, vaddq_s16(n, vdupq_n_s16(16)))
    );
}

INLINE(Vqwu,VQWU_ROTR) (Vqwu a, Rc(1, 31) b)
{
#define     VQWU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 31))               \
    ?   A                               \
    :   vsriq_n_u32(                    \
            vshlq_n_u32(A,(32-(B&31))), \
            A,                          \
            ((B&31)+(B==0))             \
        )                               \
)
    int32x4_t    n = vdupq_n_s32(b);
    n = vnegq_s32(n);
    return  vorrq_u32(
        vshlq_u32(a, n),
        vshlq_u32(a, vaddq_s32(n, vdupq_n_s32(32)))
    );
}

INLINE(Vqdu,VQDU_ROTR) (Vqdu a, Rc(1, 63) b)
{
#define     VQDU_ROTR(A, B)             \
(                                       \
    ((B < 1) || (B > 63))               \
    ?   A                               \
    :   vsriq_n_u64(                    \
            vshlq_n_u64(A,(64-(B&63))), \
            A,                          \
            ((B&63)+(B==0))             \
        )                               \
)
    int64x2_t    n = vdupq_n_s64(b);
    n = vnegq_s64(n);
    return  vorrq_u64(
        vshlq_u64(a, n),
        vshlq_u64(a, vaddq_s64(n, vdupq_n_s64(64)))
    );
}

#if _LEAVE_ARM_ROTR
}
#endif

#if _ENTER_ARM_ROVL
{
#endif

INLINE(Vdbu,VDBU_ROVL) (Vdbu a, Vdbu b) 
{
    int8x8_t   n = vreinterpret_s8_u8(b);
    uint8x8_t  r = vshl_u8(a, vadd_s8(n, vdup_n_s8(-8)));
    return  vorr_u8(r, vshl_u8(a, n));
}

INLINE(Vdbc,VDBC_ROVL) (Vdbc a, Vdbu b)
{
    return  VDBU_ASBC( ((VDBU_ROVL)(VDBC_ASBU(a), b)) );
}

INLINE(Vdhu,VDHU_ROVL) (Vdhu a, Vdhu b) 
{
    int16x4_t   n = vreinterpret_s16_u16(b);
    uint16x4_t  r = vshl_u16(a, vadd_s16(n, vdup_n_s16(-16)));
    return  vorr_u16(r, vshl_u16(a, n));
}

INLINE(Vdwu,VDWU_ROVL) (Vdwu a, Vdwu b) 
{
    int32x2_t   n = vreinterpret_s32_u32(b);
    uint32x2_t  r = vshl_u32(a, vadd_s32(n, vdup_n_s32(-32)));
    return  vorr_u32(r, vshl_u32(a, n));
}

INLINE(Vddu,VDDU_ROVL) (Vddu a, Vddu b) 
{
    int64x1_t   n = vreinterpret_s64_u64(b);
    uint64x1_t  r = vshl_u64(a, vadd_s64(n, vdup_n_s64(-64)));
    return  vorr_u64(r, vshl_u64(a, n));
}

INLINE(Vqbu,VQBU_ROVL) (Vqbu a, Vqbu b) 
{
    int8x16_t   n = vreinterpretq_s8_u8(b);
    uint8x16_t  r = vshlq_u8(a, vaddq_s8(n, vdupq_n_s8(-8)));
    return  vorrq_u8(r, vshlq_u8(a, n));
}

INLINE(Vqbc,VQBC_ROVL) (Vqbc a, Vqbu b)
{
    return  VQBU_ASBC( ((VQBU_ROVL)(VQBC_ASBU(a), b)) );
}

INLINE(Vqhu,VQHU_ROVL) (Vqhu a, Vqhu b) 
{
    int16x8_t   n = vreinterpretq_s16_u16(b);
    uint16x8_t  r = vshlq_u16(a, vaddq_s16(n, vdupq_n_s16(-16)));
    return  vorrq_u16(r, vshlq_u16(a, n));
}

INLINE(Vqwu,VQWU_ROVL) (Vqwu a, Vqwu b) 
{
    int32x4_t   n = vreinterpretq_s32_u32(b);
    uint32x4_t  r = vshlq_u32(a, vaddq_s32(n, vdupq_n_s32(-32)));
    return  vorrq_u32(r, vshlq_u32(a, n));
}

INLINE(Vqdu,VQDU_ROVL) (Vqdu a, Vqdu b) 
{
    int64x2_t   n = vreinterpretq_s64_u64(b);
    uint64x2_t  r = vshlq_u64(a, vaddq_s64(n, vdupq_n_s64(-64)));
    return  vorrq_u64(r, vshlq_u64(a, n));
}

#if _LEAVE_ARM_ROVL
}
#endif

#if _ENTER_ARM_ROVR
{
#endif

INLINE(Vdbu,VDBU_ROVR) (Vdbu a, Vdbu b) 
{
    int8x8_t   n = vreinterpret_s8_u8(b);
    uint8x8_t  r = vshl_u8(a, vneg_s8(n));
    n = vsub_s8(vdup_n_s8(8), n);
    return  vorr_u8(r, vshl_u8(a, n));
}

INLINE(Vdbc,VDBC_ROVR) (Vdbc a, Vdbu b)
{
    return  VDBU_ASBC( ((VDBU_ROVR)(VDBC_ASBU(a), b)) );
}

INLINE(Vdhu,VDHU_ROVR) (Vdhu a, Vdhu b) 
{
    int16x4_t   n = vreinterpret_s16_u16(b);
    uint16x4_t  r = vshl_u16(a, vneg_s16(n));
    n = vsub_s16(vdup_n_s16(16), n);
    return  vorr_u16(r, vshl_u16(a, n));
}

INLINE(Vdwu,VDWU_ROVR) (Vdwu a, Vdwu b) 
{
    int32x2_t   n = vreinterpret_s32_u32(b);
    uint32x2_t  r = vshl_u32(a, vneg_s32(n));
    n = vsub_s32(vdup_n_s32(32), n);
    return  vorr_u32(r, vshl_u32(a, n));
}

INLINE(Vddu,VDDU_ROVR) (Vddu a, Vddu b) 
{
    int64x1_t   n = vreinterpret_s64_u64(b);
    uint64x1_t  r = vshl_u64(a, vneg_s64(n));
    n = vsub_s64(vdup_n_s64(64), n);
    return  vorr_u64(r, vshl_u64(a, n));
}

INLINE(Vqbu,VQBU_ROVR) (Vqbu a, Vqbu b) 
{
    int8x16_t   n = vreinterpretq_s8_u8(b);
    uint8x16_t  r = vshlq_u8(a, vnegq_s8(n));
    n = vsubq_s8(vdupq_n_s8(8), n);
    return  vorrq_u8(r, vshlq_u8(a, n));
}

INLINE(Vqbc,VQBC_ROVR) (Vqbc a, Vqbu b)
{
    return  VQBU_ASBC( ((VQBU_ROVR)(VQBC_ASBU(a), b)) );
}

INLINE(Vqhu,VQHU_ROVR) (Vqhu a, Vqhu b) 
{
    int16x8_t   n = vreinterpretq_s16_u16(b);
    uint16x8_t  r = vshlq_u16(a, vnegq_s16(n));
    n = vsubq_s16(vdupq_n_s16(16), n);
    return  vorrq_u16(r, vshlq_u16(a, n));
}

INLINE(Vqwu,VQWU_ROVR) (Vqwu a, Vqwu b) 
{
    int32x4_t   n = vreinterpretq_s32_u32(b);
    uint32x4_t  r = vshlq_u32(a, vnegq_s32(n));
    n = vsubq_s32(vdupq_n_s32(32), n);
    return  vorrq_u32(r, vshlq_u32(a, n));
}

INLINE(Vqdu,VQDU_ROVR) (Vqdu a, Vqdu b) 
{
    int64x2_t   n = vreinterpretq_s64_u64(b);
    uint64x2_t  r = vshlq_u64(a, vnegq_s64(n));
    n = vsubq_s64(vdupq_n_s64(64), n);
    return  vorrq_u64(r, vshlq_u64(a, n));
}

#if _LEAVE_ARM_ROVR
}
#endif


#if _ENTER_ARM_ADDL
{
#endif

INLINE( _Bool,  BOOL_ADDL)  (_Bool a,  _Bool b) {return a^b;}
INLINE( uchar, UCHAR_ADDL)  (uchar a,  uchar b) {return a+b;}
INLINE( schar, SCHAR_ADDL)  (schar a,  schar b) {return UCHAR_ADDL(a,b);}
INLINE(  char,  CHAR_ADDL)   (char a,   char b) {return UCHAR_ADDL(a,b);}
INLINE(ushort, USHRT_ADDL) (ushort a, ushort b) {return a+b;}
INLINE( short,  SHRT_ADDL)  (short a,  short b) {return USHRT_ADDL(a,b);}
INLINE(  uint,  UINT_ADDL)   (uint a,   uint b) {return a+b;}
INLINE(   int,   INT_ADDL)    (int a,    int b) {return UINT_ADDL(a,b);}
INLINE( ulong, ULONG_ADDL)  (ulong a,  ulong b) {return a+b;}
INLINE(  long,  LONG_ADDL)   (long a,   long b) {return ULONG_ADDL(a,b);}
INLINE(ullong,ULLONG_ADDL) (ullong a, ullong b) {return a+b;}
INLINE( llong, LLONG_ADDL)  (llong a,  llong b) {return ULLONG_ADDL(a,b);}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,addlqu) (QUAD_UTYPE a, QUAD_UTYPE b) {return a+b;}
INLINE(QUAD_ITYPE,addlqi) (QUAD_ITYPE a, QUAD_ITYPE b) {return a+b;}
#endif

INLINE(Vwyu,VWYU_ADDL) (Vwyu a, Vwyu b)
{
#define     VWYU_ADDL(A, B) \
WYU_ASTV(((VWWU_ASTV(VWYU_ASWU(A)))^(VWWU_ASTV(VWYU_ASWU(B)))))
    return  VWYU_ADDL(a, b);
}

INLINE(Vwbu,VWBU_ADDL) (Vwbu a, Vwbu b) 
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    float       bm = VWBU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    az = vadd_u8(az, bz);
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_ADDL) (Vwbi a, Vwbi b) 
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    float       bm = VWBI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    az = vadd_s8(az, bz);
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_ADDL) (Vwbc a, Vwbc b) 
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ADDL(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return  VWBU_ASBC(VWBU_ADDL(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_ADDL) (Vwhu a, Vwhu b) 
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    float       bm = VWHU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    az = vadd_u16(az, bz);
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_ADDL) (Vwhi a, Vwhi b) 
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    float       bm = VWHI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    az = vadd_s16(az, bz);
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}


INLINE(Vwwu,VWWU_ADDL) (Vwwu a, Vwwu b) 
{
#define     VWWU_ADDL(A, B)  UINT32_ASTV((VWWU_ASTV(A)+VWWU_ASTV(B)))
    return  VWWU_ADDL(a, b);
}

INLINE(Vwwi,VWWI_ADDL) (Vwwi a, Vwwi b) 
{
#define     VWWI_ADDL(A, B)  INT32_ASTV((VWWI_ASTV(A)+VWWI_ASTV(B)))
    return  VWWI_ADDL(a, b);
}


INLINE(Vdyu,VDYU_ADDL) (Vdyu a, Vdyu b)
{
#define     VDYU_ADDL(A, B) DYU_ASTV(veor_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_ADDL(a, b);
}

INLINE(Vdbu,VDBU_ADDL) (Vdbu a, Vdbu b) {return vadd_u8(a, b);}
INLINE(Vdbi,VDBI_ADDL) (Vdbi a, Vdbi b) {return vadd_s8(a, b);}
INLINE(Vdbc,VDBC_ADDL) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_ADDL(A, B) VDBI_ASBC(vadd_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_ADDL(A, B) VDBU_ASBC(vadd_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_ADDL(a, b);
}

INLINE(Vdhu,VDHU_ADDL) (Vdhu a, Vdhu b) {return vadd_u16(a, b);}
INLINE(Vdhi,VDHI_ADDL) (Vdhi a, Vdhi b) {return vadd_s16(a, b);}
INLINE(Vdwu,VDWU_ADDL) (Vdwu a, Vdwu b) {return vadd_u32(a, b);}
INLINE(Vdwi,VDWI_ADDL) (Vdwi a, Vdwi b) {return vadd_s32(a, b);}
INLINE(Vddu,VDDU_ADDL) (Vddu a, Vddu b) {return vadd_u64(a, b);}
INLINE(Vddi,VDDI_ADDL) (Vddi a, Vddi b) {return vadd_s64(a, b);}

INLINE(Vqyu,VQYU_ADDL) (Vqyu a, Vqyu b)
{
#define     VQYU_ADDL(A, B) QYU_ASTV(veorq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_ADDL(a, b);
}

INLINE(Vqbu,VQBU_ADDL) (Vqbu a, Vqbu b) {return vaddq_u8(a, b);}
INLINE(Vqbi,VQBI_ADDL) (Vqbi a, Vqbi b) {return vaddq_s8(a, b);}
INLINE(Vqbc,VQBC_ADDL) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_ADDL(A, B) VQBI_ASBC(vaddq_s8(VQBC_ASBI(A), VQBC_ASBI(B)))
#else
#   define  VQBC_ADDL(A, B) VQBU_ASBC(vaddq_u8(VQBC_ASBU(A), VQBC_ASBU(B)))
#endif
    return  VQBC_ADDL(a, b);
}

INLINE(Vqhu,VQHU_ADDL) (Vqhu a, Vqhu b) {return vaddq_u16(a, b);}
INLINE(Vqhi,VQHI_ADDL) (Vqhi a, Vqhi b) {return vaddq_s16(a, b);}
INLINE(Vqwu,VQWU_ADDL) (Vqwu a, Vqwu b) {return vaddq_u32(a, b);}
INLINE(Vqwi,VQWI_ADDL) (Vqwi a, Vqwi b) {return vaddq_s32(a, b);}
INLINE(Vqdu,VQDU_ADDL) (Vqdu a, Vqdu b) {return vaddq_u64(a, b);}
INLINE(Vqdi,VQDI_ADDL) (Vqdi a, Vqdi b) {return vaddq_s64(a, b);}

#if _LEAVE_ARM_ADDL
}
#endif

#if _ENTER_ARM_ADDS
{
#endif

INLINE( _Bool,  BOOL_ADDS)  (_Bool a,  _Bool b) {return a|b;}

INLINE( uchar, UCHAR_ADDS)  (uchar a,  uchar b)
{
    return  vqaddb_u8(a, b);
}

INLINE( schar, SCHAR_ADDS)  (schar a,  schar b)
{
    return  vqaddb_s8(a, b);
}

INLINE(  char,  CHAR_ADDS)   (char a,   char b)
{
/*  Of course char+char doesn't make sense but this is
    probably the best example of practicality over purity.
    Besides, the Microsoft abi considers char compatible with
    signed char so addsbc would be required for INT8_ADDS
    anyway.
*/
#if CHAR_MIN
    return  vqaddb_s8(a, b);
#else
    return  vqaddb_u8(a, b);
#endif
}

INLINE(ushort, USHRT_ADDS) (ushort a, ushort b)
{
    return  vqaddh_u16(a, b);
}

INLINE( short,  SHRT_ADDS)  (short a,  short b)
{
    return  vqaddh_s16(a, b);
}

INLINE(  uint,  UINT_ADDS)   (uint a,   uint b)
{
    return  vqadds_u32(a, b);
}

INLINE(   int,   INT_ADDS)    (int a,    int b)
{
    return  vqadds_s32(a, b);
}

INLINE( ulong, ULONG_ADDS)  (ulong a,  ulong b)
{
#if DWRD_NLONG == 2
#   define  ULONG_ADDS(A, B) ((ulong) vqadds_u32(A, B))
#else
#   define  ULONG_ADDS(A, B) vqaddd_u64(A, B)
#endif
    return  ULONG_ADDS(a, b);
}

INLINE(  long,  LONG_ADDS)   (long a,   long b)
{
#if DWRD_NLONG == 2
#   define  LONG_ADDS(A, B) ((long) vqadds_s32(A, B))
#else
#   define  LONG_ADDS(A, B) vqaddd_s64(A, B)
#endif
    return  LONG_ADDS(a, b);
}

INLINE(ullong,ULLONG_ADDS) (ullong a, ullong b)
{
#if QUAD_NLLONG == 2
#   define  ULLONG_ADDS(A, B) ((ullong) vqaddd_u64(A, B))
#else
// ??
#   define  ULLONG_ADDS(A, B) vqaddq_u128(A, B)
#endif
    return  ULLONG_ADDS(a, b);
}

INLINE( llong, LLONG_ADDS)  (llong a,  llong b)
{
#if QUAD_NLLONG == 2
#   define  LLONG_ADDS(A, B) ((llong) vqaddd_s64(A, B))
#else
// ??
#   define  LLONG_ADDS(A, B) vqaddq_s128(A, B)
#endif
    return  LLONG_ADDS(a, b);
}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,addsqu) (QUAD_UTYPE a, QUAD_UTYPE b) 
{
    b += a;
    return (b >= a) ? b : (((QUAD_UTYPE) 0)-1);
}

INLINE(QUAD_ITYPE,addsqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    QUAD_ITYPE c = a+b;
    if (a <= 0)
    {
        if ((b < 0) && (c > a))
        {
            QUAD_UTYPE r = 1;
            r <<= 127;
            return  (QUAD_ITYPE) r;
        }
    }
    else 
    {
        if ((b > 0) && (c < a))
        {
            QUAD_UTYPE r = 0;
            r -= 1;
            r >>= 1;
            return  (QUAD_ITYPE) r;
        }
    }
    return c;
}
#endif

INLINE(Vdyu,VDYU_ADDS) (Vdyu a, Vdyu b)
{
#define     VDYU_ADDS(A, B) DYU_ASTV(vorr_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_ADDS(a, b);
}

INLINE(Vdbu,VDBU_ADDS) (Vdbu a, Vdbu b) {return vqadd_u8(a, b);}
INLINE(Vdbi,VDBI_ADDS) (Vdbi a, Vdbi b) {return vqadd_s8(a, b);}
INLINE(Vdbc,VDBC_ADDS) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_ADDS(A, B) VDBI_ASBC(vqadd_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_ADDS(A, B) VDBU_ASBC(vqadd_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_ADDS(a, b);
}
INLINE(Vdhu,VDHU_ADDS) (Vdhu a, Vdhu b) {return vqadd_u16(a, b);}
INLINE(Vdhi,VDHI_ADDS) (Vdhi a, Vdhi b) {return vqadd_s16(a, b);}
INLINE(Vdwu,VDWU_ADDS) (Vdwu a, Vdwu b) {return vqadd_u32(a, b);}
INLINE(Vdwi,VDWI_ADDS) (Vdwi a, Vdwi b) {return vqadd_s32(a, b);}
INLINE(Vddu,VDDU_ADDS) (Vddu a, Vddu b) {return vqadd_u64(a, b);}
INLINE(Vddi,VDDI_ADDS) (Vddi a, Vddi b) {return vqadd_s64(a, b);}


INLINE(Vqyu,VQYU_ADDS) (Vqyu a, Vqyu b)
{
#define     VQYU_ADDS(A, B) QYU_ASTV(vorrq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_ADDS(a, b);
}
INLINE(Vqbu,VQBU_ADDS) (Vqbu a, Vqbu b) {return vqaddq_u8(a, b);}
INLINE(Vqbi,VQBI_ADDS) (Vqbi a, Vqbi b) {return vqaddq_s8(a, b);}
INLINE(Vqbc,VQBC_ADDS) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_ADDS(A, B) VQBI_ASBC(vqaddq_s8(VQBC_ASBI(A), VQBC_ASBI(B)))
#else
#   define  VQBC_ADDS(A, B) VQBU_ASBC(vqaddq_u8(VQBC_ASBU(A), VQBC_ASBU(B)))
#endif
    return  VQBC_ADDS(a, b);
}
INLINE(Vqhu,VQHU_ADDS) (Vqhu a, Vqhu b) {return vqaddq_u16(a, b);}
INLINE(Vqhi,VQHI_ADDS) (Vqhi a, Vqhi b) {return vqaddq_s16(a, b);}
INLINE(Vqwu,VQWU_ADDS) (Vqwu a, Vqwu b) {return vqaddq_u32(a, b);}
INLINE(Vqwi,VQWI_ADDS) (Vqwi a, Vqwi b) {return vqaddq_s32(a, b);}
INLINE(Vqdu,VQDU_ADDS) (Vqdu a, Vqdu b) {return vqaddq_u64(a, b);}
INLINE(Vqdi,VQDI_ADDS) (Vqdi a, Vqdi b) {return vqaddq_s64(a, b);}

#if _LEAVE_ARM_ADDS
}
#endif

#if _ENTER_ARM_ADDH
{
#endif

INLINE(flt16_t,  BOOL_ADDH)   (_Bool a, flt16_t b) {return b+a;}
INLINE(flt16_t, UCHAR_ADDH)   (uchar a, flt16_t b) {return a+b;}
INLINE(flt16_t, SCHAR_ADDH)   (schar a, flt16_t b) {return a+b;}
INLINE(flt16_t,  CHAR_ADDH)    (char a, flt16_t b) {return a+b;}
INLINE(flt16_t, USHRT_ADDH)  (ushort a, flt16_t b) {return a+b;}
INLINE(flt16_t,  SHRT_ADDH)   (short a, flt16_t b) {return a+b;}
INLINE(flt16_t,  UINT_ADDH)    (uint a, flt16_t b) {return a+b;}
INLINE(flt16_t,   INT_ADDH)     (int a, flt16_t b) {return a+b;}
INLINE(flt16_t, ULONG_ADDH)   (ulong a, flt16_t b) {return a+b;}
INLINE(flt16_t,  LONG_ADDH)    (long a, flt16_t b) {return a+b;}
INLINE(flt16_t,ULLONG_ADDH)  (ullong a, flt16_t b) {return a+b;}
INLINE(flt16_t, LLONG_ADDH)   (llong a, flt16_t b) {return a+b;}

INLINE(flt16_t, FLT16_ADDH) (flt16_t a, flt16_t b) {return a+b;}
INLINE(float,     FLT_ADDH)   (float a, flt16_t b) {return a+b;}
INLINE(double,    DBL_ADDH)  (double a, flt16_t b) {return a+b;}

#if QUAD_NLLONG == 2
INLINE(flt16_t,addhqu) (QUAD_UTYPE a, flt16_t b) {return a+b;}
INLINE(flt16_t,addhqi) (QUAD_ITYPE a, flt16_t b) {return a+b;}
INLINE(QUAD_FTYPE,addhqf) (QUAD_FTYPE a, flt16_t b) {return a+b;}
#endif


INLINE(Vdhf,VWBU_ADDH) (Vwbu a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vadd_f16(VWBU_CVHF(a), b);
#else
    float32x4_t p = VWBU_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vaddq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBI_ADDH) (Vwbi a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vadd_f16(VWBI_CVHF(a), b);
#else
    float32x4_t p = VWBI_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vaddq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBC_ADDH) (Vwbc a, Vdhf b)
{
#if CHAR_MIN
    return VWBI_ADDH(VWBC_ASBI(a), b);
#else
    return VWBU_ADDH(VWBC_ASBU(a), b);
#endif
}


INLINE(float,WHF_ADDH) (float am, float bm)
{
#if defined(SPC_ARM_FP16_SIMD)
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    ah = vadd_f16(ah, bh);
#else
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x4_t av = vcvt_f32_f16(ah);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    float32x4_t bv = vcvt_f32_f16(bh);
    av = vaddq_f32(av, bv);
    ah = vcvt_f16_f32(av);
#endif
    aw = vreinterpret_f32_f16(ah);
    return  vget_lane_f32(aw, 0);
    
}


INLINE(Vwhf,VWHU_ADDH) (Vwhu a, Vwhf b)
{
    Vwhf    v = VWHU_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_ADDH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHI_ADDH) (Vwhi a, Vwhf b)
{
    Vwhf    v = VWHI_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_ADDH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_ADDH) (Vwhf a, Vwhf b)
{
    float   m = WHF_ADDH(VWHF_ASTM(a), VWHF_ASTM(b));
    return WHF_ASTV(m);
}


INLINE(Vqhf,VDBU_ADDH) (Vdbu a, Vqhf b) 
{
    uint16x8_t  c = vmovl_u8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_u16(c);
    return  vaddq_f16(f, b);
#else

    uint32x4_t  zl = vmovl_u16(vget_low_u16(c));
    float32x4_t fl = vcvtq_f32_u32(zl);
    uint32x4_t  zr = vmovl_u16(vget_high_u16(c));
    float32x4_t fr = vcvtq_f32_u32(zr);
    fl = vaddq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vaddq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBI_ADDH) (Vdbi a, Vqhf b) 
{
    int16x8_t   c = vmovl_s8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_s16(c);
    return  vaddq_f16(f, b);
#else
    int32x4_t   zl = vmovl_s16(vget_low_s16(c));
    float32x4_t fl = vcvtq_f32_s32(zl);
    int32x4_t   zr = vmovl_s16(vget_high_s16(c));
    float32x4_t fr = vcvtq_f32_s32(zr);
    fl = vaddq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vaddq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBC_ADDH) (Vdbc a, Vqhf b)
{
#if CHAR_MIN
    return  VDBI_ADDH(VDBC_ASBI(a), b);
#else
    return  VDBU_ADDH(VDBC_ASBU(a), b);
#endif
}


INLINE(Vdhf,VDHU_ADDH) (Vdhu a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vadd_f16(vcvt_f16_u16(a), b);
#else
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vaddq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHI_ADDH) (Vdhi a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vadd_f16(vcvt_f16_s16(a), b);
#else
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vaddq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHF_ADDH) (Vdhf a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vadd_f16(a, b);
#else
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vaddq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}


INLINE(Vwhf,VDWU_ADDH) (Vdwu a, Vwhf b)
{
    return  VWHF_ADDH(VDWU_CVHF(a), b);
}

INLINE(Vwhf,VDWI_ADDH) (Vdwi a, Vwhf b)
{
    return  VWHF_ADDH(VDWI_CVHF(a), b);
}

INLINE(Vdwf,VDWF_ADDH) (Vdwf a, Vwhf b)
{
    return vadd_f32(a, VWHF_CVWF(b));
}

/*
INLINE(Vhhf,VDDU_ADDH) (Vddu a, Vhhf b);
INLINE(Vhhf,VDDI_ADDH) (Vddi a, Vhhf b);
INLINE(Vddf,VDDF_ADDH) (Vddf a, Vhhf b);


INLINE(Vohf,VQBU_ADDH) (Vqbu a, Vohf b);
INLINE(Vohf,VQBI_ADDH) (Vqbi a, Vohf b);
INLINE(Vohf,VQBC_ADDH) (Vqbc a, Vohf b);

*/

INLINE(Vqhf,VQHU_ADDH) (Vqhu a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vaddq_f16(vcvtq_f16_u16(a), b);
#else
    return vcombine_f16(
        VDHU_ADDH(vget_low_u16(a),  vget_low_f16(b)),
        VDHU_ADDH(vget_high_u16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHI_ADDH) (Vqhi a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vaddq_f16(vcvtq_f16_s16(a), b);
#else
    return vcombine_f16(
        VDHI_ADDH(vget_low_s16(a),  vget_low_f16(b)),
        VDHI_ADDH(vget_high_s16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHF_ADDH) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vaddq_f16(a, b);
#else
    return vcombine_f16(
        VDHF_ADDH(vget_low_f16(a),  vget_low_f16(b)),
        VDHF_ADDH(vget_high_f16(a), vget_high_f16(b))
    );
#endif

}


INLINE(Vdhf,VQWU_ADDH) (Vqwu a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_u32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vaddq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vdhf,VQWI_ADDH) (Vqwi a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_s32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vaddq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vqwf,VQWF_ADDH) (Vqwf a, Vdhf b)
{
    return  vaddq_f32(a, vcvt_f32_f16(b));
}

INLINE(Vwhf,VQDU_ADDH) (Vqdu a, Vwhf b)
{
    return  VWHF_ADDH(VQDU_CVHF(a), b);
}

INLINE(Vwhf,VQDI_ADDH) (Vqdi a, Vwhf b)
{
    return  VWHF_ADDH(VQDI_CVHF(a), b);
}

INLINE(Vqdf,VQDF_ADDH) (Vqdf a, Vwhf b)
{
    return  vaddq_f64(a, VWHF_CVDF(b));
}

#if _LEAVE_ARM_ADDH
}
#endif

#if _ENTER_ARM_ADDW
{
#endif

INLINE(float,  BOOL_ADDW)   (_Bool a, float b) {return b+a;}
INLINE(float, UCHAR_ADDW)   (uchar a, float b) {return a+b;}
INLINE(float, SCHAR_ADDW)   (schar a, float b) {return a+b;}
INLINE(float,  CHAR_ADDW)    (char a, float b) {return a+b;}
INLINE(float, USHRT_ADDW)  (ushort a, float b) {return a+b;}
INLINE(float,  SHRT_ADDW)   (short a, float b) {return a+b;}
INLINE(float,  UINT_ADDW)    (uint a, float b) {return a+b;}
INLINE(float,   INT_ADDW)     (int a, float b) {return a+b;}
INLINE(float, ULONG_ADDW)   (ulong a, float b) {return a+b;}
INLINE(float,  LONG_ADDW)    (long a, float b) {return a+b;}
INLINE(float,ULLONG_ADDW)  (ullong a, float b) {return a+b;}
INLINE(float, LLONG_ADDW)   (llong a, float b) {return a+b;}

INLINE(float, FLT16_ADDW) (flt16_t a, float b) {return a+b;}
INLINE(float,   FLT_ADDW)   (float a, float b) {return a+b;}
INLINE(double,  DBL_ADDW)  (double a, float b) {return a+b;}

#if QUAD_NLLONG == 2
INLINE(float,addwqu)   (QUAD_UTYPE a, float b) {return a+b;}
INLINE(float,addwqi)   (QUAD_ITYPE a, float b) {return a+b;}
INLINE(QUAD_FTYPE,addwqf) (QUAD_FTYPE a, float b) {return a+b;}
#endif


INLINE(Vqwf,VWBU_ADDW) (Vwbu a, Vqwf b)
{
    return  vaddq_f32(VWBU_CVWF(a), b);
}

INLINE(Vqwf,VWBI_ADDW) (Vwbi a, Vqwf b)
{
    return  vaddq_f32(VWBI_CVWF(a), b);
}

INLINE(Vqwf,VWBC_ADDW) (Vwbc a, Vqwf b)
{
#if CHAR_MIN
    return  VWBI_ADDW(VWBC_ASBI(a), b);
#else
    return  VWBU_ADDW(VWBC_ASBU(a), b);
#endif
}



INLINE(Vdwf,VWHU_ADDW) (Vwhu a, Vdwf b)
{
    return  vadd_f32(VWHU_CVWF(a), b);
}

INLINE(Vdwf,VWHI_ADDW) (Vwhi a, Vdwf b)
{
    return  vadd_f32(VWHI_CVWF(a), b);
}

INLINE(Vdwf,VWHF_ADDW) (Vwhf a, Vdwf b)
{
    return  vadd_f32(VWHF_CVWF(a), b);
}


INLINE(Vwwf,VWWU_ADDW) (Vwwu a, Vwwf b)
{
    return WWF_ASTV((VWWU_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWI_ADDW) (Vwwi a, Vwwf b)
{
    return WWF_ASTV((VWWI_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWF_ADDW) (Vwwf a, Vwwf b)
{
    return WWF_ASTV((VWWF_ASTM(a)+VWWF_ASTM(b)));
}


INLINE(Vqwf,VDHU_ADDW) (Vdhu a, Vqwf b) 
{
    return  vaddq_f32(VDHU_CVWF(a), b);
}

INLINE(Vqwf,VDHI_ADDW) (Vdhi a, Vqwf b) 
{
    return  vaddq_f32(VDHI_CVWF(a), b);
}

INLINE(Vqwf,VDHF_ADDW) (Vdhf a, Vqwf b) 
{
    return  vaddq_f32(VDHF_CVWF(a), b);
}


INLINE(Vdwf,VDWU_ADDW) (Vdwu a, Vdwf b)
{
    return vadd_f32(vcvt_f32_u32(a), b);
}

INLINE(Vdwf,VDWI_ADDW) (Vdwi a, Vdwf b)
{
    return vadd_f32(vcvt_f32_s32(a), b);
}

INLINE(Vdwf,VDWF_ADDW) (Vdwf a, Vdwf b)
{
    return vadd_f32(a, b);
}


INLINE(Vwwf,VDDU_ADDW) (Vddu a, Vwwf b)
{
    return WWF_ASTV((VDDU_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vwwf,VDDI_ADDW) (Vddi a, Vwwf b)
{
    return WWF_ASTV((VDDI_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vddf,VDDF_ADDW) (Vddf a, Vwwf b)
{
    return vadd_f64(a, vdup_n_f64(VWWF_ASTM(b)));
}


INLINE(Vqwf,VQWU_ADDW) (Vqwu a, Vqwf b)
{
    return  vaddq_f32(vcvtq_f32_u32(a), b);
}

INLINE(Vqwf,VQWI_ADDW) (Vqwi a, Vqwf b)
{
    return  vaddq_f32(vcvtq_f32_s32(a), b);
}

INLINE(Vqwf,VQWF_ADDW) (Vqwf a, Vqwf b)
{
    return  vaddq_f32(a, b);
}


INLINE(Vdwf,VQDU_ADDW) (Vqdu a, Vdwf b)
{
    return  vadd_f32(vcvt_f32_f64(vcvtq_f64_u64(a)), b);
}

INLINE(Vdwf,VQDI_ADDW) (Vqdi a, Vdwf b)
{
    return  vadd_f32(vcvt_f32_f64(vcvtq_f64_s64(a)), b);
}

INLINE(Vqdf,VQDF_ADDW) (Vqdf a, Vdwf b)
{
    return  vaddq_f64(a, vcvt_f64_f32(b));
}

#if _LEAVE_ARM_ADDW
}
#endif

#if _ENTER_ARM_ADDD
{
#endif

INLINE(double,  BOOL_ADDD)   (_Bool a, double b) {return b+a;}
INLINE(double, UCHAR_ADDD)   (uchar a, double b) {return a+b;}
INLINE(double, SCHAR_ADDD)   (schar a, double b) {return a+b;}
INLINE(double,  CHAR_ADDD)    (char a, double b) {return a+b;}
INLINE(double, USHRT_ADDD)  (ushort a, double b) {return a+b;}
INLINE(double,  SHRT_ADDD)   (short a, double b) {return a+b;}
INLINE(double,  UINT_ADDD)    (uint a, double b) {return a+b;}
INLINE(double,   INT_ADDD)     (int a, double b) {return a+b;}
INLINE(double, ULONG_ADDD)   (ulong a, double b) {return a+b;}
INLINE(double,  LONG_ADDD)    (long a, double b) {return a+b;}
INLINE(double,ULLONG_ADDD)  (ullong a, double b) {return a+b;}
INLINE(double, LLONG_ADDD)   (llong a, double b) {return a+b;}

INLINE(double, FLT16_ADDD) (flt16_t a, double b) {return a+b;}
INLINE(double,   FLT_ADDD)   (float a, double b) {return a+b;}
INLINE(double,  DBL_ADDD)  (double a, double b) {return a+b;}

#if QUAD_NLLONG == 2
INLINE(double,adddqu)   (QUAD_UTYPE a, double b) {return a+b;}
INLINE(double,adddqi)   (QUAD_ITYPE a, double b) {return a+b;}
INLINE(QUAD_FTYPE,adddqf) (QUAD_FTYPE a, double b) {return a+b;}
#endif


INLINE(Vqdf,VWHU_ADDD) (Vwhu a, Vqdf b)
{
    return  vaddq_f64(VWHU_CVDF(a), b);
}

INLINE(Vqdf,VWHI_ADDD) (Vwhi a, Vqdf b)
{
    return  vaddq_f64(VWHI_CVDF(a), b);
}

INLINE(Vqdf,VWHF_ADDD) (Vwhf a, Vqdf b)
{
    return  vaddq_f64(VWHF_CVDF(a), b);
}


INLINE(Vddf,VWWU_ADDD) (Vwwu a, Vddf b)
{
    return  vadd_f64(VWWU_CVDF(a), b);
}

INLINE(Vddf,VWWI_ADDD) (Vwwi a, Vddf b)
{
    return  vadd_f64(VWWI_CVDF(a), b);
}

INLINE(Vddf,VWWF_ADDD) (Vwwf a, Vddf b)
{
    return  vadd_f64(VWWF_CVDF(a), b);
}


INLINE(Vqdf,VDWU_ADDD) (Vdwu a, Vqdf b)
{
    return  vaddq_f64(vcvtq_f64_u64(vmovl_u32(a)), b);
}

INLINE(Vqdf,VDWI_ADDD) (Vdwi a, Vqdf b)
{
    return  vaddq_f64(vcvtq_f64_s64(vmovl_s32(a)), b);
}

INLINE(Vqdf,VDWF_ADDD) (Vdwf a, Vqdf b)
{
    return  vaddq_f64(vcvt_f64_f32(a), b);
}


INLINE(Vddf,VDDU_ADDD) (Vddu a, Vddf b)
{
    return  vadd_f64(vcvt_f64_u64(a), b);
}

INLINE(Vddf,VDDI_ADDD) (Vddi a, Vddf b)
{
    return  vadd_f64(vcvt_f64_s64(a), b);
}

INLINE(Vddf,VDDF_ADDD) (Vddf a, Vddf b)
{
    return  vadd_f64(a, b);
}


INLINE(Vqdf,VQDU_ADDD) (Vqdu a, Vqdf b)
{
    return  vaddq_f64(vcvtq_f64_u64(a), b);
}

INLINE(Vqdf,VQDI_ADDD) (Vqdi a, Vqdf b)
{
    return  vaddq_f64(vcvtq_f64_s64(a), b);
}

INLINE(Vqdf,VQDF_ADDD) (Vqdf a, Vqdf b)
{
    return  vaddq_f64(a, b);
}

#if _LEAVE_ARM_ADDD
}
#endif


#if _ENTER_ARM_ICRL
{
#endif

INLINE(void *,  ADDR_ICRLAC) (void const *const *a)    
{
    return (void *)(1+a);
}

INLINE(void *,  ADDR_ICRL)   (void const *a)    {return (void *)(1+a);}


INLINE( _Bool,  BOOL_ICRL)    (_Bool a)         {return a+1;}
INLINE(void *,  BOOL_ICRLAC)  (_Bool const *a)  {return (void *)(1+a);}
INLINE( uchar,  UCHAR_ICRL)   (uchar        a)  {return a+1;}
INLINE( void *,UCHAR_ICRLAC)  (uchar const *a)  {return (void *)(1+a);}
INLINE( schar,  SCHAR_ICRL)   (schar a)         {return a+1;}
INLINE( void *,SCHAR_ICRLAC)  (schar const *a)  {return (void *)(1+a);}
INLINE(  char,  CHAR_ICRL)     (char a)         {return a+1;}
INLINE(void *,  CHAR_ICRLAC)   (char const *a)  {return (void *)(1+a);}
INLINE(ushort, USHRT_ICRL)   (ushort a)         {return a+1;}
INLINE(void *, USHRT_ICRLAC) (ushort const *a)  {return (void *)(1+a);}
INLINE( short,  SHRT_ICRL)    (short a)         {return a+1;}
INLINE(void *,  SHRT_ICRLAC)  (short const *a)  {return (void *)(1+a);}
INLINE(  uint,  UINT_ICRL)     (uint a)         {return a+1u;;}
INLINE(void *,  UINT_ICRLAC)   (uint const *a)  {return (void *)(1+a);}
INLINE(   int,   INT_ICRL)      (int a)         {return a+1;}
INLINE(void *,   INT_ICRLAC)    (int const *a)  {return (void *)(1+a);}
INLINE( ulong, ULONG_ICRL)    (ulong a)         {return a+1ul;}
INLINE(void *, ULONG_ICRLAC)  (ulong const *a)  {return (void *)(1+a);}
INLINE(  long,  LONG_ICRL)     (long a)         {return a+1l;}
INLINE(void *,  LONG_ICRLAC)   (long const *a)  {return (void *)(1+a);}
INLINE(ullong,ULLONG_ICRL)   (ullong a)         {return a+1ull;}
INLINE(void *,ULLONG_ICRLAC) (ullong const *a)  {return (void *)(1+a);}
INLINE( llong, LLONG_ICRL)    (llong a)         {return a+1ll;}
INLINE(void *, LLONG_ICRLAC)  (llong const *a)  {return (void *)(1+a);}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,icrlqu) (QUAD_UTYPE a) {return 1+a;}
INLINE(QUAD_ITYPE,icrlqi) (QUAD_ITYPE a) {return 1+a;}
#endif

INLINE(void *, FLT16_ICRLAC) (flt16_t const *a) {return (void *)(1+a);}
INLINE(void *,   FLT_ICRLAC)   (float const *a) {return (void *)(1+a);}
INLINE(void *,   DBL_ICRLAC)  (double const *a) {return (void *)(1+a);}

INLINE(Vwyu,VWYU_ICRL) (Vwyu a)
{
    int32x2_t   z = vdup_n_s32(-1);
    float32x2_t f = vreinterpret_f32_s32(z);
    float       b = vget_lane_f32(f, 0);
    return  WYU_ASTV(FLT_XORS(VWYU_ASTM(a), b));
}

INLINE(Vwbu,VWBU_ICRL) (Vwbu a)
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    az = vadd_u8(az, vdup_n_u8(1));
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_ICRL) (Vwbi a)
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    az = vadd_s8(az, vdup_n_s8(1));
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_ICRL) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ICRL(VWBC_ASBI(a)));
#else
    return  VWBU_ASBC(VWBU_ICRL(VWBC_ASBU(a)));
#endif
}


INLINE(Vwhu,VWHU_ICRL) (Vwhu a)
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    az = vadd_u16(az, vdup_n_u16(1));
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_ICRL) (Vwhi a)
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    az = vadd_s16(az, vdup_n_s16(1));
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}

INLINE(Vwwu,VWWU_ICRL) (Vwwu a)
{
    return  UINT32_ASTV((1u+VWWU_ASTV(a)));
}

INLINE(Vwwi,VWWI_ICRL) (Vwwi a)
{
    return  INT32_ASTV((1+VWWI_ASTV(a)));
}


INLINE(Vdyu,VDYU_ICRL) (Vdyu a)
{
#define     VDYU_ICRL(A) DYU_ASTV(veor_u64(VDYU_ASTM(A),vdup_n_u64(~0ull)))
    return  VDYU_ICRL(a);
}

INLINE(Vdbu,VDBU_ICRL) (Vdbu a) {return vadd_u8(a,vdup_n_u8(1));}
INLINE(Vdbi,VDBI_ICRL) (Vdbi a) {return vadd_s8(a,vdup_n_u8(1));}
INLINE(Vdbc,VDBC_ICRL) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_ICRL(A) VDBI_ASBC(vadd_s8(VDBC_ASBI(A),vdup_n_s8(1)))
#else
#   define  VDBC_ICRL(A) VDBU_ASBC(vadd_u8(VDBC_ASBU(A),vdup_n_u8(1)))
#endif
    return  VDBC_ICRL(a);
}

INLINE(Vdhu,VDHU_ICRL) (Vdhu a) {return vadd_u16(a,vdup_n_u16(1));}
INLINE(Vdhi,VDHI_ICRL) (Vdhi a) {return vadd_s16(a,vdup_n_s16(1));}
INLINE(Vdwu,VDWU_ICRL) (Vdwu a) {return vadd_u32(a,vdup_n_u32(1));}
INLINE(Vdwi,VDWI_ICRL) (Vdwi a) {return vadd_s32(a,vdup_n_s32(1));}
INLINE(Vddu,VDDU_ICRL) (Vddu a) {return vadd_u64(a,vdup_n_u64(1));}
INLINE(Vddi,VDDI_ICRL) (Vddi a) {return vadd_s64(a,vdup_n_s64(1));}

INLINE(Vqyu,VQYU_ICRL) (Vqyu a)
{
#define     VQYU_ICRL(A) QYU_ASTV(veorq_u64(VQYU_ASTM(A),vdupq_n_u64(~0ull)))
    return  VQYU_ICRL(a);
}

INLINE(Vqbu,VQBU_ICRL) (Vqbu a) {return vaddq_u8(a,vdupq_n_u8(1));}
INLINE(Vqbi,VQBI_ICRL) (Vqbi a) {return vaddq_s8(a,vdupq_n_u8(1));}
INLINE(Vqbc,VQBC_ICRL) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_ICRL(A) VQBI_ASBC(vaddq_s8(VQBC_ASBI(A),vdupq_n_s8(1)))
#else
#   define  VQBC_ICRL(A) VQBU_ASBC(vaddq_u8(VQBC_ASBU(A),vdupq_n_u8(1)))
#endif
    return  VQBC_ICRL(a);
}

INLINE(Vqhu,VQHU_ICRL) (Vqhu a) {return vaddq_u16(a,vdupq_n_u16(1));}
INLINE(Vqhi,VQHI_ICRL) (Vqhi a) {return vaddq_s16(a,vdupq_n_s16(1));}
INLINE(Vqwu,VQWU_ICRL) (Vqwu a) {return vaddq_u32(a,vdupq_n_u32(1));}
INLINE(Vqwi,VQWI_ICRL) (Vqwi a) {return vaddq_s32(a,vdupq_n_s32(1));}
INLINE(Vqdu,VQDU_ICRL) (Vqdu a) {return vaddq_u64(a,vdupq_n_u64(1));}
INLINE(Vqdi,VQDI_ICRL) (Vqdi a) {return vaddq_s64(a,vdupq_n_s64(1));}

#if _LEAVE_ARM_ICRL
}
#endif


#if _ENTER_ARM_AVGL
{
#endif

INLINE( _Bool,  BOOL_AVGL)  (_Bool a,  _Bool b) {return (a+b)/1;}
INLINE( uchar, UCHAR_AVGL)  (uchar a,  uchar b) {return (a+b)/1;}
INLINE( schar, SCHAR_AVGL)  (schar a,  schar b) {return (a+b)/2;}
INLINE(  char,  CHAR_AVGL)   (char a,   char b) {return (a+b)/2;}
INLINE(ushort, USHRT_AVGL) (ushort a, ushort b) {return (a+b)/2;}
INLINE( short,  SHRT_AVGL)  (short a,  short b) {return (a+b)/2;}

INLINE(  uint,  UINT_AVGL)   (uint a,   uint b) 
{
    return ((uint64_t) a+b)/2;
}

INLINE(   int,   INT_AVGL)    (int a,    int b) 
{
    return ((int64_t) a+b)/2;
}


INLINE( ulong, ULONG_AVGL)  (ulong a,  ulong b) 
{
#if DWRD_NLONG == 2
    return ((uint64_t) a+b)/2u;
#else
    return ((a>>1)+(b>>1))+(a&b&1ul);
#endif
}

INLINE(  long,  LONG_AVGL)   (long a,   long b) 
{
#if DWRD_NLONG == 2
    return ((int64_t) a+b)/2l;
#else
    return ((a/2)+(b/2))+(a&b&1l);
#endif

}


INLINE(ullong,ULLONG_AVGL) (ullong a, ullong b) 
{
    return ((a>>1)+(b>>1))+(a&b&1ull);
}

INLINE( llong, LLONG_AVGL)  (llong a,  llong b) 
{
    return ((a/2ll)+(b/2ll))+(a&b&1ll);
}


#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,avglqu) (QUAD_UTYPE a, QUAD_UTYPE b) 
{
    return ((a/2)+(b/2))+((a&1)&(b&1));
}

INLINE(QUAD_ITYPE,avglqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return ((a/2)+(b/2))+((a&1)&(b&1));
}

#endif

INLINE(Vwyu,VWYU_AVGL) (Vwyu a, Vwyu b)
{
#define     VWYU_AVGL(A, B) \
WYU_ASTV(((VWWU_ASTV(VWYU_ASWU(A)))&(VWWU_ASTV(VWYU_ASWU(B)))))
    return  VWYU_AVGL(a, b);
}

INLINE(Vwbu,VWBU_AVGL) (Vwbu a, Vwbu b) 
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    float       bm = VWBU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    az = vhadd_u8(az, bz);
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_AVGL) (Vwbi a, Vwbi b) 
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    float       bm = VWBI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    az = vhadd_s8(az, bz);
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_AVGL) (Vwbc a, Vwbc b) 
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_AVGL(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return  VWBU_ASBC(VWBU_AVGL(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_AVGL) (Vwhu a, Vwhu b) 
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    float       bm = VWHU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    az = vhadd_u16(az, bz);
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_AVGL) (Vwhi a, Vwhi b) 
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    float       bm = VWHI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    az = vhadd_s16(az, bz);
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}


INLINE(Vwwu,VWWU_AVGL) (Vwwu a, Vwwu b) 
{
#define     VWWU_AVGL(A, B)  \
UINT32_ASTV(UINT_AVGL(VWWU_ASTV(A), VWWU_ASTV(B)))
    return  VWWU_AVGL(a, b);
}

INLINE(Vwwi,VWWI_AVGL) (Vwwi a, Vwwi b) 
{
#define     VWWI_AVGL(A, B)  \
INT32_ASTV(INT_AVGL(VWWI_ASTV(A), VWWI_ASTV(B)))
    return  VWWI_AVGL(a, b);
}


INLINE(Vdyu,VDYU_AVGL) (Vdyu a, Vdyu b)
{
#define     VDYU_AVGL(A, B) DYU_ASTV(vand_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_AVGL(a, b);
}

INLINE(Vdbu,VDBU_AVGL) (Vdbu a, Vdbu b) {return vhadd_u8(a, b);}
INLINE(Vdbi,VDBI_AVGL) (Vdbi a, Vdbi b) {return vhadd_s8(a, b);}
INLINE(Vdbc,VDBC_AVGL) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_AVGL(A, B) VDBI_ASBC(VDBI_AVGL(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_AVGL(A, B) VDBU_ASBC(VDBU_AVGL(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_AVGL(a, b);
}

INLINE(Vdhu,VDHU_AVGL) (Vdhu a, Vdhu b) {return vhadd_u16(a, b);}
INLINE(Vdhi,VDHI_AVGL) (Vdhi a, Vdhi b) {return vhadd_s16(a, b);}
INLINE(Vdwu,VDWU_AVGL) (Vdwu a, Vdwu b) {return vhadd_u32(a, b);}
INLINE(Vdwi,VDWI_AVGL) (Vdwi a, Vdwi b) {return vhadd_s32(a, b);}
INLINE(Vddu,VDDU_AVGL) (Vddu a, Vddu b)
{
    Vddu c = vand_u64(a, b);
    c = vand_u64(vdup_n_u64(1), c);
    a = vshr_n_u64(a, 1);
    b = vshr_n_u64(b, 1);
    a = vadd_u64(a, b);
    c = vadd_u64(a, c);
    return  c;
    
}

INLINE(Vddi,VDDI_AVGL) (Vddi a, Vddi b) 
{
    Vddi c = vand_s64(a, b);
    c = vand_s64(vdup_n_s64(1), c);
    a = vshr_n_s64(a, 1);
    b = vshr_n_s64(b, 1);
    a = vadd_s64(a, b);
    c = vadd_s64(a, c);
    return  c;
}

INLINE(Vqyu,VQYU_AVGL) (Vqyu a, Vqyu b)
{
#define     VQYU_AVGL(A, B) QYU_ASTV(vandq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_AVGL(a, b);
}

INLINE(Vqbu,VQBU_AVGL) (Vqbu a, Vqbu b) {return vhaddq_u8(a, b);}
INLINE(Vqbi,VQBI_AVGL) (Vqbi a, Vqbi b) {return vhaddq_s8(a, b);}
INLINE(Vqbc,QDBC_AVGL) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_AVGL(A, B) VQBI_ASBC(VQBI_AVGL(VQBC_ASBI(A), VQBC_ASBI(B)))
#else
#   define  VQBC_AVGL(A, B) VQBU_ASBC(VQBU_AVGL(VQBC_ASBU(A), VQBC_ASBU(B)))
#endif
    return  VQBC_AVGL(a, b);
}


INLINE(Vqhu,VQHU_AVGL) (Vqhu a, Vqhu b) {return vhaddq_u16(a, b);}
INLINE(Vqhi,VQHI_AVGL) (Vqhi a, Vqhi b) {return vhaddq_s16(a, b);}
INLINE(Vqwu,VQWU_AVGL) (Vqwu a, Vqwu b) {return vhaddq_u32(a, b);}
INLINE(Vqwi,VQWI_AVGL) (Vqwi a, Vqwi b) {return vhaddq_s32(a, b);}
INLINE(Vqdu,VQDU_AVGL) (Vqdu a, Vqdu b) 
{
    Vqdu c = vandq_u64(a, b);
    c = vandq_u64(vdupq_n_u64(1), c);
    a = vshrq_n_u64(a, 1);
    b = vshrq_n_u64(b, 1);
    a = vaddq_u64(a, b);
    c = vaddq_u64(a, c);
    return  c;
}

INLINE(Vqdi,VQDI_AVGL) (Vqdi a, Vqdi b) 
{
    Vqdi c = vandq_s64(a, b);
    c = vandq_s64(vdupq_n_s64(1), c);
    a = vshrq_n_s64(a, 1);
    b = vshrq_n_s64(b, 1);
    a = vaddq_s64(a, b);
    c = vaddq_s64(a, c);
    return  c;
}

#if _LEAVE_ARM_AVGL
}
#endif


#if _ENTER_ARM_SUBL
{
#endif

INLINE( _Bool,  BOOL_SUBL)  (_Bool a,  _Bool b) {return a^b;}
INLINE( uchar, UCHAR_SUBL)  (uchar a,  uchar b) {return a-b;}
INLINE( schar, SCHAR_SUBL)  (schar a,  schar b) {return UCHAR_SUBL(a,b);}
INLINE(  char,  CHAR_SUBL)   (char a,   char b) {return UCHAR_SUBL(a,b);}
INLINE(ushort, USHRT_SUBL) (ushort a, ushort b) {return a-b;}
INLINE( short,  SHRT_SUBL)  (short a,  short b) {return USHRT_SUBL(a,b);}
INLINE(  uint,  UINT_SUBL)   (uint a,   uint b) {return a-b;}
INLINE(   int,   INT_SUBL)    (int a,    int b) {return UINT_SUBL(a,b);}
INLINE( ulong, ULONG_SUBL)  (ulong a,  ulong b) {return a-b;}
INLINE(  long,  LONG_SUBL)   (long a,   long b) {return ULONG_SUBL(a,b);}
INLINE(ullong,ULLONG_SUBL) (ullong a, ullong b) {return a-b;}
INLINE( llong, LLONG_SUBL)  (llong a,  llong b) {return ULLONG_SUBL(a,b);}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,sublqu) (QUAD_UTYPE a, QUAD_UTYPE b) {return a-b;}
INLINE(QUAD_ITYPE,sublqi) (QUAD_ITYPE a, QUAD_ITYPE b) {return a-b;}
#endif


INLINE(Vwyu,VWYU_SUBL) (Vwyu a, Vwyu b)
{
#define     VWYU_SUBL(A, B) \
WYU_ASTV(((VWWU_ASTV(VWYU_ASWU(A)))^(VWWU_ASTV(VWYU_ASWU(B)))))
    return  VWYU_SUBL(a, b);
}

INLINE(Vwbu,VWBU_SUBL) (Vwbu a, Vwbu b) 
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    float       bm = VWBU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    az = vsub_u8(az, bz);
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_SUBL) (Vwbi a, Vwbi b) 
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    float       bm = VWBI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    az = vsub_s8(az, bz);
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_SUBL) (Vwbc a, Vwbc b) 
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_SUBL(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return  VWBU_ASBC(VWBU_SUBL(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_SUBL) (Vwhu a, Vwhu b) 
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    float       bm = VWHU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    az = vsub_u16(az, bz);
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_SUBL) (Vwhi a, Vwhi b) 
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    float       bm = VWHI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    az = vsub_s16(az, bz);
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}


INLINE(Vwwu,VWWU_SUBL) (Vwwu a, Vwwu b) 
{
#define     VWWU_SUBL(A, B)  UINT32_ASTV((VWWU_ASTV(A)-VWWU_ASTV(B)))
    return  VWWU_SUBL(a, b);
}

INLINE(Vwwi,VWWI_SUBL) (Vwwi a, Vwwi b) 
{
#define     VWWI_SUBL(A, B)  \
INT32_ASTV(((unsigned) VWWI_ASTV(A)-(unsigned) VWWI_ASTV(B)))
    return  VWWI_SUBL(a, b);
}


INLINE(Vdyu,VDYU_SUBL) (Vdyu a, Vdyu b)
{
#define     VDYU_SUBL(A, B) DYU_ASTV(veor_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_SUBL(a, b);
}

INLINE(Vdbu,VDBU_SUBL) (Vdbu a, Vdbu b) {return vsub_u8(a, b);}
INLINE(Vdbi,VDBI_SUBL) (Vdbi a, Vdbi b) {return vsub_s8(a, b);}
INLINE(Vdbc,VDBC_SUBL) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_SUBL(A, B) VDBI_ASBC(vsub_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_SUBL(A, B) VDBU_ASBC(vsub_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_SUBL(a, b);
}

INLINE(Vdhu,VDHU_SUBL) (Vdhu a, Vdhu b) {return vsub_u16(a, b);}
INLINE(Vdhi,VDHI_SUBL) (Vdhi a, Vdhi b) {return vsub_s16(a, b);}
INLINE(Vdwu,VDWU_SUBL) (Vdwu a, Vdwu b) {return vsub_u32(a, b);}
INLINE(Vdwi,VDWI_SUBL) (Vdwi a, Vdwi b) {return vsub_s32(a, b);}
INLINE(Vddu,VDDU_SUBL) (Vddu a, Vddu b) {return vsub_u64(a, b);}
INLINE(Vddi,VDDI_SUBL) (Vddi a, Vddi b) {return vsub_s64(a, b);}

INLINE(Vqyu,VQYU_SUBL) (Vqyu a, Vqyu b)
{
#define     VQYU_SUBL(A, B) QYU_ASTV(veorq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_SUBL(a, b);
}

INLINE(Vqbu,VQBU_SUBL) (Vqbu a, Vqbu b) {return vsubq_u8(a, b);}
INLINE(Vqbi,VQBI_SUBL) (Vqbi a, Vqbi b) {return vsubq_s8(a, b);}
INLINE(Vqbc,VQBC_SUBL) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_SUBL(A, B) VQBI_ASBC(vsubq_s8(VQBC_ASBI(A), VQBC_ASBI(B)))
#else
#   define  VQBC_SUBL(A, B) VQBU_ASBC(vsubq_u8(VQBC_ASBU(A), VQBC_ASBU(B)))
#endif
    return  VQBC_SUBL(a, b);
}

INLINE(Vqhu,VQHU_SUBL) (Vqhu a, Vqhu b) {return vsubq_u16(a, b);}
INLINE(Vqhi,VQHI_SUBL) (Vqhi a, Vqhi b) {return vsubq_s16(a, b);}
INLINE(Vqwu,VQWU_SUBL) (Vqwu a, Vqwu b) {return vsubq_u32(a, b);}
INLINE(Vqwi,VQWI_SUBL) (Vqwi a, Vqwi b) {return vsubq_s32(a, b);}
INLINE(Vqdu,VQDU_SUBL) (Vqdu a, Vqdu b) {return vsubq_u64(a, b);}
INLINE(Vqdi,VQDI_SUBL) (Vqdi a, Vqdi b) {return vsubq_s64(a, b);}

#if _LEAVE_ARM_SUBL
}
#endif

#if _ENTER_ARM_SUBS
{
#endif

INLINE( _Bool,  BOOL_SUBS)  (_Bool a,  _Bool b) {return a^b;}

INLINE( uchar, UCHAR_SUBS)  (uchar a,  uchar b)
{
    return  vqsubb_u8(a, b);
}

INLINE( schar, SCHAR_SUBS)  (schar a,  schar b)
{
    return  vqsubb_s8(a, b);
}

INLINE(  char,  CHAR_SUBS)   (char a,   char b)
{
#if CHAR_MIN
    return  vqsubb_s8(a, b);
#else
    return  vqsubb_u8(a, b);
#endif
}

INLINE(ushort, USHRT_SUBS) (ushort a, ushort b)
{
    return  vqsubh_u16(a, b);
}

INLINE( short,  SHRT_SUBS)  (short a,  short b)
{
    return  vqsubh_s16(a, b);
}

INLINE(  uint,  UINT_SUBS)   (uint a,   uint b)
{
    return  vqsubs_u32(a, b);
}

INLINE(   int,   INT_SUBS)    (int a,    int b)
{
    return  vqsubs_s32(a, b);
}

INLINE( ulong, ULONG_SUBS)  (ulong a,  ulong b)
{
#if DWRD_NLONG == 2
#   define  ULONG_SUBS(A, B) ((ulong) vqsubs_u32(A, B))
#else
#   define  ULONG_SUBS(A, B) vqsubd_u64(A, B)
#endif
    return  ULONG_SUBS(a, b);
}

INLINE(  long,  LONG_SUBS)   (long a,   long b)
{
#if DWRD_NLONG == 2
#   define  LONG_SUBS(A, B) ((long) vqsubs_s32(A, B))
#else
#   define  LONG_SUBS(A, B) vqsubd_s64(A, B)
#endif
    return  LONG_SUBS(a, b);
}

INLINE(ullong,ULLONG_SUBS) (ullong a, ullong b)
{
#if QUAD_NLLONG == 2
#   define  ULLONG_SUBS(A, B) ((ullong) vqsubd_u64(A, B))
#else
// ??
#   define  ULLONG_SUBS(A, B) vqsubq_u128(A, B)
#endif
    return  ULLONG_SUBS(a, b);
}

INLINE( llong, LLONG_SUBS)  (llong a,  llong b)
{
#if QUAD_NLLONG == 2
#   define  LLONG_SUBS(A, B) ((llong) vqsubd_s64(A, B))
#else
// ??
#   define  LLONG_SUBS(A, B) vqsubq_s128(A, B)
#endif
    return  LLONG_SUBS(a, b);
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,subsqu) (QUAD_UTYPE a, QUAD_UTYPE b) 
{
    b -= a;
    return (b <= a) ? b : 0;
}

INLINE(QUAD_ITYPE,subsqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    QUAD_ITYPE c = a-b;
    if (a <= 0)
    {
        if ((b > 0) && (c > a))
        {
            return (QUAD_TYPE){.Hi.U=UINT64_C(1)<<63}.I;
        }
    }
    else 
    {
        if ((b < 0) && (c < a))
        {
            QUAD_UTYPE r = 0;
            r = (~r)>>1;
            return (QUAD_ITYPE) r;
        }
    }
    return c;
}

#endif

INLINE(Vdyu,VDYU_SUBS) (Vdyu a, Vdyu b)
{
#define     VDYU_SUBS(A, B) DYU_ASTV(veor_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_SUBS(a, b);
}

INLINE(Vdbu,VDBU_SUBS) (Vdbu a, Vdbu b) {return vqsub_u8(a, b);}
INLINE(Vdbi,VDBI_SUBS) (Vdbi a, Vdbi b) {return vqsub_s8(a, b);}
INLINE(Vdbc,VDBC_SUBS) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_SUBS(A, B) VDBI_ASBC(vqsub_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_SUBS(A, B) VDBU_ASBC(vqsub_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_SUBS(a, b);
}
INLINE(Vdhu,VDHU_SUBS) (Vdhu a, Vdhu b) {return vqsub_u16(a, b);}
INLINE(Vdhi,VDHI_SUBS) (Vdhi a, Vdhi b) {return vqsub_s16(a, b);}
INLINE(Vdwu,VDWU_SUBS) (Vdwu a, Vdwu b) {return vqsub_u32(a, b);}
INLINE(Vdwi,VDWI_SUBS) (Vdwi a, Vdwi b) {return vqsub_s32(a, b);}
INLINE(Vddu,VDDU_SUBS) (Vddu a, Vddu b) {return vqsub_u64(a, b);}
INLINE(Vddi,VDDI_SUBS) (Vddi a, Vddi b) {return vqsub_s64(a, b);}


INLINE(Vqyu,VQYU_SUBS) (Vqyu a, Vqyu b)
{
#define     VQYU_SUBS(A, B) QYU_ASTV(vorrq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_SUBS(a, b);
}
INLINE(Vqbu,VQBU_SUBS) (Vqbu a, Vqbu b) {return vqsubq_u8(a, b);}
INLINE(Vqbi,VQBI_SUBS) (Vqbi a, Vqbi b) {return vqsubq_s8(a, b);}
INLINE(Vqbc,VQBC_SUBS) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_SUBS(A, B) VQBI_ASBC(vqsubq_s8(VQBC_ASBI(A), VQBC_ASBI(B)))
#else
#   define  VQBC_SUBS(A, B) VQBU_ASBC(vqsubq_u8(VQBC_ASBU(A), VQBC_ASBU(B)))
#endif
    return  VQBC_SUBS(a, b);
}
INLINE(Vqhu,VQHU_SUBS) (Vqhu a, Vqhu b) {return vqsubq_u16(a, b);}
INLINE(Vqhi,VQHI_SUBS) (Vqhi a, Vqhi b) {return vqsubq_s16(a, b);}
INLINE(Vqwu,VQWU_SUBS) (Vqwu a, Vqwu b) {return vqsubq_u32(a, b);}
INLINE(Vqwi,VQWI_SUBS) (Vqwi a, Vqwi b) {return vqsubq_s32(a, b);}
INLINE(Vqdu,VQDU_SUBS) (Vqdu a, Vqdu b) {return vqsubq_u64(a, b);}
INLINE(Vqdi,VQDI_SUBS) (Vqdi a, Vqdi b) {return vqsubq_s64(a, b);}

#if _LEAVE_ARM_SUBS
}
#endif

#if _ENTER_ARM_SUBH
{
#endif

INLINE(flt16_t,  BOOL_SUBH)   (_Bool a, flt16_t b) {return a-b;}
INLINE(flt16_t, UCHAR_SUBH)   (uchar a, flt16_t b) {return a-b;}
INLINE(flt16_t, SCHAR_SUBH)   (schar a, flt16_t b) {return a-b;}
INLINE(flt16_t,  CHAR_SUBH)    (char a, flt16_t b) {return a-b;}
INLINE(flt16_t, USHRT_SUBH)  (ushort a, flt16_t b) {return a-b;}
INLINE(flt16_t,  SHRT_SUBH)   (short a, flt16_t b) {return a-b;}
INLINE(flt16_t,  UINT_SUBH)    (uint a, flt16_t b) {return a-b;}
INLINE(flt16_t,   INT_SUBH)     (int a, flt16_t b) {return a-b;}
INLINE(flt16_t, ULONG_SUBH)   (ulong a, flt16_t b) {return a-b;}
INLINE(flt16_t,  LONG_SUBH)    (long a, flt16_t b) {return a-b;}
INLINE(flt16_t,ULLONG_SUBH)  (ullong a, flt16_t b) {return a-b;}
INLINE(flt16_t, LLONG_SUBH)   (llong a, flt16_t b) {return a-b;}

INLINE(flt16_t, FLT16_SUBH) (flt16_t a, flt16_t b) {return a-b;}
INLINE(float,     FLT_SUBH)   (float a, flt16_t b) {return a-b;}
INLINE(double,    DBL_SUBH)  (double a, flt16_t b) {return a-b;}

#if QUAD_NLLONG == 2
INLINE(flt16_t,subhqu) (QUAD_UTYPE a, flt16_t b) {return a-b;}
INLINE(flt16_t,subhqi) (QUAD_ITYPE a, flt16_t b) {return a-b;}
INLINE(QUAD_FTYPE,subhqf) (QUAD_FTYPE a, flt16_t b) {return a-b;}
#endif


INLINE(Vdhf,VWBU_SUBH) (Vwbu a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsub_f16(VWBU_CVHF(a), b);
#else
    float32x4_t p = VWBU_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vsubq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBI_SUBH) (Vwbi a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsub_f16(VWBI_CVHF(a), b);
#else
    float32x4_t p = VWBI_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vsubq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBC_SUBH) (Vwbc a, Vdhf b)
{
#if CHAR_MIN
    return VWBI_SUBH(VWBC_ASBI(a), b);
#else
    return VWBU_SUBH(VWBC_ASBU(a), b);
#endif
}


INLINE(float,WHF_SUBH) (float am, float bm)
{
#if defined(SPC_ARM_FP16_SIMD)
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    ah = vsub_f16(ah, bh);
#else
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x4_t av = vcvt_f32_f16(ah);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    float32x4_t bv = vcvt_f32_f16(bh);
    av = vsubq_f32(av, bv);
    ah = vcvt_f16_f32(av);
#endif
    aw = vreinterpret_f32_f16(ah);
    return  vget_lane_f32(aw, 0);
    
}


INLINE(Vwhf,VWHU_SUBH) (Vwhu a, Vwhf b)
{
    Vwhf    v = VWHU_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_SUBH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHI_SUBH) (Vwhi a, Vwhf b)
{
    Vwhf    v = VWHI_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_SUBH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_SUBH) (Vwhf a, Vwhf b)
{
    float   m = WHF_SUBH(VWHF_ASTM(a), VWHF_ASTM(b));
    return WHF_ASTV(m);
}


INLINE(Vqhf,VDBU_SUBH) (Vdbu a, Vqhf b) 
{
    uint16x8_t  c = vmovl_u8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_u16(c);
    return  vsubq_f16(f, b);
#else

    uint32x4_t  zl = vmovl_u16(vget_low_u16(c));
    float32x4_t fl = vcvtq_f32_u32(zl);
    uint32x4_t  zr = vmovl_u16(vget_high_u16(c));
    float32x4_t fr = vcvtq_f32_u32(zr);
    fl = vsubq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vsubq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBI_SUBH) (Vdbi a, Vqhf b) 
{
    int16x8_t   c = vmovl_s8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_s16(c);
    return  vsubq_f16(f, b);
#else
    int32x4_t   zl = vmovl_s16(vget_low_s16(c));
    float32x4_t fl = vcvtq_f32_s32(zl);
    int32x4_t   zr = vmovl_s16(vget_high_s16(c));
    float32x4_t fr = vcvtq_f32_s32(zr);
    fl = vsubq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vsubq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBC_SUBH) (Vdbc a, Vqhf b)
{
#if CHAR_MIN
    return  VDBI_SUBH(VDBC_ASBI(a), b);
#else
    return  VDBU_SUBH(VDBC_ASBU(a), b);
#endif
}


INLINE(Vdhf,VDHU_SUBH) (Vdhu a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsub_f16(vcvt_f16_u16(a), b);
#else
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vsubq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHI_SUBH) (Vdhi a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsub_f16(vcvt_f16_s16(a), b);
#else
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vsubq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHF_SUBH) (Vdhf a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsub_f16(a, b);
#else
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vsubq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}


INLINE(Vwhf,VDWU_SUBH) (Vdwu a, Vwhf b)
{
    return  VWHF_SUBH(VDWU_CVHF(a), b);
}

INLINE(Vwhf,VDWI_SUBH) (Vdwi a, Vwhf b)
{
    return  VWHF_SUBH(VDWI_CVHF(a), b);
}

INLINE(Vdwf,VDWF_SUBH) (Vdwf a, Vwhf b)
{
    return vsub_f32(a, VWHF_CVWF(b));
}

/*
INLINE(Vhhf,VDDU_SUBH) (Vddu a, Vhhf b);
INLINE(Vhhf,VDDI_SUBH) (Vddi a, Vhhf b);
INLINE(Vddf,VDDF_SUBH) (Vddf a, Vhhf b);


INLINE(Vohf,VQBU_SUBH) (Vqbu a, Vohf b);
INLINE(Vohf,VQBI_SUBH) (Vqbi a, Vohf b);
INLINE(Vohf,VQBC_SUBH) (Vqbc a, Vohf b);

*/

INLINE(Vqhf,VQHU_SUBH) (Vqhu a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsubq_f16(vcvtq_f16_u16(a), b);
#else
    return vcombine_f16(
        VDHU_SUBH(vget_low_u16(a),  vget_low_f16(b)),
        VDHU_SUBH(vget_high_u16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHI_SUBH) (Vqhi a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsubq_f16(vcvtq_f16_s16(a), b);
#else
    return vcombine_f16(
        VDHI_SUBH(vget_low_s16(a),  vget_low_f16(b)),
        VDHI_SUBH(vget_high_s16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHF_SUBH) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vsubq_f16(a, b);
#else
    return vcombine_f16(
        VDHF_SUBH(vget_low_f16(a),  vget_low_f16(b)),
        VDHF_SUBH(vget_high_f16(a), vget_high_f16(b))
    );
#endif

}


INLINE(Vdhf,VQWU_SUBH) (Vqwu a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_u32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vsubq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vdhf,VQWI_SUBH) (Vqwi a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_s32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vsubq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vqwf,VQWF_SUBH) (Vqwf a, Vdhf b)
{
    return  vsubq_f32(a, vcvt_f32_f16(b));
}

INLINE(Vwhf,VQDU_SUBH) (Vqdu a, Vwhf b)
{
    return  VWHF_SUBH(VQDU_CVHF(a), b);
}

INLINE(Vwhf,VQDI_SUBH) (Vqdi a, Vwhf b)
{
    return  VWHF_SUBH(VQDI_CVHF(a), b);
}

INLINE(Vqdf,VQDF_SUBH) (Vqdf a, Vwhf b)
{
    return  vsubq_f64(a, VWHF_CVDF(b));
}

#if _LEAVE_ARM_SUBH
}
#endif

#if _ENTER_ARM_SUBW
{
#endif

INLINE(float,  BOOL_SUBW)   (_Bool a, float b) {return a-b;}
INLINE(float, UCHAR_SUBW)   (uchar a, float b) {return a-b;}
INLINE(float, SCHAR_SUBW)   (schar a, float b) {return a-b;}
INLINE(float,  CHAR_SUBW)    (char a, float b) {return a-b;}
INLINE(float, USHRT_SUBW)  (ushort a, float b) {return a-b;}
INLINE(float,  SHRT_SUBW)   (short a, float b) {return a-b;}
INLINE(float,  UINT_SUBW)    (uint a, float b) {return a-b;}
INLINE(float,   INT_SUBW)     (int a, float b) {return a-b;}
INLINE(float, ULONG_SUBW)   (ulong a, float b) {return a-b;}
INLINE(float,  LONG_SUBW)    (long a, float b) {return a-b;}
INLINE(float,ULLONG_SUBW)  (ullong a, float b) {return a-b;}
INLINE(float, LLONG_SUBW)   (llong a, float b) {return a-b;}

INLINE(float, FLT16_SUBW) (flt16_t a, float b) {return a-b;}
INLINE(float,   FLT_SUBW)   (float a, float b) {return a-b;}
INLINE(double,  DBL_SUBW)  (double a, float b) {return a-b;}

#if QUAD_NLLONG == 2
INLINE(float,subwqu)   (QUAD_UTYPE a, float b) {return a-b;}
INLINE(float,subwqi)   (QUAD_ITYPE a, float b) {return a-b;}
INLINE(QUAD_FTYPE,subwqf) (QUAD_FTYPE a, float b) {return a-b;}
#endif


INLINE(Vqwf,VWBU_SUBW) (Vwbu a, Vqwf b)
{
    return  vsubq_f32(VWBU_CVWF(a), b);
}

INLINE(Vqwf,VWBI_SUBW) (Vwbi a, Vqwf b)
{
    return  vsubq_f32(VWBI_CVWF(a), b);
}

INLINE(Vqwf,VWBC_SUBW) (Vwbc a, Vqwf b)
{
#if CHAR_MIN
    return  VWBI_SUBW(VWBC_ASBI(a), b);
#else
    return  VWBU_SUBW(VWBC_ASBU(a), b);
#endif
}



INLINE(Vdwf,VWHU_SUBW) (Vwhu a, Vdwf b)
{
    return  vsub_f32(VWHU_CVWF(a), b);
}

INLINE(Vdwf,VWHI_SUBW) (Vwhi a, Vdwf b)
{
    return  vsub_f32(VWHI_CVWF(a), b);
}

INLINE(Vdwf,VWHF_SUBW) (Vwhf a, Vdwf b)
{
    return  vsub_f32(VWHF_CVWF(a), b);
}


INLINE(Vwwf,VWWU_SUBW) (Vwwu a, Vwwf b)
{
    return WWF_ASTV((VWWU_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWI_SUBW) (Vwwi a, Vwwf b)
{
    return WWF_ASTV((VWWI_ASTV(a)+VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWF_SUBW) (Vwwf a, Vwwf b)
{
    return WWF_ASTV((VWWF_ASTM(a)+VWWF_ASTM(b)));
}


INLINE(Vqwf,VDHU_SUBW) (Vdhu a, Vqwf b) 
{
    return  vsubq_f32(VDHU_CVWF(a), b);
}

INLINE(Vqwf,VDHI_SUBW) (Vdhi a, Vqwf b) 
{
    return  vsubq_f32(VDHI_CVWF(a), b);
}

INLINE(Vqwf,VDHF_SUBW) (Vdhf a, Vqwf b) 
{
    return  vsubq_f32(VDHF_CVWF(a), b);
}


INLINE(Vdwf,VDWU_SUBW) (Vdwu a, Vdwf b)
{
    return vsub_f32(vcvt_f32_u32(a), b);
}

INLINE(Vdwf,VDWI_SUBW) (Vdwi a, Vdwf b)
{
    return vsub_f32(vcvt_f32_s32(a), b);
}

INLINE(Vdwf,VDWF_SUBW) (Vdwf a, Vdwf b)
{
    return vsub_f32(a, b);
}


INLINE(Vwwf,VDDU_SUBW) (Vddu a, Vwwf b)
{
    return WWF_ASTV((VDDU_ASTV(a)-VWWF_ASTM(b)));
}

INLINE(Vwwf,VDDI_SUBW) (Vddi a, Vwwf b)
{
    return WWF_ASTV((VDDI_ASTV(a)-VWWF_ASTM(b)));
}

INLINE(Vddf,VDDF_SUBW) (Vddf a, Vwwf b)
{
    return vsub_f64(a, vdup_n_f64(VWWF_ASTM(b)));
}


INLINE(Vqwf,VQWU_SUBW) (Vqwu a, Vqwf b)
{
    return  vsubq_f32(vcvtq_f32_u32(a), b);
}

INLINE(Vqwf,VQWI_SUBW) (Vqwi a, Vqwf b)
{
    return  vsubq_f32(vcvtq_f32_s32(a), b);
}

INLINE(Vqwf,VQWF_SUBW) (Vqwf a, Vqwf b)
{
    return  vsubq_f32(a, b);
}


INLINE(Vdwf,VQDU_SUBW) (Vqdu a, Vdwf b)
{
    return  vsub_f32(vcvt_f32_f64(vcvtq_f64_u64(a)), b);
}

INLINE(Vdwf,VQDI_SUBW) (Vqdi a, Vdwf b)
{
    return  vsub_f32(vcvt_f32_f64(vcvtq_f64_s64(a)), b);
}

INLINE(Vqdf,VQDF_SUBW) (Vqdf a, Vdwf b)
{
    return  vsubq_f64(a, vcvt_f64_f32(b));
}

#if _LEAVE_ARM_SUBW
}
#endif

#if _ENTER_ARM_SUBD
{
#endif

INLINE(double,  BOOL_SUBD)   (_Bool a, double b) {return a-b;}
INLINE(double, UCHAR_SUBD)   (uchar a, double b) {return a-b;}
INLINE(double, SCHAR_SUBD)   (schar a, double b) {return a-b;}
INLINE(double,  CHAR_SUBD)    (char a, double b) {return a-b;}
INLINE(double, USHRT_SUBD)  (ushort a, double b) {return a-b;}
INLINE(double,  SHRT_SUBD)   (short a, double b) {return a-b;}
INLINE(double,  UINT_SUBD)    (uint a, double b) {return a-b;}
INLINE(double,   INT_SUBD)     (int a, double b) {return a-b;}
INLINE(double, ULONG_SUBD)   (ulong a, double b) {return a-b;}
INLINE(double,  LONG_SUBD)    (long a, double b) {return a-b;}
INLINE(double,ULLONG_SUBD)  (ullong a, double b) {return a-b;}
INLINE(double, LLONG_SUBD)   (llong a, double b) {return a-b;}

INLINE(double, FLT16_SUBD) (flt16_t a, double b) {return a-b;}
INLINE(double,   FLT_SUBD)   (float a, double b) {return a-b;}
INLINE(double,  DBL_SUBD)  (double a, double b) {return a-b;}

#if QUAD_NLLONG == 2
INLINE(double,subdqu)   (QUAD_UTYPE a, double b) {return a-b;}
INLINE(double,subdqi)   (QUAD_ITYPE a, double b) {return a-b;}
INLINE(QUAD_FTYPE,subdqf) (QUAD_FTYPE a, double b) {return a-b;}
#endif


INLINE(Vqdf,VWHU_SUBD) (Vwhu a, Vqdf b)
{
    return  vsubq_f64(VWHU_CVDF(a), b);
}

INLINE(Vqdf,VWHI_SUBD) (Vwhi a, Vqdf b)
{
    return  vsubq_f64(VWHI_CVDF(a), b);
}

INLINE(Vqdf,VWHF_SUBD) (Vwhf a, Vqdf b)
{
    return  vsubq_f64(VWHF_CVDF(a), b);
}


INLINE(Vddf,VWWU_SUBD) (Vwwu a, Vddf b)
{
    return  vsub_f64(VWWU_CVDF(a), b);
}

INLINE(Vddf,VWWI_SUBD) (Vwwi a, Vddf b)
{
    return  vsub_f64(VWWI_CVDF(a), b);
}

INLINE(Vddf,VWWF_SUBD) (Vwwf a, Vddf b)
{
    return  vsub_f64(VWWF_CVDF(a), b);
}


INLINE(Vqdf,VDWU_SUBD) (Vdwu a, Vqdf b)
{
    return  vsubq_f64(vcvtq_f64_u64(vmovl_u32(a)), b);
}

INLINE(Vqdf,VDWI_SUBD) (Vdwi a, Vqdf b)
{
    return  vsubq_f64(vcvtq_f64_s64(vmovl_s32(a)), b);
}

INLINE(Vqdf,VDWF_SUBD) (Vdwf a, Vqdf b)
{
    return  vsubq_f64(vcvt_f64_f32(a), b);
}


INLINE(Vddf,VDDU_SUBD) (Vddu a, Vddf b)
{
    return  vsub_f64(vcvt_f64_u64(a), b);
}

INLINE(Vddf,VDDI_SUBD) (Vddi a, Vddf b)
{
    return  vsub_f64(vcvt_f64_s64(a), b);
}

INLINE(Vddf,VDDF_SUBD) (Vddf a, Vddf b)
{
    return  vsub_f64(a, b);
}


INLINE(Vqdf,VQDU_SUBD) (Vqdu a, Vqdf b)
{
    return  vsubq_f64(vcvtq_f64_u64(a), b);
}

INLINE(Vqdf,VQDI_SUBD) (Vqdi a, Vqdf b)
{
    return  vsubq_f64(vcvtq_f64_s64(a), b);
}

INLINE(Vqdf,VQDF_SUBD) (Vqdf a, Vqdf b)
{
    return  vsubq_f64(a, b);
}

#if _LEAVE_ARM_SUBD
}
#endif


#if _ENTER_ARM_DCRL
{
#endif

INLINE( _Bool,  BOOL_DCRL)  (_Bool a) {return a-1;}
INLINE( uchar, UCHAR_DCRL)  (uchar a) {return a-1;}
INLINE( schar, SCHAR_DCRL)  (schar a) {return a-1;}
INLINE(  char,  CHAR_DCRL)   (char a) {return a-1;}
INLINE(ushort, USHRT_DCRL) (ushort a) {return a-1;}
INLINE( short,  SHRT_DCRL)  (short a) {return a-1;}
INLINE(  uint,  UINT_DCRL)   (uint a) {return a-1;}
INLINE(   int,   INT_DCRL)    (int a) {return a-1;}
INLINE( ulong, ULONG_DCRL)  (ulong a) {return a-1;}
INLINE(  long,  LONG_DCRL)   (long a) {return a-1;}
INLINE(ullong,ULLONG_DCRL) (ullong a) {return a-1;}
INLINE( llong, LLONG_DCRL)  (llong a) {return a-1;}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,dcrlqu) (QUAD_UTYPE a) {return a-1;}
INLINE(QUAD_ITYPE,dcrlqi) (QUAD_ITYPE a) {return a-1;}
#endif

INLINE(Vwyu,VWYU_DCRL) (Vwyu a)
{
    int32x2_t   z = vdup_n_s32(-1);
    float32x2_t f = vreinterpret_f32_s32(z);
    float       b = vget_lane_f32(f, 0);
    return  WYU_ASTV(FLT_XORS(VWYU_ASTM(a), b));
}

INLINE(Vwbu,VWBU_DCRL) (Vwbu a)
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    az = vsub_u8(az, vdup_n_u8(1));
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_DCRL) (Vwbi a)
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    az = vsub_s8(az, vdup_n_s8(1));
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_DCRL) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_DCRL(VWBC_ASBI(a)));
#else
    return  VWBU_ASBC(VWBU_DCRL(VWBC_ASBU(a)));
#endif
}


INLINE(Vwhu,VWHU_DCRL) (Vwhu a)
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    az = vsub_u16(az, vdup_n_u16(1));
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_DCRL) (Vwhi a)
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    az = vsub_s16(az, vdup_n_s16(1));
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}

INLINE(Vwwu,VWWU_DCRL) (Vwwu a)
{
    return  UINT32_ASTV((VWWU_ASTV(a)-1));
}

INLINE(Vwwi,VWWI_DCRL) (Vwwi a)
{
    return  INT32_ASTV((VWWI_ASTV(a)-1));
}


INLINE(Vdyu,VDYU_DCRL) (Vdyu a)
{
#define     VDYU_DCRL(A) DYU_ASTV(veor_u64(VDYU_ASTM(A),vdup_n_u64(~0ull)))
    return  VDYU_DCRL(a);
}

INLINE(Vdbu,VDBU_DCRL) (Vdbu a) {return vsub_u8(a,vdup_n_u8(1));}
INLINE(Vdbi,VDBI_DCRL) (Vdbi a) {return vsub_s8(a,vdup_n_u8(1));}
INLINE(Vdbc,VDBC_DCRL) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_DCRL(A) VDBI_ASBC(vsub_s8(VDBC_ASBI(A),vdup_n_s8(1)))
#else
#   define  VDBC_DCRL(A) VDBU_ASBC(vsub_u8(VDBC_ASBU(A),vdup_n_u8(1)))
#endif
    return  VDBC_DCRL(a);
}

INLINE(Vdhu,VDHU_DCRL) (Vdhu a) {return vsub_u16(a,vdup_n_u16(1));}
INLINE(Vdhi,VDHI_DCRL) (Vdhi a) {return vsub_s16(a,vdup_n_s16(1));}
INLINE(Vdwu,VDWU_DCRL) (Vdwu a) {return vsub_u32(a,vdup_n_u32(1));}
INLINE(Vdwi,VDWI_DCRL) (Vdwi a) {return vsub_s32(a,vdup_n_s32(1));}
INLINE(Vddu,VDDU_DCRL) (Vddu a) {return vsub_u64(a,vdup_n_u64(1));}
INLINE(Vddi,VDDI_DCRL) (Vddi a) {return vsub_s64(a,vdup_n_s64(1));}

INLINE(Vqyu,VQYU_DCRL) (Vqyu a)
{
#define     VQYU_DCRL(A) QYU_ASTV(veorq_u64(VQYU_ASTM(A),vdupq_n_u64(~0ull)))
    return  VQYU_DCRL(a);
}

INLINE(Vqbu,VQBU_DCRL) (Vqbu a) {return vsubq_u8(a,vdupq_n_u8(1));}
INLINE(Vqbi,VQBI_DCRL) (Vqbi a) {return vsubq_s8(a,vdupq_n_u8(1));}
INLINE(Vqbc,VQBC_DCRL) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_DCRL(A) VQBI_ASBC(vsubq_s8(VQBC_ASBI(A),vdupq_n_s8(1)))
#else
#   define  VQBC_DCRL(A) VQBU_ASBC(vsubq_u8(VQBC_ASBU(A),vdupq_n_u8(1)))
#endif
    return  VQBC_DCRL(a);
}

INLINE(Vqhu,VQHU_DCRL) (Vqhu a) {return vsubq_u16(a,vdupq_n_u16(1));}
INLINE(Vqhi,VQHI_DCRL) (Vqhi a) {return vsubq_s16(a,vdupq_n_s16(1));}
INLINE(Vqwu,VQWU_DCRL) (Vqwu a) {return vsubq_u32(a,vdupq_n_u32(1));}
INLINE(Vqwi,VQWI_DCRL) (Vqwi a) {return vsubq_s32(a,vdupq_n_s32(1));}
INLINE(Vqdu,VQDU_DCRL) (Vqdu a) {return vsubq_u64(a,vdupq_n_u64(1));}
INLINE(Vqdi,VQDI_DCRL) (Vqdi a) {return vsubq_s64(a,vdupq_n_s64(1));}

#if _LEAVE_ARM_DCRL
}
#endif

#if _ENTER_ARM_MULL
{
#endif

INLINE( _Bool,  BOOL_MULL)  (_Bool a,  _Bool b) {return a*b;}
INLINE( uchar, UCHAR_MULL)  (uchar a,  uchar b) {return a*b;}
INLINE( schar, SCHAR_MULL)  (schar a,  schar b) {return UCHAR_MULL(a,b);}
INLINE(  char,  CHAR_MULL)   (char a,   char b) {return UCHAR_MULL(a,b);}
INLINE(ushort, USHRT_MULL) (ushort a, ushort b) {return a*b;}
INLINE( short,  SHRT_MULL)  (short a,  short b) {return USHRT_MULL(a,b);}
INLINE(  uint,  UINT_MULL)   (uint a,   uint b) {return a*b;}
INLINE(   int,   INT_MULL)    (int a,    int b) {return UINT_MULL(a,b);}
INLINE( ulong, ULONG_MULL)  (ulong a,  ulong b) {return a*b;}
INLINE(  long,  LONG_MULL)   (long a,   long b) {return ULONG_MULL(a,b);}
INLINE(ullong,ULLONG_MULL) (ullong a, ullong b) {return a*b;}
INLINE( llong, LLONG_MULL)  (llong a,  llong b) {return ULLONG_MULL(a,b);}


INLINE(Vwyu,VWYU_MULL) (Vwyu a, Vwyu b)
{
#define     VWYU_MULL(A, B) \
WYU_ASTV(((VWWU_ASTV(VWYU_ASWU(A)))&(VWWU_ASTV(VWYU_ASWU(B)))))
    return  VWYU_MULL(a, b);
}

INLINE(Vwbu,VWBU_MULL) (Vwbu a, Vwbu b) 
{
    float       am = VWBU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint8x8_t   az = vreinterpret_u8_f32(af);
    float       bm = VWBU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint8x8_t   bz = vreinterpret_u8_f32(bf);
    az = vmul_u8(az, bz);
    af = vreinterpret_f32_u8(az);
    am = vget_lane_f32(af, 0);
    return  WBU_ASTV(am);
}

INLINE(Vwbi,VWBI_MULL) (Vwbi a, Vwbi b) 
{
    float       am = VWBI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int8x8_t    az = vreinterpret_s8_f32(af);
    float       bm = VWBI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int8x8_t    bz = vreinterpret_s8_f32(bf);
    az = vmul_s8(az, bz);
    af = vreinterpret_f32_s8(az);
    am = vget_lane_f32(af, 0);
    return  WBI_ASTV(am);
}

INLINE(Vwbc,VWBC_MULL) (Vwbc a, Vwbc b) 
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_MULL(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return  VWBU_ASBC(VWBU_MULL(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_MULL) (Vwhu a, Vwhu b) 
{
    float       am = VWHU_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    uint16x4_t  az = vreinterpret_u16_f32(af);
    float       bm = VWHU_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    uint16x4_t  bz = vreinterpret_u16_f32(bf);
    az = vmul_u16(az, bz);
    af = vreinterpret_f32_u16(az);
    am = vget_lane_f32(af, 0);
    return  WHU_ASTV(am);
}

INLINE(Vwhi,VWHI_MULL) (Vwhi a, Vwhi b) 
{
    float       am = VWHI_ASTM(a);
    float32x2_t af = vdup_n_f32(am);
    int16x4_t   az = vreinterpret_s16_f32(af);
    float       bm = VWHI_ASTM(b);
    float32x2_t bf = vdup_n_f32(bm);
    int16x4_t   bz = vreinterpret_s16_f32(bf);
    az = vmul_s16(az, bz);
    af = vreinterpret_f32_s16(az);
    am = vget_lane_f32(af, 0);
    return  WHI_ASTV(am);
}


INLINE(Vwwu,VWWU_MULL) (Vwwu a, Vwwu b) 
{
#define     VWWU_MULL(A, B)  UINT32_ASTV((VWWU_ASTV(A)*VWWU_ASTV(B)))
    return  VWWU_MULL(a, b);
}

INLINE(Vwwi,VWWI_MULL) (Vwwi a, Vwwi b) 
{
#define     VWWI_MULL(A, B)  \
INT32_ASTV(((unsigned) VWWI_ASTV(A)*(unsigned) VWWI_ASTV(B)))
    return  VWWI_MULL(a, b);
}


INLINE(Vdyu,VDYU_MULL) (Vdyu a, Vdyu b)
{
#define     VDYU_MULL(A, B) DYU_ASTV(vand_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_MULL(a, b);
}

INLINE(Vdbu,VDBU_MULL) (Vdbu a, Vdbu b) {return vmul_u8(a, b);}
INLINE(Vdbi,VDBI_MULL) (Vdbi a, Vdbi b) {return vmul_s8(a, b);}
INLINE(Vdbc,VDBC_MULL) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_MULL(A, B) VDBI_ASBC(vmul_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_MULL(A, B) VDBU_ASBC(vmul_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_MULL(a, b);
}

INLINE(Vdhu,VDHU_MULL) (Vdhu a, Vdhu b) {return vmul_u16(a, b);}
INLINE(Vdhi,VDHI_MULL) (Vdhi a, Vdhi b) {return vmul_s16(a, b);}
INLINE(Vdwu,VDWU_MULL) (Vdwu a, Vdwu b) {return vmul_u32(a, b);}
INLINE(Vdwi,VDWI_MULL) (Vdwi a, Vdwi b) {return vmul_s32(a, b);}
INLINE(Vddu,VDDU_MULL) (Vddu a, Vddu b)
{
    return  vdup_n_u64(vget_lane_u64(a, 0)*vget_lane_u64(b, 0));
}

INLINE(Vddi,VDDI_MULL) (Vddi a, Vddi b)
{
    return  VDDU_ASDI(
        VDDU_MULL(VDDI_ASDU(a), VDDI_ASDU(b))
    );
}

INLINE(Vqyu,VQYU_MULL) (Vqyu a, Vqyu b)
{
#define     VQYU_MULL(A, B) QYU_ASTV(vandq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_MULL(a, b);
}

INLINE(Vqbu,VQBU_MULL) (Vqbu a, Vqbu b) {return vmulq_u8(a, b);}
INLINE(Vqbi,VQBI_MULL) (Vqbi a, Vqbi b) {return vmulq_s8(a, b);}
INLINE(Vqbc,VQBC_MULL) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_MULL(A, B) VQBI_ASBC(vmulq_s8(VQBC_ASBI(A), VQBC_ASBI(B)))
#else
#   define  VQBC_MULL(A, B) VQBU_ASBC(vmulq_u8(VQBC_ASBU(A), VQBC_ASBU(B)))
#endif
    return  VQBC_MULL(a, b);
}

INLINE(Vqhu,VQHU_MULL) (Vqhu a, Vqhu b) {return vmulq_u16(a, b);}
INLINE(Vqhi,VQHI_MULL) (Vqhi a, Vqhi b) {return vmulq_s16(a, b);}
INLINE(Vqwu,VQWU_MULL) (Vqwu a, Vqwu b) {return vmulq_u32(a, b);}
INLINE(Vqwi,VQWI_MULL) (Vqwi a, Vqwi b) {return vmulq_s32(a, b);}
INLINE(Vqdu,VQDU_MULL) (Vqdu a, Vqdu b)
{
    a = vsetq_lane_u64(
        (vgetq_lane_u64(a, V2_K0)*vgetq_lane_u64(b, V2_K0)),
        a,
        V2_K0
    );
    a = vsetq_lane_u64(
        (vgetq_lane_u64(a, V2_K1)*vgetq_lane_u64(b, V2_K1)),
        a,
        V2_K1
    );
    return a;
}

INLINE(Vqdi,VQDI_MULL) (Vqdi a, Vqdi b)
{
    return  VQDU_ASDI(
        VQDU_MULL(VQDI_ASDU(a), VQDI_ASDU(b))
    );
}

#if _LEAVE_ARM_MULL
}
#endif

#if _ENTER_ARM_MUL2
{
#endif

INLINE(   uint16_t, UCHAR_MUL2)   (uchar a,   uchar b)
{
    return (unsigned) a*(unsigned) b;
}

INLINE(    int16_t, SCHAR_MUL2)   (schar a,   schar b)
{
    return (signed) a*(signed) b;
}

#if CHAR_MIN

INLINE(    int16_t,  CHAR_MUL2)    (char a,    char b)
{
    return (signed) a*(signed) b;
}

#else

INLINE(   uint16_t,  CHAR_MUL2)    (char a,    char b)
{
    return (unsigned) a*(unsigned) b;
}

#endif

INLINE(   uint32_t, USHRT_MUL2)  (ushort a,  ushort b)
{
    return  (unsigned) a*b;
}

INLINE(    int32_t,  SHRT_MUL2)   (short a,   short b)
{
    return  (signed) a*b;
}


INLINE(   uint64_t,  UINT_MUL2)    (uint a,    uint b)
{
    return  (uint64_t) a*b;
}

INLINE(    int64_t,   INT_MUL2)     (int a,     int b)
{
    return  (int64_t) a*b;
}

#if DWRD_NLONG == 2

INLINE(     ullong, ULONG_MUL2)   (ulong a,   ulong b)
{
    return  (uint64_t) a*b;
}

INLINE(       long,  LONG_MUL2)    (long a,    long b)
{
    return  (int64_t) a*b;
}

#elif QUAD_NLLONG == 2

INLINE(unsigned __int128, ULONG_MUL2)   (ulong a,   ulong b)
{
    return  (unsigned __int128) a*b;
}

INLINE(  signed __int128,  LONG_MUL2)    (long a,    long b)
{
    return  (signed __int128) a*b;
}

#else

INLINE(ullong, ULONG_MUL2)   (ulong a,   ulong b)
{
    return  (ullong) a*b;
}

INLINE( llong,  LONG_MUL2)    (long a,    long b)
{
    return  (llong) a*b;
}

#endif


INLINE(      float, FLT16_MUL2) (flt16_t a, flt16_t b)
{
    return (float) a*b;
}

INLINE(     double,   FLT_MUL2)   (float a,   float b)
{
    return (double) a*b;
}

INLINE(long double,   DBL_MUL2)  (double a,  double b)
{
    return (long double) a*b;
}



INLINE(Vqhu,VDBU_MUL2) (Vdbu a, Vdbu b)
{
    return  vmull_u8(a, b);
}

INLINE(Vqhi,VDBI_MUL2) (Vdbi a, Vdbi b)
{
    return  vmull_s8(a, b);
}

#if CHAR_MIN

INLINE(Vqhi,VDBC_MUL2) (Vdbc a, Vdbc b)
{
    return  vmull_s8(VDBC_ASBI(a), VDBC_ASBI(b));
}

#else

INLINE(Vqhu,VDBC_MUL2) (Vdbc a, Vdbc b)
{
    return  vmull_u8(VDBC_ASBU(a), VDBC_ASBU(b));
}

#endif

INLINE(Vqwu,VDHU_MUL2) (Vdhu a, Vdhu b)
{
    return  vmull_u16(a, b);
}

INLINE(Vqwi,VDHI_MUL2) (Vdhi a, Vdhi b)
{
    return  vmull_s16(a, b);
}

INLINE(Vqwf,VDHF_MUL2) (Vdhf a, Vdhf b)
{
    return  vmulq_f32(vcvt_f32_f16(a), vcvt_f32_f16(b));
}

INLINE(Vqdu,VDWU_MUL2) (Vdwu a, Vdwu b)
{
    return  vmull_u32(a, b);
}

INLINE(Vqdi,VDWI_MUL2) (Vdwi a, Vdwi b)
{
    return  vmull_s32(a, b);
}

INLINE(Vqdf,VDWF_MUL2) (Vdwf a, Vdwf b)
{
    return  vmulq_f32(vcvt_f64_f32(a), vcvt_f64_f32(b));
}

INLINE(Vqqu,VDDU_MUL2) (Vddu a, Vddu b)
{
    return  (Vqqu)
    {
        ((unsigned __int128) vget_lane_u64(a, 0))
    *   vget_lane_u64(b, 0)
    };
}

INLINE(Vqqi,VDDI_MUL2) (Vddi a, Vddi b)
{
    return  (Vqqi)
    {
        ((signed __int128) vget_lane_s64(a, 0))
    *   vget_lane_s64(b, 0)
    };
}

INLINE(Vqqf,VQQF_MUL2) (Vddf a, Vddf b)
{
    return  (Vqqf)
    {
        ((long double) vget_lane_f64(a, 0))
    *   vget_lane_f64(b, 0)
    };
}

#if _LEAVE_ARM_MUL2
}
#endif

#if _ENTER_ARM_MULS
{
#endif

INLINE( _Bool,  BOOL_MULS)  (_Bool a,  _Bool b) {return a&b;}


INLINE( uchar, UCHAR_MULS)  (uchar a,  uchar b)
{
    return  vqmovnh_u16((a*b));
}

INLINE( schar, SCHAR_MULS)  (schar a,  schar b)
{
    return  vqmovnh_s16((a*b));
}

INLINE(  char,  CHAR_MULS)   (char a,   char b)
{
#if CHAR_MIN
    return  vqmovnh_s16((a*b));
#else
    return  vqmovnh_u16((a*b));
#endif
}


INLINE(ushort, USHRT_MULS) (ushort a, ushort b)
{
    return  vqmovns_u32((a*b));
}

INLINE( short,  SHRT_MULS)  (short a,  short b)
{
    return  vqmovns_s32((a*b));
}


INLINE(  uint,  UINT_MULS)   (uint a,   uint b)
{
    return  vqmovnd_u64(((uint64_t) a*b));
}

INLINE(   int,   INT_MULS)    (int a,    int b)
{
    return  vqmovnd_s64(((int64_t) a*b));
}


INLINE( ulong, ULONG_MULS)  (ulong a,  ulong b)
{
#if DWRD_NLONG == 2
    return  vqmovnd_u64(((uint64_t) a*b));
#else
    return (b*=a) < a ? UINT64_MAX : b;
#endif
}

INLINE(  long,  LONG_MULS)   (long a,   long b)
{
#if DWRD_NLONG == 2
    return  vqmovnd_s64(((int64_t) a*b));
#else
/* TODO: fix this
*/
    QUAD_ITYPE c = (QUAD_ITYPE) a*(QUAD_ITYPE) b;
    if (c > INT64_MAX)
        return  INT64_MAX;
    if (c < INT64_MIN)
        return  INT64_MIN;
    return c;
#endif
}


INLINE(ullong,ULLONG_MULS) (ullong a, ullong b)
{
    return (b*=a) < a ? ULLONG_MAX : b;
}

INLINE( llong, LLONG_MULS)  (llong a,  llong b)
{
    QUAD_ITYPE c = (QUAD_ITYPE) a*(QUAD_ITYPE) b;
    if (c > LLONG_MAX)
        return LLONG_MAX;
    if (c < LLONG_MIN)
        return LLONG_MIN;
    return c;
}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,mulsqu) (QUAD_UTYPE a, QUAD_UTYPE b) 
{
    return (b*=a) >= a ? b : (((QUAD_UTYPE) 0)-1);
}

INLINE(QUAD_ITYPE,mulsqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    QUAD_ITYPE c = a*b;
    if (a <= 0)
    {
        if ((b < 0) && (c > a))
        {
            QUAD_UTYPE r = 1;
            r <<= 127;
            return  (QUAD_ITYPE) r;
        }
    }
    else 
    {
        if ((b > 0) && (c < a))
        {
            QUAD_UTYPE r = 0;
            r -= 1;
            r >>= 1;
            return  (QUAD_ITYPE) r;
        }
    }
    return c;
}

#endif

INLINE(Vwyu,VWYU_MULS) (Vwyu a, Vwyu b)
{
    float ml = VWYU_ASTM(a);
    float mr = VWYU_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    uint8x8_t   zl = vreinterpret_u8_f32(vl);
    uint8x8_t   zr = vreinterpret_u8_f32(vr);
    zl = vand_u8(zl, zr);
    vl = vreinterpret_f32_u8(zl);
    ml = vget_lane_f32(vl, 0);
    return  WYU_ASTV(ml);
}


INLINE(Vwbu,VWBU_MULS) (Vwbu a, Vwbu b)
{
    float       ml = VWBU_ASTM(a);
    float       mr = VWBU_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    uint8x8_t   zl = vreinterpret_u8_f32(vl);
    uint8x8_t   zr = vreinterpret_u8_f32(vr);
    zl = vqmovn_u16(vmull_u8(zl, zr));
    vl = vreinterpret_f32_u8(zl);
    ml = vget_lane_f32(vl, 0);
    return  WBU_ASTV(ml);
}

INLINE(Vwbi,VWBI_MULS) (Vwbi a, Vwbi b)
{
    float       ml = VWBI_ASTM(a);
    float       mr = VWBI_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    int8x8_t    zl = vreinterpret_s8_f32(vl);
    int8x8_t    zr = vreinterpret_s8_f32(vr);
    zl = vqmovn_s16(vmull_s8(zl, zr));
    vl = vreinterpret_f32_s8(zl);
    ml = vget_lane_f32(vl, 0);
    return  WBI_ASTV(ml);
}

INLINE(Vwbc,VWBC_MULS) (Vwbc a, Vwbc b)
{
#if CHAR_MIN
    return VWBI_ASBC(VWBI_MULS(VWBC_ASBI(a), VWBC_ASBI(b)));
#else
    return VWBU_ASBC(VWBU_MULS(VWBC_ASBU(a), VWBC_ASBU(b)));
#endif
}


INLINE(Vwhu,VWHU_MULS) (Vwhu a, Vwhu b)
{
    float       ml = VWHU_ASTM(a);
    float       mr = VWHU_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    uint16x4_t  zl = vreinterpret_u16_f32(vl);
    uint16x4_t  zr = vreinterpret_u16_f32(vr);
    zl = vqmovn_u32(vmull_u16(zl, zr));
    vl = vreinterpret_f32_u16(zl);
    ml = vget_lane_f32(vl, 0);
    return  WHU_ASTV(ml);
}

INLINE(Vwhi,VWHI_MULS) (Vwhi a, Vwhi b)
{
    float       ml = VWHI_ASTM(a);
    float       mr = VWHI_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    int16x4_t   zl = vreinterpret_s16_f32(vl);
    int16x4_t   zr = vreinterpret_s16_f32(vr);
    zl = vqmovn_u32(vmull_s16(zl, zr));
    vl = vreinterpret_f32_s16(zl);
    ml = vget_lane_f32(vl, 0);
    return  WHI_ASTV(ml);
}


INLINE(Vwwu,VWWU_MULS) (Vwwu a, Vwwu b)
{
    float       ml = VWWU_ASTM(a);
    float       mr = VWWU_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    uint32x2_t  zl = vreinterpret_u32_f32(vl);
    uint32x2_t  zr = vreinterpret_u32_f32(vr);
    zl = vqmovn_u64(vmull_u32(zl, zr));
    vl = vreinterpret_f32_u32(zl);
    ml = vget_lane_f32(vl, 0);
    return  WWU_ASTV(ml);
}

INLINE(Vwwi,VWWI_MULS) (Vwwi a, Vwwi b)
{
    float       ml = VWWI_ASTM(a);
    float       mr = VWWI_ASTM(b);
    float32x2_t vl = vdup_n_f32(ml);
    float32x2_t vr = vdup_n_f32(mr);
    uint32x2_t  zl = vreinterpret_s32_f32(vl);
    uint32x2_t  zr = vreinterpret_s32_f32(vr);
    zl = vqmovn_s64(vmull_s32(zl, zr));
    vl = vreinterpret_f32_s32(zl);
    ml = vget_lane_f32(vl, 0);
    return  WWI_ASTV(ml);
}


INLINE(Vdyu,VDYU_MULS) (Vdyu a, Vdyu b)
{
    return VDDU_ASYU(vand_u64(VDYU_ASDU(a), VDYU_ASDU(b)));
}


INLINE(Vdbu,VDBU_MULS) (Vdbu a, Vdbu b) 
{
    return  vqmovn_u16(vmull_u8(a, b));
}

INLINE(Vdbi,VDBI_MULS) (Vdbi a, Vdbi b) 
{
    return  vqmovn_s16(vmull_s8(a, b));
}

INLINE(Vdbc,VDBC_MULS) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_MULS(VDBC_ASBI(a), VDBC_ASBI(b)));
#else
    return  VDBU_ASBC(VDBU_MULS(VDBC_ASBU(a), VDBC_ASBU(b)));
#endif

}


INLINE(Vdhu,VDHU_MULS) (Vdhu a, Vdhu b) 
{
    return  vqmovn_u32(vmull_u16(a, b));
}

INLINE(Vdhi,VDHI_MULS) (Vdhi a, Vdhi b) 
{
    return  vqmovn_s32(vmull_s16(a, b));
}

INLINE(Vdwu,VDWU_MULS) (Vdwu a, Vdwu b) 
{
    return  vqmovn_u64(vmull_u32(a, b));
}

INLINE(Vdwi,VDWI_MULS) (Vdwi a, Vdwi b) 
{
    return  vqmovn_s64(vmull_s32(a, b));
}

INLINE(Vddu,VDDU_MULS) (Vddu a, Vddu b) 
{
    uint64x1_t c = vdup_n_u64(
        (vget_lane_u64(a, 0)*vget_lane_u64(b, 0))
    );
    uint64x1_t m = vcgt_u64(b, c);
    m = vand_u64(m, vdup_n_u64(UINT64_MAX));
    return  vorr_u64(c, m);
}
    

INLINE(Vddi,VDDI_MULS) (Vddi a, Vddi b) 
{
    int64_t     l = vget_lane_s64(a, 0);
    int64_t     r = vget_lane_s64(b, 0);
    uint64_t    c = (uint64_t) l*(uint64_t) r;
    if (l < 0)
    {
        if ((r < 0)  && (c < INT64_MAX))
            c = INT64_MIN;
    }
    else
    {
        if ((r > 0) && (c > INT64_MAX))
            c = INT64_MAX;
    }
    return  vdup_n_s64(c);
}



INLINE(Vqyu,VQYU_MULS) (Vqyu a, Vqyu b)
{
    return VQDU_ASYU(vandq_u64(VQYU_ASDU(a), VQYU_ASDU(b)));
}


INLINE(Vqbu,VQBU_MULS) (Vqbu a, Vqbu b) 
{
    return  vcombine_u8(
        VDBU_MULS(vget_low_u8(a), vget_low_u8(b)),
        VDBU_MULS(vget_high_u8(a),vget_high_u8(b))
    );
}

INLINE(Vqbi,VQBI_MULS) (Vqbi a, Vqbi b) 
{
    return  vcombine_s8(
        VDBI_MULS(vget_low_s8(a), vget_low_s8(b)),
        VDBI_MULS(vget_high_s8(a),vget_high_s8(b))
    );
}

INLINE(Vqbc,VQBC_MULS) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_MULS(VQBC_ASBI(a), VQBC_ASBI(b)));
#else
    return  VQBU_ASBC(VQBU_MULS(VQBC_ASBU(a), VQBC_ASBU(b)));
#endif

}


INLINE(Vqhu,VQHU_MULS) (Vqhu a, Vqhu b) 
{
    return  vcombine_u16(
        VDHU_MULS(vget_low_u16(a), vget_low_u16(b)),
        VDHU_MULS(vget_high_u16(a),vget_high_u16(b))
    );
}

INLINE(Vqhi,VQHI_MULS) (Vqhi a, Vqhi b) 
{
    return  vcombine_s16(
        VDHI_MULS(vget_low_s16(a), vget_low_s16(b)),
        VDHI_MULS(vget_high_s16(a),vget_high_s16(b))
    );
}


INLINE(Vqwu,VQWU_MULS) (Vqwu a, Vqwu b) 
{
    return  vcombine_u32(
        VDWU_MULS(vget_low_u32(a), vget_low_u32(b)),
        VDWU_MULS(vget_high_u32(a),vget_high_u32(b))
    );
}

INLINE(Vqwi,VQWI_MULS) (Vqwi a, Vqwi b) 
{
    return  vcombine_s32(
        VDWI_MULS(vget_low_s32(a), vget_low_s32(b)),
        VDWI_MULS(vget_high_s32(a),vget_high_s32(b))
    );
}


INLINE(Vqdu,VQDU_MULS) (Vqdu a, Vqdu b) 
{
    return  vcombine_u64(
        VDDU_MULS(vget_low_u64(a), vget_low_u64(b)),
        VDDU_MULS(vget_high_u64(a),vget_high_u64(b))
    );
}

INLINE(Vqdi,VQDI_MULS) (Vqdi a, Vqdi b) 
{
    return  vcombine_s64(
        VDDI_MULS(vget_low_s64(a), vget_low_s64(b)),
        VDDI_MULS(vget_high_s64(a),vget_high_s64(b))
    );
}

#if _LEAVE_ARM_MULS
}
#endif

#if _ENTER_ARM_MULH
{
#endif

INLINE(flt16_t,  BOOL_MULH)   (_Bool a, flt16_t b) {return a*b;}
INLINE(flt16_t, UCHAR_MULH)   (uchar a, flt16_t b) {return a*b;}
INLINE(flt16_t, SCHAR_MULH)   (schar a, flt16_t b) {return a*b;}
INLINE(flt16_t,  CHAR_MULH)    (char a, flt16_t b) {return a*b;}
INLINE(flt16_t, USHRT_MULH)  (ushort a, flt16_t b) {return a*b;}
INLINE(flt16_t,  SHRT_MULH)   (short a, flt16_t b) {return a*b;}
INLINE(flt16_t,  UINT_MULH)    (uint a, flt16_t b) {return a*b;}
INLINE(flt16_t,   INT_MULH)     (int a, flt16_t b) {return a*b;}
INLINE(flt16_t, ULONG_MULH)   (ulong a, flt16_t b) {return a*b;}
INLINE(flt16_t,  LONG_MULH)    (long a, flt16_t b) {return a*b;}
INLINE(flt16_t,ULLONG_MULH)  (ullong a, flt16_t b) {return a*b;}
INLINE(flt16_t, LLONG_MULH)   (llong a, flt16_t b) {return a*b;}

INLINE(flt16_t, FLT16_MULH) (flt16_t a, flt16_t b) {return a*b;}
INLINE(float,     FLT_MULH)   (float a, flt16_t b) {return a*b;}
INLINE(double,    DBL_MULH)  (double a, flt16_t b) {return a*b;}

#if QUAD_NLLONG == 2
INLINE(flt16_t,mulhqu) (QUAD_UTYPE a, flt16_t b) {return a*b;}
INLINE(flt16_t,mulhqi) (QUAD_ITYPE a, flt16_t b) {return a*b;}
INLINE(QUAD_FTYPE,mulhqf) (QUAD_FTYPE a, flt16_t b) {return a*b;}
#endif


INLINE(Vdhf,VWBU_MULH) (Vwbu a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmul_f16(VWBU_CVHF(a), b);
#else
    float32x4_t p = VWBU_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vmulq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBI_MULH) (Vwbi a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmul_f16(VWBI_CVHF(a), b);
#else
    float32x4_t p = VWBI_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vmulq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBC_MULH) (Vwbc a, Vdhf b)
{
#if CHAR_MIN
    return VWBI_MULH(VWBC_ASBI(a), b);
#else
    return VWBU_MULH(VWBC_ASBU(a), b);
#endif
}


INLINE(float,WHF_MULH) (float am, float bm)
{
#if defined(SPC_ARM_FP16_SIMD)
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    ah = vmul_f16(ah, bh);
#else
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x4_t av = vcvt_f32_f16(ah);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    float32x4_t bv = vcvt_f32_f16(bh);
    av = vmulq_f32(av, bv);
    ah = vcvt_f16_f32(av);
#endif
    aw = vreinterpret_f32_f16(ah);
    return  vget_lane_f32(aw, 0);
    
}


INLINE(Vwhf,VWHU_MULH) (Vwhu a, Vwhf b)
{
    Vwhf    v = VWHU_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_MULH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHI_MULH) (Vwhi a, Vwhf b)
{
    Vwhf    v = VWHI_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_MULH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_MULH) (Vwhf a, Vwhf b)
{
    float   m = WHF_MULH(VWHF_ASTM(a), VWHF_ASTM(b));
    return WHF_ASTV(m);
}


INLINE(Vqhf,VDBU_MULH) (Vdbu a, Vqhf b) 
{
    uint16x8_t  c = vmovl_u8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_u16(c);
    return  vmulq_f16(f, b);
#else

    uint32x4_t  zl = vmovl_u16(vget_low_u16(c));
    float32x4_t fl = vcvtq_f32_u32(zl);
    uint32x4_t  zr = vmovl_u16(vget_high_u16(c));
    float32x4_t fr = vcvtq_f32_u32(zr);
    fl = vmulq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vmulq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBI_MULH) (Vdbi a, Vqhf b) 
{
    int16x8_t   c = vmovl_s8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_s16(c);
    return  vmulq_f16(f, b);
#else
    int32x4_t   zl = vmovl_s16(vget_low_s16(c));
    float32x4_t fl = vcvtq_f32_s32(zl);
    int32x4_t   zr = vmovl_s16(vget_high_s16(c));
    float32x4_t fr = vcvtq_f32_s32(zr);
    fl = vmulq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vmulq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBC_MULH) (Vdbc a, Vqhf b)
{
#if CHAR_MIN
    return  VDBI_MULH(VDBC_ASBI(a), b);
#else
    return  VDBU_MULH(VDBC_ASBU(a), b);
#endif
}


INLINE(Vdhf,VDHU_MULH) (Vdhu a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmul_f16(vcvt_f16_u16(a), b);
#else
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vmulq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHI_MULH) (Vdhi a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmul_f16(vcvt_f16_s16(a), b);
#else
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vmulq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHF_MULH) (Vdhf a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmul_f16(a, b);
#else
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vmulq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}


INLINE(Vwhf,VDWU_MULH) (Vdwu a, Vwhf b)
{
    return  VWHF_MULH(VDWU_CVHF(a), b);
}

INLINE(Vwhf,VDWI_MULH) (Vdwi a, Vwhf b)
{
    return  VWHF_MULH(VDWI_CVHF(a), b);
}

INLINE(Vdwf,VDWF_MULH) (Vdwf a, Vwhf b)
{
    return vmul_f32(a, VWHF_CVWF(b));
}

/*
INLINE(Vhhf,VDDU_MULH) (Vddu a, Vhhf b);
INLINE(Vhhf,VDDI_MULH) (Vddi a, Vhhf b);
INLINE(Vddf,VDDF_MULH) (Vddf a, Vhhf b);


INLINE(Vohf,VQBU_MULH) (Vqbu a, Vohf b);
INLINE(Vohf,VQBI_MULH) (Vqbi a, Vohf b);
INLINE(Vohf,VQBC_MULH) (Vqbc a, Vohf b);

*/

INLINE(Vqhf,VQHU_MULH) (Vqhu a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmulq_f16(vcvtq_f16_u16(a), b);
#else
    return vcombine_f16(
        VDHU_MULH(vget_low_u16(a),  vget_low_f16(b)),
        VDHU_MULH(vget_high_u16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHI_MULH) (Vqhi a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmulq_f16(vcvtq_f16_s16(a), b);
#else
    return vcombine_f16(
        VDHI_MULH(vget_low_s16(a),  vget_low_f16(b)),
        VDHI_MULH(vget_high_s16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHF_MULH) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmulq_f16(a, b);
#else
    return vcombine_f16(
        VDHF_MULH(vget_low_f16(a),  vget_low_f16(b)),
        VDHF_MULH(vget_high_f16(a), vget_high_f16(b))
    );
#endif

}


INLINE(Vdhf,VQWU_MULH) (Vqwu a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_u32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vmulq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vdhf,VQWI_MULH) (Vqwi a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_s32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vmulq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vqwf,VQWF_MULH) (Vqwf a, Vdhf b)
{
    return  vmulq_f32(a, vcvt_f32_f16(b));
}

INLINE(Vwhf,VQDU_MULH) (Vqdu a, Vwhf b)
{
    return  VWHF_MULH(VQDU_CVHF(a), b);
}

INLINE(Vwhf,VQDI_MULH) (Vqdi a, Vwhf b)
{
    return  VWHF_MULH(VQDI_CVHF(a), b);
}

INLINE(Vqdf,VQDF_MULH) (Vqdf a, Vwhf b)
{
    return  vmulq_f64(a, VWHF_CVDF(b));
}

#if _LEAVE_ARM_MULH
}
#endif

#if _ENTER_ARM_MULW
{
#endif

INLINE(float,  BOOL_MULW)   (_Bool a, float b) 
{
    uint32x2_t m = vdup_n_u32(a);
    m = vtst_u32(m, vdup_n_u32(UINT32_MAX));
    m = vorr_u32(m, vdup_n_u64(UINT32_C(0x80000000)));
    float32x2_t f = vdup_n_f32(b);
    uint32x2_t  r = vreinterpret_u32_f32(f);
    r = vand_u32(r, m);
    f = vreinterpret_f32_u32(r);
    return  vget_lane_f32(f, 0);
}

INLINE(float, UCHAR_MULW)   (uchar a, float b) {return a*b;}
INLINE(float, SCHAR_MULW)   (schar a, float b) {return a*b;}
INLINE(float,  CHAR_MULW)    (char a, float b) {return a*b;}
INLINE(float, USHRT_MULW)  (ushort a, float b) {return a*b;}
INLINE(float,  SHRT_MULW)   (short a, float b) {return a*b;}
INLINE(float,  UINT_MULW)    (uint a, float b) {return a*b;}
INLINE(float,   INT_MULW)     (int a, float b) {return a*b;}
INLINE(float, ULONG_MULW)   (ulong a, float b) {return a*b;}
INLINE(float,  LONG_MULW)    (long a, float b) {return a*b;}
INLINE(float,ULLONG_MULW)  (ullong a, float b) {return a*b;}
INLINE(float, LLONG_MULW)   (llong a, float b) {return a*b;}

INLINE(float, FLT16_MULW) (flt16_t a, float b) {return a*b;}
INLINE(float,   FLT_MULW)   (float a, float b) {return a*b;}
INLINE(double,  DBL_MULW)  (double a, float b) {return a*b;}

#if QUAD_NLLONG == 2
INLINE(float,mulwqu)   (QUAD_UTYPE a, float b) {return a*b;}
INLINE(float,mulwqi)   (QUAD_ITYPE a, float b) {return a*b;}
INLINE(QUAD_FTYPE,mulwqf) (QUAD_FTYPE a, float b) {return a*b;}
#endif


INLINE(Vqwf,VWBU_MULW) (Vwbu a, Vqwf b)
{
    return  vmulq_f32(VWBU_CVWF(a), b);
}

INLINE(Vqwf,VWBI_MULW) (Vwbi a, Vqwf b)
{
    return  vmulq_f32(VWBI_CVWF(a), b);
}

INLINE(Vqwf,VWBC_MULW) (Vwbc a, Vqwf b)
{
#if CHAR_MIN
    return  VWBI_MULW(VWBC_ASBI(a), b);
#else
    return  VWBU_MULW(VWBC_ASBU(a), b);
#endif
}



INLINE(Vdwf,VWHU_MULW) (Vwhu a, Vdwf b)
{
    return  vmul_f32(VWHU_CVWF(a), b);
}

INLINE(Vdwf,VWHI_MULW) (Vwhi a, Vdwf b)
{
    return  vmul_f32(VWHI_CVWF(a), b);
}

INLINE(Vdwf,VWHF_MULW) (Vwhf a, Vdwf b)
{
    return  vmul_f32(VWHF_CVWF(a), b);
}


INLINE(Vwwf,VWWU_MULW) (Vwwu a, Vwwf b)
{
    return WWF_ASTV((VWWU_ASTV(a)*VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWI_MULW) (Vwwi a, Vwwf b)
{
    return WWF_ASTV((VWWI_ASTV(a)*VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWF_MULW) (Vwwf a, Vwwf b)
{
    return WWF_ASTV((VWWF_ASTM(a)*VWWF_ASTM(b)));
}


INLINE(Vqwf,VDHU_MULW) (Vdhu a, Vqwf b) 
{
    return  vmulq_f32(VDHU_CVWF(a), b);
}

INLINE(Vqwf,VDHI_MULW) (Vdhi a, Vqwf b) 
{
    return  vmulq_f32(VDHI_CVWF(a), b);
}

INLINE(Vqwf,VDHF_MULW) (Vdhf a, Vqwf b) 
{
    return  vmulq_f32(VDHF_CVWF(a), b);
}


INLINE(Vdwf,VDWU_MULW) (Vdwu a, Vdwf b)
{
    return vmul_f32(vcvt_f32_u32(a), b);
}

INLINE(Vdwf,VDWI_MULW) (Vdwi a, Vdwf b)
{
    return vmul_f32(vcvt_f32_s32(a), b);
}

INLINE(Vdwf,VDWF_MULW) (Vdwf a, Vdwf b)
{
    return vmul_f32(a, b);
}


INLINE(Vwwf,VDDU_MULW) (Vddu a, Vwwf b)
{
    return WWF_ASTV((VDDU_ASTV(a)*VWWF_ASTM(b)));
}

INLINE(Vwwf,VDDI_MULW) (Vddi a, Vwwf b)
{
    return WWF_ASTV((VDDI_ASTV(a)*VWWF_ASTM(b)));
}

INLINE(Vddf,VDDF_MULW) (Vddf a, Vwwf b)
{
    return vmul_f64(a, vdup_n_f64(VWWF_ASTM(b)));
}


INLINE(Vqwf,VQWU_MULW) (Vqwu a, Vqwf b)
{
    return  vmulq_f32(vcvtq_f32_u32(a), b);
}

INLINE(Vqwf,VQWI_MULW) (Vqwi a, Vqwf b)
{
    return  vmulq_f32(vcvtq_f32_s32(a), b);
}

INLINE(Vqwf,VQWF_MULW) (Vqwf a, Vqwf b)
{
    return  vmulq_f32(a, b);
}


INLINE(Vdwf,VQDU_MULW) (Vqdu a, Vdwf b)
{
    return  vmul_f32(vcvt_f32_f64(vcvtq_f64_u64(a)), b);
}

INLINE(Vdwf,VQDI_MULW) (Vqdi a, Vdwf b)
{
    return  vmul_f32(vcvt_f32_f64(vcvtq_f64_s64(a)), b);
}

INLINE(Vqdf,VQDF_MULW) (Vqdf a, Vdwf b)
{
    return  vmulq_f64(a, vcvt_f64_f32(b));
}

#if _LEAVE_ARM_MULW
}
#endif

#if _ENTER_ARM_MULD
{
#endif

INLINE(double,  BOOL_MULD)   (_Bool a, double b) 
{
    uint64x1_t m = vdup_n_u64(a);
    m = vtst_u64(m, vdup_n_u64(UINT64_MAX));
    m = vorr_u64(m, vdup_n_u64(UINT64_C(0x8000000000000000)));
    float64x1_t f = vdup_n_f64(b);
    uint64x1_t  r = vreinterpret_u64_f64(f);
    r = vand_u64(r, m);
    f = vreinterpret_f64_u64(r);
    return  vget_lane_f64(f, 0);
}

INLINE(double, UCHAR_MULD)   (uchar a, double b) {return a*b;}
INLINE(double, SCHAR_MULD)   (schar a, double b) {return a*b;}
INLINE(double,  CHAR_MULD)    (char a, double b) {return a*b;}
INLINE(double, USHRT_MULD)  (ushort a, double b) {return a*b;}
INLINE(double,  SHRT_MULD)   (short a, double b) {return a*b;}
INLINE(double,  UINT_MULD)    (uint a, double b) {return a*b;}
INLINE(double,   INT_MULD)     (int a, double b) {return a*b;}
INLINE(double, ULONG_MULD)   (ulong a, double b) {return a*b;}
INLINE(double,  LONG_MULD)    (long a, double b) {return a*b;}
INLINE(double,ULLONG_MULD)  (ullong a, double b) {return a*b;}
INLINE(double, LLONG_MULD)   (llong a, double b) {return a*b;}

INLINE(double, FLT16_MULD) (flt16_t a, double b) {return a*b;}
INLINE(double,   FLT_MULD)   (float a, double b) {return a*b;}
INLINE(double,   DBL_MULD)  (double a, double b) {return a*b;}

#if QUAD_NLLONG == 2
INLINE(double,muldqu)   (QUAD_UTYPE a, double b) {return a*b;}
INLINE(double,muldqi)   (QUAD_ITYPE a, double b) {return a*b;}
INLINE(QUAD_FTYPE,muldqf) (QUAD_FTYPE a, double b) {return a*b;}
#endif


INLINE(Vqdf,VWHU_MULD) (Vwhu a, Vqdf b)
{
    return  vmulq_f64(VWHU_CVDF(a), b);
}

INLINE(Vqdf,VWHI_MULD) (Vwhi a, Vqdf b)
{
    return  vmulq_f64(VWHI_CVDF(a), b);
}

INLINE(Vqdf,VWHF_MULD) (Vwhf a, Vqdf b)
{
    return  vmulq_f64(VWHF_CVDF(a), b);
}


INLINE(Vddf,VWWU_MULD) (Vwwu a, Vddf b)
{
    return  vmul_f64(VWWU_CVDF(a), b);
}

INLINE(Vddf,VWWI_MULD) (Vwwi a, Vddf b)
{
    return  vmul_f64(VWWI_CVDF(a), b);
}

INLINE(Vddf,VWWF_MULD) (Vwwf a, Vddf b)
{
    return  vmul_f64(VWWF_CVDF(a), b);
}


INLINE(Vqdf,VDWU_MULD) (Vdwu a, Vqdf b)
{
    return  vmulq_f64(vcvtq_f64_u64(vmovl_u32(a)), b);
}

INLINE(Vqdf,VDWI_MULD) (Vdwi a, Vqdf b)
{
    return  vmulq_f64(vcvtq_f64_s64(vmovl_s32(a)), b);
}

INLINE(Vqdf,VDWF_MULD) (Vdwf a, Vqdf b)
{
    return  vmulq_f64(vcvt_f64_f32(a), b);
}


INLINE(Vddf,VDDU_MULD) (Vddu a, Vddf b)
{
    return  vmul_f64(vcvt_f64_u64(a), b);
}

INLINE(Vddf,VDDI_MULD) (Vddi a, Vddf b)
{
    return  vmul_f64(vcvt_f64_s64(a), b);
}

INLINE(Vddf,VDDF_MULD) (Vddf a, Vddf b)
{
    return  vmul_f64(a, b);
}


INLINE(Vqdf,VQDU_MULD) (Vqdu a, Vqdf b)
{
    return  vmulq_f64(vcvtq_f64_u64(a), b);
}

INLINE(Vqdf,VQDI_MULD) (Vqdi a, Vqdf b)
{
    return  vmulq_f64(vcvtq_f64_s64(a), b);
}

INLINE(Vqdf,VQDF_MULD) (Vqdf a, Vqdf b)
{
    return  vmulq_f64(a, b);
}

#if _LEAVE_ARM_MULD
}
#endif

#if _ENTER_ARM_FAML
{
#endif

INLINE(  _Bool,  BOOL_FAML)  (_Bool a,   _Bool b,   _Bool c) {return a+b*c;}
INLINE(  uchar, UCHAR_FAML)  (uchar a,   uchar b,   uchar c) {return a+b*c;}
INLINE(  schar, SCHAR_FAML)  (schar a,   schar b,   schar c) {return a+b*c;}
INLINE(   char,  CHAR_FAML)   (char a,    char b,    char c) {return a+b*c;}
INLINE( ushort, USHRT_FAML) (ushort a,  ushort b,  ushort c) {return a+b*c;}
INLINE(  short,  SHRT_FAML)  (short a,   short b,   short c) {return a+b*c;}
INLINE(   uint,  UINT_FAML)   (uint a,    uint b,    uint c) {return a+b*c;}
INLINE(    int,   INT_FAML)    (int a,     int b,     int c) {return a+b*c;}
INLINE(  ulong, ULONG_FAML)  (ulong a,   ulong b,   ulong c) {return a+b*c;}
INLINE(   long,  LONG_FAML)   (long a,    long b,    long c) {return a+b*c;}
INLINE( ullong,ULLONG_FAML) (ullong a,  ullong b,  ullong c) {return a+b*c;}
INLINE(  llong, LLONG_FAML)  (llong a,   llong b,   llong c) {return a+b*c;}

INLINE(flt16_t, FLT16_FAML)(flt16_t a, flt16_t b, flt16_t c) {return a+b*c;}
INLINE(  float,   FLT_FAML)  (float a,   float b,   float c) {return a+b*c;}
INLINE(  float,   DBL_FAML) (double a,  double b,  double c) {return a+b*c;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,famlqu) (QUAD_UTYPE a, QUAD_UTYPE b, QUAD_UTYPE c)
{
    return  a+(b*c);
}

INLINE(QUAD_ITYPE,famlqi) (QUAD_ITYPE a, QUAD_ITYPE b, QUAD_ITYPE c)
{
    return  a+(b*c);
}

INLINE(QUAD_FTYPE,famlqf) (QUAD_FTYPE a, QUAD_FTYPE b, QUAD_FTYPE c)
{
    return  a+(b*c);
}

#endif

INLINE(Vdyu,VDYU_FAML) (Vdyu a, Vdyu b, Vdyu c)
{
    return  VDYU_ADDL(a, VDYU_MULL(b, c));
}

INLINE(Vdbu,VDBU_FAML) (Vdbu a, Vdbu b, Vdbu c) {return vmla_u8(a, b, c);}
INLINE(Vdbi,VDBI_FAML) (Vdbi a, Vdbi b, Vdbi c) {return vmla_s8(a, b, c);}
INLINE(Vdbc,VDBC_FAML) (Vdbc a, Vdbc b, Vdbc c)
{
#if CHAR_MIN
#   define  VDBC_FAML(A, B) VDBI_ASBC(vmul_s8(VDBC_ASBI(A), VDBC_ASBI(B)))
#else
#   define  VDBC_FAML(A, B) VDBU_ASBC(vmul_u8(VDBC_ASBU(A), VDBC_ASBU(B)))
#endif
    return  VDBC_FAML(a, b);
}

INLINE(Vdhu,VDHU_FAML) (Vdhu a, Vdhu b, Vdhu c) {return vmla_u16(a, b, c);}
INLINE(Vdhi,VDHI_FAML) (Vdhi a, Vdhi b, Vdhi c) {return vmla_s16(a, b, c);}
INLINE(Vdhf,VDHF_FAML) (Vdhf a, Vdhf b, Vdhf c)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmla_f16(a, b, c);
#else
    return vcvt_f16_f32(
        vmlaq_f32(
            vcvt_f32_f16(a),
            vcvt_f32_f16(b),
            vcvt_f32_f16(c)
        )
    );
#endif
}

INLINE(Vdwu,VDWU_FAML) (Vdwu a, Vdwu b, Vdwu c) {return vmla_u32(a, b, c);}
INLINE(Vdwi,VDWI_FAML) (Vdwi a, Vdwi b, Vdwi c) {return vmla_s32(a, b, c);}
INLINE(Vddu,VDDU_FAML) (Vddu a, Vddu b, Vddu c)
{
    return  vadd_u64(
        a,
        vdup_n_u64(vget_lane_u64(b, 0)*vget_lane_u64(c, 0))
    );
}

INLINE(Vddi,VDDI_FAML) (Vddi a, Vddi b, Vddi c)
{
    return  vadd_s64(
        a,
        vdup_n_s64(vget_lane_s64(b, 0)*vget_lane_s64(c, 0))
    );
}


INLINE(Vqyu,VQYU_FAML) (Vqyu a, Vqyu b, Vqyu c)
{
    return  VQYU_ADDL(a, VQYU_MULL(b, c));
}

INLINE(Vqbu,VQBU_FAML) (Vqbu a, Vqbu b, Vqbu c) {return vmlaq_u8(a, b, c);}
INLINE(Vqbi,VQBI_FAML) (Vqbi a, Vqbi b, Vqbi c) {return vmlaq_s8(a, b, c);}
INLINE(Vqbc,VQBC_FAML) (Vqbc a, Vqbc b, Vqbc c)
{
#if CHAR_MIN
    return  VQBI_ASBC(
        VQBI_FAML(VQBC_ASBI(a), VQBC_ASBI(a), VQBC_ASBI(c))
    );
#else
    return  VQBU_ASBC(
        VQBU_FAML(VQBC_ASBU(a), VQBC_ASBU(a), VQBC_ASBU(c))
    );
#endif
}

INLINE(Vqhu,VQHU_FAML) (Vqhu a, Vqhu b, Vqhu c) {return vmlaq_u16(a, b, c);}
INLINE(Vqhi,VQHI_FAML) (Vqhi a, Vqhi b, Vqhi c) {return vmlaq_s16(a, b, c);}
INLINE(Vqhf,VQHF_FAML) (Vqhf a, Vqhf b, Vqhf c)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmlaq_f16(a, b, c);
#else
    return  vcombine_f16(
        VDHF_FAML(vget_low_f16( a), vget_low_f16( b), vget_low_f16( c)),
        VDHF_FAML(vget_high_f16(a), vget_high_f16(b), vget_high_f16(c))
    );
#endif
}

INLINE(Vqwu,VQWU_FAML) (Vqwu a, Vqwu b, Vqwu c) {return vmlaq_u32(a, b, c);}
INLINE(Vqwi,VQWI_FAML) (Vqwi a, Vqwi b, Vqwi c) {return vmlaq_s32(a, b, c);}
INLINE(Vqwf,VQWF_FAML) (Vqwf a, Vqwi b, Vqwi c) {return vmlaq_s32(a, b, c);}
INLINE(Vqdu,VQDU_FAML) (Vqdu a, Vqdu b, Vqdu c)
{
    return  vaddq_u64(
        a,
        vcombine_u64(
            vdup_n_u64(vgetq_lane_u64(b,0)*vgetq_lane_u64(c,0)),
            vdup_n_u64(vgetq_lane_u64(b,1)*vgetq_lane_u64(c,1))
        )
    );
}

INLINE(Vqdi,VQDI_FAML) (Vqdi a, Vqdi b, Vqdi c)
{
    return  vaddq_s64(
        a,
        vcombine_s64(
            vdup_n_s64(vgetq_lane_s64(b,0)*vgetq_lane_s64(c,0)),
            vdup_n_s64(vgetq_lane_s64(b,1)*vgetq_lane_s64(c,1))
        )
    );
}

#if _LEAVE_ARM_FAML
}
#endif

#if _ENTER_ARM_FAM2
{
#endif

INLINE(ushort, USHRT_FAM2) (ushort a,  uint8_t b,  uint8_t c)
{
    return a+(uint) b*c;
}

INLINE( short,  SHRT_FAM2)  (short a,   int8_t b,   int8_t c)
{
    return a+(int) b*c;
}

INLINE(  uint,  UINT_FAM2)   (uint a, uint16_t b, uint16_t c)
{
    return a+(uint) b*c;
}

INLINE(   int,   INT_FAM2)    (int a,  int16_t b,  int16_t c)
{
    return a+(int) b*c;
}

#if DWRD_NLONG == 2

INLINE( ulong, ULONG_FAM2)  (ulong a, uint16_t b, uint16_t c)
{
    return a+(ulong) b*c;
}

INLINE(  long,  LONG_FAM2)   (long a,  int16_t b,  int16_t c)
{
    return a+(long) b*c;
}

#else

INLINE( ulong, ULONG_FAM2)  (ulong a, uint32_t b, uint32_t c)
{
    return a+(ulong) b*c;
}

INLINE(  long,  LONG_FAM2)   (long a,  int32_t b,  int32_t c)
{
    return a+(long) b*c;
}

#endif

#if QUAD_NLLONG == 2

INLINE(ullong,ULLONG_FAM2) (ullong a, uint32_t b, uint32_t c)
{
    return a+(ullong) b*c;
}

INLINE( llong, LLONG_FAM2)  (llong a,  int32_t b,  int32_t c)
{
    return a+(llong) b*c;
}

INLINE(QUAD_UTYPE,fam2qu) (QUAD_UTYPE a, uint64_t b, uint64_t c)
{
    return a+(unsigned __int128) b*c;
}

INLINE(QUAD_ITYPE,fam2qi) (QUAD_ITYPE a,  int64_t b,  int64_t c)
{
    return a+(signed __int128) b*c;
}

#else

INLINE(ullong,ULLONG_FAM2) (ullong a, uint64_t b, uint64_t c)
{
    return a+(ullong) b*c;
}

INLINE( llong, LLONG_FAM2)  (llong a,  int64_t b,  int64_t c)
{
    return a+(llong) b*c;
}

#endif

INLINE(Vdhu,VDHU_FAM2) (Vdhu a, Vwbu b, Vwbu c)
{
    return  vget_low_u16(
        vmlal_u8(
            vcombine_u16(a, a),
            vreinterpret_u8_f32(vdup_n_f32(VWBU_ASTM(b))),
            vreinterpret_u8_f32(vdup_n_f32(VWBU_ASTM(c)))
        )
    );
}

INLINE(Vdhi,VDHI_FAM2) (Vdhi a, Vwbi b, Vwbi c)
{
    return  vget_low_s16(
        vmlal_s8(
            vcombine_s16(a, a),
            vreinterpret_s8_f32(vdup_n_f32(VWBI_ASTM(b))),
            vreinterpret_s8_f32(vdup_n_f32(VWBI_ASTM(c)))
        )
    );
}

INLINE(Vdwu,VDWU_FAM2) (Vdwu a, Vwhu b, Vwhu c)
{
    return  vget_low_u32(
        vmlal_u16(
            vcombine_u32(a, a),
            vreinterpret_u16_f32(vdup_n_f32(VWHU_ASTM(b))),
            vreinterpret_u16_f32(vdup_n_f32(VWHU_ASTM(c)))
        )
    );
}

INLINE(Vdwi,VDWI_FAM2) (Vdwi a, Vwhi b, Vwhi c)
{
    return  vget_low_s32(
        vmlal_s16(
            vcombine_s32(a, a),
            vreinterpret_s16_f32(vdup_n_f32(VWHI_ASTM(b))),
            vreinterpret_s16_f32(vdup_n_f32(VWHI_ASTM(c)))
        )
    );
}

INLINE(Vddu,VDDU_FAM2) (Vddu a, Vwwu b, Vwwu c)
{
    return  vget_low_u64(
        vmlal_u32(
            vcombine_u64(a, a),
            vreinterpret_u32_f32(vdup_n_f32(VWWU_ASTM(b))),
            vreinterpret_u32_f32(vdup_n_f32(VWWU_ASTM(c)))
        )
    );
}

INLINE(Vddi,VDDI_FAM2) (Vddi a, Vwwi b, Vwwi c)
{
    return  vget_low_s64(
        vmlal_s32(
            vcombine_s64(a, a),
            vreinterpret_s32_f32(vdup_n_f32(VWWI_ASTM(b))),
            vreinterpret_s32_f32(vdup_n_f32(VWWI_ASTM(c)))
        )
    );
}


INLINE(Vqhu,VQHU_FAM2) (Vqhu a, Vdbu b, Vdbu c) {return  vmlal_u8(a, b, c);}
INLINE(Vqhi,VQHI_FAM2) (Vqhi a, Vdbi b, Vdbi c) {return  vmlal_s8(a, b, c);}
INLINE(Vqwu,VQWU_FAM2) (Vqwu a, Vdhu b, Vdhu c) {return  vmlal_u16(a, b, c);}
INLINE(Vqwi,VQWI_FAM2) (Vqwi a, Vdhi b, Vdhi c) {return  vmlal_s16(a, b, c);}
INLINE(Vqdu,VQDU_FAM2) (Vqdu a, Vdwu b, Vdwu c) {return  vmlal_u32(a, b, c);}
INLINE(Vqdi,VQDI_FAM2) (Vqdi a, Vdwi b, Vdwi c) {return  vmlal_s32(a, b, c);}

#if _LEAVE_ARM_FAM2
}
#endif

#if _ENTER_ARM_FAMF
{
#endif

INLINE(flt16_t, FLT16_FAMF) (flt16_t a, flt16_t b, flt16_t c)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vfmah_f16(a, b, c);
#else
    return a+b*c;
#endif

}

INLINE(float, FLT_FAMF) (float a, float b, float c)
{
    return a+b*c;
}

INLINE(float, DBL_FAMF) (double a, double b, double c)
{
    return a+b*c;
}


INLINE(Vwhf,VWHF_FAMF) (Vwhf a, Vwhf b, Vwhf c)
{
    float ma = VWHF_ASTM(a);
    float mb = VWHF_ASTM(b);
    float mc = VWHF_ASTM(c);
    float32x2_t va = vdup_n_f32(ma);
    float32x2_t vb = vdup_n_f32(mb);
    float32x2_t vc = vdup_n_f32(mc);
    float16x4_t fa = vreinterpret_f16_f32(va);
    float16x4_t fb = vreinterpret_f16_f32(vb);
    float16x4_t fc = vreinterpret_f16_f32(vc);
#if defined(SPC_ARM_FP16_SIMD)
    fa = vfma_f16(fa, fb, fc);
#else
    float32x4_t qa = vcvt_f32_f16(fa);
    float32x4_t qb = vcvt_f32_f16(fb);
    float32x4_t qc = vcvt_f32_f16(fc);
    qa = vfmaq_f32(qa, qb, qc);
    fa = vcvt_f16_f32(qa);
#endif
    va = vreinterpret_f32_f16(fa);
    ma = vget_lane_f32(va, 0);
    return  WHF_ASTV(ma);
}

INLINE(Vwwf,VWWF_FAMF) (Vwwf a, Vwwf b, Vwwf c)
{
    float x = VWWF_ASTM(a);
    float y = VWWF_ASTM(b);
    float z = VWWF_ASTM(c);
    return WWF_ASTV((x+y*z));
}


INLINE(Vdhf,VDHF_FAMF) (Vdhf a, Vdhf b, Vdhf c)
{
#if defined(SPC_ARM_FP16_SIMD)
#   define  VDHF_FAMF(A, B, C) vfma_f16(A,B,C)
#else
#   define  VDHF_FAMF(A, B, C) \
 vcvt_f16_f32(              \
    vfmaq_f32(              \
        vcvt_f32_f16(A),    \
        vcvt_f32_f16(B),    \
        vcvt_f32_f16(C)     \
    )                       \
)
#endif

    return  VDHF_FAMF(a, b, c);
}

INLINE(Vdwf,VDWF_FAMF) (Vdwf a, Vdwf b, Vdwf c)
{
#define     VDWF_FAMF(A, B, C) vfma_f32(A,B,C)
    return  VDWF_FAMF(a, b, c);
}

INLINE(Vddf,VDDF_FAMF) (Vddf a, Vddf b, Vddf c)
{
#define     VDDF_FAMF(A, B, C) vfma_f64(A,B,C)
    return  VDDF_FAMF(a, b, c);
}



INLINE(Vqhf,VQHF_FAMF) (Vqhf a, Vqhf b, Vqhf c)
{
#if defined(SPC_ARM_FP16_SIMD)
#   define  VQHF_FAMF(A, B, C) vfmaq_f16(A,B,C)
    return  VDHF_FAMF(a, b, c);
#else
    return  vcombine_f16(
        VDHF_FAMF(
            vget_low_f16(a),
            vget_low_f16(b),
            vget_low_f16(c)
        ),
        VDHF_FAMF(
            vget_high_f16(a),
            vget_high_f16(b),
            vget_high_f16(c)
        )
    );
#endif

}

INLINE(Vqwf,VQWF_FAMF) (Vqwf a, Vqwf b, Vqwf c)
{
#define     VQWF_FAMF(A, B, C) vfmaq_f32(A,B,C)
    return  VQWF_FAMF(a, b, c);
}

INLINE(Vqdf,VQDF_FAMF) (Vqdf a, Vqdf b, Vqdf c)
{
#define     VQDF_FAMF(A, B, C) vfmaq_f64(A,B,C)
    return  VQDF_FAMF(a, b, c);
}

#if _LEAVE_ARM_FAMF
}
#endif


#if _ENTER_ARM_SUML
{
#endif

INLINE(   _Bool,VWYU_SUML) (Vwyu a)
{
    uint32_t v = VWWU_ASTV(VWYU_ASWU(a));
    v = v^(v>>16);
    v = v^(v>>8);
    v = v^(v>>4);
    v = v^(v>>2);
    v = v^(v>>1);
    return v;
}

INLINE( uint8_t,VWBU_SUML) (Vwbu a)
{
    uint8x8_t   v = vdup_n_u8(0);
    float32x2_t m = vreinterpret_f32_u8(v);
    m = vset_lane_f32(VWBU_ASTM(a), m, 0);
    return  vaddv_u8(v);
}

INLINE(  int8_t,VWBI_SUML) (Vwbi a)
{
    int8x8_t    v = vdup_n_s8(0);
    float32x2_t m = vreinterpret_f32_s8(v);
    m = vset_lane_f32(VWBI_ASTM(a), m, 0);
    return  vaddv_s8(v);
}


INLINE(    char,VWBC_SUML) (Vwbc a)
{
#if CHAR_MIN
    return VWBI_SUML(VWBC_ASBI(a));
#else
    return VWBU_SUML(VWBC_ASBU(a));
#endif
}

INLINE(uint16_t,VWHU_SUML) (Vwhu a)
{
    uint16x4_t  v = vdup_n_u16(0);
    float32x2_t m = vreinterpret_f32_u16(v);
    m = vset_lane_f32(VWHU_ASTM(a), m, 0);
    return  vaddv_u16(v);
}

INLINE( int16_t,VWHI_SUML) (Vwhi a)
{
    int16x4_t   v = vdup_n_s16(0);
    float32x2_t m = vreinterpret_f32_s16(v);
    m = vset_lane_f32(VWHI_ASTM(a), m, 0);
    return  vaddv_s16(v);
}


INLINE(   _Bool,VDYU_SUML) (Vdyu a)
{
    uint64x1_t  m = VDYU_ASTM(a);
    uint8x8_t   b = vreinterpret_u8_u64(m);
    return  vaddv_u8(vcnt_u8(b));
}

INLINE( uint8_t,VDBU_SUML) (Vdbu a) {return vaddv_u8(a);}
INLINE(  int8_t,VDBI_SUML) (Vdbi a) {return vaddv_s8(a);}
INLINE(    char,VDBC_SUML) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_SUML(A) vaddv_s8(VDBC_ASBI(A))
#else
#   define  VDBC_SUML(A) vaddv_u8(VDBC_ASBU(A))
#endif
    return  VDBC_SUML(a);
}

INLINE( uint16_t,VDHU_SUML) (Vdhu a) {return vaddv_u16(a);}
INLINE(  int16_t,VDHI_SUML) (Vdhi a) {return vaddv_s16(a);}
INLINE(  flt16_t,VDHF_SUML) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
// no such thing as vaddv_f16
#endif
    return  vaddvq_f32(vcvt_f32_f16(a));
}

INLINE(uint32_t,VDWU_SUML) (Vdwu a) {return vaddv_u32(a);}
INLINE( int32_t,VDWI_SUML) (Vdwi a) {return vaddv_s32(a);}
INLINE(   float,VDWF_SUML) (Vdwf a) {return vaddv_f32(a);}

INLINE(   _Bool,VQYU_SUML) (Vqyu a)
{
    uint64x2_t  m = VQYU_ASTM(a);
    uint8x16_t  b = vreinterpretq_u8_u64(m);
    return  vaddvq_u8(vcntq_u8(b));
}

INLINE( uint8_t,VQBU_SUML) (Vqbu a) {return vaddvq_u8(a);}
INLINE(  int8_t,VQBI_SUML) (Vqbi a) {return vaddvq_s8(a);}
INLINE(    char,VQBC_SUML) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_SUML(A, B) vaddvq_s8(VQBC_ASBI(A))
#else
#   define  VQBC_SUML(A, B) vaddvq_u8(VQBC_ASBU(A))
#endif
    return  VQBC_SUML(a, b);
}

INLINE(uint16_t,VQHU_SUML) (Vqhu a) {return vaddvq_u16(a);}
INLINE( int16_t,VQHI_SUML) (Vqhi a) {return vaddvq_s16(a);}
INLINE( flt16_t,VQHF_SUML) (Vqhf a)
{
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    return  vaddvq_f32(wl)+vaddvq_f32(wr);
}

INLINE(uint32_t,VQWU_SUML) (Vqwu a) {return vaddvq_u32(a);}
INLINE( int32_t,VQWI_SUML) (Vqwi a) {return vaddvq_s32(a);}
INLINE(   float,VQWF_SUML) (Vqwf a) {return vaddvq_f32(a);}
INLINE(uint64_t,VQDU_SUML) (Vqdu a) {return vaddvq_u64(a);}
INLINE( int64_t,VQDI_SUML) (Vqdi a) {return vaddvq_s64(a);}
INLINE(  double,VQDF_SUML) (Vqdf a) {return vaddvq_f64(a);}

#if _LEAVE_ARM_SUML
}
#endif

#if _ENTER_ARM_SUM2
{
#endif

INLINE( uint16_t,VWBU_SUM2) (Vwbu a)
{
    uint8x8_t   v = vdup_n_u8(0);
    float32x2_t m = vreinterpret_f32_u8(v);
    m = vset_lane_f32(VWBU_ASTM(a), m, 0);
    return  vaddlv_u8(v);
}

INLINE(  int16_t,VWBI_SUM2) (Vwbi a)
{
    int8x8_t    v = vdup_n_s8(0);
    float32x2_t m = vreinterpret_f32_s8(v);
    m = vset_lane_f32(VWBI_ASTM(a), m, 0);
    return  vaddlv_s8(v);
}

#if CHAR_MIN

INLINE( int16_t,VWBC_SUM2) (Vwbc a)
{
    return VWBI_SUM2(VWBC_ASBI(a));
}

#else

INLINE(uint16_t,VWBC_SUM2) (Vwbc a)
{
    return VWBU_SUM2(VWBC_ASBU(a));
}

#endif

INLINE(uint32_t,VWHU_SUM2) (Vwhu a)
{
    uint16x4_t  v = vdup_n_u16(0);
    float32x2_t m = vreinterpret_f32_u16(v);
    m = vset_lane_f32(VWHU_ASTM(a), m, 0);
    return  vaddlv_u16(v);
}

INLINE( int32_t,VWHI_SUM2) (Vwhi a)
{
    int16x4_t   v = vdup_n_s16(0);
    float32x2_t m = vreinterpret_f32_s16(v);
    m = vset_lane_f32(VWHI_ASTM(a), m, 0);
    return  vaddlv_s16(v);
}

INLINE(uint16_t,VDBU_SUM2) (Vdbu a) {return vaddlv_u8(a);}
INLINE( int16_t,VDBI_SUM2) (Vdbi a) {return vaddlv_s8(a);}
#if CHAR_MIN
INLINE( int16_t,VDBC_SUM2) (Vdbc a)
{
#   define  VDBC_SUM2(A) vaddlv_s8(VDBC_ASBI(A))
    return  VDBC_SUM2(a);
}
#else
INLINE(uint16_t,VDBC_SUM2) (Vdbc a)
{
#   define  VDBC_SUM2(A) vaddlv_u8(VDBC_ASBU(A))
    return  VDBC_SUM2(a);
}
#endif

INLINE(uint32_t,VDHU_SUM2) (Vdhu a) {return vaddlv_u16(a);}
INLINE( int32_t,VDHI_SUM2) (Vdhi a) {return vaddlv_s16(a);}


INLINE(uint64_t,VDWU_SUM2) (Vdwu a) {return vaddlv_u32(a);}
INLINE( int64_t,VDWI_SUM2) (Vdwi a) {return vaddlv_s32(a);}
INLINE(  double,VDWF_SUM2) (Vdwf a) {return vaddvq_f64(vcvt_f64_f32(a));}

INLINE(uint16_t,VQBU_SUM2) (Vqbu a) {return vaddlvq_u8(a);}
INLINE( int16_t,VQBI_SUM2) (Vqbi a) {return vaddlvq_s8(a);}
#if CHAR_MIN
INLINE( int16_t,VQBC_SUM2) (Vqbc a)
{
#   define  VQBC_SUM2(A) vaddlvq_s8(VDBC_ASBI(A))
    return  VQBC_SUM2(a);
}

#else

INLINE(uint16_t,VQBC_SUM2) (Vqbc a)
{
#   define  VQBC_SUM2(A) vaddlvq_u8(VQBC_ASBU(A))
    return  VQBC_SUM2(a);
}

#endif

INLINE(uint32_t,VQHU_SUM2) (Vqhu a) {return vaddlvq_u16(a);}
INLINE( int32_t,VQHI_SUM2) (Vqhi a) {return vaddlvq_s16(a);}
INLINE(   float,VQHF_SUM2) (Vqhf a)
{
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    return  vaddvq_f32(wl)+vaddvq_f32(wr);
}

INLINE(uint64_t,VQWU_SUM2) (Vqwu a) {return vaddlvq_u32(a);}
INLINE( int64_t,VQWI_SUM2) (Vqwi a) {return vaddlvq_s32(a);}
INLINE(  double,VQWF_SUM2) (Vqwf a)
{
    float32x2_t hl = vget_low_f32(a);
    float32x2_t hr = vget_high_f32(a);
    float64x2_t wl = vcvt_f64_f32(hl);
    float64x2_t wr = vcvt_f64_f32(hr);
    return  vaddvq_f64(wl)+vaddvq_f64(wr);
}

#if _LEAVE_ARM_SUM2
}
#endif

#if _ENTER_ARM_SUMS
{
#endif

INLINE(   _Bool,VWYU_SUMS) (Vwyu a)
{
    float32x2_t m = vdup_n_f32(VWYU_ASTM(a));
    uint32x2_t  r = vdup_n_u32(~0u);
    uint32x2_t  l = vreinterpret_u32_f32(m);
    l = vtst_u32(l, r);
    return  vget_lane_u32(l, V2_K0);
}

INLINE( uint8_t,VWBU_SUMS) (Vwbu a)
{
    float32x2_t m = vdup_n_f32(0);
    m = vset_lane_f32(VWBU_ASTM(a), m, V2_K0);
    return  vqmovnh_u16(vaddlv_u8(vreinterpret_u8_f32(m)));
}

INLINE(  int8_t,VWBI_SUMS) (Vwbi a)
{
    float32x2_t m = vdup_n_f32(0);
    m = vset_lane_f32(VWBI_ASTM(a), m, V2_K0);
    return  vqmovnh_s16(vaddlv_s8(vreinterpret_s8_f32(m)));
}


INLINE(    char,VWBC_SUMS) (Vwbc a)
{
#if CHAR_MIN
#   define  VWBC_SUMS(A) ((char) VWBI_SUMS(VWBC_ASBI(A)))
#else
#   define  VWBC_SUMS(A) ((char) VWBU_SUMS(VWBC_ASBU(A)))
#endif
    return  VWBC_SUMS(a);
}

INLINE(uint16_t,VWHU_SUMS) (Vwhu a)
{
    float32x2_t m = vdup_n_f32(0);
    m = vset_lane_f32(VWHU_ASTM(a), m, V2_K0);
    return  vqmovns_u32(vaddlv_u16(vreinterpret_u16_f32(m)));
}

INLINE( int16_t,VWHI_SUMS) (Vwhi a)
{
    float32x2_t m = vdup_n_f32(0);
    m = vset_lane_f32(VWHI_ASTM(a), m, V2_K0);
    return  vqmovns_s32(vaddlv_s16(vreinterpret_s16_f32(m)));
}


INLINE(   _Bool,VDYU_SUMS) (Vdyu a)
{
    uint64x1_t  m = VDYU_ASTM(a);
    m = vtst_u64(m, vdup_n_u64(~0ull));
    return vget_lane_u64(m, 0);
}


INLINE( uint8_t,VDBU_SUMS) (Vdbu a)
{
    return  vqmovnh_u16(vaddlv_u8(a));
}

INLINE(  int8_t,VDBI_SUMS) (Vdbi a)
{
    return vqmovnh_s16(vaddlv_s8(a));
}

INLINE(    char,VDBC_SUMS) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_SUMS(A) ((char) VDBI_SUMS(VDBC_ASBI(A)))
#else
#   define  VDBC_SUMS(A) ((char) VDBU_SUMS(VDBC_ASBI(A)))
#endif
    return  VDBC_SUMS(a);
}


INLINE(uint16_t,VDHU_SUMS) (Vdhu a)
{
    return vqmovns_u32(vaddlv_u16(a));
}

INLINE( int16_t,VDHI_SUMS) (Vdhi a)
{
    return vqmovns_s32(vaddlv_s16(a));
}


INLINE(uint32_t,VDWU_SUMS) (Vdwu a)
{
    return  vqmovnd_u64(vaddlv_u32(a));
}

INLINE( int32_t,VDWI_SUMS) (Vdwi a)
{
    return  vqmovnd_s64(vaddlv_s32(a));
}


INLINE(_Bool,VQYU_SUMS) (Vqyu a)
{
    uint64x2_t n = VQYU_ASTM(a);
    n = vtstq_u64(n, vdupq_n_u64(~0ull));
    uint64x1_t r = vorr_u64(
        vget_low_u64(n),
        vget_high_u64(n)
    );
    return  vget_lane_u64(r, 0);
}


INLINE( uint8_t,VQBU_SUMS) (Vqbu a)
{
    return  vqmovnh_u16(vaddlvq_u8(a));
}

INLINE(  int8_t,VQBI_SUMS) (Vqbi a)
{
    return  vqmovnh_s16(vaddlvq_s8(a));
}

INLINE(    char,VQBC_SUMS) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_SUMS(A) ((char) vqmovnh_s16(vaddlvq_s8(VDBC_ASBI(A))))
#else
#   define  VQBC_SUMS(A) ((char) vqmovnh_u16(vaddlvq_u8(VQBC_ASBU(A))))
#endif
    return  VQBC_SUMS(a);
}


INLINE(uint16_t,VQHU_SUMS) (Vqhu a)
{
    return  vqmovns_u32(vaddlvq_u16(a));
}

INLINE( int16_t,VQHI_SUMS) (Vqhi a)
{
    return  vqmovns_u32(vaddlvq_s16(a));
}


INLINE(uint32_t,VQWU_SUMS) (Vqwu a)
{
    return  vqmovnd_u64(vaddlvq_u32(a));
}

INLINE( int32_t,VQWI_SUMS) (Vqwi a)
{
    return  vqmovnd_s64(vaddlvq_s32(a));
}


INLINE(uint64_t,VQDU_SUMS) (Vqdu a)
{
    return  vqaddd_u64(vget_low_u64(a), vget_high_u64(a));
}

INLINE( int64_t,VQDI_SUMS) (Vqdi a)
{
    return  vqaddd_s64(vget_low_s64(a), vget_high_s64(a));
}

#if _LEAVE_ARM_SUMS
}
#endif


#if _ENTER_ARM_SUMF
{
#endif

INLINE(flt16_t,VWHF_SUMF) (Vwhf a)
{
    float16x4_t h = vdup_n_f16(0.0f16);
    float32x2_t w = vreinterpret_f32_f16(h);
    w = vset_lane_f32(VWHF_ASTM(a), w, 0);
    float32x4_t q = vcvt_f32_f16(w);
    return vaddvq_f32(q);
}


INLINE(flt16_t,VDHF_SUMF) (Vdhf a)
{
    return vaddvq_f32(vcvt_f32_f16(a));
}

INLINE(  float,VDWF_SUMF) (Vdwf a)
{
    return  vaddv_f32(a);
}


INLINE(flt16_t,VQHF_SUMF) (Vqhf a)
{
    float32x4_t l = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t r = vcvt_f32_f16(vget_high_f16(a));
    return vaddvq_f32(l)+vaddvq_f32(r);
}

INLINE(  float,VQWF_SUMF) (Vqwf a)
{
    return  vaddvq_f32(a);
}

INLINE( double,VQDF_SUMF) (Vqdf a)
{
    return  vaddvq_f64(a);
}


#if _LEAVE_ARM_SUMF
}
#endif

#if _ENTER_ARM_MODL
{
#endif

INLINE( uchar, UCHAR_MODL)  (uchar a,  uchar b) {return a%b;}
INLINE( schar, SCHAR_MODL)  (schar a,  schar b) {return a%b;}
INLINE(  char,  CHAR_MODL)   (char a,   char b) {return a%b;}
INLINE(ushort, USHRT_MODL) (ushort a, ushort b) {return a%b;}
INLINE( short,  SHRT_MODL)  (short a,  short b) {return a%b;}
INLINE(  uint,  UINT_MODL)   (uint a,   uint b) {return a%b;}
INLINE(   int,   INT_MODL)    (int a,    int b) {return a%b;}
INLINE( ulong, ULONG_MODL)  (ulong a,  ulong b) {return a%b;}
INLINE(  long,  LONG_MODL)   (long a,   long b) {return a%b;}
INLINE(ullong,ULLONG_MODL) (ullong a, ullong b) {return a%b;}
INLINE( llong, LLONG_MODL)  (llong a,  llong b) {return a%b;}
#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,modloqu) (QUAD_UTYPE a, QUAD_UTYPE b) {return a%b;}
INLINE(QUAD_ITYPE,modloqi) (QUAD_ITYPE a, QUAD_UTYPE b) {return a%b;}
#endif

INLINE(Vwbu,VWBU_MODL) (Vwbu a, Vwbu b)
{
    return  VWBU_NEWL(
        (VWBU_GET1(a, 0)%VWBU_GET1(b, 0)),
        (VWBU_GET1(a, 1)%VWBU_GET1(b, 1)),
        (VWBU_GET1(a, 2)%VWBU_GET1(b, 2)),
        (VWBU_GET1(a, 3)%VWBU_GET1(b, 3))
    );
}

INLINE(Vwbi,VWBI_MODL) (Vwbi a, Vwbi b)
{
    return  VWBI_NEWL(
        (VWBI_GET1(a, 0)%VWBI_GET1(b, 0)),
        (VWBI_GET1(a, 1)%VWBI_GET1(b, 1)),
        (VWBI_GET1(a, 2)%VWBI_GET1(b, 2)),
        (VWBI_GET1(a, 3)%VWBI_GET1(b, 3))
    );
}

INLINE(Vwbc,VWBC_MODL) (Vwbc a, Vwbc b)
{
    return  VWBC_NEWL(
        (VWBC_GET1(a, 0)%VWBC_GET1(b, 0)),
        (VWBC_GET1(a, 1)%VWBC_GET1(b, 1)),
        (VWBC_GET1(a, 2)%VWBC_GET1(b, 2)),
        (VWBC_GET1(a, 3)%VWBC_GET1(b, 3))
    );
}

INLINE(Vwhu,VWHU_MODL) (Vwhu a, Vwhu b)
{
    return  VWHU_NEWL(
        (VWHU_GET1(a, 0)%VWHU_GET1(b, 0)),
        (VWHU_GET1(a, 1)%VWHU_GET1(b, 1))
    );
}

INLINE(Vwhi,VWHI_MODL) (Vwhi a, Vwhi b)
{
    return  VWHI_NEWL(
        (VWHI_GET1(a, 0)%VWHI_GET1(b, 0)),
        (VWHI_GET1(a, 1)%VWHI_GET1(b, 1))
    );
}

INLINE(Vwwu,VWWU_MODL) (Vwwu a, Vwwu b)
{
    return  UINT_ASTV( (VWWU_ASTV(a)%VWWU_ASTV(b)) );
}

INLINE(Vwwi,VWWI_MODL) (Vwwi a, Vwwi b)
{
    return  INT_ASTV( (VWWI_ASTV(a)%VWWI_ASTV(b)) );
}


INLINE(Vdbu,VDBU_MODL) (Vdbu a, Vdbu b)
{
    return  VDBU_NEWL(
        (VDBU_GET1(a,0)%VDBU_GET1(b,0)),
        (VDBU_GET1(a,1)%VDBU_GET1(b,1)),
        (VDBU_GET1(a,2)%VDBU_GET1(b,2)),
        (VDBU_GET1(a,3)%VDBU_GET1(b,3)),
        (VDBU_GET1(a,4)%VDBU_GET1(b,4)),
        (VDBU_GET1(a,5)%VDBU_GET1(b,5)),
        (VDBU_GET1(a,6)%VDBU_GET1(b,6)),
        (VDBU_GET1(a,7)%VDBU_GET1(b,7))
    );
}

INLINE(Vdbi,VDBI_MODL) (Vdbi a, Vdbi b)
{
    return  VDBI_NEWL(
        (VDBI_GET1(a,0)%VDBI_GET1(b,0)),
        (VDBI_GET1(a,1)%VDBI_GET1(b,1)),
        (VDBI_GET1(a,2)%VDBI_GET1(b,2)),
        (VDBI_GET1(a,3)%VDBI_GET1(b,3)),
        (VDBI_GET1(a,4)%VDBI_GET1(b,4)),
        (VDBI_GET1(a,5)%VDBI_GET1(b,5)),
        (VDBI_GET1(a,6)%VDBI_GET1(b,6)),
        (VDBI_GET1(a,7)%VDBI_GET1(b,7))
    );
}

INLINE(Vdbc,VDBC_MODL) (Vdbc a, Vdbc b)
{
    return  VDBC_NEWL(
        (VDBC_GET1(a,0)%VDBC_GET1(b,0)),
        (VDBC_GET1(a,1)%VDBC_GET1(b,1)),
        (VDBC_GET1(a,2)%VDBC_GET1(b,2)),
        (VDBC_GET1(a,3)%VDBC_GET1(b,3)),
        (VDBC_GET1(a,4)%VDBC_GET1(b,4)),
        (VDBC_GET1(a,5)%VDBC_GET1(b,5)),
        (VDBC_GET1(a,6)%VDBC_GET1(b,6)),
        (VDBC_GET1(a,7)%VDBC_GET1(b,7))
    );
}

INLINE(Vdhu,VDHU_MODL) (Vdhu a, Vdhu b)
{
    return VDHU_NEWL(
        (VDHU_GET1(a, 0)%VDHU_GET1(b, 0)),
        (VDHU_GET1(a, 1)%VDHU_GET1(b, 1)),
        (VDHU_GET1(a, 2)%VDHU_GET1(b, 2)),
        (VDHU_GET1(a, 3)%VDHU_GET1(b, 3))
    );
}

INLINE(Vdhi,VDHI_MODL) (Vdhi a, Vdhi b)
{
    return VDHI_NEWL(
        (VDHI_GET1(a, 0)%VDHI_GET1(b, 0)),
        (VDHI_GET1(a, 1)%VDHI_GET1(b, 1)),
        (VDHI_GET1(a, 2)%VDHI_GET1(b, 2)),
        (VDHI_GET1(a, 3)%VDHI_GET1(b, 3))
    );
}

INLINE(Vdwu,VDWU_MODL) (Vdwu a, Vdwu b)
{
    return VDWU_NEWL(
        (VDWU_GET1(a, 0)%VDWU_GET1(b, 0)),
        (VDWU_GET1(a, 1)%VDWU_GET1(b, 1))
    );
}

INLINE(Vdwi,VDWI_MODL) (Vdwi a, Vdwi b)
{
    return VDWI_NEWL(
        (VDWI_GET1(a, 0)%VDWI_GET1(b, 0)),
        (VDWI_GET1(a, 1)%VDWI_GET1(b, 1))
    );
}

INLINE(Vddu,VDDU_MODL) (Vddu a, Vddu b)
{
    return  UINT64_ASTV( (VDDU_ASTV(a)%VDDU_ASTV(b)) );
}

INLINE(Vddi,VDDI_MODL) (Vddi a, Vddi b)
{
    return  INT64_ASTV( (VDDI_ASTV(a)%VDDI_ASTV(b)) );
}


INLINE(Vqbu,VQBU_MODL) (Vqbu a, Vqbu b)
{
    return  VQBU_NEWL(
        (VQBU_GET1(a, 0)%VQBU_GET1(b, 0)),
        (VQBU_GET1(a, 1)%VQBU_GET1(b, 1)),
        (VQBU_GET1(a, 2)%VQBU_GET1(b, 2)),
        (VQBU_GET1(a, 3)%VQBU_GET1(b, 3)),
        (VQBU_GET1(a, 4)%VQBU_GET1(b, 4)),
        (VQBU_GET1(a, 5)%VQBU_GET1(b, 5)),
        (VQBU_GET1(a, 6)%VQBU_GET1(b, 6)),
        (VQBU_GET1(a, 7)%VQBU_GET1(b, 7)),
        (VQBU_GET1(a, 8)%VQBU_GET1(b, 8)),
        (VQBU_GET1(a, 9)%VQBU_GET1(b, 9)),
        (VQBU_GET1(a,10)%VQBU_GET1(b,10)),
        (VQBU_GET1(a,11)%VQBU_GET1(b,11)),
        (VQBU_GET1(a,12)%VQBU_GET1(b,12)),
        (VQBU_GET1(a,13)%VQBU_GET1(b,13)),
        (VQBU_GET1(a,14)%VQBU_GET1(b,14)),
        (VQBU_GET1(a,15)%VQBU_GET1(b,15))
    );
}

INLINE(Vqbi,VQBI_MODL) (Vqbi a, Vqbi b)
{
    return  VQBI_NEWL(
        (VQBI_GET1(a, 0)%VQBI_GET1(b, 0)),
        (VQBI_GET1(a, 1)%VQBI_GET1(b, 1)),
        (VQBI_GET1(a, 2)%VQBI_GET1(b, 2)),
        (VQBI_GET1(a, 3)%VQBI_GET1(b, 3)),
        (VQBI_GET1(a, 4)%VQBI_GET1(b, 4)),
        (VQBI_GET1(a, 5)%VQBI_GET1(b, 5)),
        (VQBI_GET1(a, 6)%VQBI_GET1(b, 6)),
        (VQBI_GET1(a, 7)%VQBI_GET1(b, 7)),
        (VQBI_GET1(a, 8)%VQBI_GET1(b, 8)),
        (VQBI_GET1(a, 9)%VQBI_GET1(b, 9)),
        (VQBI_GET1(a,10)%VQBI_GET1(b,10)),
        (VQBI_GET1(a,11)%VQBI_GET1(b,11)),
        (VQBI_GET1(a,12)%VQBI_GET1(b,12)),
        (VQBI_GET1(a,13)%VQBI_GET1(b,13)),
        (VQBI_GET1(a,14)%VQBI_GET1(b,14)),
        (VQBI_GET1(a,15)%VQBI_GET1(b,15))
    );
}


INLINE(Vqbc,VQBC_MODL) (Vqbc a, Vqbc b)
{
    return  VQBC_NEWL(
        (VQBC_GET1(a, 0)%VQBC_GET1(b, 0)),
        (VQBC_GET1(a, 1)%VQBC_GET1(b, 1)),
        (VQBC_GET1(a, 2)%VQBC_GET1(b, 2)),
        (VQBC_GET1(a, 3)%VQBC_GET1(b, 3)),
        (VQBC_GET1(a, 4)%VQBC_GET1(b, 4)),
        (VQBC_GET1(a, 5)%VQBC_GET1(b, 5)),
        (VQBC_GET1(a, 6)%VQBC_GET1(b, 6)),
        (VQBC_GET1(a, 7)%VQBC_GET1(b, 7)),
        (VQBC_GET1(a, 8)%VQBC_GET1(b, 8)),
        (VQBC_GET1(a, 9)%VQBC_GET1(b, 9)),
        (VQBC_GET1(a,10)%VQBC_GET1(b,10)),
        (VQBC_GET1(a,11)%VQBC_GET1(b,11)),
        (VQBC_GET1(a,12)%VQBC_GET1(b,12)),
        (VQBC_GET1(a,13)%VQBC_GET1(b,13)),
        (VQBC_GET1(a,14)%VQBC_GET1(b,14)),
        (VQBC_GET1(a,15)%VQBC_GET1(b,15))
    );
}

INLINE(Vqhu,VQHU_MODL) (Vqhu a, Vqhu b)
{
    return  VQHU_NEWL(
        (VQHU_GET1(a, 0)%VQHU_GET1(b, 0)),
        (VQHU_GET1(a, 1)%VQHU_GET1(b, 1)),
        (VQHU_GET1(a, 2)%VQHU_GET1(b, 2)),
        (VQHU_GET1(a, 3)%VQHU_GET1(b, 3)),
        (VQHU_GET1(a, 4)%VQHU_GET1(b, 4)),
        (VQHU_GET1(a, 5)%VQHU_GET1(b, 5)),
        (VQHU_GET1(a, 6)%VQHU_GET1(b, 6)),
        (VQHU_GET1(a, 7)%VQHU_GET1(b, 7))
    );
}

INLINE(Vqhi,VQHI_MODL) (Vqhi a, Vqhi b)
{
    return  VQHI_NEWL(
        (VQHI_GET1(a, 0)%VQHI_GET1(b, 0)),
        (VQHI_GET1(a, 1)%VQHI_GET1(b, 1)),
        (VQHI_GET1(a, 2)%VQHI_GET1(b, 2)),
        (VQHI_GET1(a, 3)%VQHI_GET1(b, 3)),
        (VQHI_GET1(a, 4)%VQHI_GET1(b, 4)),
        (VQHI_GET1(a, 5)%VQHI_GET1(b, 5)),
        (VQHI_GET1(a, 6)%VQHI_GET1(b, 6)),
        (VQHI_GET1(a, 7)%VQHI_GET1(b, 7))
    );
}

INLINE(Vqwu,VQWU_MODL) (Vqwu a, Vqwu b)
{
    return  VQWU_NEWL(
        (VQWU_GET1(a, 0)%VQWU_GET1(b, 0)),
        (VQWU_GET1(a, 1)%VQWU_GET1(b, 1)),
        (VQWU_GET1(a, 2)%VQWU_GET1(b, 2)),
        (VQWU_GET1(a, 3)%VQWU_GET1(b, 3))
    );
}

INLINE(Vqwi,VQWI_MODL) (Vqwi a, Vqwi b)
{
    return  VQWI_NEWL(
        (VQWI_GET1(a, 0)%VQWI_GET1(b, 0)),
        (VQWI_GET1(a, 1)%VQWI_GET1(b, 1)),
        (VQWI_GET1(a, 2)%VQWI_GET1(b, 2)),
        (VQWI_GET1(a, 3)%VQWI_GET1(b, 3))
    );
}

INLINE(Vqdu,VQDU_MODL) (Vqdu a, Vqdu b)
{
    return  VQDU_NEWL(
        (VQDU_GET1(a, 0)%VQDU_GET1(b, 0)),
        (VQDU_GET1(a, 1)%VQDU_GET1(b, 1))
    );
}

INLINE(Vqdi,VQDI_MODL) (Vqdi a, Vqdi b)
{
    return  VQDI_NEWL(
        (VQDI_GET1(a, 0)%VQDI_GET1(b, 0)),
        (VQDI_GET1(a, 1)%VQDI_GET1(b, 1))
    );
}

#if _LEAVE_ARM_MODL
}
#endif

#if _ENTER_ARM_MOD2
{
#endif


INLINE( uint8_t, USHRT_MOD2) (ushort a,  uint8_t b)
{
#define     USHRT_MOD(A, B) ((uint8_t) (((ushort) A)%((uint8_t) B)))
    return  a%b;
}

INLINE(  int8_t,  SHRT_MOD2)  (short a,   int8_t b)
{
#define     SHRT_MOD(A, B) ((int8_t) (((short) A)%((int8_t) B)))
    return  a%b;
}


INLINE(uint16_t,  UINT_MOD2)   (uint a, uint16_t b)
{
#define     UINT_MOD(A, B) ((uint16_t) (((uint) A)%((uint16_t) B)))
    return  a%b;
}

INLINE( int16_t,   INT_MOD2)    (int a,  int16_t b)
{
#define     INT_MOD(A, B) ((int16_t) (((int) A)%((int16_t) B)))
    return  a%b;
}

#if DWRD_NLONG == 2

INLINE(uint16_t, ULONG_MOD2)  (ulong a, uint16_t b)
{
#define     ULONG_MOD(A, B) ((uint16_t) (((ulong) A)%((uint16_t) B)))
    return  a%b;
}

INLINE( int16_t,  LONG_MOD2)   (long a,  int16_t b)
{
#define     LONG_MOD(A, B) ((int16_t) (((long) A)%((int16_t) B)))
    return  a%b;
}

#else

INLINE(uint32_t, ULONG_MOD2)  (ulong a, uint32_t b)
{
#define     ULONG_MOD(A, B) ((uint32_t) (((ulong) A)%((uint32_t) B)))
    return  a%b;
}

INLINE(int32_t,   LONG_MOD2)   (long a,  int32_t b)
{
#define     LONG_MOD(A, B) ((int32_t) (((long) A)%((int32_t) B)))
    return  a%b;
}

#endif


#if QUAD_NLLONG == 2

INLINE(uint32_t,ULLONG_MOD2) (ullong a, uint32_t b)
{
#define     ULLONG_MOD(A, B) ((uint32_t) (((ullong) A)%((uint32_t) B)))
    return  a%b;
}

INLINE( int32_t, LLONG_MOD2)  (llong a,  int32_t b)
{
#define     LLONG_MOD(A, B) ((int32_t) (((llong) A)%((int32_t) B)))
    return  a%b;
}


INLINE( uint64_t,mod2qu)  (QUAD_UTYPE a,   uint64_t b)
{
    return  a%b;
}

INLINE(  int64_t,mod2qi) (QUAD_ITYPE a,    int64_t b)
{
    return  a%b;
}

#else

INLINE(uint64_t,ULLONG_MOD2) (ullong a, uint64_t b)
{
#define     ULLONG_MOD(A, B) ((uint64_t) (((ullong) A)%((uint64_t) B)))
    return  a%b;
}

INLINE( int64_t, LLONG_MOD2)  (llong a,  int64_t b)
{
#define     LLONG_MOD(A, B) ((int64_t) (((llong) A)%((int64_t) B)))
    return  a%b;
}

#endif

INLINE(Vwbu,VDHU_MOD2) (Vdhu a, Vwbu b)
{
    return  VWBU_NEWL(
        (vget_lane_u16(a, 0)%VWBU_GET1(b, 0)),
        (vget_lane_u16(a, 1)%VWBU_GET1(b, 1)),
        (vget_lane_u16(a, 2)%VWBU_GET1(b, 2)),
        (vget_lane_u16(a, 3)%VWBU_GET1(b, 3))
    );
}

INLINE(Vwbi,VDHI_MOD2) (Vdhi a, Vwbi b)
{
    return  VWBI_NEWL(
        (vget_lane_s16(a, 0)%VWBI_GET1(b, 0)),
        (vget_lane_s16(a, 1)%VWBI_GET1(b, 1)),
        (vget_lane_s16(a, 2)%VWBI_GET1(b, 2)),
        (vget_lane_s16(a, 3)%VWBI_GET1(b, 3))
    );
}


INLINE(Vwhu,VDWU_MOD2) (Vdwu a, Vwhu b)
{
    return  VWHU_NEWL(
        (vget_lane_u32(a, 0)%VWHU_GET1(b, 0)),
        (vget_lane_u32(a, 1)%VWHU_GET1(b, 1))
    );
}

INLINE(Vwhi,VDWI_MOD2) (Vdwi a, Vwhi b)
{
    return  VWHI_NEWL(
        (vget_lane_s32(a, 0)%VWHI_GET1(b, 0)),
        (vget_lane_s32(a, 1)%VWHI_GET1(b, 1))
    );
}


INLINE(Vwwu,VDDU_MOD2) (Vddu a, Vwwu b)
{
    return  UINT32_ASTV((vget_lane_u64(a, 0)%VWWU_ASTV(b)));
}

INLINE(Vwwi,VDDI_MOD2) (Vddi a, Vwwi b)
{
    return  INT32_ASTV((vget_lane_s64(a, 0)%VWWI_ASTV(b)));
}


INLINE(Vdbu,VQHU_MOD2) (Vqhu a, Vdbu b)
{
    return  VDBU_NEWL(
        (vgetq_lane_u16(a, 0)%vget_lane_u8(b, 0)),
        (vgetq_lane_u16(a, 1)%vget_lane_u8(b, 1)),
        (vgetq_lane_u16(a, 2)%vget_lane_u8(b, 2)),
        (vgetq_lane_u16(a, 3)%vget_lane_u8(b, 3)),
        (vgetq_lane_u16(a, 4)%vget_lane_u8(b, 4)),
        (vgetq_lane_u16(a, 5)%vget_lane_u8(b, 5)),
        (vgetq_lane_u16(a, 6)%vget_lane_u8(b, 6)),
        (vgetq_lane_u16(a, 7)%vget_lane_u8(b, 7))
    );
}

INLINE(Vdbi,VQHI_MOD2) (Vqhi a, Vdbi b)
{
    return  VDBU_NEWL(
        (vgetq_lane_s16(a, 0)%vget_lane_s8(b, 0)),
        (vgetq_lane_s16(a, 1)%vget_lane_s8(b, 1)),
        (vgetq_lane_s16(a, 2)%vget_lane_s8(b, 2)),
        (vgetq_lane_s16(a, 3)%vget_lane_s8(b, 3)),
        (vgetq_lane_s16(a, 4)%vget_lane_s8(b, 4)),
        (vgetq_lane_s16(a, 5)%vget_lane_s8(b, 5)),
        (vgetq_lane_s16(a, 6)%vget_lane_s8(b, 6)),
        (vgetq_lane_s16(a, 7)%vget_lane_s8(b, 7))
    );
}


INLINE(Vdhu,VQWU_MOD2) (Vqwu a, Vdhu b)
{
    return  VDHU_NEWL(
        (vgetq_lane_u32(a, 0)%vget_lane_u16(b, 0)),
        (vgetq_lane_u32(a, 1)%vget_lane_u16(b, 1)),
        (vgetq_lane_u32(a, 2)%vget_lane_u16(b, 2)),
        (vgetq_lane_u32(a, 3)%vget_lane_u16(b, 3))
    );
}

INLINE(Vdhi,VQWI_MOD2) (Vqwi a, Vdhi b)
{
    return  VDHI_NEWL(
        (vgetq_lane_s32(a, 0)%vget_lane_s16(b, 0)),
        (vgetq_lane_s32(a, 1)%vget_lane_s16(b, 1)),
        (vgetq_lane_s32(a, 2)%vget_lane_s16(b, 2)),
        (vgetq_lane_s32(a, 3)%vget_lane_s16(b, 3))
    );
}


INLINE(Vdwu,VQDU_MOD2) (Vqdu a, Vdwu b)
{
    return  VDWU_NEWL(
        (vgetq_lane_u64(a, 0)%vget_lane_u32(b, 0)),
        (vgetq_lane_u64(a, 1)%vget_lane_u32(b, 1))
    );
}

INLINE(Vdwi,VQDI_MOD2) (Vqdi a, Vdwi b)
{
    return  VDWI_NEWL(
        (vgetq_lane_s64(a, 0)%vget_lane_s32(b, 0)),
        (vgetq_lane_s64(a, 1)%vget_lane_s32(b, 1))
    );
}

#if _LEAVE_ARM_MOD2
}
#endif

#if _ENTER_ARM_TSTS
{
#endif

INLINE( uchar, UCHAR_TSTS)  (uchar a,  uchar b) {return 0000-(0 != (a&b));}
INLINE( schar, SCHAR_TSTS)  (schar a,  schar b) {return 0000-(0 != (a&b));}
INLINE(  char,  CHAR_TSTS)   (char a,   char b) {return 0000-(0 != (a&b));}
INLINE(ushort, USHRT_TSTS) (ushort a, ushort b) {return 0000-(0 != (a&b));}
INLINE( short,  SHRT_TSTS)  (short a,  short b) {return 0000-(0 != (a&b));}
INLINE(  uint,  UINT_TSTS)   (uint a,   uint b) {return 000u-(0 != (a&b));}
INLINE(   int,   INT_TSTS)    (int a,    int b) {return 0000-(0 != (a&b));}
INLINE( ulong, ULONG_TSTS)  (ulong a,  ulong b) {return 00ul-(0 != (a&b));}
INLINE(  long,  LONG_TSTS)   (long a,   long b) {return 000l-(0 != (a&b));}
INLINE(ullong,ULLONG_TSTS) (ullong a, ullong b) {return 0ull-(0 != (a&b));}
INLINE( llong, LLONG_TSTS)  (llong a,  llong b) {return 00ll-(0 != (a&b));}
INLINE(QUAD_UTYPE,tstsqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  ((QUAD_UTYPE) 0)-(0 != (a&b));
}

INLINE(QUAD_ITYPE,tstsqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  ((QUAD_ITYPE) 0)-(0 != (a&b));
}


INLINE(float,WBU_TSTS) (float a, float b) 
{
    uint8x8_t l = vreinterpret_u8_f32(vdup_n_f32(a));
    uint8x8_t r = vreinterpret_u8_f32(vdup_n_f32(a));
    l = vtst_u8(l, r);
    return  vget_lane_f32(vreinterpret_f32_u8(l), 0);
}

INLINE(float,WHU_TSTS) (float a, float b) 
{
    uint16x4_t l = vreinterpret_u16_f32(vdup_n_f32(a));
    uint16x4_t r = vreinterpret_u16_f32(vdup_n_f32(a));
    l = vtst_u16(l, r);
    return  vget_lane_f32(vreinterpret_f32_u16(l), 0);
}

INLINE(float,WWU_TSTS) (float a, float b) 
{
    uint32x2_t l = vreinterpret_u32_f32(vdup_n_f32(a));
    uint32x2_t r = vreinterpret_u32_f32(vdup_n_f32(a));
    l = vtst_u32(l, r);
    return  vget_lane_f32(vreinterpret_f32_u32(l), 0);
}


INLINE(Vwbu,VWBU_TSTS) (Vwbu a, Vwbu b) 
{
    return  WBU_ASTV(WBU_TSTS(VWBU_ASTM(a),VWBU_ASTM(b)));
}

INLINE(Vwbi,VWBI_TSTS) (Vwbi a, Vwbi b) 
{
    return  WBI_ASTV(WBU_TSTS(VWBI_ASTM(a),VWBI_ASTM(b)));
}

INLINE(Vwbc,VWBC_TSTS) (Vwbc a, Vwbc b) 
{
    return  WBC_ASTV(WBU_TSTS(VWBC_ASTM(a),VWBC_ASTM(b)));
}

INLINE(Vwhu,VWHU_TSTS) (Vwhu a, Vwhu b)
{
    return  WHU_ASTV(WHU_TSTS(VWHU_ASTM(a),VWHU_ASTM(b)));
}

INLINE(Vwhi,VWHI_TSTS) (Vwhi a, Vwhi b)
{
    return  WHI_ASTV(WHU_TSTS(VWHI_ASTM(a),VWHI_ASTM(b)));
}

INLINE(Vwwu,VWWU_TSTS) (Vwwu a, Vwwu b)
{
    return  WWU_ASTV(WWU_TSTS(VWWU_ASTM(a),VWWU_ASTM(b)));
}

INLINE(Vwwi,VWWI_TSTS) (Vwwi a, Vwwi b)
{
    return  WWI_ASTV(WWU_TSTS(VWWI_ASTM(a),VWWI_ASTM(b)));
}


INLINE(Vdbu,VDBU_TSTS) (Vdbu a, Vdbu b) {return vtst_u8(a, b);}
INLINE(Vdbi,VDBI_TSTS) (Vdbi a, Vdbi b) 
{
    return  vreinterpret_s8_u8(vtst_s8(a, b));
}

INLINE(Vdbc,VDBC_TSTS) (Vdbc a, Vdbc b) 
{
    return  VDBU_ASBC(vtst_u8(VDBC_ASBU(a), VDBC_ASBU(b)));
}


INLINE(Vdhu,VDHU_TSTS) (Vdhu a, Vdhu b) {return vtst_u16(a, b);}
INLINE(Vdhi,VDHI_TSTS) (Vdhi a, Vdhi b) 
{
    return  vreinterpret_s16_u16(vtst_s16(a, b));
}

INLINE(Vdwu,VDWU_TSTS) (Vdwu a, Vdwu b) {return vtst_u32(a, b);}
INLINE(Vdwi,VDWI_TSTS) (Vdwi a, Vdwi b) 
{
    return  vreinterpret_s32_u32(vtst_s32(a, b));
}

INLINE(Vddu,VDDU_TSTS) (Vddu a, Vddu b) {return vtst_u64(a, b);}
INLINE(Vddi,VDDI_TSTS) (Vddi a, Vddi b) 
{
    return  vreinterpret_s64_u64(vtst_s64(a, b));
}

INLINE(Vqbu,VQBU_TSTS) (Vqbu a, Vqbu b) {return vtstq_u8(a, b);}
INLINE(Vqbi,VQBI_TSTS) (Vqbi a, Vqbi b) 
{
    return  vreinterpretq_s8_u8(vtstq_s8(a, b));
}

INLINE(Vqbc,VQBC_TSTS) (Vqbc a, Vqbc b) 
{
    return  VQBU_ASBC(vtstq_u8(VQBC_ASBU(a), VQBC_ASBU(b)));
}


INLINE(Vqhu,VQHU_TSTS) (Vqhu a, Vqhu b) {return vtstq_u16(a, b);}
INLINE(Vqhi,VQHI_TSTS) (Vqhi a, Vqhi b) 
{
    return  vreinterpretq_s16_u16(vtstq_s16(a, b));
}

INLINE(Vqwu,VQWU_TSTS) (Vqwu a, Vqwu b) {return vtstq_u32(a, b);}
INLINE(Vqwi,VQWI_TSTS) (Vqwi a, Vqwi b) 
{
    return  vreinterpretq_s32_u32(vtstq_s32(a, b));
}

INLINE(Vqdu,VQDU_TSTS) (Vqdu a, Vqdu b) {return vtstq_u64(a, b);}
INLINE(Vqdi,VQDI_TSTS) (Vqdi a, Vqdi b) 
{
    return  vreinterpretq_s64_u64(vtstq_s64(a, b));
}

#if _LEAVE_ARM_TSTS
}
#endif


#if _ENTER_ARM_TSTY
{
#endif

INLINE( uchar, UCHAR_TSTY)  (uchar a,  uchar b) {return (0 != (a&b));}
INLINE( schar, SCHAR_TSTY)  (schar a,  schar b) {return (0 != (a&b));}
INLINE(  char,  CHAR_TSTY)   (char a,   char b) {return (0 != (a&b));}
INLINE(ushort, USHRT_TSTY) (ushort a, ushort b) {return (0 != (a&b));}
INLINE( short,  SHRT_TSTY)  (short a,  short b) {return (0 != (a&b));}
INLINE(  uint,  UINT_TSTY)   (uint a,   uint b) {return (0 != (a&b));}
INLINE(   int,   INT_TSTY)    (int a,    int b) {return (0 != (a&b));}
INLINE( ulong, ULONG_TSTY)  (ulong a,  ulong b) {return (0 != (a&b));}
INLINE(  long,  LONG_TSTY)   (long a,   long b) {return (0 != (a&b));}
INLINE(ullong,ULLONG_TSTY) (ullong a, ullong b) {return (0 != (a&b));}
INLINE( llong, LLONG_TSTY)  (llong a,  llong b) {return (0 != (a&b));}

INLINE(QUAD_UTYPE,tstyqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  0 != (a&b);
}

INLINE(QUAD_ITYPE,tstyqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  0 != (a&b);
}


INLINE(float,WBU_TSTY) (float a, float b) 
{
    uint8x8_t l = vreinterpret_u8_f32(vdup_n_f32(a));
    uint8x8_t r = vreinterpret_u8_f32(vdup_n_f32(a));
    l = vtst_u8(l, r);
    l = vshr_n_u8(l,7);
    return  vget_lane_f32(vreinterpret_f32_u8(l), 0);
}

INLINE(float,WHU_TSTY) (float a, float b) 
{
    uint16x4_t l = vreinterpret_u16_f32(vdup_n_f32(a));
    uint16x4_t r = vreinterpret_u16_f32(vdup_n_f32(a));
    l = vtst_u16(l, r);
    l = vshr_n_u16(l,15);
    return  vget_lane_f32(vreinterpret_f32_u16(l), 0);
}

INLINE(float,WWU_TSTY) (float a, float b) 
{
    uint32x2_t l = vreinterpret_u32_f32(vdup_n_f32(a));
    uint32x2_t r = vreinterpret_u32_f32(vdup_n_f32(a));
    l = vtst_u32(l, r);
    l = vshr_n_u32(l,31);
    return  vget_lane_f32(vreinterpret_f32_u32(l), 0);
}

INLINE(Vwbu,VWBU_TSTY) (Vwbu a, Vwbu b) 
{
    return  WBU_ASTV(WBU_TSTY(VWBU_ASTM(a),VWBU_ASTM(b)));
}

INLINE(Vwbi,VWBI_TSTY) (Vwbi a, Vwbi b) 
{
    return  WBI_ASTV(WBU_TSTY(VWBI_ASTM(a),VWBI_ASTM(b)));
}

INLINE(Vwbc,VWBC_TSTY) (Vwbc a, Vwbc b) 
{
    return  WBC_ASTV(WBU_TSTY(VWBC_ASTM(a),VWBC_ASTM(b)));
}

INLINE(Vwhu,VWHU_TSTY) (Vwhu a, Vwhu b)
{
    return  WHU_ASTV(WHU_TSTY(VWHU_ASTM(a),VWHU_ASTM(b)));
}

INLINE(Vwhi,VWHI_TSTY) (Vwhi a, Vwhi b)
{
    return  WHI_ASTV(WHU_TSTY(VWHI_ASTM(a),VWHI_ASTM(b)));
}

INLINE(Vwwu,VWWU_TSTY) (Vwwu a, Vwwu b)
{
    return  WWU_ASTV(WWU_TSTY(VWWU_ASTM(a),VWWU_ASTM(b)));
}

INLINE(Vwwi,VWWI_TSTY) (Vwwi a, Vwwi b)
{
    return  WWI_ASTV(WWU_TSTY(VWWI_ASTM(a),VWWI_ASTM(b)));
}


INLINE(Vdbu,VDBU_TSTY) (Vdbu a, Vdbu b) 
{
    return  vshr_n_u8(vtst_u8(a, b), 7);
}

INLINE(Vdbi,VDBI_TSTY) (Vdbi a, Vdbi b) 
{
    return  vreinterpret_s8_u8(vshr_n_u8(vtst_s8(a, b), 7));
}

INLINE(Vdbc,VDBC_TSTY) (Vdbc a, Vdbc b) 
{
    return  VDBU_ASBC(VDBU_TSTY(VDBC_ASBU(a), VDBC_ASBU(b)));
}


INLINE(Vdhu,VDHU_TSTY) (Vdhu a, Vdhu b) 
{
    return  vshr_n_u16(vtst_u16(a, b), 15);
}

INLINE(Vdhi,VDHI_TSTY) (Vdhi a, Vdhi b) 
{
    return  vreinterpret_s16_u16(vshr_n_u16(vtst_s16(a, b), 15));
}

INLINE(Vdwu,VDWU_TSTY) (Vdwu a, Vdwu b) 
{
    return  vshr_n_u32(vtst_u32(a, b), 31);
}
INLINE(Vdwi,VDWI_TSTY) (Vdwi a, Vdwi b) 
{
    return  vreinterpret_s32_u32(vshr_n_u32(vtst_s32(a, b), 31));
}

INLINE(Vddu,VDDU_TSTY) (Vddu a, Vddu b) 
{
    return  vshr_n_u64(vtst_u64(a, b), 63);
}

INLINE(Vddi,VDDI_TSTY) (Vddi a, Vddi b) 
{
    return  vreinterpret_s64_u64(vshr_n_u64(vtst_s64(a, b), 63));
}


INLINE(Vqbu,VQBU_TSTY) (Vqbu a, Vqbu b) 
{
    return  vshrq_n_u8(vtstq_u8(a, b), 7);
}

INLINE(Vqbi,VQBI_TSTY) (Vqbi a, Vqbi b) 
{
    return  vreinterpretq_s8_u8(vshrq_n_u8(vtstq_s8(a, b), 7));
}

INLINE(Vqbc,VQBC_TSTY) (Vqbc a, Vqbc b) 
{
    return  VQBU_ASBC(VQBU_TSTY(VQBC_ASBU(a), VQBC_ASBU(b)));
}


INLINE(Vqhu,VQHU_TSTY) (Vqhu a, Vqhu b) 
{
    return  vshrq_n_u16(vtstq_u16(a, b), 15);
}

INLINE(Vqhi,VQHI_TSTY) (Vqhi a, Vqhi b) 
{
    return  vreinterpretq_s16_u16(vshrq_n_u16(vtstq_s16(a, b), 15));
}

INLINE(Vqwu,VQWU_TSTY) (Vqwu a, Vqwu b)
{
    return  vshrq_n_u32(vtstq_u32(a, b), 31);
}

INLINE(Vqwi,VQWI_TSTY) (Vqwi a, Vqwi b) 
{
    return  vreinterpretq_s32_u32(vshrq_n_u32(vtstq_s32(a, b), 31));
}

INLINE(Vqdu,VQDU_TSTY) (Vqdu a, Vqdu b) 
{
    return  vshrq_n_u64(vtstq_u64(a, b), 63);
}

INLINE(Vqdi,VQDI_TSTY) (Vqdi a, Vqdi b) 
{
    return  vreinterpretq_s64_u64(vshrq_n_u64(vtstq_s64(a, b), 63));
}

#if _LEAVE_ARM_TSTS
}
#endif


#if _ENTER_ARM_CEQS
{
#endif

INLINE(ptrdiff_t, ADDR_CEQS)
(
    void volatile const *a,
    void volatile const *b
)
{
    return (a == b) ? (NULL-((void *) INTPTR_C(1))) : 0;
}

INLINE(  _Bool,   BOOL_CEQS)   (_Bool a,   _Bool b) {return a==b;}
INLINE(  uchar,  UCHAR_CEQS)   (uchar a,   uchar b)
{
    return  (a == b) ? UCHAR_MAX : 0;
}

INLINE(  schar,  SCHAR_CEQS)   (schar a,   schar b) {return  0-(a==b);}

INLINE(   char,   CHAR_CEQS)    (char a,    char b)
{
    return  (a == b) ? '\xff' : '\x00';
}

INLINE( ushort,  USHRT_CEQS)  (ushort a,  ushort b)
{
    return  (a == b) ? USHRT_MAX : 0;
}

INLINE(  short,   SHRT_CEQS)   (short a,   short b) {return  0-(a==b);}

INLINE(   uint,   UINT_CEQS)    (uint a,    uint b)
{
    return  (a == b) ? UINT_MAX : 0u;
}

INLINE(    int,    INT_CEQS)     (int a,     int b) {return  0-(a==b);}

INLINE(  ulong,  ULONG_CEQS)   (ulong a,   ulong b)
{
    return  (a==b) ? ULONG_MAX : 0ul;
}

INLINE(   long,   LONG_CEQS)    (long a,    long b) {return  0l-(a==b);}

INLINE( ullong, ULLONG_CEQS)  (ullong a,  ullong b)
{
    return  (a==b) ? ULLONG_MAX : 0ull;
}

INLINE(  llong,  LLONG_CEQS)   (llong a,   llong b) {return  0ll-(a==b);}

INLINE(int16_t,  FLT16_CEQS) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vceqh_f16(a, b);
#else
    return  0-(a==b);
#endif
}

INLINE(int32_t,    FLT_CEQS)   (float a,   float b) {return vceqs_f32(a, b);}
INLINE(int64_t,    DBL_CEQS)  (double a,  double b) {return vceqd_f64(a, b);}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,ceqsqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (QUAD_UTYPE) 0-(a==b);
}

INLINE(QUAD_ITYPE,ceqsqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (QUAD_ITYPE) 0-(a==b);
}

INLINE(QUAD_ITYPE,ceqsqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (QUAD_ITYPE) 0-(a==b);
}

#endif

#define MY_CEQD(U, S, F, A, B)  \
vreinterpret_##S(               \
    vceq_##F(                   \
        vreinterpret_##U(A),    \
        vreinterpret_##U(B)     \
    )                           \
)

#define MY_CEQQ(U, S, F, A, B)  \
vreinterpretq_##S(              \
    vceqq_##F(                  \
        vreinterpretq_##U(A),   \
        vreinterpretq_##U(B)    \
    )                           \
)

#define     DYU_CEQS            vand_u64
#define     DBU_CEQS            vceq_u8
#define     DBI_CEQS(A, B)      MY_CEQD(s8_u8,u8_s8,u8,A,B)
#if CHAR_MIN
#   define  DBC_CEQS            DBI_CEQS
#else
#   define  DBC_CEQS            DBU_CEQS
#endif

#define     DHU_CEQS            vceq_u16
#define     DHI_CEQS(A, B)      MY_CEQD(s16_u16,u16_s16,u16,A,B)
INLINE(int16x4_t,DHF_CEQS) (float16x4_t a, float16x4_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vreinterpret_s16_u16(vceq_f16(a, b));
#else
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(a);
    uint32x4_t  m = vceqq_f32(l, r);
    return vreinterpret_s16_u16(vmovn_u32(m));
#endif
}

#define     DWU_CEQS            vceq_u32
#define     DWI_CEQS(A, B)      MY_CEQD(s32_u32,u32_s32,u32,A,B)
#define     DWF_CEQS(A, B)      MY_CEQD(f32_u32,u32_s32,u32,A,B)
#define     DDU_CEQS            vceq_u64
#define     DDI_CEQS(A, B)      MY_CEQD(s64_u64,u64_s64,u64,A,B)
#define     DDF_CEQS(A, B)      MY_CEQD(f64_u64,u64_s64,u64,A,B)

#define     QYU_CEQS            vandq_u64
#define     QBU_CEQS            vceqq_u8
#if CHAR_MIN
#   define  QBC_CEQS            QBI_CEQS
#else
#   define  QBC_CEQS            QBU_CEQS
#endif

#define     QBI_CEQS(A, B)      MY_CEQQ(s8_u8,u8_s8,u8,A,B)
#define     QHU_CEQS            vceqq_u16
#define     QHI_CEQS(A, B)      MY_CEQQ(s16_u16,u16_s16,u16,A,B)
INLINE(int16x8_t,QHF_CEQS) (float16x8_t a, float16x8_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vreinterpretq_s16_u16(vceqq_f16(a, b));
#else
    float32x4_t al = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t ar = vcvt_f32_f16(vget_high_f16(a));
    float32x4_t bl = vcvt_f32_f16(vget_low_f16(b));
    float32x4_t br = vcvt_f32_f16(vget_high_f16(b));
    uint32x4_t  cl = vceqq_f32(al, bl);
    uint32x4_t  cr = vceqq_f32(ar, br);
    return vreinterpretq_s16_u16(
        vcombine_u16(
            vmovn_u32(cl),
            vmovn_u32(cr)
        )
    );
#endif
}

#define     QWU_CEQS            vceqq_u32
#define     QWI_CEQS(A, B)      MY_CEQQ(s32_u32,u32_s32,u32,A,B)
#define     QWF_CEQS(A, B)      MY_CEQQ(f32_u32,u32_s32,u32,A,B)
#define     QDU_CEQS            vceqq_u64
#define     QDI_CEQS(A, B)      MY_CEQQ(s64_u64,u64_s64,u64,A,B)
#define     QDF_CEQS(A, B)      MY_CEQQ(f64_u64,u64_s64,u64,A,B)

INLINE(Vwyu,VWYU_CEQS) (Vwyu a, Vwyu b)
{
    return  WYU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(
                vand_u32(
                    vreinterpret_u32_f32(vdup_n_f32(VWYU_ASTM(a))),
                    vreinterpret_u32_f32(vdup_n_f32(VWYU_ASTM(b)))
                )
            ),
            V2_K0
        )
    );
}

INLINE(Vwbu,VWBU_CEQS) (Vwbu a, Vwbu b)
{
    float32x2_t l = vset_lane_f32(VWBU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBU_ASTM(a), r, V2_K0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CEQS(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbi,VWBI_CEQS) (Vwbi a, Vwbi b)
{
    float32x2_t l = vset_lane_f32(VWBI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBI_ASTM(a), r, V2_K0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CEQS(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbc,VWBC_CEQS) (Vwbc a, Vwbc b)
{
    float32x2_t l = vset_lane_f32(VWBC_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBC_ASTM(a), r, V2_K0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return  WBC_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CEQS(p, q)),
            V2_K0
        )
    );
}


INLINE(Vwhu,VWHU_CEQS) (Vwhu a, Vwhu b)
{
    float32x2_t l = vset_lane_f32(VWHU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHU_ASTM(a), r, V2_K0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(DHU_CEQS(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwhi,VWHI_CEQS) (Vwhi a, Vwhi b)
{
    float32x2_t l = vset_lane_f32(VWHI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHI_ASTM(a), r, V2_K0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(DHU_CEQS(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwhi,VWHF_CEQS) (Vwhf a, Vwhf b)
{
    float32x2_t l = vset_lane_f32(VWHF_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHF_ASTM(a), r, V2_K0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(DHU_CEQS(p, q)),
            V2_K0
        )
    );
}


INLINE(Vwwu,VWWU_CEQS) (Vwwu a, Vwwu b)
{
    float32x2_t l = vset_lane_f32(VWWU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWU_ASTM(a), r, V2_K0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(DWU_CEQS(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWI_CEQS) (Vwwi a, Vwwi b)
{
    float32x2_t l = vset_lane_f32(VWWI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWI_ASTM(a), r, V2_K0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(DWU_CEQS(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWF_CEQS) (Vwwf a, Vwwf b)
{
    float32x2_t l = vset_lane_f32(VWWF_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWF_ASTM(a), r, V2_K0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(DWU_CEQS(p, q)),
            V2_K0
        )
    );
}


INLINE(Vdbu,VDBU_CEQS) (Vdbu a, Vdbu b) {return DBU_CEQS(a, b);}
INLINE(Vdbi,VDBI_CEQS) (Vdbi a, Vdbi b) {return DBI_CEQS(a, b);}
INLINE(Vdbc,VDBC_CEQS) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(DBC_CEQS(VDBC_ASTM(b), VDBC_ASTM(b)));
}

INLINE(Vdhu,VDHU_CEQS) (Vdhu a, Vdhu b) {return DHU_CEQS(a, b);}
INLINE(Vdhi,VDHI_CEQS) (Vdhi a, Vdhi b) {return DHI_CEQS(a, b);}
INLINE(Vdhi,VDHF_CEQS) (Vdhf a, Vdhf b) {return DHF_CEQS(a, b);}
INLINE(Vdwu,VDWU_CEQS) (Vdwu a, Vdwu b) {return DWU_CEQS(a, b);}
INLINE(Vdwi,VDWI_CEQS) (Vdwi a, Vdwi b) {return DWI_CEQS(a, b);}
INLINE(Vdwi,VDWF_CEQS) (Vdwf a, Vdwf b) {return DWF_CEQS(a, b);}
INLINE(Vddu,VDDU_CEQS) (Vddu a, Vddu b) {return DDU_CEQS(a, b);}
INLINE(Vddi,VDDI_CEQS) (Vddi a, Vddi b) {return DDI_CEQS(a, b);}
INLINE(Vddi,VDDF_CEQS) (Vddf a, Vddf b) {return DDF_CEQS(a, b);}


INLINE(Vqbu,VQBU_CEQS) (Vqbu a, Vqbu b) {return QBU_CEQS(a, b);}
INLINE(Vqbi,VQBI_CEQS) (Vqbi a, Vqbi b) {return QBI_CEQS(a, b);}
INLINE(Vqbc,VQBC_CEQS) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CEQS(VQBC_ASTM(b), VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CEQS) (Vqhu a, Vqhu b) {return QHU_CEQS(a, b);}
INLINE(Vqhi,VQHI_CEQS) (Vqhi a, Vqhi b) {return QHI_CEQS(a, b);}
INLINE(Vqhi,VQHF_CEQS) (Vqhf a, Vqhf b) {return QHF_CEQS(a, b);}
INLINE(Vqwu,VQWU_CEQS) (Vqwu a, Vqwu b) {return QWU_CEQS(a, b);}
INLINE(Vqwi,VQWI_CEQS) (Vqwi a, Vqwi b) {return QWI_CEQS(a, b);}
INLINE(Vqwi,VQWF_CEQS) (Vqwf a, Vqwf b) {return QWF_CEQS(a, b);}
INLINE(Vqdu,VQDU_CEQS) (Vqdu a, Vqdu b) {return QDU_CEQS(a, b);}
INLINE(Vqdi,VQDI_CEQS) (Vqdi a, Vqdi b) {return QDI_CEQS(a, b);}
INLINE(Vqdi,VQDF_CEQS) (Vqdf a, Vqdf b) {return QDF_CEQS(a, b);}

#if _LEAVE_ARM_CEQS
}
#endif

#if _ENTER_ARM_CEQY
{
#endif

INLINE(ptrdiff_t, ADDR_CEQY) (void volatile const *a, void volatile const *b)
{
    return  a==b;
}

INLINE(  _Bool,  BOOL_CEQY)   (_Bool a,   _Bool b) {return a==b;}
INLINE(  uchar, UCHAR_CEQY)   (uchar a,   uchar b) {return a==b;}
INLINE(  schar, SCHAR_CEQY)   (schar a,   schar b) {return a==b;}
INLINE(   char,  CHAR_CEQY)    (char a,    char b) {return a==b;}
INLINE( ushort, USHRT_CEQY)  (ushort a,  ushort b) {return a==b;}
INLINE(  short,  SHRT_CEQY)   (short a,   short b) {return a==b;}
INLINE(   uint,  UINT_CEQY)    (uint a,    uint b) {return a==b;}
INLINE(    int,   INT_CEQY)     (int a,     int b) {return a==b;}
INLINE(  ulong, ULONG_CEQY)   (ulong a,   ulong b) {return a==b;}
INLINE(   long,  LONG_CEQY)    (long a,    long b) {return a==b;}
INLINE( ullong,ULLONG_CEQY)  (ullong a,  ullong b) {return a==b;}
INLINE(  llong, LLONG_CEQY)   (llong a,   llong b) {return a==b;}
INLINE(int16_t, FLT16_CEQY) (flt16_t a, flt16_t b) {return a==b;}
INLINE(int32_t,   FLT_CEQY)   (float a,   float b) {return a==b;}
INLINE(int64_t,   DBL_CEQY)  (double a,  double b) {return a==b;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,ceqyqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (a==b);
}

INLINE(QUAD_ITYPE,ceqyqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (a==b);
}

INLINE(QUAD_ITYPE,ceqyqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (a==b);
}

#endif

INLINE( Wbu,WBU_CEQY) (Wbu a, Wbu b)
{
#define     WBU_CEQY    WBU_CEQY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    uint8x8_t   r = vceq_u8(
        vreinterpret_u8_f32(p),
        vreinterpret_u8_f32(q)
    );
    return  vget_lane_f32(
        vreinterpret_f32_u8(vshr_n_u8(r, 7)),
        V2_K0
    );
}

INLINE( Wbu,WBI_CEQY) (Wbi a, Wbi b)
{
#define     WBI_CEQY    WBI_CEQY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vshr_n_u8(
                vceq_s8(
                    vreinterpret_s8_f32(p),
                    vreinterpret_s8_f32(q)
                ),
                7
            )
        ),
        V2_K0
    );
}

#if CHAR_MIN
#   define  WBC_CEQY    WBI_CEQY
#else
#   define  WBC_CEQY    WBU_CEQY
#endif

INLINE( Whu,WHU_CEQY) (Whu a, Whu b)
{
#define     WHU_CEQY    WHU_CEQY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vceq_u16(
                    vreinterpret_u16_f32(p),
                    vreinterpret_u16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
}

INLINE( Whu,WHI_CEQY) (Whi a, Whi b)
{
#define     WHI_CEQY    WHI_CEQY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vceq_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
}

INLINE( Whu,WHF_CEQY) (Whf a, Whf b)
{
#define     WHF_CEQY    WHF_CEQY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vceq_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
#else
    float32x4_t l = vcvt_f32_f16(vreinterpret_f16_f32(p));
    float32x4_t r = vcvt_f32_f16(vreinterpret_f16_f32(q));
    uint16x4_t  v = vshr_n_u16(vmovn_u32(vceqq_f32(l, r)), 15);
    return  vget_lane_f32(vreinterpret_f32_u16(v), V2_K0);
#endif
}

INLINE( Wwu,WWU_CEQY) (Wwu a, Wwu b)
{
#define     WWU_CEQY    WWU_CEQY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vceq_u32(
                    vreinterpret_u32_f32(p),
                    vreinterpret_u32_f32(q)
                ),
                31
            )
        ),
        V2_K0
    );
}

INLINE( Wwu,WWI_CEQY) (Wwi a, Wwi b)
{
#define     WWI_CEQY    WWI_CEQY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vceq_s32(
                    vreinterpret_s32_f32(p),
                    vreinterpret_s32_f32(q)
                ),
                31
            )
        ),
        V2_K0
    );
}

INLINE( Wwu,WWF_CEQY) (Wwf a, Wwf b)
{
#define     WWF_CEQY    WWF_CEQY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(vshr_n_u32(vceq_f32(p, q), 31)),
        V2_K0
    );
}

#define     DBU_CEQY(A, B)            vshr_n_u8(vceq_u8(A,B),7)
#define     DBI_CEQY(A, B)  VDBU_ASTI(vshr_n_u8(vceq_s8(A,B),7))
#if CHAR_MIN
#   define  DBC_CEQY        DBI_CEQY
#else
#   define  DBC_CEQY        DBU_CEQY
#endif

#define     DHU_CEQY(A, B)            vshr_n_u16(vceq_u16(A,B),15)
#define     DHI_CEQY(A, B)  VDHU_ASTI(vshr_n_u16(vceq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CEQY(A, B)  VDHU_ASTI(vshr_n_u16(vceq_f16(A,B),15))
#else
#   define  DHF_CEQY(A, B)      \
VDHU_ASHI(                      \
    vshr_n_u16(                 \
        vmovn_u32(              \
            vceqq_f32(          \
                vcvt_f32_f16(a),\
                vcvt_f32_f16(b) \
            )                   \
        ),                      \
        15                      \
    )                           \
)
#endif

#define     DWU_CEQY(A, B)            vshr_n_u32(vceq_u32(A,B),31)
#define     DWI_CEQY(A, B)  VDWU_ASTI(vshr_n_u32(vceq_s32(A,B),31))
#define     DWF_CEQY(A, B)  VDWU_ASTI(vshr_n_u32(vceq_f32(A,B),31))

#define     DDU_CEQY(A, B)            vshr_n_u64(vceq_u64(A,B),63)
#define     DDI_CEQY(A, B)  VDDU_ASTI(vshr_n_u64(vceq_s64(A,B),63))
#define     DDF_CEQY(A, B)  VDDU_ASTI(vshr_n_u64(vceq_f64(A,B),63))


#define     QBU_CEQY(A, B)            vshrq_n_u8(vceqq_u8(A,B),7)
#define     QBI_CEQY(A, B)  VQBU_ASTI(vshrq_n_u8(vceqq_s8(A,B),7))
#if CHAR_MIN
#   define  QBC_CEQY        QBI_CEQY
#else
#   define  QBC_CEQY        QBU_CEQY
#endif

#define     QHU_CEQY(A, B)            vshrq_n_u16(vceqq_u16(A,B),15)
#define     QHI_CEQY(A, B)  VQHU_ASTI(vshrq_n_u16(vceqq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CEQY(A, B)  VQHU_ASTI(vshrq_n_u16(vceqq_f16(A,B),15))
#else
#   define  QHF_CEQY(A, B)                          \
vreinterpretq_s16_u16(                              \
    vshrq_n_u16(                                    \
        vcombine_u16(                               \
            vmovn_u32(                              \
                vceqq_f32(                          \
                    vcvt_f32_f16(vget_low_f16(A)),  \
                    vcvt_f32_f16(vget_low_f16(B))   \
                )                                   \
            ),                                      \
            vmovn_u32(                              \
                vceqq_f32(                          \
                    vcvt_f32_f16(vget_high_f16(A)), \
                    vcvt_f32_f16(vget_high_f16(B))  \
                )                                   \
            )                                       \
        ),                                          \
        15                                          \
    )                                               \
)
#endif

#define     QWU_CEQY(A, B)            vshrq_n_u32(vceqq_u32(A,B),31)
#define     QWI_CEQY(A, B)  VQWU_ASTI(vshrq_n_u32(vceqq_s32(A,B),31))
#define     QWF_CEQY(A, B)  VQWU_ASTI(vshrq_n_u32(vceqq_f32(A,B),31))

#define     QDU_CEQY(A, B)            vshrq_n_u64(vceqq_u64(A,B),63)
#define     QDI_CEQY(A, B)  VQDU_ASTI(vshrq_n_u64(vceqq_s64(A,B),63))
#define     QDF_CEQY(A, B)  VQDU_ASTI(vshrq_n_u64(vceqq_f64(A,B),63))


INLINE(Vwbu,VWBU_CEQY) (Vwbu a, Vwbu b)
{
#define     VWBU_CEQY(A, B) WBU_ASTV(WBU_CEQY(VWBU_ASTM(A),VWBU_ASTM(B)))
    return  VWBU_CEQY(a, b);
}

INLINE(Vwbi,VWBI_CEQY) (Vwbi a, Vwbi b)
{
#define     VWBI_CEQY(A, B) WBI_ASTV(WBI_CEQY(VWBI_ASTM(A),VWBI_ASTM(B)))
    return  VWBI_CEQY(a, b);
}

INLINE(Vwbc,VWBC_CEQY) (Vwbc a, Vwbc b)
{
#define     VWBC_CEQY(A, B) WBC_ASTV(WBC_CEQY(VWBC_ASTM(A),VWBC_ASTM(B)))
    return  VWBC_CEQY(a, b);
}


INLINE(Vwhu,VWHU_CEQY) (Vwhu a, Vwhu b)
{
#define     VWHU_CEQY(A, B) WHU_ASTV(WHU_CEQY(VWHU_ASTM(A),VWHU_ASTM(B)))
    return  VWHU_CEQY(a, b);
}

INLINE(Vwhi,VWHI_CEQY) (Vwhi a, Vwhi b)
{
#define     VWHI_CEQY(A, B) WHI_ASTV(WHI_CEQY(VWHI_ASTM(A),VWHI_ASTM(B)))
    return  VWHI_CEQY(a, b);
}

INLINE(Vwhi,VWHF_CEQY) (Vwhf a, Vwhf b)
{
#define     VWHF_CEQY(A, B) WHI_ASTV(WHF_CEQY(VWHF_ASTM(A),VWHF_ASTM(B)))
    return  VWHF_CEQY(a, b);
}


INLINE(Vwwu,VWWU_CEQY) (Vwwu a, Vwwu b)
{
#define     VWWU_CEQY(A, B) WWU_ASTV(WWU_CEQY(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_CEQY(a, b);
}

INLINE(Vwwi,VWWI_CEQY) (Vwwi a, Vwwi b)
{
#define     VWWI_CEQY(A, B) WWI_ASTV(WWI_CEQY(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_CEQY(a, b);
}

INLINE(Vwwi,VWWF_CEQY) (Vwwf a, Vwwf b)
{
#define     VWWF_CEQY(A, B) WWI_ASTV(WWF_CEQY(VWWF_ASTM(A),VWWF_ASTM(B)))
    return  VWWF_CEQY(a, b);
}


INLINE(Vdbu,VDBU_CEQY) (Vdbu a, Vdbu b) {return DBU_CEQY(a, b);}
INLINE(Vdbi,VDBI_CEQY) (Vdbi a, Vdbi b) {return DBI_CEQY(a, b);}
INLINE(Vdbc,VDBC_CEQY) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(
        DBC_CEQY(
            VDBC_ASTM(a),
            VDBC_ASTM(b)
        )
    );
}

INLINE(Vdhu,VDHU_CEQY) (Vdhu a, Vdhu b) {return DHU_CEQY(a, b);}
INLINE(Vdhi,VDHI_CEQY) (Vdhi a, Vdhi b) {return DHI_CEQY(a, b);}
INLINE(Vdhi,VDHF_CEQY) (Vdhf a, Vdhf b) {return DHF_CEQY(a, b);}

INLINE(Vdwu,VDWU_CEQY) (Vdwu a, Vdwu b) {return DWU_CEQY(a, b);}
INLINE(Vdwi,VDWI_CEQY) (Vdwi a, Vdwi b) {return DWI_CEQY(a, b);}
INLINE(Vdwi,VDWF_CEQY) (Vdwf a, Vdwf b) {return DWF_CEQY(a, b);}

INLINE(Vddu,VDDU_CEQY) (Vddu a, Vddu b) {return DDU_CEQY(a, b);}
INLINE(Vddi,VDDI_CEQY) (Vddi a, Vddi b) {return DDI_CEQY(a, b);}
INLINE(Vddi,VDDF_CEQY) (Vddf a, Vddf b) {return DDF_CEQY(a, b);}


INLINE(Vqbu,VQBU_CEQY) (Vqbu a, Vqbu b) {return QBU_CEQY(a,b);}
INLINE(Vqbi,VQBI_CEQY) (Vqbi a, Vqbi b) {return QBI_CEQY(a,b);}
INLINE(Vqbc,VQBC_CEQY) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CEQY(VQBC_ASTM(a),VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CEQY) (Vqhu a, Vqhu b) {return QHU_CEQY(a,b);}
INLINE(Vqhi,VQHI_CEQY) (Vqhi a, Vqhi b) {return QHI_CEQY(a,b);}
INLINE(Vqhi,VQHF_CEQY) (Vqhf a, Vqhf b) {return QHF_CEQY(a,b);}
INLINE(Vqwu,VQWU_CEQY) (Vqwu a, Vqwu b) {return QWU_CEQY(a,b);}
INLINE(Vqwi,VQWI_CEQY) (Vqwi a, Vqwi b) {return QWI_CEQY(a,b);}
INLINE(Vqwi,VQWF_CEQY) (Vqwf a, Vqwf b) {return QWF_CEQY(a,b);}
INLINE(Vqdu,VQDU_CEQY) (Vqdu a, Vqdu b) {return QDU_CEQY(a,b);}
INLINE(Vqdi,VQDI_CEQY) (Vqdi a, Vqdi b) {return QDI_CEQY(a,b);}
INLINE(Vqdi,VQDF_CEQY) (Vqdf a, Vqdf b) {return QDF_CEQY(a,b);}

#if _LEAVE_ARM_CEQY
}
#endif

#if _ENTER_ARM_CNES
{
#endif

INLINE(ptrdiff_t, ADDR_CNES)
(
    void volatile const *a,
    void volatile const *b
)
{
    return (a != b) ? NULL-((void *) INTPTR_C(1)) : 0;
}

INLINE(  _Bool,   BOOL_CNES)   (_Bool a,   _Bool b) {return a!=b;}
INLINE(  uchar,  UCHAR_CNES)   (uchar a,   uchar b)
{
    return  (a != b) ? UCHAR_MAX : 0;
}

INLINE(  schar,  SCHAR_CNES)   (schar a,   schar b) {return  0-(a!=b);}
INLINE(   char,   CHAR_CNES)    (char a,    char b)
{
    return  (a !=b ) ? '\xff' : '\x00';
}

INLINE( ushort,  USHRT_CNES)  (ushort a,  ushort b)
{
    return  (a !=b ) ? USHRT_MAX : 0;
}

INLINE(  short,   SHRT_CNES)   (short a,   short b) {return  0-(a!=b);}

INLINE(   uint,   UINT_CNES)    (uint a,    uint b)
{
    return  (a != b) ? UINT_MAX : 0u;
}

INLINE(    int,    INT_CNES)     (int a,     int b) {return  0-(a!=b);}

INLINE(  ulong,  ULONG_CNES)   (ulong a,   ulong b)
{
    return  (a != b) ? ULONG_MAX : 0ul;
}

INLINE(   long,   LONG_CNES)    (long a,    long b) {return  0l-(a!=b);}

INLINE( ullong, ULLONG_CNES)  (ullong a,  ullong b)
{
    return  (a != b) ? ULLONG_MAX : 0ull;
}

INLINE(  llong,  LLONG_CNES)   (llong a,   llong b) {return  0ll-(a!=b);}
INLINE(int16_t,  FLT16_CNES) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return ~vceqh_f16(a, b);
#else
    return 0-(a!=b);
#endif
}

INLINE(int32_t,    FLT_CNES)   (float a,   float b) {return ~vceqs_f32(a, b);}
INLINE(int64_t,    DBL_CNES)  (double a,  double b) {return ~vceqd_f64(a, b);}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cnesqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (QUAD_UTYPE) 0-(a!=b);
}

INLINE(QUAD_ITYPE,cnesqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (QUAD_ITYPE) 0-(a!=b);
}

INLINE(QUAD_ITYPE,cnesqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (QUAD_ITYPE) 0-(a!=b);
}

#endif

#define MY_CNED(R, S, A, B)  \
vreinterpret_##R##_u8(             \
    vmvn_u8(                       \
        vceq_u8(                   \
            vreinterpret_u8_##S(A),\
            vreinterpret_u8_##S(B) \
        )                           \
    )                               \
)

#define     DBU_CNES(A, B)      vmvn_u8(vceq_u8(A,B))
#define     DBI_CNES(A, B)      MY_CNED(s8,s8,A,B)
#if CHAR_MIN
#   define  DBC_CNES(A, B)      MY_CNED(s8,u8,A,B)
#else
#   define  DBC_CNES(A, B)      vmvn_u8(vceq_u8(A,B))
#endif

#define     DHU_CNES(A, B)      vmvn_u16(vceq_u16(A,B))
#define     DHI_CNES(A, B)      MY_CNED(s16,s16,A,B)
#define     DHF_CNES(A, B)      MY_CNED(s16,f16,A,B)

#define     DWU_CNES(A, B)      vmvn_u32(vceq_u32(A,B))
#define     DWI_CNES(A, B)      MY_CNED(s32,s32,A,B)
#define     DWF_CNES(A, B)      MY_CNED(s32,f32,A,B)
#define     DDU_CNES(A, B)      MY_CNED(u64,u64,A,B)
#define     DDI_CNES(A, B)      MY_CNED(s64,s64,A,B)
#define     DDF_CNES(A, B)      MY_CNED(s64,f64,A,B)


#define MY_CNEQ(R, S, A, B)         \
vreinterpretq_##R##_u8(             \
    vmvnq_u8(                       \
        vceqq_u8(                   \
            vreinterpretq_u8_##S(A),\
            vreinterpretq_u8_##S(B) \
        )                           \
    )                               \
)
#define     QBU_CNES(A, B)      vmvnq_u8(vceqq_u8(A,B))
#define     QBI_CNES(A, B)      MY_CNEQ(s8,s8,A,B)
#if CHAR_MIN
#   define  QBC_CNES(A, B)      MY_CNEQ(s8,s8,A,B)
#else
#   define  QBC_CNES(A, B)      vmvnq_u8(vceqq_u8(A,B))
#endif

#define     QHU_CNES(A, B)      vmvnq_u16(vceqq_u8(A,B))
#define     QHI_CNES(A, B)      MY_CNEQ(s16,s16,A,B)
#define     QHF_CNES(A, B)      MY_CNEQ(s16,f16,A,B)

#define     QWU_CNES(A, B)      vmvnq_u32(vceqq_u8(A,B))
#define     QWI_CNES(A, B)      MY_CNEQ(s32,s32,A,B)
#define     QWF_CNES(A, B)      MY_CNEQ(s32,f32,A,B)
#define     QDU_CNES(A, B)      MY_CNEQ(u64,u64,A,A)
#define     QDI_CNES(A, B)      MY_CNEQ(s64,s64,A,B)
#define     QDF_CNES(A, B)      MY_CNEQ(s64,f64,A,B)


INLINE(Vwbu,VWBU_CNES) (Vwbu a, Vwbu b)
{
    float32x2_t l = vset_lane_f32(VWBU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBU_ASTM(a), r, V2_K0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CNES(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbi,VWBI_CNES) (Vwbi a, Vwbi b)
{
    float32x2_t l = vset_lane_f32(VWBI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBI_ASTM(a), r, V2_K0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CNES(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbc,VWBC_CNES) (Vwbc a, Vwbc b)
{
    float32x2_t l = vset_lane_f32(VWBC_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBC_ASTM(a), r, V2_K0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return  WBC_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CNES(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwhu,VWHU_CNES) (Vwhu a, Vwhu b)
{
    float32x2_t l = vset_lane_f32(VWHU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHU_ASTM(a), r, V2_K0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(DHU_CNES(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwhi,VWHI_CNES) (Vwhi a, Vwhi b)
{
    float32x2_t l = vset_lane_f32(VWHI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHI_ASTM(a), r, V2_K0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(DHU_CNES(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwhf,VWHF_CNES) (Vwhf a, Vwhf b)
{
    float32x2_t l = vset_lane_f32(VWHF_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHF_ASTM(a), r, V2_K0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHF_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(DHU_CNES(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwu,VWWU_CNES) (Vwwu a, Vwwu b)
{
    float32x2_t l = vset_lane_f32(VWWU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWU_ASTM(a), r, V2_K0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(DWU_CNES(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWI_CNES) (Vwwi a, Vwwi b)
{
    float32x2_t l = vset_lane_f32(VWWI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWI_ASTM(a), r, V2_K0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(DWU_CNES(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwf,VWWF_CNES) (Vwwf a, Vwwf b)
{
    float32x2_t l = vset_lane_f32(VWWF_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWF_ASTM(a), r, V2_K0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWF_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(DWU_CNES(p, q)),
            V2_K0
        )
    );
}


INLINE(Vdbu,VDBU_CNES) (Vdbu a, Vdbu b) {return DBU_CNES(a, b);}
INLINE(Vdbi,VDBI_CNES) (Vdbi a, Vdbi b) {return DBI_CNES(a, b);}
INLINE(Vdbc,VDBC_CNES) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(DBC_CNES(VDBC_ASTM(b), VDBC_ASTM(b)));
}

INLINE(Vdhu,VDHU_CNES) (Vdhu a, Vdhu b) {return DHU_CNES(a, b);}
INLINE(Vdhi,VDHI_CNES) (Vdhi a, Vdhi b) {return DHI_CNES(a, b);}
INLINE(Vdhi,VDHF_CNES) (Vdhf a, Vdhf b) {return DHF_CNES(a, b);}
INLINE(Vdwu,VDWU_CNES) (Vdwu a, Vdwu b) {return DWU_CNES(a, b);}
INLINE(Vdwi,VDWI_CNES) (Vdwi a, Vdwi b) {return DWI_CNES(a, b);}
INLINE(Vdwi,VDWF_CNES) (Vdwf a, Vdwf b) {return DWF_CNES(a, b);}
INLINE(Vddu,VDDU_CNES) (Vddu a, Vddu b) {return DDU_CNES(a, b);}
INLINE(Vddi,VDDI_CNES) (Vddi a, Vddi b) {return DDI_CNES(a, b);}
INLINE(Vddi,VDDF_CNES) (Vddf a, Vddf b) {return DDF_CNES(a, b);}


INLINE(Vqbu,VQBU_CNES) (Vqbu a, Vqbu b) {return QBU_CNES(a, b);}
INLINE(Vqbi,VQBI_CNES) (Vqbi a, Vqbi b) {return QBI_CNES(a, b);}
INLINE(Vqbc,VQBC_CNES) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CNES(VQBC_ASTM(b), VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CNES) (Vqhu a, Vqhu b) {return QHU_CNES(a, b);}
INLINE(Vqhi,VQHI_CNES) (Vqhi a, Vqhi b) {return QHI_CNES(a, b);}
INLINE(Vqhi,VQHF_CNES) (Vqhf a, Vqhf b) {return QHF_CNES(a, b);}
INLINE(Vqwu,VQWU_CNES) (Vqwu a, Vqwu b) {return QWU_CNES(a, b);}
INLINE(Vqwi,VQWI_CNES) (Vqwi a, Vqwi b) {return QWI_CNES(a, b);}
INLINE(Vqwi,VQWF_CNES) (Vqwf a, Vqwf b) {return QWF_CNES(a, b);}
INLINE(Vqdu,VQDU_CNES) (Vqdu a, Vqdu b) {return QDU_CNES(a, b);}
INLINE(Vqdi,VQDI_CNES) (Vqdi a, Vqdi b) {return QDI_CNES(a, b);}
INLINE(Vqdi,VQDF_CNES) (Vqdf a, Vqdf b) {return QDF_CNES(a, b);}


#if _LEAVE_ARM_CNES
}
#endif

#if _ENTER_ARM_CNEY
{
#endif

INLINE(ptrdiff_t,ADDR_CNEY) (void volatile const *a, void volatile const *b)
{
    return  a!=b;
}

INLINE(  _Bool,  BOOL_CNEY)   (_Bool a,   _Bool b) {return a!=b;}
INLINE(  uchar, UCHAR_CNEY)   (uchar a,   uchar b) {return a!=b;}
INLINE(  schar, SCHAR_CNEY)   (schar a,   schar b) {return a!=b;}
INLINE(   char,  CHAR_CNEY)    (char a,    char b) {return a!=b;}
INLINE( ushort, USHRT_CNEY)  (ushort a,  ushort b) {return a!=b;}
INLINE(  short,  SHRT_CNEY)   (short a,   short b) {return a!=b;}
INLINE(   uint,  UINT_CNEY)    (uint a,    uint b) {return a!=b;}
INLINE(    int,   INT_CNEY)     (int a,     int b) {return a!=b;}
INLINE(  ulong, ULONG_CNEY)   (ulong a,   ulong b) {return a!=b;}
INLINE(   long,  LONG_CNEY)    (long a,    long b) {return a!=b;}
INLINE( ullong,ULLONG_CNEY)  (ullong a,  ullong b) {return a!=b;}
INLINE(  llong, LLONG_CNEY)   (llong a,   llong b) {return a!=b;}
INLINE(int16_t, FLT16_CNEY) (flt16_t a, flt16_t b) {return a!=b;}
INLINE(int32_t,   FLT_CNEY)   (float a,   float b) {return a!=b;}
INLINE(int64_t,   DBL_CNEY)  (double a,  double b) {return a!=b;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cneyqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (a!=b);
}

INLINE(QUAD_ITYPE,cneyqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (a!=b);
}

INLINE(QUAD_ITYPE,cneyqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (a!=b);
}

#endif

INLINE(float,WBU_CNEY) (float a, float b)
{
    float32x2_t p = vdup_n_f32(a);
    float32x2_t q = vdup_n_f32(b);
    uint8x8_t   r = vceq_u8(
        vreinterpret_u8_f32(p),
        vreinterpret_u8_f32(q)
    );
    r = vmvn_u8(r);
    r = vshr_n_u8(r, 7);
    p = vreinterpret_f32_u8(r);
    return  vget_lane_f32(p, V2_K0);
}

INLINE(float,WBI_CNEY) (float a, float b)
{
    float32x2_t p = vdup_n_f32(a);
    float32x2_t q = vdup_n_f32(b);
    uint8x8_t   r = vceq_s8(
        vreinterpret_s8_f32(p),
        vreinterpret_s8_f32(q)
    );
    r = vmvn_u8(r);
    r = vshr_n_u8(r, 7);
    p = vreinterpret_f32_u8(r);
    return  vget_lane_f32(p, V2_K0);
}

#if CHAR_MIN
#   define  WBC_CNEY WBI_CNEY
#else
#   define  WBC_CNEY WBU_CNEY
#endif

INLINE(float,WHU_CNEY) (float a, float b)
{
    float32x2_t p = vdup_n_f32(a);
    float32x2_t q = vdup_n_f32(b);
    uint16x4_t  r = vceq_u16(
        vreinterpret_u16_f32(p),
        vreinterpret_u16_f32(q)
    );
    r = vmvn_u16(r);
    r = vshr_n_u16(r, 15);
    p = vreinterpret_f32_u16(r);
    return  vget_lane_f32(p, V2_K0);
}

INLINE(float,WHI_CNEY) (float a, float b)
{
    float32x2_t p = vdup_n_f32(a);
    float32x2_t q = vdup_n_f32(b);
    uint16x4_t  r = vceq_s16(
        vreinterpret_s16_f32(p),
        vreinterpret_s16_f32(q)
    );
    r = vmvn_u16(r);
    r = vshr_n_u16(r, 15);
    p = vreinterpret_f32_u16(r);
    return  vget_lane_f32(p, V2_K0);
}

INLINE(float,WHF_CNEY) (float a, float b)
{
    float32x2_t p = vdup_n_f32(a);
    float32x2_t q = vdup_n_f32(b);
    uint16x4_t  m;
#if defined(SPC_ARM_FP16_SIMD)
    m = vceq_f16(
        vreinterpret_f16_f32(p),
        vreinterpret_f16_f32(q)
    );
#else
    float32x4_t l = vcvt_f32_f16(vreinterpret_f16_f32(p));
    float32x4_t r = vcvt_f32_f16(vreinterpret_f16_f32(p));
    m = vmovn_u32(vceqq_f32(l, r));
#endif
    m = vmvn_u16(m);
    m = vshr_n_u16(m, 15);
    p = vreinterpret_f32_u16(m);
    return  vget_lane_f32(p, V2_K0);
}


INLINE(float,WWU_CNEY) (float a, float b)
{
    float32x2_t p = vdup_n_f32(a);
    float32x2_t q = vdup_n_f32(b);
    uint32x2_t  r = vceq_u32(
        vreinterpret_u32_f32(p),
        vreinterpret_u32_f32(q)
    );
    r = vmvn_u32(r);
    r = vshr_n_u32(r, 31);
    p = vreinterpret_f32_u32(r);
    return  vget_lane_f32(p, V2_K0);
}

INLINE(float,WWI_CNEY) (float a, float b)
{
    float32x2_t p = vdup_n_f32(a);
    float32x2_t q = vdup_n_f32(b);
    uint32x2_t  r = vceq_s32(
        vreinterpret_s32_f32(p),
        vreinterpret_s32_f32(q)
    );
    r = vmvn_u32(r);
    r = vshr_n_u32(r, 31);
    p = vreinterpret_f32_u32(r);
    return  vget_lane_f32(p, V2_K0);
}

INLINE(float,WWF_CNEY) (float a, float b)
{
    return  INT_ASWF((a!=b));
}

#define     DBU_CNEY(A, B)            vshr_n_u8(vmvn_u8(vceq_u8(A,B)),7)
#define     DBI_CNEY(A, B)  VDBU_ASTI(vshr_n_u8(vmvn_u8(vceq_s8(A,B)),7))
#if CHAR_MIN
#   define  DBC_CNEY        DBI_CNEY
#else
#   define  DBC_CNEY        DBU_CNEY
#endif

#define     DHU_CNEY(A, B)            vshr_n_u16(vmvn_u16(vceq_u16(A,B)),15)
#define     DHI_CNEY(A, B)  VDHU_ASTI(vshr_n_u16(vmvn_u16(vceq_s16(A,B)),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CNEY(A, B)  \
VDHU_ASTI(vshr_n_u16(vmvn_u16(vceqz_f16(vsub_f16(A,B))),15))
#else

INLINE(int16x4_t,DHF_CNEY) (float16x4_t a, float16x4_t b)
{
/*  TODO: figure out the following macro doesn't work
    since the error messages are completely nonsensical

#define    MY_DHF_CNEY(A, B)        \
vreinterpret_s16_u16(               \
    vshr_n_u16(                     \
        vmovn_u32(                  \
            vmvnq_u32(              \
                vceqq_f32(          \
                    vcvt_f32_f16(a),\
                    vcvt_f32_f16(b) \
                )                   \
            )                       \
        ),                          \
        15                          \
    )                               \
)

*/
    float32x4_t aq = vcvt_f32_f16(a);
    float32x4_t bq = vcvt_f32_f16(b);
    aq = vsubq_f32(aq, bq);
    uint32x4_t  yq = vceqzq_f32(aq);
    yq = vmvnq_u32(yq);
    yq = vshrq_n_u32(yq, 31);
    return vreinterpret_s16_u16(vmovn_u32(yq));
}

#endif

#define     DWU_CNEY(A, B)            vshr_n_u32(vmvn_u32(vceq_u32(A,B)),31)
#define     DWI_CNEY(A, B)  VDWU_ASWI(vshr_n_u32(vmvn_u32(vceq_s32(A,B)),31))
#define     DWF_CNEY(A, B)  VDWU_ASWI(vshr_n_u32(vmvn_u32(vceqz_f32((A-B))),31))

#define     DDU_CNEY(A, B)          \
vshr_n_u64(                         \
    vreinterpret_u64_u32(           \
        vmvn_u32(                   \
            vreinterpret_u32_u64(   \
                vceq_u64(A,B)       \
            )                       \
        )                           \
    ),                              \
    63                              \
)

#define     DDI_CNEY(A, B)              \
vreinterpret_s64_u64(                   \
    vshr_n_u64(                         \
        vreinterpret_u64_u32(           \
            vmvn_u32(                   \
                vreinterpret_u32_u64(   \
                    vceq_s64(A,B)       \
                )                       \
            )                           \
        ),                              \
        63                              \
    )                                   \
)

#define     DDF_CNEY(A, B)              \
vreinterpret_s64_u64(                   \
    vshr_n_u64(                         \
        vreinterpret_u64_u32(           \
            vmvn_u32(                   \
                vreinterpret_u32_u64(   \
                    vceqz_f64((A-B))    \
                )                       \
            )                           \
        ),                              \
        63                              \
    )                                   \
)

#define     QBU_CNEY(A, B)            vshrq_n_u8(vmvnq_u8(vceqq_u8(A,B)),7)
#define     QBI_CNEY(A, B)  VQBU_ASTI(vshrq_n_u8(vmvnq_u8(vceqq_s8(A,B)),7))
#if CHAR_MIN
#   define  QBC_CNEY        QBI_CNEY
#else
#   define  QBC_CNEY        QBU_CNEY
#endif

#define     QHU_CNEY(A, B)            vshrq_n_u16(vmvnq_u16(vceqq_u16(A,B)),15)
#define     QHI_CNEY(A, B)  VQHU_ASTI(vshrq_n_u16(vmvnq_u16(vceqq_s16(A,B)),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CNEY(A, B)  VQHU_ASTI(vshrq_n_u16(vmvnq_u16(vceqq_f16(A,B)),15))
#else
INLINE(int16x8_t,QHF_CNEY) (float16x8_t a, float16x8_t b)
{
    int16x4_t l = DHF_CNEY(
        vget_low_f16(a),
        vget_low_f16(b)
    );
    int16x4_t r = DHF_CNEY(
        vget_high_f16(a),
        vget_high_f16(b)
    );
    return vcombine_s16(l, r);
}

#endif

#define     QWU_CNEY(A, B)            vshrq_n_u32(vmvnq_u32(vceqq_u32(A,B)),31)
#define     QWI_CNEY(A, B)  VQWU_ASWI(vshrq_n_u32(vmvnq_u32(vceqq_s32(A,B)),31))
#define     QWF_CNEY(A, B)  VQWU_ASWI(vshrq_n_u32(vmvnq_u32(vceqq_f32(A,B)),31))
#define     QDU_CNEY(A, B)          \
vshrq_n_u64(                        \
    vreinterpretq_u64_u32(          \
        vmvnq_u32(                  \
            vreinterpretq_u32_u64(  \
                vceqq_u64(A,B)      \
            )                       \
        )                           \
    ),                              \
    63                              \
)

#define     QDI_CNEY(A, B)              \
vreinterpretq_s64_u64(                  \
    vshrq_n_u64(                        \
        vreinterpretq_u64_u32(          \
            vmvnq_u32(                  \
                vreinterpretq_u32_u64(  \
                    vceqq_s64(A,B)      \
                )                       \
            )                           \
        ),                              \
        63                              \
    )                                   \
)

#define     QDF_CNEY(A, B)              \
vreinterpretq_s64_u64(                  \
    vshrq_n_u64(                        \
        vreinterpretq_u64_u32(          \
            vmvnq_u32(                  \
                vreinterpretq_u32_u64(  \
                    vceqq_f64(A,B)      \
                )                       \
            )                           \
        ),                              \
        63                              \
    )                                   \
)


INLINE(Vwbu,VWBU_CNEY) (Vwbu a, Vwbu b)
{
#define     VWBU_CNEY(A, B) WBU_ASTV(WBU_CNEY(VWBU_ASTM(A),VWBU_ASTM(B)))
    return  VWBU_CNEY(a, b);
}

INLINE(Vwbi,VWBI_CNEY) (Vwbi a, Vwbi b)
{
#define     VWBI_CNEY(A, B) WBI_ASTV(WBI_CNEY(VWBI_ASTM(A),VWBI_ASTM(B)))
    return  VWBI_CNEY(a, b);
}

INLINE(Vwbc,VWBC_CNEY) (Vwbc a, Vwbc b)
{
#define     VWBC_CNEY(A, B) WBC_ASTV(WBC_CNEY(VWBC_ASTM(A),VWBC_ASTM(B)))
    return  VWBC_CNEY(a, b);
}


INLINE(Vwhu,VWHU_CNEY) (Vwhu a, Vwhu b)
{
#define     VWHU_CNEY(A, B) WHU_ASTV(WHU_CNEY(VWHU_ASTM(A),VWHU_ASTM(B)))
    return  VWHU_CNEY(a, b);
}

INLINE(Vwhi,VWHI_CNEY) (Vwhi a, Vwhi b)
{
#define     VWHI_CNEY(A, B) WHI_ASTV(WHI_CNEY(VWHI_ASTM(A),VWHI_ASTM(B)))
    return  VWHI_CNEY(a, b);
}

INLINE(Vwhi,VWHF_CNEY) (Vwhf a, Vwhf b)
{
#define     VWHF_CNEY(A, B) WHI_ASTV(WHF_CNEY(VWHF_ASTM(A),VWHF_ASTM(B)))
    return  VWHF_CNEY(a, b);
}


INLINE(Vwwu,VWWU_CNEY) (Vwwu a, Vwwu b)
{
#define     VWWU_CNEY(A, B) WWU_ASTV(WWU_CNEY(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_CNEY(a, b);
}

INLINE(Vwwi,VWWI_CNEY) (Vwwi a, Vwwi b)
{
#define     VWWI_CNEY(A, B) WWI_ASTV(WWI_CNEY(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_CNEY(a, b);
}

INLINE(Vwwi,VWWF_CNEY) (Vwwf a, Vwwf b)
{
#define     VWWF_CNEY(A, B) WWI_ASTV(WWF_CNEY(VWWF_ASTM(A),VWWF_ASTM(B)))
    return  VWWF_CNEY(a, b);
}



INLINE(Vdbu,VDBU_CNEY) (Vdbu a, Vdbu b) {return DBU_CNEY(a, b);}
INLINE(Vdbi,VDBI_CNEY) (Vdbi a, Vdbi b) {return DBI_CNEY(a, b);}
INLINE(Vdbc,VDBC_CNEY) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(
        DBC_CNEY(
            VDBC_ASTM(a),
            VDBC_ASTM(b)
        )
    );
}

INLINE(Vdhu,VDHU_CNEY) (Vdhu a, Vdhu b) {return DHU_CNEY(a, b);}
INLINE(Vdhi,VDHI_CNEY) (Vdhi a, Vdhi b) {return DHI_CNEY(a, b);}
INLINE(Vdhi,VDHF_CNEY) (Vdhf a, Vdhf b) {return DHF_CNEY(a, b);}

INLINE(Vdwu,VDWU_CNEY) (Vdwu a, Vdwu b) {return DWU_CNEY(a, b);}
INLINE(Vdwi,VDWI_CNEY) (Vdwi a, Vdwi b) {return DWI_CNEY(a, b);}
INLINE(Vdwi,VDWF_CNEY) (Vdwf a, Vdwf b) {return DWF_CNEY(a, b);}

INLINE(Vddu,VDDU_CNEY) (Vddu a, Vddu b) {return DDU_CNEY(a, b);}
INLINE(Vddi,VDDI_CNEY) (Vddi a, Vddi b) {return DDI_CNEY(a, b);}
INLINE(Vddi,VDDF_CNEY) (Vddf a, Vddf b) {return DDF_CNEY(a, b);}


INLINE(Vqbu,VQBU_CNEY) (Vqbu a, Vqbu b) {return QBU_CNEY(a,b);}
INLINE(Vqbi,VQBI_CNEY) (Vqbi a, Vqbi b) {return QBI_CNEY(a,b);}
INLINE(Vqbc,VQBC_CNEY) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CNEY(VQBC_ASTM(a),VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CNEY) (Vqhu a, Vqhu b) {return QHU_CNEY(a,b);}
INLINE(Vqhi,VQHI_CNEY) (Vqhi a, Vqhi b) {return QHI_CNEY(a,b);}
INLINE(Vqhi,VQHF_CNEY) (Vqhf a, Vqhf b) {return QHF_CNEY(a,b);}

INLINE(Vqwu,VQWU_CNEY) (Vqwu a, Vqwu b) {return QWU_CNEY(a,b);}
INLINE(Vqwi,VQWI_CNEY) (Vqwi a, Vqwi b) {return QWI_CNEY(a,b);}
INLINE(Vqwi,VQWF_CNEY) (Vqwf a, Vqwf b) {return QWF_CNEY(a,b);}
INLINE(Vqdu,VQDU_CNEY) (Vqdu a, Vqdu b) {return QDU_CNEY(a,b);}
INLINE(Vqdi,VQDI_CNEY) (Vqdi a, Vqdi b) {return QDI_CNEY(a,b);}
INLINE(Vqdi,VQDF_CNEY) (Vqdf a, Vqdf b) {return QDF_CNEY(a,b);}

#if _LEAVE_ARM_CNEY
}
#endif


#if _ENTER_ARM_CLTS
{
#endif

INLINE(ptrdiff_t, ADDR_CLTS)
(
    void volatile const *a,
    void volatile const *b
)
{
    return (a != b) ? NULL-((void *) INTPTR_C(1)) : 0;
}

INLINE(  _Bool,   BOOL_CLTS)   (_Bool a,   _Bool b) {return a<b;}
INLINE(  uchar,  UCHAR_CLTS)   (uchar a,   uchar b)
{
    return  (a<b) ? UCHAR_MAX : 0;
}

INLINE(  schar,  SCHAR_CLTS)   (schar a,   schar b) {return  0-(a<b);}
INLINE(   char,   CHAR_CLTS)    (char a,    char b)
{
    return  (a<b) ? '\xff' : '\x00';
}

INLINE( ushort,  USHRT_CLTS)  (ushort a,  ushort b)
{
    return  (a<b) ? USHRT_MAX : 0;
}

INLINE(  short,   SHRT_CLTS)   (short a,   short b) {return  0-(a<b);}

INLINE(   uint,   UINT_CLTS)    (uint a,    uint b)
{
    return  (a<b) ? UINT_MAX : 0u;
}

INLINE(    int,    INT_CLTS)     (int a,     int b) {return  0-(a<b);}

INLINE(  ulong,  ULONG_CLTS)   (ulong a,   ulong b)
{
    return  (a<b) ? ULONG_MAX : 0ul;
}

INLINE(   long,   LONG_CLTS)    (long a,    long b) {return  0l-(a<b);}

INLINE( ullong, ULLONG_CLTS)  (ullong a,  ullong b)
{
    return  (a<b) ? ULLONG_MAX : 0ull;
}

INLINE(  llong,  LLONG_CLTS)   (llong a,   llong b) {return  0ll-(a<b);}
INLINE(int16_t,  FLT16_CLTS) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vclth_f16(a, b);
#else
    return  0-(a<b);
#endif
}
INLINE(int32_t,    FLT_CLTS)   (float a,   float b) {return  vclts_f32(a, b);}
INLINE(int64_t,    DBL_CLTS)  (double a,  double b) {return  vcltd_f64(a, b);}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cltsqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (QUAD_UTYPE) 0-(a<b);
}

INLINE(QUAD_ITYPE,cltsqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (QUAD_ITYPE) 0-(a<b);
}

INLINE(QUAD_ITYPE,cltsqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (QUAD_ITYPE) 0-(a<b);
}

#endif

#define     DBU_CLTS            vclt_u8
#define     DBI_CLTS(A, B)      vreinterpret_s16_u16(vclt_s8(A,B))
#define     DBC_CLTS            VDBC_BASE(CLTS)
#define     DHU_CLTS            vclt_u16
#define     DHI_CLTS(A, B)      vreinterpret_s16_u16(vclt_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CLTS(A, B)      vreinterpret_s16_u16(vclt_f16(A,B))
#else
INLINE(int16x4_t,DHF_CLTS) (float16x4_t a, float16x4_t b)
{
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(a);
    uint32x4_t  v = vcltq_f32(l, r);
    return  vreinterpret_s16_u16(vmovn_u32(v));
    //MY_NOT_IMPLEMENTED(0, __func__);
    //return  DHF_VOID;
}
#endif

#define     DWU_CLTS                             vclt_u32
#define     DWI_CLTS(A, B)  vreinterpret_s32_u32(vclt_s32(A,B))
#define     DWF_CLTS(A, B)  vreinterpret_s32_u32(vclt_f32(A,B))
#define     DDU_CLTS                             vclt_u64
#define     DDI_CLTS(A, B)  vreinterpret_s64_u64(vclt_s32(A,B))
#define     DDF_CLTS(A, B)  vreinterpret_s64_u64(vclt_f32(A,B))


#define     QBU_CLTS                              vcltq_u8
#define     QBI_CLTS(A, B)  vreinterpretq_s16_u16(vcltq_s8(A,B))
#define     QBC_CLTS        VQBC_BASE(CLTS)
#define     QHU_CLTS                              vcltq_u16
#define     QHI_CLTS(A, B)  vreinterpretq_s16_u16(vcltq_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CLTS(A, B)  vreinterpretq_s16_u16(vcltq_f16(A,B))
#else
INLINE(int16x8_t,QHF_CLTS) (float16x8_t a, float16x8_t b)
{
    return vcombine_s16(
        DHF_CLTS(vget_low_f16(a), vget_low_f16(b)),
        DHF_CLTS(vget_high_f16(a), vget_high_f16(b))
    );
}
#endif

#define     QWU_CLTS                              vcltq_u32
#define     QWI_CLTS(A, B)  vreinterpretq_s32_u32(vcltq_s32(A,B))
#define     QWF_CLTS(A, B)  vreinterpretq_s32_u32(vcltq_f32(A,B))
#define     QDU_CLTS                              vcltq_u64
#define     QDI_CLTS(A, B)  vreinterpretq_s64_u64(vcltq_s32(A,B))
#define     QDF_CLTS(A, B)  vreinterpretq_s64_u64(vcltq_f32(A,B))


INLINE(Vwbu,VWBU_CLTS) (Vwbu a, Vwbu b)
{
    float32x2_t l = vset_lane_f32(VWBU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBU_ASTM(a), r, V2_K0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CLTS(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbi,VWBI_CLTS) (Vwbi a, Vwbi b)
{
    float32x2_t l = vset_lane_f32(VWBI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBI_ASTM(a), r, V2_K0);
    int8x8_t    p = vreinterpret_s8_f32(l);
    int8x8_t    q = vreinterpret_s8_f32(r);
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(vclt_s8(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbc,VWBC_CLTS) (Vwbc a, Vwbc b)
{
    float32x2_t x = vset_lane_f32(VWBC_ASTM(a), x, V2_K0);
    float32x2_t y = vset_lane_f32(VWBC_ASTM(a), y, V2_K0);
    float32x2_t r;
#if CHAR_MIN
    int8x8_t    p = vreinterpret_s8_f32(x);
    int8x8_t    q = vreinterpret_s8_f32(y);
    r = vreinterpret_f32_u8(vclt_s8(p, q));
#else
    uint8x8_t   p = vreinterpret_u8_f32(x);
    uint8x8_t   q = vreinterpret_u8_f32(y);
    r = vreinterpret_f32_u8(vclt_u8(p, q));
#endif
    return  WBC_ASTV(vget_lane_f32(r, V2_K0));
}


INLINE(Vwhu,VWHU_CLTS) (Vwhu a, Vwhu b)
{
    float32x2_t l = vset_lane_f32(VWHU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHU_ASTM(a), r, V2_K0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vclt_u16(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwhi,VWHI_CLTS) (Vwhi a, Vwhi b)
{
    float32x2_t l = vset_lane_f32(VWHI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHI_ASTM(a), r, V2_K0);
    int16x4_t   p = vreinterpret_s16_f32(l);
    int16x4_t   q = vreinterpret_s16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vclt_s16(p, q)),
            V2_K0
        )
    );
}

#if defined(SPC_ARM_FP16_SIMD)

INLINE(Vwhi,VWHF_CLTS) (Vwhf a, Vwhf b)
{
    float32x2_t l = vset_lane_f32(VWHF_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHF_ASTM(a), r, V2_K0);
    float16x4_t p = vreinterpret_f16_f32(l);
    float16x4_t q = vreinterpret_f16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vclt_f16(p, q)),
            V2_K0
        )
    );
}

#else

INLINE(Vwhf,VWHF_CLTS) (Vwhf a, Vwhf b)
{
    MY_NOT_IMPLEMENTED(0, __func__);
    return  VWHF_VOID;
}

#endif


INLINE(Vwwu,VWWU_CLTS) (Vwwu a, Vwwu b)
{
    float32x2_t l = vset_lane_f32(VWWU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWU_ASTM(a), r, V2_K0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vclt_u32(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWI_CLTS) (Vwwi a, Vwwi b)
{
    float32x2_t l = vset_lane_f32(VWWI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWI_ASTM(a), r, V2_K0);
    int32x2_t   p = vreinterpret_s32_f32(l);
    int32x2_t   q = vreinterpret_s32_f32(r);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vclt_s32(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWF_CLTS) (Vwwf a, Vwwf b)
{
    float32x2_t p = vset_lane_f32(VWWF_ASTM(a), p, V2_K0);
    float32x2_t q = vset_lane_f32(VWWF_ASTM(a), q, V2_K0);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vclt_f32(p, q)),
            V2_K0
        )
    );
}


INLINE(Vdbu,VDBU_CLTS) (Vdbu a, Vdbu b) {return DBU_CLTS(a, b);}
INLINE(Vdbi,VDBI_CLTS) (Vdbi a, Vdbi b) {return DBI_CLTS(a, b);}
INLINE(Vdbc,VDBC_CLTS) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(DBC_CLTS(VDBC_ASTM(b), VDBC_ASTM(b)));
}

INLINE(Vdhu,VDHU_CLTS) (Vdhu a, Vdhu b) {return DHU_CLTS(a, b);}
INLINE(Vdhi,VDHI_CLTS) (Vdhi a, Vdhi b) {return DHI_CLTS(a, b);}
INLINE(Vdhi,VDHF_CLTS) (Vdhf a, Vdhf b) {return DHF_CLTS(a, b);}
INLINE(Vdwu,VDWU_CLTS) (Vdwu a, Vdwu b) {return DWU_CLTS(a, b);}
INLINE(Vdwi,VDWI_CLTS) (Vdwi a, Vdwi b) {return DWI_CLTS(a, b);}
INLINE(Vdwi,VDWF_CLTS) (Vdwf a, Vdwf b) {return DWF_CLTS(a, b);}
INLINE(Vddu,VDDU_CLTS) (Vddu a, Vddu b) {return DDU_CLTS(a, b);}
INLINE(Vddi,VDDI_CLTS) (Vddi a, Vddi b) {return DDI_CLTS(a, b);}
INLINE(Vddi,VDDF_CLTS) (Vddf a, Vddf b) {return DDF_CLTS(a, b);}


INLINE(Vqbu,VQBU_CLTS) (Vqbu a, Vqbu b) {return QBU_CLTS(a, b);}
INLINE(Vqbi,VQBI_CLTS) (Vqbi a, Vqbi b) {return QBI_CLTS(a, b);}
INLINE(Vqbc,VQBC_CLTS) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CLTS(VQBC_ASTM(b), VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CLTS) (Vqhu a, Vqhu b) {return QHU_CLTS(a, b);}
INLINE(Vqhi,VQHI_CLTS) (Vqhi a, Vqhi b) {return QHI_CLTS(a, b);}
INLINE(Vqhi,VQHF_CLTS) (Vqhf a, Vqhf b) {return QHF_CLTS(a, b);}
INLINE(Vqwu,VQWU_CLTS) (Vqwu a, Vqwu b) {return QWU_CLTS(a, b);}
INLINE(Vqwi,VQWI_CLTS) (Vqwi a, Vqwi b) {return QWI_CLTS(a, b);}
INLINE(Vqwi,VQWF_CLTS) (Vqwf a, Vqwf b) {return QWF_CLTS(a, b);}
INLINE(Vqdu,VQDU_CLTS) (Vqdu a, Vqdu b) {return QDU_CLTS(a, b);}
INLINE(Vqdi,VQDI_CLTS) (Vqdi a, Vqdi b) {return QDI_CLTS(a, b);}
INLINE(Vqdi,VQDF_CLTS) (Vqdf a, Vqdf b) {return QDF_CLTS(a, b);}

#if _LEAVE_ARM_CLTS
}
#endif

#if _ENTER_ARM_CLTY
{
#endif

INLINE(ptrdiff_t, ADDR_CLTY) (void volatile const *a, void volatile const *b)
{
    return  a<b;
}

INLINE(  _Bool,  BOOL_CLTY)   (_Bool a,   _Bool b) {return a<b;}
INLINE(  uchar, UCHAR_CLTY)   (uchar a,   uchar b) {return a<b;}
INLINE(  schar, SCHAR_CLTY)   (schar a,   schar b) {return a<b;}
INLINE(   char,  CHAR_CLTY)    (char a,    char b) {return a<b;}
INLINE( ushort, USHRT_CLTY)  (ushort a,  ushort b) {return a<b;}
INLINE(  short,  SHRT_CLTY)   (short a,   short b) {return a<b;}
INLINE(   uint,  UINT_CLTY)    (uint a,    uint b) {return a<b;}
INLINE(    int,   INT_CLTY)     (int a,     int b) {return a<b;}
INLINE(  ulong, ULONG_CLTY)   (ulong a,   ulong b) {return a<b;}
INLINE(   long,  LONG_CLTY)    (long a,    long b) {return a<b;}
INLINE( ullong,ULLONG_CLTY)  (ullong a,  ullong b) {return a<b;}
INLINE(  llong, LLONG_CLTY)   (llong a,   llong b) {return a<b;}
INLINE(int16_t, FLT16_CLTY) (flt16_t a, flt16_t b) {return a<b;}
INLINE(int32_t,   FLT_CLTY)   (float a,   float b) {return a<b;}
INLINE(int64_t,   DBL_CLTY)  (double a,  double b) {return a<b;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cltyqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (a<b);
}

INLINE(QUAD_ITYPE,cltyqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (a<b);
}

INLINE(QUAD_ITYPE,cltyqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (a<b);
}

#endif

INLINE( Wbu,WBU_CLTY) (Wbu a, Wbu b)
{
#define     WBU_CLTY    WBU_CLTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    uint8x8_t   r = vclt_u8(
        vreinterpret_u8_f32(p),
        vreinterpret_u8_f32(q)
    );
    return  vget_lane_f32(
        vreinterpret_f32_u8(vshr_n_u8(r, 7)),
        V2_K0
    );
}

INLINE( Wbu,WBI_CLTY) (Wbi a, Wbi b)
{
#define     WBI_CLTY    WBI_CLTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vshr_n_u8(
                vclt_s8(
                    vreinterpret_s8_f32(p),
                    vreinterpret_s8_f32(q)
                ),
                7
            )
        ),
        V2_K0
    );
}

#if CHAR_MIN
#   define  WBC_CLTY    WBI_CLTY
#else
#   define  WBC_CLTY    WBU_CLTY
#endif

INLINE( Whu,WHU_CLTY) (Whu a, Whu b)
{
#define     WHU_CLTY    WHU_CLTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vclt_u16(
                    vreinterpret_u16_f32(p),
                    vreinterpret_u16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
}

INLINE( Whu,WHI_CLTY) (Whi a, Whi b)
{
#define     WHI_CLTY    WHI_CLTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vclt_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
}

INLINE( Whu,WHF_CLTY) (Whf a, Whf b)
{
#define     WHF_CLTY    WHF_CLTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vclt_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
#else
    float32x4_t l = vcvt_f32_f16(vreinterpret_f16_f32(p));
    float32x4_t r = vcvt_f32_f16(vreinterpret_f16_f32(q));
    uint16x4_t  v = vshr_n_u16(vmovn_u32(vcltq_f32(l, r)), 15);
    return  vget_lane_f32(vreinterpret_f32_u16(v), V2_K0);
#endif
}

INLINE( Wwu,WWU_CLTY) (Wwu a, Wwu b)
{
#define     WWU_CLTY    WWU_CLTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vclt_u32(
                    vreinterpret_u32_f32(p),
                    vreinterpret_u32_f32(q)
                ),
                31
            )
        ),
        V2_K0
    );
}

INLINE( Wwu,WWI_CLTY) (Wwi a, Wwi b)
{
#define     WWI_CLTY    WWI_CLTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vclt_s32(
                    vreinterpret_s32_f32(p),
                    vreinterpret_s32_f32(q)
                ),
                31
            )
        ),
        V2_K0
    );
}

INLINE( Wwu,WWF_CLTY) (Wwf a, Wwf b)
{
#define     WWF_CLTY    WWF_CLTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(vshr_n_u32(vclt_f32(p, q), 31)),
        V2_K0
    );
}

#define     DBU_CLTY(A, B)            vshr_n_u8(vclt_u8(A,B),7)
#define     DBI_CLTY(A, B)  VDBU_ASTI(vshr_n_u8(vclt_s8(A,B),7))
#if CHAR_MIN
#   define  DBC_CLTY        DBI_CLTY
#else
#   define  DBC_CLTY        DBU_CLTY
#endif

#define     DHU_CLTY(A, B)            vshr_n_u16(vclt_u16(A,B),15)
#define     DHI_CLTY(A, B)  VDHU_ASTI(vshr_n_u16(vclt_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CLTY(A, B)  VDHU_ASTI(vshr_n_u16(vclt_f16(A,B),15))
#else
#   define  DHF_CLTY(A, B)      \
VDHU_ASHI(                      \
    vshr_n_u16(                 \
        vmovn_u32(              \
            vcltq_f32(          \
                vcvt_f32_f16(a),\
                vcvt_f32_f16(b) \
            )                   \
        ),                      \
        15                      \
    )                           \
)
#endif

#define     DWU_CLTY(A, B)            vshr_n_u32(vclt_u32(A,B),31)
#define     DWI_CLTY(A, B)  VDWU_ASTI(vshr_n_u32(vclt_s32(A,B),31))
#define     DWF_CLTY(A, B)  VDWU_ASTI(vshr_n_u32(vclt_f32(A,B),31))

#define     DDU_CLTY(A, B)            vshr_n_u64(vclt_u64(A,B),63)
#define     DDI_CLTY(A, B)  VDDU_ASTI(vshr_n_u64(vclt_s64(A,B),63))
#define     DDF_CLTY(A, B)  VDDU_ASTI(vshr_n_u64(vclt_f64(A,B),63))


#define     QBU_CLTY(A, B)            vshrq_n_u8(vcltq_u8(A,B),7)
#define     QBI_CLTY(A, B)  VQBU_ASTI(vshrq_n_u8(vcltq_s8(A,B),7))
#if CHAR_MIN
#   define  QBC_CLTY        QBI_CLTY
#else
#   define  QBC_CLTY        QBU_CLTY
#endif

#define     QHU_CLTY(A, B)            vshrq_n_u16(vcltq_u16(A,B),15)
#define     QHI_CLTY(A, B)  VQHU_ASTI(vshrq_n_u16(vcltq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CLTY(A, B)  VQHU_ASTI(vshrq_n_u16(vcltq_f16(A,B),15))
#else
#   define  QHF_CLTY(A, B)                          \
vreinterpretq_s16_u16(                              \
    vshrq_n_u16(                                    \
        vcombine_u16(                               \
            vmovn_u32(                              \
                vcltq_f32(                          \
                    vcvt_f32_f16(vget_low_f16(A)),  \
                    vcvt_f32_f16(vget_low_f16(B))   \
                )                                   \
            ),                                      \
            vmovn_u32(                              \
                vcltq_f32(                          \
                    vcvt_f32_f16(vget_high_f16(A)), \
                    vcvt_f32_f16(vget_high_f16(B))  \
                )                                   \
            )                                       \
        ),                                          \
        15                                          \
    )                                               \
)
#endif

#define     QWU_CLTY(A, B)            vshrq_n_u32(vcltq_u32(A,B),31)
#define     QWI_CLTY(A, B)  VQWU_ASTI(vshrq_n_u32(vcltq_s32(A,B),31))
#define     QWF_CLTY(A, B)  VQWU_ASTI(vshrq_n_u32(vcltq_f32(A,B),31))

#define     QDU_CLTY(A, B)            vshrq_n_u64(vcltq_u64(A,B),63)
#define     QDI_CLTY(A, B)  VQDU_ASTI(vshrq_n_u64(vcltq_s64(A,B),63))
#define     QDF_CLTY(A, B)  VQDU_ASTI(vshrq_n_u64(vcltq_f64(A,B),63))


INLINE(Vwbu,VWBU_CLTY) (Vwbu a, Vwbu b)
{
#define     VWBU_CLTY(A, B) WBU_ASTV(WBU_CLTY(VWBU_ASTM(A),VWBU_ASTM(B)))
    return  VWBU_CLTY(a, b);
}

INLINE(Vwbi,VWBI_CLTY) (Vwbi a, Vwbi b)
{
#define     VWBI_CLTY(A, B) WBI_ASTV(WBI_CLTY(VWBI_ASTM(A),VWBI_ASTM(B)))
    return  VWBI_CLTY(a, b);
}

INLINE(Vwbc,VWBC_CLTY) (Vwbc a, Vwbc b)
{
#define     VWBC_CLTY(A, B) WBC_ASTV(WBC_CLTY(VWBC_ASTM(A),VWBC_ASTM(B)))
    return  VWBC_CLTY(a, b);
}


INLINE(Vwhu,VWHU_CLTY) (Vwhu a, Vwhu b)
{
#define     VWHU_CLTY(A, B) WHU_ASTV(WHU_CLTY(VWHU_ASTM(A),VWHU_ASTM(B)))
    return  VWHU_CLTY(a, b);
}

INLINE(Vwhi,VWHI_CLTY) (Vwhi a, Vwhi b)
{
#define     VWHI_CLTY(A, B) WHI_ASTV(WHI_CLTY(VWHI_ASTM(A),VWHI_ASTM(B)))
    return  VWHI_CLTY(a, b);
}

INLINE(Vwhi,VWHF_CLTY) (Vwhf a, Vwhf b)
{
#define     VWHF_CLTY(A, B) WHI_ASTV(WHF_CLTY(VWHF_ASTM(A),VWHF_ASTM(B)))
    return  VWHF_CLTY(a, b);
}


INLINE(Vwwu,VWWU_CLTY) (Vwwu a, Vwwu b)
{
#define     VWWU_CLTY(A, B) WWU_ASTV(WWU_CLTY(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_CLTY(a, b);
}

INLINE(Vwwi,VWWI_CLTY) (Vwwi a, Vwwi b)
{
#define     VWWI_CLTY(A, B) WWI_ASTV(WWI_CLTY(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_CLTY(a, b);
}

INLINE(Vwwi,VWWF_CLTY) (Vwwf a, Vwwf b)
{
#define     VWWF_CLTY(A, B) WWI_ASTV(WWF_CLTY(VWWF_ASTM(A),VWWF_ASTM(B)))
    return  VWWF_CLTY(a, b);
}


INLINE(Vdbu,VDBU_CLTY) (Vdbu a, Vdbu b) {return DBU_CLTY(a, b);}
INLINE(Vdbi,VDBI_CLTY) (Vdbi a, Vdbi b) {return DBI_CLTY(a, b);}
INLINE(Vdbc,VDBC_CLTY) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(
        DBC_CLTY(
            VDBC_ASTM(a),
            VDBC_ASTM(b)
        )
    );
}

INLINE(Vdhu,VDHU_CLTY) (Vdhu a, Vdhu b) {return DHU_CLTY(a, b);}
INLINE(Vdhi,VDHI_CLTY) (Vdhi a, Vdhi b) {return DHI_CLTY(a, b);}
INLINE(Vdhi,VDHF_CLTY) (Vdhf a, Vdhf b) {return DHF_CLTY(a, b);}

INLINE(Vdwu,VDWU_CLTY) (Vdwu a, Vdwu b) {return DWU_CLTY(a, b);}
INLINE(Vdwi,VDWI_CLTY) (Vdwi a, Vdwi b) {return DWI_CLTY(a, b);}
INLINE(Vdwi,VDWF_CLTY) (Vdwf a, Vdwf b) {return DWF_CLTY(a, b);}

INLINE(Vddu,VDDU_CLTY) (Vddu a, Vddu b) {return DDU_CLTY(a, b);}
INLINE(Vddi,VDDI_CLTY) (Vddi a, Vddi b) {return DDI_CLTY(a, b);}
INLINE(Vddi,VDDF_CLTY) (Vddf a, Vddf b) {return DDF_CLTY(a, b);}


INLINE(Vqbu,VQBU_CLTY) (Vqbu a, Vqbu b) {return QBU_CLTY(a,b);}
INLINE(Vqbi,VQBI_CLTY) (Vqbi a, Vqbi b) {return QBI_CLTY(a,b);}
INLINE(Vqbc,VQBC_CLTY) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CLTY(VQBC_ASTM(a),VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CLTY) (Vqhu a, Vqhu b) {return QHU_CLTY(a,b);}
INLINE(Vqhi,VQHI_CLTY) (Vqhi a, Vqhi b) {return QHI_CLTY(a,b);}
INLINE(Vqhi,VQHF_CLTY) (Vqhf a, Vqhf b) {return QHF_CLTY(a,b);}
INLINE(Vqwu,VQWU_CLTY) (Vqwu a, Vqwu b) {return QWU_CLTY(a,b);}
INLINE(Vqwi,VQWI_CLTY) (Vqwi a, Vqwi b) {return QWI_CLTY(a,b);}
INLINE(Vqwi,VQWF_CLTY) (Vqwf a, Vqwf b) {return QWF_CLTY(a,b);}
INLINE(Vqdu,VQDU_CLTY) (Vqdu a, Vqdu b) {return QDU_CLTY(a,b);}
INLINE(Vqdi,VQDI_CLTY) (Vqdi a, Vqdi b) {return QDI_CLTY(a,b);}
INLINE(Vqdi,VQDF_CLTY) (Vqdf a, Vqdf b) {return QDF_CLTY(a,b);}

#if _LEAVE_ARM_CLTY
}
#endif


#if _ENTER_ARM_CLES
{
#endif

INLINE(ptrdiff_t, ADDR_CLES)
(
    void volatile const *a,
    void volatile const *b
)
{
    return (a != b) ? NULL-((void *) INTPTR_C(1)) : 0;
}

INLINE(  _Bool,   BOOL_CLES)   (_Bool a,   _Bool b) {return a<=b;}
INLINE(  uchar,  UCHAR_CLES)   (uchar a,   uchar b)
{
    return  (a<=b) ? UCHAR_MAX : 0;
}

INLINE(  schar,  SCHAR_CLES)   (schar a,   schar b) {return  0-(a<=b);}
INLINE(   char,   CHAR_CLES)    (char a,    char b)
{
    return  (a<=b) ? '\xff' : '\x00';
}

INLINE( ushort,  USHRT_CLES)  (ushort a,  ushort b)
{
    return  (a<=b) ? USHRT_MAX : 0;
}

INLINE(  short,   SHRT_CLES)   (short a,   short b) {return  0-(a<=b);}

INLINE(   uint,   UINT_CLES)    (uint a,    uint b)
{
    return  (a<=b) ? UINT_MAX : 0u;
}

INLINE(    int,    INT_CLES)     (int a,     int b) {return  0-(a<=b);}

// NOTE: vclth_f16 / vcltd_u64 / vcltd_s64 exist
INLINE(  ulong,  ULONG_CLES)   (ulong a,   ulong b)
{
    return  (a<=b) ? ULONG_MAX : 0ul;
}

INLINE(   long,   LONG_CLES)    (long a,    long b) {return  0l-(a<=b);}

INLINE( ullong, ULLONG_CLES)  (ullong a,  ullong b)
{
    return  (a<=b) ? ULLONG_MAX : 0ull;
}

INLINE(  llong,  LLONG_CLES)   (llong a,   llong b) {return  0ll-(a<=b);}

INLINE(int16_t,  FLT16_CLES) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcleh_f16(a, b);
#else
    return  0-(a<=b);
#endif
}

INLINE(int32_t,    FLT_CLES)   (float a,   float b) {return  vcles_f32(a, b);}
INLINE(int64_t,    DBL_CLES)  (double a,  double b) {return  vcled_f64(a, b);}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,clesqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (QUAD_UTYPE) 0-(a<=b);
}

INLINE(QUAD_ITYPE,clesqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (QUAD_ITYPE) 0-(a<=b);
}

INLINE(QUAD_ITYPE,clesqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (QUAD_ITYPE) 0-(a<=b);
}

#endif

#define     DBU_CLES                             vcle_u8
#define     DBI_CLES(A, B)  vreinterpret_s16_u16(vcle_s8(A,B))
#define     DBC_CLES        VDBC_BASE(CLES)
#define     DHU_CLES                             vcle_u16
#define     DHI_CLES(A, B)  vreinterpret_s16_u16(vcle_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CLES(A, B)  vreinterpret_s16_u16(vcle_f16(A,B))
#else
INLINE(float16x4_t,DHF_CLES) (float16x4_t a, float16x4_t b)
{
#   define  DHF_CLES    DHF_CLES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  DHF_VOID;
}
#endif
#define     DWU_CLES                             vcle_u32
#define     DWI_CLES(A, B)  vreinterpret_s32_u32(vcle_s32(A,B))
#define     DWF_CLES(A, B)  vreinterpret_s32_u32(vcle_f32(A,B))
#define     DDU_CLES                             vcle_u64
#define     DDI_CLES(A, B)  vreinterpret_s64_u64(vcle_s32(A,B))
#define     DDF_CLES(A, B)  vreinterpret_s64_u64(vcle_f32(A,B))


#define     QBU_CLES                              vcleq_u8
#define     QBI_CLES(A, B)  vreinterpretq_s16_u16(vcleq_s8(A,B))
#define     QBC_CLES        VQBC_BASE(CLES)
#define     QHU_CLES                              vcleq_u16
#define     QHI_CLES(A, B)  vreinterpretq_s16_u16(vcleq_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CLES(A, B)  vreinterpretq_s16_u16(vcleq_f16(A,B))
#else
INLINE(float16x8_t,QHF_CLES) (float16x8_t a, float16x8_t b)
{
#   define  QHF_CLES    QHF_CLES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  QHF_VOID;
}
#endif

#define     QWU_CLES                              vcleq_u32
#define     QWI_CLES(A, B)  vreinterpretq_s32_u32(vcleq_s32(A,B))
#define     QWF_CLES(A, B)  vreinterpretq_s32_u32(vcleq_f32(A,B))
#define     QDU_CLES                              vcleq_u64
#define     QDI_CLES(A, B)  vreinterpretq_s64_u64(vcleq_s32(A,B))
#define     QDF_CLES(A, B)  vreinterpretq_s64_u64(vcleq_f32(A,B))


INLINE(Vwbu,VWBU_CLES) (Vwbu a, Vwbu b)
{
    float32x2_t l = vset_lane_f32(VWBU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBU_ASTM(a), r, V2_K0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CLES(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbi,VWBI_CLES) (Vwbi a, Vwbi b)
{
    float32x2_t l = vset_lane_f32(VWBI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBI_ASTM(a), r, V2_K0);
    int8x8_t    p = vreinterpret_s8_f32(l);
    int8x8_t    q = vreinterpret_s8_f32(r);
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(vcle_s8(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbc,VWBC_CLES) (Vwbc a, Vwbc b)
{
    float32x2_t x = vset_lane_f32(VWBC_ASTM(a), x, V2_K0);
    float32x2_t y = vset_lane_f32(VWBC_ASTM(a), y, V2_K0);
    float32x2_t r;
#if CHAR_MIN
    int8x8_t    p = vreinterpret_s8_f32(x);
    int8x8_t    q = vreinterpret_s8_f32(y);
    r = vreinterpret_f32_u8(vcle_s8(p, q));
#else
    uint8x8_t   p = vreinterpret_u8_f32(x);
    uint8x8_t   q = vreinterpret_u8_f32(y);
    r = vreinterpret_f32_u8(vcle_u8(p, q));
#endif
    return  WBC_ASTV(vget_lane_f32(r, V2_K0));
}


INLINE(Vwhu,VWHU_CLES) (Vwhu a, Vwhu b)
{
    float32x2_t l = vset_lane_f32(VWHU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHU_ASTM(a), r, V2_K0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcle_u16(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwhi,VWHI_CLES) (Vwhi a, Vwhi b)
{
    float32x2_t l = vset_lane_f32(VWHI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHI_ASTM(a), r, V2_K0);
    int16x4_t   p = vreinterpret_s16_f32(l);
    int16x4_t   q = vreinterpret_s16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcle_s16(p, q)),
            V2_K0
        )
    );
}

#if defined(SPC_ARM_FP16_SIMD)
INLINE(Vwhi,VWHF_CLES) (Vwhf a, Vwhf b)
{
#   define  VWHF_CLES VWHF_CLES
    float32x2_t l = vset_lane_f32(VWHF_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHF_ASTM(a), r, V2_K0);
    float16x4_t p = vreinterpret_f16_f32(l);
    float16x4_t q = vreinterpret_f16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcle_f16(p, q)),
            V2_K0
        )
    );
}
#else
INLINE(Vwhf,VWHF_CLES) (Vwhf a, Vwhf b)
{
#   define  VWHF_CLES    VWHF_CLES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  VWHF_VOID;
}
#endif


INLINE(Vwwu,VWWU_CLES) (Vwwu a, Vwwu b)
{
    float32x2_t l = vset_lane_f32(VWWU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWU_ASTM(a), r, V2_K0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcle_u32(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWI_CLES) (Vwwi a, Vwwi b)
{
    float32x2_t l = vset_lane_f32(VWWI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWI_ASTM(a), r, V2_K0);
    int32x2_t   p = vreinterpret_s32_f32(l);
    int32x2_t   q = vreinterpret_s32_f32(r);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcle_s32(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWF_CLES) (Vwwf a, Vwwf b)
{
    float32x2_t p = vset_lane_f32(VWWF_ASTM(a), p, V2_K0);
    float32x2_t q = vset_lane_f32(VWWF_ASTM(a), q, V2_K0);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcle_f32(p, q)),
            V2_K0
        )
    );
}



INLINE(Vdbu,VDBU_CLES) (Vdbu a, Vdbu b) {return DBU_CLES(a, b);}
INLINE(Vdbi,VDBI_CLES) (Vdbi a, Vdbi b) {return DBI_CLES(a, b);}
INLINE(Vdbc,VDBC_CLES) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(DBC_CLES(VDBC_ASTM(b), VDBC_ASTM(b)));
}

INLINE(Vdhu,VDHU_CLES) (Vdhu a, Vdhu b) {return DHU_CLES(a, b);}
INLINE(Vdhi,VDHI_CLES) (Vdhi a, Vdhi b) {return DHI_CLES(a, b);}
INLINE(Vdhi,VDHF_CLES) (Vdhf a, Vdhf b) {return DHF_CLES(a, b);}
INLINE(Vdwu,VDWU_CLES) (Vdwu a, Vdwu b) {return DWU_CLES(a, b);}
INLINE(Vdwi,VDWI_CLES) (Vdwi a, Vdwi b) {return DWI_CLES(a, b);}
INLINE(Vdwi,VDWF_CLES) (Vdwf a, Vdwf b) {return DWF_CLES(a, b);}
INLINE(Vddu,VDDU_CLES) (Vddu a, Vddu b) {return DDU_CLES(a, b);}
INLINE(Vddi,VDDI_CLES) (Vddi a, Vddi b) {return DDI_CLES(a, b);}
INLINE(Vddi,VDDF_CLES) (Vddf a, Vddf b) {return DDF_CLES(a, b);}


INLINE(Vqbu,VQBU_CLES) (Vqbu a, Vqbu b) {return QBU_CLES(a, b);}
INLINE(Vqbi,VQBI_CLES) (Vqbi a, Vqbi b) {return QBI_CLES(a, b);}
INLINE(Vqbc,VQBC_CLES) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CLES(VQBC_ASTM(b), VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CLES) (Vqhu a, Vqhu b) {return QHU_CLES(a, b);}
INLINE(Vqhi,VQHI_CLES) (Vqhi a, Vqhi b) {return QHI_CLES(a, b);}
INLINE(Vqhi,VQHF_CLES) (Vqhf a, Vqhf b) {return QHF_CLES(a, b);}
INLINE(Vqwu,VQWU_CLES) (Vqwu a, Vqwu b) {return QWU_CLES(a, b);}
INLINE(Vqwi,VQWI_CLES) (Vqwi a, Vqwi b) {return QWI_CLES(a, b);}
INLINE(Vqwi,VQWF_CLES) (Vqwf a, Vqwf b) {return QWF_CLES(a, b);}
INLINE(Vqdu,VQDU_CLES) (Vqdu a, Vqdu b) {return QDU_CLES(a, b);}
INLINE(Vqdi,VQDI_CLES) (Vqdi a, Vqdi b) {return QDI_CLES(a, b);}
INLINE(Vqdi,VQDF_CLES) (Vqdf a, Vqdf b) {return QDF_CLES(a, b);}

#if _LEAVE_ARM_CLES
}
#endif

#if _ENTER_ARM_CLEY
{
#endif

INLINE(ptrdiff_t, ADDR_CLEY) (void volatile const *a, void volatile const *b)
{
    return  a<=b;
}

INLINE(  _Bool,  BOOL_CLEY)   (_Bool a,   _Bool b) {return a<=b;}
INLINE(  uchar, UCHAR_CLEY)   (uchar a,   uchar b) {return a<=b;}
INLINE(  schar, SCHAR_CLEY)   (schar a,   schar b) {return a<=b;}
INLINE(   char,  CHAR_CLEY)    (char a,    char b) {return a<=b;}
INLINE( ushort, USHRT_CLEY)  (ushort a,  ushort b) {return a<=b;}
INLINE(  short,  SHRT_CLEY)   (short a,   short b) {return a<=b;}
INLINE(   uint,  UINT_CLEY)    (uint a,    uint b) {return a<=b;}
INLINE(    int,   INT_CLEY)     (int a,     int b) {return a<=b;}
INLINE(  ulong, ULONG_CLEY)   (ulong a,   ulong b) {return a<=b;}
INLINE(   long,  LONG_CLEY)    (long a,    long b) {return a<=b;}
INLINE( ullong,ULLONG_CLEY)  (ullong a,  ullong b) {return a<=b;}
INLINE(  llong, LLONG_CLEY)   (llong a,   llong b) {return a<=b;}
INLINE(int16_t, FLT16_CLEY) (flt16_t a, flt16_t b) {return a<=b;}
INLINE(int32_t,   FLT_CLEY)   (float a,   float b) {return a<=b;}
INLINE(int64_t,   DBL_CLEY)  (double a,  double b) {return a<=b;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cleyqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (a<=b);
}

INLINE(QUAD_ITYPE,cleyqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (a<=b);
}

INLINE(QUAD_ITYPE,cleyqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (a<=b);
}

#endif
INLINE( Wbu,WBU_CLEY) (Wbu a, Wbu b)
{
#define     WBU_CLEY    WBU_CLEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    uint8x8_t   r = vcle_u8(
        vreinterpret_u8_f32(p),
        vreinterpret_u8_f32(q)
    );
    return  vget_lane_f32(
        vreinterpret_f32_u8(vshr_n_u8(r, 7)),
        V2_K0
    );
}

INLINE( Wbu,WBI_CLEY) (Wbi a, Wbi b)
{
#define     WBI_CLEY    WBI_CLEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vshr_n_u8(
                vcle_s8(
                    vreinterpret_s8_f32(p),
                    vreinterpret_s8_f32(q)
                ),
                7
            )
        ),
        V2_K0
    );
}

#if CHAR_MIN
#   define  WBC_CLEY    WBI_CLEY
#else
#   define  WBC_CLEY    WBU_CLEY
#endif

INLINE( Whu,WHU_CLEY) (Whu a, Whu b)
{
#define     WHU_CLEY    WHU_CLEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcle_u16(
                    vreinterpret_u16_f32(p),
                    vreinterpret_u16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
}

INLINE( Whu,WHI_CLEY) (Whi a, Whi b)
{
#define     WHI_CLEY    WHI_CLEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcle_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
}

INLINE( Whu,WHF_CLEY) (Whf a, Whf b)
{
#define     WHF_CLEY    WHF_CLEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcle_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
#else
    float32x4_t l = vcvt_f32_f16(vreinterpret_f16_f32(p));
    float32x4_t r = vcvt_f32_f16(vreinterpret_f16_f32(q));
    uint16x4_t  v = vshr_n_u16(vmovn_u32(vcleq_f32(l, r)), 15);
    return  vget_lane_f32(vreinterpret_f32_u16(v), V2_K0);
#endif
}

INLINE( Wwu,WWU_CLEY) (Wwu a, Wwu b)
{
#define     WWU_CLEY    WWU_CLEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcle_u32(
                    vreinterpret_u32_f32(p),
                    vreinterpret_u32_f32(q)
                ),
                31
            )
        ),
        V2_K0
    );
}

INLINE( Wwu,WWI_CLEY) (Wwi a, Wwi b)
{
#define     WWI_CLEY    WWI_CLEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcle_s32(
                    vreinterpret_s32_f32(p),
                    vreinterpret_s32_f32(q)
                ),
                31
            )
        ),
        V2_K0
    );
}

INLINE( Wwu,WWF_CLEY) (Wwf a, Wwf b)
{
#define     WWF_CLEY    WWF_CLEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(vshr_n_u32(vcle_f32(p, q), 31)),
        V2_K0
    );
}

#define     DBU_CLEY(A, B)            vshr_n_u8(vcle_u8(A,B),7)
#define     DBI_CLEY(A, B)  VDBU_ASTI(vshr_n_u8(vcle_s8(A,B),7))
#if CHAR_MIN
#   define  DBC_CLEY        DBI_CLEY
#else
#   define  DBC_CLEY        DBU_CLEY
#endif

#define     DHU_CLEY(A, B)            vshr_n_u16(vcle_u16(A,B),15)
#define     DHI_CLEY(A, B)  VDHU_ASTI(vshr_n_u16(vcle_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CLEY(A, B)  VDHU_ASTI(vshr_n_u16(vcle_f16(A,B),15))
#else
#   define  DHF_CLEY(A, B)      \
VDHU_ASHI(                      \
    vshr_n_u16(                 \
        vmovn_u32(              \
            vcleq_f32(          \
                vcvt_f32_f16(a),\
                vcvt_f32_f16(b) \
            )                   \
        ),                      \
        15                      \
    )                           \
)
#endif

#define     DWU_CLEY(A, B)            vshr_n_u32(vcle_u32(A,B),31)
#define     DWI_CLEY(A, B)  VDWU_ASTI(vshr_n_u32(vcle_s32(A,B),31))
#define     DWF_CLEY(A, B)  VDWU_ASTI(vshr_n_u32(vcle_f32(A,B),31))

#define     DDU_CLEY(A, B)            vshr_n_u64(vcle_u64(A,B),63)
#define     DDI_CLEY(A, B)  VDDU_ASTI(vshr_n_u64(vcle_s64(A,B),63))
#define     DDF_CLEY(A, B)  VDDU_ASTI(vshr_n_u64(vcle_f64(A,B),63))


#define     QBU_CLEY(A, B)            vshrq_n_u8(vcleq_u8(A,B),7)
#define     QBI_CLEY(A, B)  VQBU_ASTI(vshrq_n_u8(vcleq_s8(A,B),7))
#if CHAR_MIN
#   define  QBC_CLEY        QBI_CLEY
#else
#   define  QBC_CLEY        QBU_CLEY
#endif

#define     QHU_CLEY(A, B)            vshrq_n_u16(vcleq_u16(A,B),15)
#define     QHI_CLEY(A, B)  VQHU_ASTI(vshrq_n_u16(vcleq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CLEY(A, B)  VQHU_ASTI(vshrq_n_u16(vcleq_f16(A,B),15))
#else
#   define  QHF_CLEY(A, B)                          \
vreinterpretq_s16_u16(                              \
    vshrq_n_u16(                                    \
        vcombine_u16(                               \
            vmovn_u32(                              \
                vcleq_f32(                          \
                    vcvt_f32_f16(vget_low_f16(A)),  \
                    vcvt_f32_f16(vget_low_f16(B))   \
                )                                   \
            ),                                      \
            vmovn_u32(                              \
                vcleq_f32(                          \
                    vcvt_f32_f16(vget_high_f16(A)), \
                    vcvt_f32_f16(vget_high_f16(B))  \
                )                                   \
            )                                       \
        ),                                          \
        15                                          \
    )                                               \
)
#endif

#define     QWU_CLEY(A, B)            vshrq_n_u32(vcleq_u32(A,B),31)
#define     QWI_CLEY(A, B)  VQWU_ASTI(vshrq_n_u32(vcleq_s32(A,B),31))
#define     QWF_CLEY(A, B)  VQWU_ASTI(vshrq_n_u32(vcleq_f32(A,B),31))

#define     QDU_CLEY(A, B)            vshrq_n_u64(vcleq_u64(A,B),63)
#define     QDI_CLEY(A, B)  VQDU_ASTI(vshrq_n_u64(vcleq_s64(A,B),63))
#define     QDF_CLEY(A, B)  VQDU_ASTI(vshrq_n_u64(vcleq_f64(A,B),63))


INLINE(Vwbu,VWBU_CLEY) (Vwbu a, Vwbu b)
{
#define     VWBU_CLEY(A, B) WBU_ASTV(WBU_CLEY(VWBU_ASTM(A),VWBU_ASTM(B)))
    return  VWBU_CLEY(a, b);
}

INLINE(Vwbi,VWBI_CLEY) (Vwbi a, Vwbi b)
{
#define     VWBI_CLEY(A, B) WBI_ASTV(WBI_CLEY(VWBI_ASTM(A),VWBI_ASTM(B)))
    return  VWBI_CLEY(a, b);
}

INLINE(Vwbc,VWBC_CLEY) (Vwbc a, Vwbc b)
{
#define     VWBC_CLEY(A, B) WBC_ASTV(WBC_CLEY(VWBC_ASTM(A),VWBC_ASTM(B)))
    return  VWBC_CLEY(a, b);
}


INLINE(Vwhu,VWHU_CLEY) (Vwhu a, Vwhu b)
{
#define     VWHU_CLEY(A, B) WHU_ASTV(WHU_CLEY(VWHU_ASTM(A),VWHU_ASTM(B)))
    return  VWHU_CLEY(a, b);
}

INLINE(Vwhi,VWHI_CLEY) (Vwhi a, Vwhi b)
{
#define     VWHI_CLEY(A, B) WHI_ASTV(WHI_CLEY(VWHI_ASTM(A),VWHI_ASTM(B)))
    return  VWHI_CLEY(a, b);
}

INLINE(Vwhi,VWHF_CLEY) (Vwhf a, Vwhf b)
{
#define     VWHF_CLEY(A, B) WHI_ASTV(WHF_CLEY(VWHF_ASTM(A),VWHF_ASTM(B)))
    return  VWHF_CLEY(a, b);
}


INLINE(Vwwu,VWWU_CLEY) (Vwwu a, Vwwu b)
{
#define     VWWU_CLEY(A, B) WWU_ASTV(WWU_CLEY(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_CLEY(a, b);
}

INLINE(Vwwi,VWWI_CLEY) (Vwwi a, Vwwi b)
{
#define     VWWI_CLEY(A, B) WWI_ASTV(WWI_CLEY(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_CLEY(a, b);
}

INLINE(Vwwi,VWWF_CLEY) (Vwwf a, Vwwf b)
{
#define     VWWF_CLEY(A, B) WWI_ASTV(WWF_CLEY(VWWF_ASTM(A),VWWF_ASTM(B)))
    return  VWWF_CLEY(a, b);
}


INLINE(Vdbu,VDBU_CLEY) (Vdbu a, Vdbu b) {return DBU_CLEY(a, b);}
INLINE(Vdbi,VDBI_CLEY) (Vdbi a, Vdbi b) {return DBI_CLEY(a, b);}
INLINE(Vdbc,VDBC_CLEY) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(
        DBC_CLEY(
            VDBC_ASTM(a),
            VDBC_ASTM(b)
        )
    );
}

INLINE(Vdhu,VDHU_CLEY) (Vdhu a, Vdhu b) {return DHU_CLEY(a, b);}
INLINE(Vdhi,VDHI_CLEY) (Vdhi a, Vdhi b) {return DHI_CLEY(a, b);}
INLINE(Vdhi,VDHF_CLEY) (Vdhf a, Vdhf b) {return DHF_CLEY(a, b);}

INLINE(Vdwu,VDWU_CLEY) (Vdwu a, Vdwu b) {return DWU_CLEY(a, b);}
INLINE(Vdwi,VDWI_CLEY) (Vdwi a, Vdwi b) {return DWI_CLEY(a, b);}
INLINE(Vdwi,VDWF_CLEY) (Vdwf a, Vdwf b) {return DWF_CLEY(a, b);}

INLINE(Vddu,VDDU_CLEY) (Vddu a, Vddu b) {return DDU_CLEY(a, b);}
INLINE(Vddi,VDDI_CLEY) (Vddi a, Vddi b) {return DDI_CLEY(a, b);}
INLINE(Vddi,VDDF_CLEY) (Vddf a, Vddf b) {return DDF_CLEY(a, b);}


INLINE(Vqbu,VQBU_CLEY) (Vqbu a, Vqbu b) {return QBU_CLEY(a,b);}
INLINE(Vqbi,VQBI_CLEY) (Vqbi a, Vqbi b) {return QBI_CLEY(a,b);}
INLINE(Vqbc,VQBC_CLEY) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CLEY(VQBC_ASTM(a),VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CLEY) (Vqhu a, Vqhu b) {return QHU_CLEY(a,b);}
INLINE(Vqhi,VQHI_CLEY) (Vqhi a, Vqhi b) {return QHI_CLEY(a,b);}
INLINE(Vqhi,VQHF_CLEY) (Vqhf a, Vqhf b) {return QHF_CLEY(a,b);}
INLINE(Vqwu,VQWU_CLEY) (Vqwu a, Vqwu b) {return QWU_CLEY(a,b);}
INLINE(Vqwi,VQWI_CLEY) (Vqwi a, Vqwi b) {return QWI_CLEY(a,b);}
INLINE(Vqwi,VQWF_CLEY) (Vqwf a, Vqwf b) {return QWF_CLEY(a,b);}
INLINE(Vqdu,VQDU_CLEY) (Vqdu a, Vqdu b) {return QDU_CLEY(a,b);}
INLINE(Vqdi,VQDI_CLEY) (Vqdi a, Vqdi b) {return QDI_CLEY(a,b);}
INLINE(Vqdi,VQDF_CLEY) (Vqdf a, Vqdf b) {return QDF_CLEY(a,b);}

#if _LEAVE_ARM_CLEY
}
#endif


#if _ENTER_ARM_CGTS
{
#endif

INLINE(ptrdiff_t, ADDR_CGTS)
(
    void volatile const *a,
    void volatile const *b
)
{
    return (a > b) ? NULL-((void *) INTPTR_C(1)) : 0;
}

INLINE(  _Bool,   BOOL_CGTS)   (_Bool a,   _Bool b) {return a>b;}
INLINE(  uchar,  UCHAR_CGTS)   (uchar a,   uchar b)
{
    return  (a>b) ? UCHAR_MAX : 0;
}

INLINE(  schar,  SCHAR_CGTS)   (schar a,   schar b) {return  0-(a>b);}
INLINE(   char,   CHAR_CGTS)    (char a,    char b)
{
    return  (a>b) ? '\xff' : '\x00';
}

INLINE( ushort,  USHRT_CGTS)  (ushort a,  ushort b)
{
    return  (a>b) ? USHRT_MAX : 0;
}

INLINE(  short,   SHRT_CGTS)   (short a,   short b) {return  0-(a>b);}

INLINE(   uint,   UINT_CGTS)    (uint a,    uint b)
{
    return  (a>b) ? UINT_MAX : 0u;
}

INLINE(    int,    INT_CGTS)     (int a,     int b) {return  0-(a>b);}

INLINE(  ulong,  ULONG_CGTS)   (ulong a,   ulong b)
{
    return  (a>b) ? ULONG_MAX : 0ul;
}

INLINE(   long,   LONG_CGTS)    (long a,    long b) {return  0l-(a>b);}

INLINE( ullong, ULLONG_CGTS)  (ullong a,  ullong b)
{
    return  (a>b) ? ULLONG_MAX : 0ull;
}

INLINE(  llong,  LLONG_CGTS)   (llong a,   llong b) {return  0ll-(a>b);}

INLINE(int16_t,  FLT16_CGTS) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcgth_f16(a, b);
#else
    return  0-(a>b);
#endif
}

INLINE(int32_t,    FLT_CGTS)   (float a,   float b) {return  vcgts_f32(a, b);}
INLINE(int64_t,    DBL_CGTS)  (double a,  double b) {return  vcgtd_f64(a, b);}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cgtsqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (QUAD_UTYPE) 0-(a>b);
}

INLINE(QUAD_ITYPE,cgtsqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (QUAD_ITYPE) 0-(a>b);
}

INLINE(QUAD_ITYPE,cgtsqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (QUAD_ITYPE) 0-(a>b);
}

#endif

#define     DBU_CGTS                             vcgt_u8
#define     DBI_CGTS(A, B)  vreinterpret_s16_u16(vcgt_s8(A,B))
#define     DBC_CGTS        VDBC_BASE(CGTS)
#define     DHU_CGTS                             vcgt_u16
#define     DHI_CGTS(A, B)  vreinterpret_s16_u16(vcgt_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CGTS(A, B)  vreinterpret_s16_u16(vcgt_f16(A,B))
#else
INLINE(float16x4_t,DHF_CGTS) (float16x4_t a, float16x4_t b)
{
#   define  DHF_CGTS    DHF_CGTS
    MY_NOT_IMPLEMENTED(0, __func__);
    return  DHF_VOID;
}
#endif
#define     DWU_CGTS                             vcgt_u32
#define     DWI_CGTS(A, B)  vreinterpret_s32_u32(vcgt_s32(A,B))
#define     DWF_CGTS(A, B)  vreinterpret_s32_u32(vcgt_f32(A,B))
#define     DDU_CGTS                             vcgt_u64
#define     DDI_CGTS(A, B)  vreinterpret_s64_u64(vcgt_s32(A,B))
#define     DDF_CGTS(A, B)  vreinterpret_s64_u64(vcgt_f32(A,B))


#define     QBU_CGTS                              vcgtq_u8
#define     QBI_CGTS(A, B)  vreinterpretq_s16_u16(vcgtq_s8(A,B))
#define     QBC_CGTS        VQBC_BASE(CGTS)
#define     QHU_CGTS                              vcgtq_u16
#define     QHI_CGTS(A, B)  vreinterpretq_s16_u16(vcgtq_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CGTS(A, B)  vreinterpretq_s16_u16(vcgtq_f16(A,B))
#else
INLINE(float16x8_t,QHF_CGTS) (float16x8_t a, float16x8_t b)
{
#   define  QHF_CGTS    QHF_CGTS
    MY_NOT_IMPLEMENTED(0, __func__);
    return  QHF_VOID;
}
#endif

#define     QWU_CGTS                              vcgtq_u32
#define     QWI_CGTS(A, B)  vreinterpretq_s32_u32(vcgtq_s32(A,B))
#define     QWF_CGTS(A, B)  vreinterpretq_s32_u32(vcgtq_f32(A,B))
#define     QDU_CGTS                              vcgtq_u64
#define     QDI_CGTS(A, B)  vreinterpretq_s64_u64(vcgtq_s32(A,B))
#define     QDF_CGTS(A, B)  vreinterpretq_s64_u64(vcgtq_f32(A,B))


INLINE(Vwbu,VWBU_CGTS) (Vwbu a, Vwbu b)
{
    float32x2_t l = vset_lane_f32(VWBU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBU_ASTM(a), r, V2_K0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CGTS(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbi,VWBI_CGTS) (Vwbi a, Vwbi b)
{
    float32x2_t l = vset_lane_f32(VWBI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBI_ASTM(a), r, V2_K0);
    int8x8_t    p = vreinterpret_s8_f32(l);
    int8x8_t    q = vreinterpret_s8_f32(r);
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(vcgt_s8(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbc,VWBC_CGTS) (Vwbc a, Vwbc b)
{
    float32x2_t x = vset_lane_f32(VWBC_ASTM(a), x, V2_K0);
    float32x2_t y = vset_lane_f32(VWBC_ASTM(a), y, V2_K0);
    float32x2_t r;
#if CHAR_MIN
    int8x8_t    p = vreinterpret_s8_f32(x);
    int8x8_t    q = vreinterpret_s8_f32(y);
    r = vreinterpret_f32_u8(vcgt_s8(p, q));
#else
    uint8x8_t   p = vreinterpret_u8_f32(x);
    uint8x8_t   q = vreinterpret_u8_f32(y);
    r = vreinterpret_f32_u8(vcgt_u8(p, q));
#endif
    return  WBC_ASTV(vget_lane_f32(r, V2_K0));
}


INLINE(Vwhu,VWHU_CGTS) (Vwhu a, Vwhu b)
{
    float32x2_t l = vset_lane_f32(VWHU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHU_ASTM(a), r, V2_K0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcgt_u16(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwhi,VWHI_CGTS) (Vwhi a, Vwhi b)
{
    float32x2_t l = vset_lane_f32(VWHI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHI_ASTM(a), r, V2_K0);
    int16x4_t   p = vreinterpret_s16_f32(l);
    int16x4_t   q = vreinterpret_s16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcgt_s16(p, q)),
            V2_K0
        )
    );
}

#if defined(SPC_ARM_FP16_SIMD)
INLINE(Vwhi,VWHF_CGTS) (Vwhf a, Vwhf b)
{
#   define  VWHF_CGTS VWHF_CGTS
    float32x2_t l = vset_lane_f32(VWHF_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHF_ASTM(a), r, V2_K0);
    float16x4_t p = vreinterpret_f16_f32(l);
    float16x4_t q = vreinterpret_f16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcgt_f16(p, q)),
            V2_K0
        )
    );
}
#else
INLINE(Vwhf,VWHF_CGTS) (Vwhf a, Vwhf b)
{
#   define  VWHF_CGTS    VWHF_CGTS
    MY_NOT_IMPLEMENTED(0, __func__);
    return  VWHF_VOID;
}
#endif


INLINE(Vwwu,VWWU_CGTS) (Vwwu a, Vwwu b)
{
    float32x2_t l = vset_lane_f32(VWWU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWU_ASTM(a), r, V2_K0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcgt_u32(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWI_CGTS) (Vwwi a, Vwwi b)
{
    float32x2_t l = vset_lane_f32(VWWI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWI_ASTM(a), r, V2_K0);
    int32x2_t   p = vreinterpret_s32_f32(l);
    int32x2_t   q = vreinterpret_s32_f32(r);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcgt_s32(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWF_CGTS) (Vwwf a, Vwwf b)
{
    float32x2_t p = vset_lane_f32(VWWF_ASTM(a), p, V2_K0);
    float32x2_t q = vset_lane_f32(VWWF_ASTM(a), q, V2_K0);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcgt_f32(p, q)),
            V2_K0
        )
    );
}


INLINE(Vdbu,VDBU_CGTS) (Vdbu a, Vdbu b) {return DBU_CGTS(a, b);}
INLINE(Vdbi,VDBI_CGTS) (Vdbi a, Vdbi b) {return DBI_CGTS(a, b);}
INLINE(Vdbc,VDBC_CGTS) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(DBC_CGTS(VDBC_ASTM(b), VDBC_ASTM(b)));
}

INLINE(Vdhu,VDHU_CGTS) (Vdhu a, Vdhu b) {return DHU_CGTS(a, b);}
INLINE(Vdhi,VDHI_CGTS) (Vdhi a, Vdhi b) {return DHI_CGTS(a, b);}
INLINE(Vdhi,VDHF_CGTS) (Vdhf a, Vdhf b) {return DHF_CGTS(a, b);}
INLINE(Vdwu,VDWU_CGTS) (Vdwu a, Vdwu b) {return DWU_CGTS(a, b);}
INLINE(Vdwi,VDWI_CGTS) (Vdwi a, Vdwi b) {return DWI_CGTS(a, b);}
INLINE(Vdwi,VDWF_CGTS) (Vdwf a, Vdwf b) {return DWF_CGTS(a, b);}
INLINE(Vddu,VDDU_CGTS) (Vddu a, Vddu b) {return DDU_CGTS(a, b);}
INLINE(Vddi,VDDI_CGTS) (Vddi a, Vddi b) {return DDI_CGTS(a, b);}
INLINE(Vddi,VDDF_CGTS) (Vddf a, Vddf b) {return DDF_CGTS(a, b);}


INLINE(Vqbu,VQBU_CGTS) (Vqbu a, Vqbu b) {return QBU_CGTS(a, b);}
INLINE(Vqbi,VQBI_CGTS) (Vqbi a, Vqbi b) {return QBI_CGTS(a, b);}
INLINE(Vqbc,VQBC_CGTS) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CGTS(VQBC_ASTM(b), VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CGTS) (Vqhu a, Vqhu b) {return QHU_CGTS(a, b);}
INLINE(Vqhi,VQHI_CGTS) (Vqhi a, Vqhi b) {return QHI_CGTS(a, b);}
INLINE(Vqhi,VQHF_CGTS) (Vqhf a, Vqhf b) {return QHF_CGTS(a, b);}
INLINE(Vqwu,VQWU_CGTS) (Vqwu a, Vqwu b) {return QWU_CGTS(a, b);}
INLINE(Vqwi,VQWI_CGTS) (Vqwi a, Vqwi b) {return QWI_CGTS(a, b);}
INLINE(Vqwi,VQWF_CGTS) (Vqwf a, Vqwf b) {return QWF_CGTS(a, b);}
INLINE(Vqdu,VQDU_CGTS) (Vqdu a, Vqdu b) {return QDU_CGTS(a, b);}
INLINE(Vqdi,VQDI_CGTS) (Vqdi a, Vqdi b) {return QDI_CGTS(a, b);}
INLINE(Vqdi,VQDF_CGTS) (Vqdf a, Vqdf b) {return QDF_CGTS(a, b);}


#if _LEAVE_ARM_CGTS
}
#endif

#if _ENTER_ARM_CGTY
{
#endif

INLINE(ptrdiff_t, ADDR_CGTY) (void volatile const *a, void volatile const *b)
{
    return  a>b;
}

INLINE(  _Bool,  BOOL_CGTY)   (_Bool a,   _Bool b) {return a>b;}
INLINE(  uchar, UCHAR_CGTY)   (uchar a,   uchar b) {return a>b;}
INLINE(  schar, SCHAR_CGTY)   (schar a,   schar b) {return a>b;}
INLINE(   char,  CHAR_CGTY)    (char a,    char b) {return a>b;}
INLINE( ushort, USHRT_CGTY)  (ushort a,  ushort b) {return a>b;}
INLINE(  short,  SHRT_CGTY)   (short a,   short b) {return a>b;}
INLINE(   uint,  UINT_CGTY)    (uint a,    uint b) {return a>b;}
INLINE(    int,   INT_CGTY)     (int a,     int b) {return a>b;}
INLINE(  ulong, ULONG_CGTY)   (ulong a,   ulong b) {return a>b;}
INLINE(   long,  LONG_CGTY)    (long a,    long b) {return a>b;}
INLINE( ullong,ULLONG_CGTY)  (ullong a,  ullong b) {return a>b;}
INLINE(  llong, LLONG_CGTY)   (llong a,   llong b) {return a>b;}
INLINE(int16_t, FLT16_CGTY) (flt16_t a, flt16_t b) {return a>b;}
INLINE(int32_t,   FLT_CGTY)   (float a,   float b) {return a>b;}
INLINE(int64_t,   DBL_CGTY)  (double a,  double b) {return a>b;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cgtyqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (a==b);
}

INLINE(QUAD_ITYPE,cgtyqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (a==b);
}

INLINE(QUAD_ITYPE,cgtyqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (a==b);
}

#endif
INLINE( Wbu,WBU_CGTY) (Wbu a, Wbu b)
{
#define     WBU_CGTY    WBU_CGTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    uint8x8_t   r = vcgt_u8(
        vreinterpret_u8_f32(p),
        vreinterpret_u8_f32(q)
    );
    return  vget_lane_f32(
        vreinterpret_f32_u8(vshr_n_u8(r, 7)),
        V2_K0
    );
}

INLINE( Wbu,WBI_CGTY) (Wbi a, Wbi b)
{
#define     WBI_CGTY    WBI_CGTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vshr_n_u8(
                vcgt_s8(
                    vreinterpret_s8_f32(p),
                    vreinterpret_s8_f32(q)
                ),
                7
            )
        ),
        V2_K0
    );
}

#if CHAR_MIN
#   define  WBC_CGTY    WBI_CGTY
#else
#   define  WBC_CGTY    WBU_CGTY
#endif

INLINE( Whu,WHU_CGTY) (Whu a, Whu b)
{
#define     WHU_CGTY    WHU_CGTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcgt_u16(
                    vreinterpret_u16_f32(p),
                    vreinterpret_u16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
}

INLINE( Whu,WHI_CGTY) (Whi a, Whi b)
{
#define     WHI_CGTY    WHI_CGTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcgt_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
}

INLINE( Whu,WHF_CGTY) (Whf a, Whf b)
{
#define     WHF_CGTY    WHF_CGTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcgt_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
#else
    float32x4_t l = vcvt_f32_f16(vreinterpret_f16_f32(p));
    float32x4_t r = vcvt_f32_f16(vreinterpret_f16_f32(q));
    uint16x4_t  v = vshr_n_u16(vmovn_u32(vcgtq_f32(l, r)), 15);
    return  vget_lane_f32(vreinterpret_f32_u16(v), V2_K0);
#endif
}

INLINE( Wwu,WWU_CGTY) (Wwu a, Wwu b)
{
#define     WWU_CGTY    WWU_CGTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcgt_u32(
                    vreinterpret_u32_f32(p),
                    vreinterpret_u32_f32(q)
                ),
                31
            )
        ),
        V2_K0
    );
}

INLINE( Wwu,WWI_CGTY) (Wwi a, Wwi b)
{
#define     WWI_CGTY    WWI_CGTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcgt_s32(
                    vreinterpret_s32_f32(p),
                    vreinterpret_s32_f32(q)
                ),
                31
            )
        ),
        V2_K0
    );
}

INLINE( Wwu,WWF_CGTY) (Wwf a, Wwf b)
{
#define     WWF_CGTY    WWF_CGTY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(vshr_n_u32(vcgt_f32(p, q), 31)),
        V2_K0
    );
}

#define     DBU_CGTY(A, B)            vshr_n_u8(vcgt_u8(A,B),7)
#define     DBI_CGTY(A, B)  VDBU_ASTI(vshr_n_u8(vcgt_s8(A,B),7))
#if CHAR_MIN
#   define  DBC_CGTY        DBI_CGTY
#else
#   define  DBC_CGTY        DBU_CGTY
#endif

#define     DHU_CGTY(A, B)            vshr_n_u16(vcgt_u16(A,B),15)
#define     DHI_CGTY(A, B)  VDHU_ASTI(vshr_n_u16(vcgt_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CGTY(A, B)  VDHU_ASTI(vshr_n_u16(vcgt_f16(A,B),15))
#else
#   define  DHF_CGTY(A, B)      \
VDHU_ASHI(                      \
    vshr_n_u16(                 \
        vmovn_u32(              \
            vcgtq_f32(          \
                vcvt_f32_f16(a),\
                vcvt_f32_f16(b) \
            )                   \
        ),                      \
        15                      \
    )                           \
)
#endif

#define     DWU_CGTY(A, B)            vshr_n_u32(vcgt_u32(A,B),31)
#define     DWI_CGTY(A, B)  VDWU_ASTI(vshr_n_u32(vcgt_s32(A,B),31))
#define     DWF_CGTY(A, B)  VDWU_ASTI(vshr_n_u32(vcgt_f32(A,B),31))

#define     DDU_CGTY(A, B)            vshr_n_u64(vcgt_u64(A,B),63)
#define     DDI_CGTY(A, B)  VDDU_ASTI(vshr_n_u64(vcgt_s64(A,B),63))
#define     DDF_CGTY(A, B)  VDDU_ASTI(vshr_n_u64(vcgt_f64(A,B),63))


#define     QBU_CGTY(A, B)            vshrq_n_u8(vcgtq_u8(A,B),7)
#define     QBI_CGTY(A, B)  VQBU_ASTI(vshrq_n_u8(vcgtq_s8(A,B),7))
#if CHAR_MIN
#   define  QBC_CGTY        QBI_CGTY
#else
#   define  QBC_CGTY        QBU_CGTY
#endif

#define     QHU_CGTY(A, B)            vshrq_n_u16(vcgtq_u16(A,B),15)
#define     QHI_CGTY(A, B)  VQHU_ASTI(vshrq_n_u16(vcgtq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CGTY(A, B)  VQHU_ASTI(vshrq_n_u16(vcgtq_f16(A,B),15))
#else
#   define  QHF_CGTY(A, B)                          \
vreinterpretq_s16_u16(                              \
    vshrq_n_u16(                                    \
        vcombine_u16(                               \
            vmovn_u32(                              \
                vcgtq_f32(                          \
                    vcvt_f32_f16(vget_low_f16(A)),  \
                    vcvt_f32_f16(vget_low_f16(B))   \
                )                                   \
            ),                                      \
            vmovn_u32(                              \
                vcgtq_f32(                          \
                    vcvt_f32_f16(vget_high_f16(A)), \
                    vcvt_f32_f16(vget_high_f16(B))  \
                )                                   \
            )                                       \
        ),                                          \
        15                                          \
    )                                               \
)
#endif

#define     QWU_CGTY(A, B)            vshrq_n_u32(vcgtq_u32(A,B),31)
#define     QWI_CGTY(A, B)  VQWU_ASTI(vshrq_n_u32(vcgtq_s32(A,B),31))
#define     QWF_CGTY(A, B)  VQWU_ASTI(vshrq_n_u32(vcgtq_f32(A,B),31))

#define     QDU_CGTY(A, B)            vshrq_n_u64(vcgtq_u64(A,B),63)
#define     QDI_CGTY(A, B)  VQDU_ASTI(vshrq_n_u64(vcgtq_s64(A,B),63))
#define     QDF_CGTY(A, B)  VQDU_ASTI(vshrq_n_u64(vcgtq_f64(A,B),63))


INLINE(Vwbu,VWBU_CGTY) (Vwbu a, Vwbu b)
{
#define     VWBU_CGTY(A, B) WBU_ASTV(WBU_CGTY(VWBU_ASTM(A),VWBU_ASTM(B)))
    return  VWBU_CGTY(a, b);
}

INLINE(Vwbi,VWBI_CGTY) (Vwbi a, Vwbi b)
{
#define     VWBI_CGTY(A, B) WBI_ASTV(WBI_CGTY(VWBI_ASTM(A),VWBI_ASTM(B)))
    return  VWBI_CGTY(a, b);
}

INLINE(Vwbc,VWBC_CGTY) (Vwbc a, Vwbc b)
{
#define     VWBC_CGTY(A, B) WBC_ASTV(WBC_CGTY(VWBC_ASTM(A),VWBC_ASTM(B)))
    return  VWBC_CGTY(a, b);
}


INLINE(Vwhu,VWHU_CGTY) (Vwhu a, Vwhu b)
{
#define     VWHU_CGTY(A, B) WHU_ASTV(WHU_CGTY(VWHU_ASTM(A),VWHU_ASTM(B)))
    return  VWHU_CGTY(a, b);
}

INLINE(Vwhi,VWHI_CGTY) (Vwhi a, Vwhi b)
{
#define     VWHI_CGTY(A, B) WHI_ASTV(WHI_CGTY(VWHI_ASTM(A),VWHI_ASTM(B)))
    return  VWHI_CGTY(a, b);
}

INLINE(Vwhi,VWHF_CGTY) (Vwhf a, Vwhf b)
{
#define     VWHF_CGTY(A, B) WHI_ASTV(WHF_CGTY(VWHF_ASTM(A),VWHF_ASTM(B)))
    return  VWHF_CGTY(a, b);
}


INLINE(Vwwu,VWWU_CGTY) (Vwwu a, Vwwu b)
{
#define     VWWU_CGTY(A, B) WWU_ASTV(WWU_CGTY(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_CGTY(a, b);
}

INLINE(Vwwi,VWWI_CGTY) (Vwwi a, Vwwi b)
{
#define     VWWI_CGTY(A, B) WWI_ASTV(WWI_CGTY(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_CGTY(a, b);
}

INLINE(Vwwi,VWWF_CGTY) (Vwwf a, Vwwf b)
{
#define     VWWF_CGTY(A, B) WWI_ASTV(WWF_CGTY(VWWF_ASTM(A),VWWF_ASTM(B)))
    return  VWWF_CGTY(a, b);
}


INLINE(Vdbu,VDBU_CGTY) (Vdbu a, Vdbu b) {return DBU_CGTY(a, b);}
INLINE(Vdbi,VDBI_CGTY) (Vdbi a, Vdbi b) {return DBI_CGTY(a, b);}
INLINE(Vdbc,VDBC_CGTY) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(
        DBC_CGTY(
            VDBC_ASTM(a),
            VDBC_ASTM(b)
        )
    );
}

INLINE(Vdhu,VDHU_CGTY) (Vdhu a, Vdhu b) {return DHU_CGTY(a, b);}
INLINE(Vdhi,VDHI_CGTY) (Vdhi a, Vdhi b) {return DHI_CGTY(a, b);}
INLINE(Vdhi,VDHF_CGTY) (Vdhf a, Vdhf b) {return DHF_CGTY(a, b);}

INLINE(Vdwu,VDWU_CGTY) (Vdwu a, Vdwu b) {return DWU_CGTY(a, b);}
INLINE(Vdwi,VDWI_CGTY) (Vdwi a, Vdwi b) {return DWI_CGTY(a, b);}
INLINE(Vdwi,VDWF_CGTY) (Vdwf a, Vdwf b) {return DWF_CGTY(a, b);}

INLINE(Vddu,VDDU_CGTY) (Vddu a, Vddu b) {return DDU_CGTY(a, b);}
INLINE(Vddi,VDDI_CGTY) (Vddi a, Vddi b) {return DDI_CGTY(a, b);}
INLINE(Vddi,VDDF_CGTY) (Vddf a, Vddf b) {return DDF_CGTY(a, b);}


INLINE(Vqbu,VQBU_CGTY) (Vqbu a, Vqbu b) {return QBU_CGTY(a,b);}
INLINE(Vqbi,VQBI_CGTY) (Vqbi a, Vqbi b) {return QBI_CGTY(a,b);}
INLINE(Vqbc,VQBC_CGTY) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CGTY(VQBC_ASTM(a),VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CGTY) (Vqhu a, Vqhu b) {return QHU_CGTY(a,b);}
INLINE(Vqhi,VQHI_CGTY) (Vqhi a, Vqhi b) {return QHI_CGTY(a,b);}
INLINE(Vqhi,VQHF_CGTY) (Vqhf a, Vqhf b) {return QHF_CGTY(a,b);}
INLINE(Vqwu,VQWU_CGTY) (Vqwu a, Vqwu b) {return QWU_CGTY(a,b);}
INLINE(Vqwi,VQWI_CGTY) (Vqwi a, Vqwi b) {return QWI_CGTY(a,b);}
INLINE(Vqwi,VQWF_CGTY) (Vqwf a, Vqwf b) {return QWF_CGTY(a,b);}
INLINE(Vqdu,VQDU_CGTY) (Vqdu a, Vqdu b) {return QDU_CGTY(a,b);}
INLINE(Vqdi,VQDI_CGTY) (Vqdi a, Vqdi b) {return QDI_CGTY(a,b);}
INLINE(Vqdi,VQDF_CGTY) (Vqdf a, Vqdf b) {return QDF_CGTY(a,b);}

#if _LEAVE_ARM_CGTY
}
#endif


#if _ENTER_ARM_CGES
{
#endif

INLINE(ptrdiff_t, ADDR_CGES)
(
    void volatile const *a,
    void volatile const *b
)
{
    return (a >= b) ? NULL-((void *) INTPTR_C(1)) : 0;
}

INLINE(  _Bool,   BOOL_CGES)   (_Bool a,   _Bool b) {return a>=b;}
INLINE(  uchar,  UCHAR_CGES)   (uchar a,   uchar b)
{
    return  (a>=b) ? UCHAR_MAX : 0;
}

INLINE(  schar,  SCHAR_CGES)   (schar a,   schar b) {return  0-(a>=b);}
INLINE(   char,   CHAR_CGES)    (char a,    char b)
{
    return  (a>=b) ? '\xff' : '\x00';
}

INLINE( ushort,  USHRT_CGES)  (ushort a,  ushort b)
{
    return  (a>=b) ? USHRT_MAX : 0;
}

INLINE(  short,   SHRT_CGES)   (short a,   short b) {return  0-(a>=b);}

INLINE(   uint,   UINT_CGES)    (uint a,    uint b)
{
    return  (a>=b) ? UINT_MAX : 0u;
}

INLINE(    int,    INT_CGES)     (int a,     int b) {return  0-(a>=b);}

INLINE(  ulong,  ULONG_CGES)   (ulong a,   ulong b)
{
    return  (a>=b) ? ULONG_MAX : 0ul;
}

INLINE(   long,   LONG_CGES)    (long a,    long b) {return  0l-(a>=b);}

INLINE( ullong, ULLONG_CGES)  (ullong a,  ullong b)
{
    return  (a>=b) ? ULLONG_MAX : 0ull;
}

INLINE(  llong,  LLONG_CGES)   (llong a,   llong b) {return  0ll-(a>=b);}

INLINE(int16_t,  FLT16_CGES) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcgeh_f16(a, b);
#else
    return  0-(a>=b);
#endif
}

INLINE(int32_t,    FLT_CGES)   (float a,   float b) {return  vcges_f32(a, b);}
INLINE(int64_t,    DBL_CGES)  (double a,  double b) {return  vcged_f64(a, b);}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cgesqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (QUAD_UTYPE) 0-(a>=b);
}

INLINE(QUAD_ITYPE,cgesqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (QUAD_ITYPE) 0-(a>=b);
}

INLINE(QUAD_ITYPE,cgesqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (QUAD_ITYPE) 0-(a>=b);
}

#endif

#define     DBU_CGES                             vcge_u8
#define     DBI_CGES(A, B)  vreinterpret_s16_u16(vcge_s8(A,B))
#define     DBC_CGES        VDBC_BASE(CGES)
#define     DHU_CGES                             vcge_u16
#define     DHI_CGES(A, B)  vreinterpret_s16_u16(vcge_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CGES(A, B)  vreinterpret_s16_u16(vcge_f16(A,B))
#else
INLINE(float16x4_t,DHF_CGES) (float16x4_t a, float16x4_t b)
{
#   define  DHF_CGES    DHF_CGES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  DHF_VOID;
}
#endif
#define     DWU_CGES                             vcge_u32
#define     DWI_CGES(A, B)  vreinterpret_s32_u32(vcge_s32(A,B))
#define     DWF_CGES(A, B)  vreinterpret_s32_u32(vcge_f32(A,B))
#define     DDU_CGES                             vcge_u64
#define     DDI_CGES(A, B)  vreinterpret_s64_u64(vcge_s32(A,B))
#define     DDF_CGES(A, B)  vreinterpret_s64_u64(vcge_f32(A,B))


#define     QBU_CGES                              vcgeq_u8
#define     QBI_CGES(A, B)  vreinterpretq_s16_u16(vcgeq_s8(A,B))
#define     QBC_CGES        VQBC_BASE(CGES)
#define     QHU_CGES                              vcgeq_u16
#define     QHI_CGES(A, B)  vreinterpretq_s16_u16(vcgeq_s16(A,B))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CGES(A, B)  vreinterpretq_s16_u16(vcgeq_f16(A,B))
#else
INLINE(float16x8_t,QHF_CGES) (float16x8_t a, float16x8_t b)
{
#   define  QHF_CGES    QHF_CGES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  QHF_VOID;
}
#endif

#define     QWU_CGES                              vcgeq_u32
#define     QWI_CGES(A, B)  vreinterpretq_s32_u32(vcgeq_s32(A,B))
#define     QWF_CGES(A, B)  vreinterpretq_s32_u32(vcgeq_f32(A,B))
#define     QDU_CGES                              vcgeq_u64
#define     QDI_CGES(A, B)  vreinterpretq_s64_u64(vcgeq_s32(A,B))
#define     QDF_CGES(A, B)  vreinterpretq_s64_u64(vcgeq_f32(A,B))


INLINE(Vwbu,VWBU_CGES) (Vwbu a, Vwbu b)
{
    float32x2_t l = vset_lane_f32(VWBU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBU_ASTM(a), r, V2_K0);
    uint8x8_t   p = vreinterpret_u8_f32(l);
    uint8x8_t   q = vreinterpret_u8_f32(r);
    return WBU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(DBU_CGES(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbi,VWBI_CGES) (Vwbi a, Vwbi b)
{
    float32x2_t l = vset_lane_f32(VWBI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWBI_ASTM(a), r, V2_K0);
    int8x8_t    p = vreinterpret_s8_f32(l);
    int8x8_t    q = vreinterpret_s8_f32(r);
    return  WBI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u8(vcge_s8(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwbc,VWBC_CGES) (Vwbc a, Vwbc b)
{
    float32x2_t x = vset_lane_f32(VWBC_ASTM(a), x, V2_K0);
    float32x2_t y = vset_lane_f32(VWBC_ASTM(a), y, V2_K0);
    float32x2_t r;
#if CHAR_MIN
    int8x8_t    p = vreinterpret_s8_f32(x);
    int8x8_t    q = vreinterpret_s8_f32(y);
    r = vreinterpret_f32_u8(vcge_s8(p, q));
#else
    uint8x8_t   p = vreinterpret_u8_f32(x);
    uint8x8_t   q = vreinterpret_u8_f32(y);
    r = vreinterpret_f32_u8(vcge_u8(p, q));
#endif
    return  WBC_ASTV(vget_lane_f32(r, V2_K0));
}


INLINE(Vwhu,VWHU_CGES) (Vwhu a, Vwhu b)
{
    float32x2_t l = vset_lane_f32(VWHU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHU_ASTM(a), r, V2_K0);
    uint16x4_t  p = vreinterpret_u16_f32(l);
    uint16x4_t  q = vreinterpret_u16_f32(r);
    return  WHU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcge_u16(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwhi,VWHI_CGES) (Vwhi a, Vwhi b)
{
    float32x2_t l = vset_lane_f32(VWHI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHI_ASTM(a), r, V2_K0);
    int16x4_t   p = vreinterpret_s16_f32(l);
    int16x4_t   q = vreinterpret_s16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcge_s16(p, q)),
            V2_K0
        )
    );
}

#if defined(SPC_ARM_FP16_SIMD)
INLINE(Vwhi,VWHF_CGES) (Vwhf a, Vwhf b)
{
#   define  VWHF_CGES VWHF_CGES
    float32x2_t l = vset_lane_f32(VWHF_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWHF_ASTM(a), r, V2_K0);
    float16x4_t p = vreinterpret_f16_f32(l);
    float16x4_t q = vreinterpret_f16_f32(r);
    return  WHI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcge_f16(p, q)),
            V2_K0
        )
    );
}
#else
INLINE(Vwhf,VWHF_CGES) (Vwhf a, Vwhf b)
{
#   define  VWHF_CGES    VWHF_CGES
    MY_NOT_IMPLEMENTED(0, __func__);
    return  VWHF_VOID;
}
#endif


INLINE(Vwwu,VWWU_CGES) (Vwwu a, Vwwu b)
{
    float32x2_t l = vset_lane_f32(VWWU_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWU_ASTM(a), r, V2_K0);
    uint32x2_t  p = vreinterpret_u32_f32(l);
    uint32x2_t  q = vreinterpret_u32_f32(r);
    return  WWU_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u16(vcge_u32(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWI_CGES) (Vwwi a, Vwwi b)
{
    float32x2_t l = vset_lane_f32(VWWI_ASTM(a), l, V2_K0);
    float32x2_t r = vset_lane_f32(VWWI_ASTM(a), r, V2_K0);
    int32x2_t   p = vreinterpret_s32_f32(l);
    int32x2_t   q = vreinterpret_s32_f32(r);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcge_s32(p, q)),
            V2_K0
        )
    );
}

INLINE(Vwwi,VWWF_CGES) (Vwwf a, Vwwf b)
{
    float32x2_t p = vset_lane_f32(VWWF_ASTM(a), p, V2_K0);
    float32x2_t q = vset_lane_f32(VWWF_ASTM(a), q, V2_K0);
    return WWI_ASTV(
        vget_lane_f32(
            vreinterpret_f32_u32(vcge_f32(p, q)),
            V2_K0
        )
    );
}


INLINE(Vdbu,VDBU_CGES) (Vdbu a, Vdbu b) {return DBU_CGES(a, b);}
INLINE(Vdbi,VDBI_CGES) (Vdbi a, Vdbi b) {return DBI_CGES(a, b);}
INLINE(Vdbc,VDBC_CGES) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(DBC_CGES(VDBC_ASTM(b), VDBC_ASTM(b)));
}

INLINE(Vdhu,VDHU_CGES) (Vdhu a, Vdhu b) {return DHU_CGES(a, b);}
INLINE(Vdhi,VDHI_CGES) (Vdhi a, Vdhi b) {return DHI_CGES(a, b);}
INLINE(Vdhi,VDHF_CGES) (Vdhf a, Vdhf b) {return DHF_CGES(a, b);}
INLINE(Vdwu,VDWU_CGES) (Vdwu a, Vdwu b) {return DWU_CGES(a, b);}
INLINE(Vdwi,VDWI_CGES) (Vdwi a, Vdwi b) {return DWI_CGES(a, b);}
INLINE(Vdwi,VDWF_CGES) (Vdwf a, Vdwf b) {return DWF_CGES(a, b);}
INLINE(Vddu,VDDU_CGES) (Vddu a, Vddu b) {return DDU_CGES(a, b);}
INLINE(Vddi,VDDI_CGES) (Vddi a, Vddi b) {return DDI_CGES(a, b);}
INLINE(Vddi,VDDF_CGES) (Vddf a, Vddf b) {return DDF_CGES(a, b);}


INLINE(Vqbu,VQBU_CGES) (Vqbu a, Vqbu b) {return QBU_CGES(a, b);}
INLINE(Vqbi,VQBI_CGES) (Vqbi a, Vqbi b) {return QBI_CGES(a, b);}
INLINE(Vqbc,VQBC_CGES) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CGES(VQBC_ASTM(b), VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CGES) (Vqhu a, Vqhu b) {return QHU_CGES(a, b);}
INLINE(Vqhi,VQHI_CGES) (Vqhi a, Vqhi b) {return QHI_CGES(a, b);}
INLINE(Vqhi,VQHF_CGES) (Vqhf a, Vqhf b) {return QHF_CGES(a, b);}
INLINE(Vqwu,VQWU_CGES) (Vqwu a, Vqwu b) {return QWU_CGES(a, b);}
INLINE(Vqwi,VQWI_CGES) (Vqwi a, Vqwi b) {return QWI_CGES(a, b);}
INLINE(Vqwi,VQWF_CGES) (Vqwf a, Vqwf b) {return QWF_CGES(a, b);}
INLINE(Vqdu,VQDU_CGES) (Vqdu a, Vqdu b) {return QDU_CGES(a, b);}
INLINE(Vqdi,VQDI_CGES) (Vqdi a, Vqdi b) {return QDI_CGES(a, b);}
INLINE(Vqdi,VQDF_CGES) (Vqdf a, Vqdf b) {return QDF_CGES(a, b);}


#if _LEAVE_ARM_CGES
}
#endif

#if _ENTER_ARM_CGEY
{
#endif

INLINE(ptrdiff_t, ADDR_CGEY) (void volatile const *a, void volatile const *b)
{
    return  a>=b;
}

INLINE(  _Bool,  BOOL_CGEY)   (_Bool a,   _Bool b) {return a>=b;}
INLINE(  uchar, UCHAR_CGEY)   (uchar a,   uchar b) {return a>=b;}
INLINE(  schar, SCHAR_CGEY)   (schar a,   schar b) {return a>=b;}
INLINE(   char,  CHAR_CGEY)    (char a,    char b) {return a>=b;}
INLINE( ushort, USHRT_CGEY)  (ushort a,  ushort b) {return a>=b;}
INLINE(  short,  SHRT_CGEY)   (short a,   short b) {return a>=b;}
INLINE(   uint,  UINT_CGEY)    (uint a,    uint b) {return a>=b;}
INLINE(    int,   INT_CGEY)     (int a,     int b) {return a>=b;}
INLINE(  ulong, ULONG_CGEY)   (ulong a,   ulong b) {return a>=b;}
INLINE(   long,  LONG_CGEY)    (long a,    long b) {return a>=b;}
INLINE( ullong,ULLONG_CGEY)  (ullong a,  ullong b) {return a>=b;}
INLINE(  llong, LLONG_CGEY)   (llong a,   llong b) {return a>=b;}
INLINE(int16_t, FLT16_CGEY) (flt16_t a, flt16_t b) {return a>=b;}
INLINE(int32_t,   FLT_CGEY)   (float a,   float b) {return a>=b;}
INLINE(int64_t,   DBL_CGEY)  (double a,  double b) {return a>=b;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cgeyqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return  (a==b);
}

INLINE(QUAD_ITYPE,cgeyqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return  (a==b);
}

INLINE(QUAD_ITYPE,cgeyqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return  (a==b);
}

#endif
INLINE( Wbu,WBU_CGEY) (Wbu a, Wbu b)
{
#define     WBU_CGEY    WBU_CGEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    uint8x8_t   r = vcge_u8(
        vreinterpret_u8_f32(p),
        vreinterpret_u8_f32(q)
    );
    return  vget_lane_f32(
        vreinterpret_f32_u8(vshr_n_u8(r, 7)),
        V2_K0
    );
}

INLINE( Wbu,WBI_CGEY) (Wbi a, Wbi b)
{
#define     WBI_CGEY    WBI_CGEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u8(
            vshr_n_u8(
                vcge_s8(
                    vreinterpret_s8_f32(p),
                    vreinterpret_s8_f32(q)
                ),
                7
            )
        ),
        V2_K0
    );
}

#if CHAR_MIN
#   define  WBC_CGEY    WBI_CGEY
#else
#   define  WBC_CGEY    WBU_CGEY
#endif

INLINE( Whu,WHU_CGEY) (Whu a, Whu b)
{
#define     WHU_CGEY    WHU_CGEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcge_u16(
                    vreinterpret_u16_f32(p),
                    vreinterpret_u16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
}

INLINE( Whu,WHI_CGEY) (Whi a, Whi b)
{
#define     WHI_CGEY    WHI_CGEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcge_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
}

INLINE( Whu,WHF_CGEY) (Whf a, Whf b)
{
#define     WHF_CGEY    WHF_CGEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
#if defined(SPC_ARM_FP16_SIMD)
    return  vget_lane_f32(
        vreinterpret_f32_u16(
            vshr_n_u16(
                vcge_s16(
                    vreinterpret_s16_f32(p),
                    vreinterpret_s16_f32(q)
                ),
                15
            )
        ),
        V2_K0
    );
#else
    float32x4_t l = vcvt_f32_f16(vreinterpret_f16_f32(p));
    float32x4_t r = vcvt_f32_f16(vreinterpret_f16_f32(q));
    uint16x4_t  v = vshr_n_u16(vmovn_u32(vcgeq_f32(l, r)), 15);
    return  vget_lane_f32(vreinterpret_f32_u16(v), V2_K0);
#endif
}

INLINE( Wwu,WWU_CGEY) (Wwu a, Wwu b)
{
#define     WWU_CGEY    WWU_CGEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcge_u32(
                    vreinterpret_u32_f32(p),
                    vreinterpret_u32_f32(q)
                ),
                31
            )
        ),
        V2_K0
    );
}

INLINE( Wwu,WWI_CGEY) (Wwi a, Wwi b)
{
#define     WWI_CGEY    WWI_CGEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(
            vshr_n_u32(
                vcge_s32(
                    vreinterpret_s32_f32(p),
                    vreinterpret_s32_f32(q)
                ),
                31
            )
        ),
        V2_K0
    );
}

INLINE( Wwu,WWF_CGEY) (Wwf a, Wwf b)
{
#define     WWF_CGEY    WWF_CGEY
    float32x2_t p = vset_lane_f32(a, p, V2_K0);
    float32x2_t q = vset_lane_f32(b, q, V2_K0);
    return  vget_lane_f32(
        vreinterpret_f32_u32(vshr_n_u32(vcge_f32(p, q), 31)),
        V2_K0
    );
}

#define     DBU_CGEY(A, B)            vshr_n_u8(vcge_u8(A,B),7)
#define     DBI_CGEY(A, B)  VDBU_ASTI(vshr_n_u8(vcge_s8(A,B),7))
#if CHAR_MIN
#   define  DBC_CGEY        DBI_CGEY
#else
#   define  DBC_CGEY        DBU_CGEY
#endif

#define     DHU_CGEY(A, B)            vshr_n_u16(vcge_u16(A,B),15)
#define     DHI_CGEY(A, B)  VDHU_ASTI(vshr_n_u16(vcge_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_CGEY(A, B)  VDHU_ASTI(vshr_n_u16(vcge_f16(A,B),15))
#else
#   define  DHF_CGEY(A, B)      \
VDHU_ASHI(                      \
    vshr_n_u16(                 \
        vmovn_u32(              \
            vcgeq_f32(          \
                vcvt_f32_f16(a),\
                vcvt_f32_f16(b) \
            )                   \
        ),                      \
        15                      \
    )                           \
)
#endif

#define     DWU_CGEY(A, B)            vshr_n_u32(vcge_u32(A,B),31)
#define     DWI_CGEY(A, B)  VDWU_ASTI(vshr_n_u32(vcge_s32(A,B),31))
#define     DWF_CGEY(A, B)  VDWU_ASTI(vshr_n_u32(vcge_f32(A,B),31))

#define     DDU_CGEY(A, B)            vshr_n_u64(vcge_u64(A,B),63)
#define     DDI_CGEY(A, B)  VDDU_ASTI(vshr_n_u64(vcge_s64(A,B),63))
#define     DDF_CGEY(A, B)  VDDU_ASTI(vshr_n_u64(vcge_f64(A,B),63))


#define     QBU_CGEY(A, B)            vshrq_n_u8(vcgeq_u8(A,B),7)
#define     QBI_CGEY(A, B)  VQBU_ASTI(vshrq_n_u8(vcgeq_s8(A,B),7))
#if CHAR_MIN
#   define  QBC_CGEY        QBI_CGEY
#else
#   define  QBC_CGEY        QBU_CGEY
#endif

#define     QHU_CGEY(A, B)            vshrq_n_u16(vcgeq_u16(A,B),15)
#define     QHI_CGEY(A, B)  VQHU_ASTI(vshrq_n_u16(vcgeq_s16(A,B),15))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_CGEY(A, B)  VQHU_ASTI(vshrq_n_u16(vcgeq_f16(A,B),15))
#else
#   define  QHF_CGEY(A, B)                          \
vreinterpretq_s16_u16(                              \
    vshrq_n_u16(                                    \
        vcombine_u16(                               \
            vmovn_u32(                              \
                vcgeq_f32(                          \
                    vcvt_f32_f16(vget_low_f16(A)),  \
                    vcvt_f32_f16(vget_low_f16(B))   \
                )                                   \
            ),                                      \
            vmovn_u32(                              \
                vcgeq_f32(                          \
                    vcvt_f32_f16(vget_high_f16(A)), \
                    vcvt_f32_f16(vget_high_f16(B))  \
                )                                   \
            )                                       \
        ),                                          \
        15                                          \
    )                                               \
)
#endif

#define     QWU_CGEY(A, B)            vshrq_n_u32(vcgeq_u32(A,B),31)
#define     QWI_CGEY(A, B)  VQWU_ASTI(vshrq_n_u32(vcgeq_s32(A,B),31))
#define     QWF_CGEY(A, B)  VQWU_ASTI(vshrq_n_u32(vcgeq_f32(A,B),31))

#define     QDU_CGEY(A, B)            vshrq_n_u64(vcgeq_u64(A,B),63)
#define     QDI_CGEY(A, B)  VQDU_ASTI(vshrq_n_u64(vcgeq_s64(A,B),63))
#define     QDF_CGEY(A, B)  VQDU_ASTI(vshrq_n_u64(vcgeq_f64(A,B),63))


INLINE(Vwbu,VWBU_CGEY) (Vwbu a, Vwbu b)
{
#define     VWBU_CGEY(A, B) WBU_ASTV(WBU_CGEY(VWBU_ASTM(A),VWBU_ASTM(B)))
    return  VWBU_CGEY(a, b);
}

INLINE(Vwbi,VWBI_CGEY) (Vwbi a, Vwbi b)
{
#define     VWBI_CGEY(A, B) WBI_ASTV(WBI_CGEY(VWBI_ASTM(A),VWBI_ASTM(B)))
    return  VWBI_CGEY(a, b);
}

INLINE(Vwbc,VWBC_CGEY) (Vwbc a, Vwbc b)
{
#define     VWBC_CGEY(A, B) WBC_ASTV(WBC_CGEY(VWBC_ASTM(A),VWBC_ASTM(B)))
    return  VWBC_CGEY(a, b);
}


INLINE(Vwhu,VWHU_CGEY) (Vwhu a, Vwhu b)
{
#define     VWHU_CGEY(A, B) WHU_ASTV(WHU_CGEY(VWHU_ASTM(A),VWHU_ASTM(B)))
    return  VWHU_CGEY(a, b);
}

INLINE(Vwhi,VWHI_CGEY) (Vwhi a, Vwhi b)
{
#define     VWHI_CGEY(A, B) WHI_ASTV(WHI_CGEY(VWHI_ASTM(A),VWHI_ASTM(B)))
    return  VWHI_CGEY(a, b);
}

INLINE(Vwhi,VWHF_CGEY) (Vwhf a, Vwhf b)
{
#define     VWHF_CGEY(A, B) WHI_ASTV(WHF_CGEY(VWHF_ASTM(A),VWHF_ASTM(B)))
    return  VWHF_CGEY(a, b);
}


INLINE(Vwwu,VWWU_CGEY) (Vwwu a, Vwwu b)
{
#define     VWWU_CGEY(A, B) WWU_ASTV(WWU_CGEY(VWWU_ASTM(A),VWWU_ASTM(B)))
    return  VWWU_CGEY(a, b);
}

INLINE(Vwwi,VWWI_CGEY) (Vwwi a, Vwwi b)
{
#define     VWWI_CGEY(A, B) WWI_ASTV(WWI_CGEY(VWWI_ASTM(A),VWWI_ASTM(B)))
    return  VWWI_CGEY(a, b);
}

INLINE(Vwwi,VWWF_CGEY) (Vwwf a, Vwwf b)
{
#define     VWWF_CGEY(A, B) WWI_ASTV(WWF_CGEY(VWWF_ASTM(A),VWWF_ASTM(B)))
    return  VWWF_CGEY(a, b);
}


INLINE(Vdbu,VDBU_CGEY) (Vdbu a, Vdbu b) {return DBU_CGEY(a, b);}
INLINE(Vdbi,VDBI_CGEY) (Vdbi a, Vdbi b) {return DBI_CGEY(a, b);}
INLINE(Vdbc,VDBC_CGEY) (Vdbc a, Vdbc b)
{
    return  DBC_ASTV(
        DBC_CGEY(
            VDBC_ASTM(a),
            VDBC_ASTM(b)
        )
    );
}

INLINE(Vdhu,VDHU_CGEY) (Vdhu a, Vdhu b) {return DHU_CGEY(a, b);}
INLINE(Vdhi,VDHI_CGEY) (Vdhi a, Vdhi b) {return DHI_CGEY(a, b);}
INLINE(Vdhi,VDHF_CGEY) (Vdhf a, Vdhf b) {return DHF_CGEY(a, b);}

INLINE(Vdwu,VDWU_CGEY) (Vdwu a, Vdwu b) {return DWU_CGEY(a, b);}
INLINE(Vdwi,VDWI_CGEY) (Vdwi a, Vdwi b) {return DWI_CGEY(a, b);}
INLINE(Vdwi,VDWF_CGEY) (Vdwf a, Vdwf b) {return DWF_CGEY(a, b);}

INLINE(Vddu,VDDU_CGEY) (Vddu a, Vddu b) {return DDU_CGEY(a, b);}
INLINE(Vddi,VDDI_CGEY) (Vddi a, Vddi b) {return DDI_CGEY(a, b);}
INLINE(Vddi,VDDF_CGEY) (Vddf a, Vddf b) {return DDF_CGEY(a, b);}


INLINE(Vqbu,VQBU_CGEY) (Vqbu a, Vqbu b) {return QBU_CGEY(a,b);}
INLINE(Vqbi,VQBI_CGEY) (Vqbi a, Vqbi b) {return QBI_CGEY(a,b);}
INLINE(Vqbc,VQBC_CGEY) (Vqbc a, Vqbc b)
{
    return  QBC_ASTV(QBC_CGEY(VQBC_ASTM(a),VQBC_ASTM(b)));
}

INLINE(Vqhu,VQHU_CGEY) (Vqhu a, Vqhu b) {return QHU_CGEY(a,b);}
INLINE(Vqhi,VQHI_CGEY) (Vqhi a, Vqhi b) {return QHI_CGEY(a,b);}
INLINE(Vqhi,VQHF_CGEY) (Vqhf a, Vqhf b) {return QHF_CGEY(a,b);}
INLINE(Vqwu,VQWU_CGEY) (Vqwu a, Vqwu b) {return QWU_CGEY(a,b);}
INLINE(Vqwi,VQWI_CGEY) (Vqwi a, Vqwi b) {return QWI_CGEY(a,b);}
INLINE(Vqwi,VQWF_CGEY) (Vqwf a, Vqwf b) {return QWF_CGEY(a,b);}
INLINE(Vqdu,VQDU_CGEY) (Vqdu a, Vqdu b) {return QDU_CGEY(a,b);}
INLINE(Vqdi,VQDI_CGEY) (Vqdi a, Vqdi b) {return QDI_CGEY(a,b);}
INLINE(Vqdi,VQDF_CGEY) (Vqdf a, Vqdf b) {return QDF_CGEY(a,b);}

#if _LEAVE_ARM_CGEY
}
#endif


#if _ENTER_ARM_ZEQY
{
#endif

INLINE(ptrdiff_t,ADDR_ZEQY) (void volatile const *a)
{
    return  (NULL==a);
}

INLINE(  _Bool,  BOOL_ZEQY)   (_Bool a) {return !a;}
INLINE(  uchar, UCHAR_ZEQY)   (uchar a) {return !a;}
INLINE(  schar, SCHAR_ZEQY)   (schar a) {return !a;}
INLINE(   char,  CHAR_ZEQY)    (char a) {return !a;}
INLINE( ushort, USHRT_ZEQY)  (ushort a) {return !a;}
INLINE(  short,  SHRT_ZEQY)   (short a) {return !a;}
INLINE(   uint,  UINT_ZEQY)    (uint a) {return !a;}
INLINE(    int,   INT_ZEQY)     (int a) {return !a;}
INLINE(  ulong, ULONG_ZEQY)   (ulong a) {return !a;}
INLINE(   long,  LONG_ZEQY)    (long a) {return !a;}
INLINE( ullong,ULLONG_ZEQY)  (ullong a) {return !a;}
INLINE(  llong, LLONG_ZEQY)   (llong a) {return !a;}
INLINE(int16_t, FLT16_ZEQY) (flt16_t a) {return !a;}
INLINE(int32_t,   FLT_ZEQY)   (float a) {return !a;}
INLINE(int64_t,   DBL_ZEQY)  (double a) {return !a;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,zeqyqu) (QUAD_UTYPE a)
{
    return  (QUAD_UTYPE) (!a);
}

INLINE(QUAD_ITYPE,zeqyqi) (QUAD_ITYPE a)
{
    return  (QUAD_ITYPE) (!a);
}

INLINE(QUAD_UTYPE,zeqyqf) (QUAD_FTYPE a)
{
    return  (QUAD_ITYPE) (!a);
}

#endif

#define     DYU_ZEQY(A) vreinterpret_u64_u8(vmvn_u8(vreinterpret_u8_u64(A)))
  
#define     DBU_ZEQY(A) vand_u8(vceqz_u8(A),vdup_n_u8(1))
#define     DBI_ZEQY(A) \
vreinterpret_s8_u8(vand_u8(vceqz_s8(A),vdup_n_u8(1)))

#if CHAR_MIN
#   define  DBC_ZEQY    DBI_ZEQY
#else
#   define  DBC_ZEQY    DBU_ZEQY
#endif

#define     DHU_ZEQY(A) vand_u16(vceqz_u16(A),vdup_n_u16(1))
#define     DHI_ZEQY(A) \
vreinterpret_s16_u16(vand_u16(vceqz_s16(A),vdup_n_u16(1)))

#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_ZEQY(A) vand_u16(vceqz_f16(A),vdup_n_u16(1))
#else
#   define  DHF_ZEQY(A)                         \
vreinterpret_s16_u16(                           \
    vand_u16(                                   \
        vorr_u16(                               \
            vceqz_u16(vreinterpret_u16_f16(A)), \
            vceq_u16(                           \
                vreinterpret_u16_f16(A),        \
                vdup_n_u16(0x8000)              \
            )                                   \
        ),                                      \
        vdup_n_u16(1)                           \
    )                                           \
)

#endif

#define     DWU_ZEQY(A) vand_u32(vceqz_u32(A),vdup_n_u32(1))
#define     DWI_ZEQY(A) \
vreinterpret_s32_u32(vand_u32(vceqz_s32(A),vdup_n_u32(1)))

#define     DWF_ZEQY(A) \
vreinterpret_s32_u32(vand_u32(vceqz_f32(A),vdup_n_u32(1)))

#define     DDU_ZEQY(A) vand_u64(vceqz_u64(A),vdup_n_u64(1))
#define     DDI_ZEQY(A) \
vreinterpret_s64_u64(vand_u64(vceqz_s64(A),vdup_n_u64(1)))

#define     DDF_ZEQY(A) \
vreinterpret_s64_u64(vand_u64(vceqz_f64(A),vdup_n_u64(1)))

#define     QYU_ZEQY(A) \
vreinterpretq_u64_u8(vmvnq_u8(vreinterpretq_u8_u64(A)))
  
#define     QBU_ZEQY(A) vandq_u8(vceqzq_u8(A),vdupq_n_u8(1))
#define     QBI_ZEQY(A) \
vreinterpretq_s8_u8(vandq_u8(vceqzq_s8(A),vdupq_n_u8(1)))

#if CHAR_MIN
#   define  QBC_ZEQY    QBI_ZEQY
#else
#   define  QBC_ZEQY    QBU_ZEQY
#endif

#define     QHU_ZEQY(A) vandq_u16(vceqzq_u16(A),vdupq_n_u16(1))
#define     QHI_ZEQY(A) \
vreinterpretq_s16_u16(vandq_u16(vceqzq_s16(A),vdupq_n_u16(1)))

#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_ZEQY(A) \
vreinterpretq_s16_u16(vandq_u16(vceqzq_f16(A),vdupq_n_u16(1)))

#else
#   define  QHF_ZEQY(A)                             \
vreinterpretq_s16_u16(                              \
    vandq_u16(                                      \
        vorrq_u16(                                  \
            vceqzq_u16(vreinterpretq_u16_f16(A)),   \
            vceqq_u16(                              \
                vreinterpretq_u16_f16(A),           \
                vdupq_n_u16(0x8000)                 \
            )                                       \
        ),                                          \
        vdupq_n_u16(1)                              \
    )                                               \
)

#endif

#define     QWU_ZEQY(A) vandq_u32(vceqzq_u32(A),vdupq_n_u32(1))
#define     QWI_ZEQY(A) \
vreinterpretq_s32_u32(vandq_u32(vceqzq_s32(A),vdupq_n_u32(1)))

#define     QWF_ZEQY(A) vandq_u32(vceqzq_f32(A),vdupq_n_u32(1))


#define     QDU_ZEQY(A) vandq_u64(vceqzq_u64(A),vdupq_n_u64(1))
#define     QDI_ZEQY(A) \
vreinterpretq_s64_u64(vandq_u64(vceqzq_s64(A),vdupq_n_u64(1)))

#define     QDF_ZEQY(A) \
vreinterpretq_s64_u64(vandq_u64(vceqzq_f64(A),vdupq_n_u64(1)))

INLINE(Vwyu,VWYU_ZEQY) (Vwyu a)
{
    float       f = VWYU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vmvn_u32(v);
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WYU_ASTV(f);
}

INLINE(Vwbu,VWBU_ZEQY) (Vwbu a)
{
    float       f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_ZEQY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZEQY) (Vwbc a)
{
    float       f = VWBC_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
}


INLINE(Vwhu,VWHU_ZEQY) (Vwhu a)
{
    float       f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vceqz_u16(v);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_ZEQY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vceqz_u16(v);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZEQY) (Vwhf a)
{
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    float16x4_t r = vreinterpret_f16_u16(v);
    v = DHF_ZEQY(v);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_ZEQY) (Vwwu a)
{
    float       f = VWWU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vceqz_u32(v);
    v = vand_u32(v, vdup_n_u32(1));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_ZEQY) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vceqz_u32(v);
    v = vand_u32(v, vdup_n_u32(1));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZEQY) (Vwwf a)
{
    float       f = VWWF_ASTM(a);
    return WWI_ASTV(!f);
}


INLINE(Vdyu,VDYU_ZEQY) (Vdyu a) {return VDBU_ASYU(vmvn_u8(VDYU_ASBU(a)));}
INLINE(Vdbu,VDBU_ZEQY) (Vdbu a) {return DBU_ZEQY(a);}
INLINE(Vdbi,VDBI_ZEQY) (Vdbi a) {return DBI_ZEQY(a);}
INLINE(Vdbc,VDBC_ZEQY) (Vdbc a) {return DBC_ASTV(DBC_ZEQY(VDBC_ASTM(a)));}
INLINE(Vdhu,VDHU_ZEQY) (Vdhu a) {return DHU_ZEQY(a);}
INLINE(Vdhi,VDHI_ZEQY) (Vdhi a) {return DHI_ZEQY(a);}
INLINE(Vdhf,VDHF_ZEQY) (Vdhf a) {return DHF_ZEQY(a);}
INLINE(Vdwu,VDWU_ZEQY) (Vdwu a) {return DWU_ZEQY(a);}
INLINE(Vdwu,VDWI_ZEQY) (Vdwi a) {return DWI_ZEQY(a);}
INLINE(Vdwu,VDWF_ZEQY) (Vdwf a) {return DWF_ZEQY(a);}
INLINE(Vddu,VDDU_ZEQY) (Vddu a) {return DDU_ZEQY(a);}
INLINE(Vddu,VDDI_ZEQY) (Vddi a) {return DDI_ZEQY(a);}
INLINE(Vddu,VDDF_ZEQY) (Vddf a) {return DDF_ZEQY(a);}

INLINE(Vqyu,VQYU_ZEQY) (Vqyu a) {return VQBU_ASYU(vmvnq_u8(VQYU_ASBU(a)));}
INLINE(Vqbu,VQBU_ZEQY) (Vqbu a) {return QBU_ZEQY(a);}
INLINE(Vqbi,VQBI_ZEQY) (Vqbi a) {return QBI_ZEQY(a);}
INLINE(Vqbc,VQBC_ZEQY) (Vqbc a) {return QBC_ASTV(QBC_ZEQY(VQBC_ASTM(a)));}
INLINE(Vqhu,VQHU_ZEQY) (Vqhu a) {return QHU_ZEQY(a);}
INLINE(Vqhi,VQHI_ZEQY) (Vqhi a) {return QHI_ZEQY(a);}
INLINE(Vqhi,VQHF_ZEQY) (Vqhf a) {return QHF_ZEQY(a);}
INLINE(Vqwu,VQWU_ZEQY) (Vqwu a) {return QWU_ZEQY(a);}
INLINE(Vqwi,VQWI_ZEQY) (Vqwi a) {return QWI_ZEQY(a);}
INLINE(Vqwi,VQWF_ZEQY) (Vqwf a) {return QWF_ZEQY(a);}
INLINE(Vqdu,VQDU_ZEQY) (Vqdu a) {return QDU_ZEQY(a);}
INLINE(Vqdi,VQDI_ZEQY) (Vqdi a) {return QDI_ZEQY(a);}
INLINE(Vqdi,VQDF_ZEQY) (Vqdf a) {return QDF_ZEQY(a);}

#if _LEAVE_ARM_ZEQY
}
#endif

#if _ENTER_ARM_ZEQS
{
#endif

INLINE(ptrdiff_t,ADDR_ZEQS) (void volatile const *a)
{
    return  PTRDIFF_C(0)-(NULL==a);
}

INLINE(  uchar, UCHAR_ZEQS)   (uchar a) {return -(!a);}
INLINE(  schar, SCHAR_ZEQS)   (schar a) {return -(!a);}
INLINE(   char,  CHAR_ZEQS)    (char a) {return -(!a);}
INLINE( ushort, USHRT_ZEQS)  (ushort a) {return -(!a);}
INLINE(  short,  SHRT_ZEQS)   (short a) {return -(!a);}
INLINE(   uint,  UINT_ZEQS)    (uint a) {return -(!a);}
INLINE(    int,   INT_ZEQS)     (int a) {return -(!a);}
INLINE(  ulong, ULONG_ZEQS)   (ulong a) {return -(!a);}
INLINE(   long,  LONG_ZEQS)    (long a) {return -(!a);}
INLINE( ullong,ULLONG_ZEQS)  (ullong a) {return -(!a);}
INLINE(  llong, LLONG_ZEQS)   (llong a) {return -(!a);}
INLINE(int16_t, FLT16_ZEQS) (flt16_t a) {return -(!a);}
INLINE(int32_t,   FLT_ZEQS)   (float a) {return -(!a);}
INLINE(int64_t,   DBL_ZEQS)  (double a) {return -(!a);}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,zeqsqu) (QUAD_UTYPE a)
{
    return  (QUAD_UTYPE) 0-(a==0);
}

INLINE(QUAD_ITYPE,zeqsqi) (QUAD_ITYPE a)
{
    return  (QUAD_ITYPE) 0-(a==0);
}

INLINE(QUAD_ITYPE,zeqsqf) (QUAD_FTYPE a)
{
    return  (QUAD_ITYPE) 0-(a==0);
}

#endif

#define     DBU_ZEQS            vceqz_u8
#define     DBI_ZEQS(A)         vreinterpret_s8_u8(vceqz_s8(A))

#if CHAR_MIN
#   define  DBC_ZEQS            DBI_ZEQS
#else
#   define  DBC_ZEQS            DBU_ZEQS
#endif

#define     DHU_ZEQS            vceqz_u16
#define     DHI_ZEQS(A)         vreinterpret_s16_u16(vceqz_s16(A))
#if defined(SPC_ARM_FP16_SIMD)
#   define  DHF_ZEQS(A)         vreinterpret_s16_u16(vceqz_f16(A))
#else
#   define  DHF_ZEQS(A)                     \
vreinterpret_s16_u16(                       \
    vorr_u16(                               \
        vceqz_u16(vreinterpret_u16_f16(A)), \
        vceq_u16(                           \
            vreinterpret_u16_f16(A),        \
            vdup_n_u16(0x8000)              \
        )                                   \
    )                                       \
)

#endif

#define     DWU_ZEQS            vceqz_u32
#define     DWI_ZEQS(A)         vreinterpret_s32_u32(vceqz_s32(A))
#define     DWF_ZEQS(A)         vreinterpret_s32_u32(vceqz_f32(A))

#define     DDU_ZEQS            vceqz_u64
#define     DDI_ZEQS(A)         vreinterpret_s64_u64(vceqz_s64(A))
#define     DDF_ZEQS(A)         vreinterpret_s64_u64(vceqz_f64(A))

#define     QBU_ZEQS            vceqzq_u8
#define     QBI_ZEQS(A)         vreinterpretq_s8_u8(vceqzq_s8(A))
#if CHAR_MIN
#   define  QBC_ZEQS            QBI_ZEQS
#else
#   define  QBC_ZEQS            QBU_ZEQS
#endif

#define     QHU_ZEQS            vceqzq_u16
#define     QHI_ZEQS(A)         vreinterpretq_s16_u16(vceqzq_s16(A))
#if defined(SPC_ARM_FP16_SIMD)
#   define  QHF_ZEQS(A)         vreinterpretq_s16_u16(vceqz_f16(A))
#else
#   define  QHF_ZEQS(A)                         \
vreinterpretq_s16_u16(                          \
    vorrq_u16(                                  \
        vceqzq_u16(vreinterpretq_u16_f16(A)),   \
        vceqq_u16(                              \
            vreinterpretq_u16_f16(A),           \
            vdupq_n_u16(0x8000)                 \
        )                                       \
    )                                           \
)

#endif

#define     QWU_ZEQS            vceqzq_u32
#define     QWI_ZEQS(A)         vreinterpretq_s32_u32(vceqzq_s32(A))
#define     QWF_ZEQS(A)         vreinterpretq_s32_u32(vceqzq_f32(A))

#define     QDU_ZEQS            vceqzq_u64
#define     QDI_ZEQS(A)         vreinterpretq_s64_u64(vceqzq_s64(A))
#define     QDF_ZEQS(A)         vreinterpretq_s64_u64(vceqzq_f64(A))

INLINE(Vwbu,VWBU_ZEQS) (Vwbu a)
{
    float       f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_ZEQS) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZEQS) (Vwbc a)
{
    float       f = VWBC_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
}


INLINE(Vwhu,VWHU_ZEQS) (Vwhu a)
{
    float       f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vceqz_u16(v);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_ZEQS) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vceqz_u16(v);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZEQS) (Vwhf a)
{
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    float16x4_t r = vreinterpret_f16_u16(v);
    v = DHF_ZEQS(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_ZEQS) (Vwwu a)
{
    float       f = VWWU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vceqz_u32(v);
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_ZEQS) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vceqz_u32(v);
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZEQS) (Vwwf a)
{
    float       f = VWWF_ASTM(a);
    return WWI_ASTV(0-(!f));
}


INLINE(Vdbu,VDBU_ZEQS) (Vdbu a) {return DBU_ZEQS(a);}
INLINE(Vdbi,VDBI_ZEQS) (Vdbi a) {return DBI_ZEQS(a);}
INLINE(Vdbc,VDBC_ZEQS) (Vdbc a) {return DBC_ASTV(DBC_ZEQS(VDBC_ASTM(a)));}
INLINE(Vdhu,VDHU_ZEQS) (Vdhu a) {return DHU_ZEQS(a);}
INLINE(Vdhi,VDHI_ZEQS) (Vdhi a) {return DHI_ZEQS(a);}
INLINE(Vdhi,VDHF_ZEQS) (Vdhf a) {return DHF_ZEQS(a);}
INLINE(Vdwu,VDWU_ZEQS) (Vdwu a) {return DWU_ZEQS(a);}
INLINE(Vdwi,VDWI_ZEQS) (Vdwi a) {return DWI_ZEQS(a);}
INLINE(Vdwi,VDWF_ZEQS) (Vdwf a) {return DWF_ZEQS(a);}
INLINE(Vddu,VDDU_ZEQS) (Vddu a) {return DDU_ZEQS(a);}
INLINE(Vddi,VDDI_ZEQS) (Vddi a) {return DDI_ZEQS(a);}
INLINE(Vddi,VDDF_ZEQS) (Vddf a) {return DDF_ZEQS(a);}

INLINE(Vqbu,VQBU_ZEQS) (Vqbu a) {return QBU_ZEQS(a);}
INLINE(Vqbi,VQBI_ZEQS) (Vqbi a) {return QBI_ZEQS(a);}
INLINE(Vqbc,VQBC_ZEQS) (Vqbc a) {return QBC_ASTV(QBC_ZEQS(VQBC_ASTM(a)));}
INLINE(Vqhu,VQHU_ZEQS) (Vqhu a) {return QHU_ZEQS(a);}
INLINE(Vqhi,VQHI_ZEQS) (Vqhi a) {return QHI_ZEQS(a);}
INLINE(Vqhi,VQHF_ZEQS) (Vqhf a) {return QHF_ZEQS(a);}
INLINE(Vqwu,VQWU_ZEQS) (Vqwu a) {return QWU_ZEQS(a);}
INLINE(Vqwi,VQWI_ZEQS) (Vqwi a) {return QWI_ZEQS(a);}
INLINE(Vqwi,VQWF_ZEQS) (Vqwf a) {return QWF_ZEQS(a);}
INLINE(Vqdu,VQDU_ZEQS) (Vqdu a) {return QDU_ZEQS(a);}
INLINE(Vqdi,VQDI_ZEQS) (Vqdi a) {return QDI_ZEQS(a);}
INLINE(Vqdi,VQDF_ZEQS) (Vqdf a) {return QDF_ZEQS(a);}

#if _LEAVE_ARM_ZEQS
}
#endif


#if _ENTER_ARM_ZNEY
{
#endif

INLINE(ptrdiff_t, ADDR_ZNEY) (void volatile const *a)
{
    return a!=NULL;
}

INLINE(  _Bool,  BOOL_ZNEY)   (_Bool a) {return a;}
INLINE(  uchar, UCHAR_ZNEY)   (uchar a) {return a!=0;}
INLINE(  schar, SCHAR_ZNEY)   (schar a) {return a!=0;}
INLINE(   char,  CHAR_ZNEY)    (char a) {return a!=0;}
INLINE( ushort, USHRT_ZNEY)  (ushort a) {return a!=0;}
INLINE(  short,  SHRT_ZNEY)   (short a) {return a!=0;}
INLINE(   uint,  UINT_ZNEY)    (uint a) {return a!=0u;}
INLINE(    int,   INT_ZNEY)     (int a) {return a!=0;}
INLINE(  ulong, ULONG_ZNEY)   (ulong a) {return a!=0ul;}
INLINE(   long,  LONG_ZNEY)    (long a) {return a!=0l;}
INLINE( ullong,ULLONG_ZNEY)  (ullong a) {return a!=0ull;}
INLINE(  llong, LLONG_ZNEY)   (llong a) {return a!=0ll;}
INLINE(int16_t, FLT16_ZNEY) (flt16_t a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  1&vceqzh_f16(a);
#else
    return  (a!=0.0f16);
#endif
}

INLINE(int32_t,   FLT_ZNEY)   (float a) {return a!=0.0f;}
INLINE(int64_t,   DBL_ZNEY)  (double a) {return a!=0.0;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,zneyqu) (QUAD_UTYPE a)
{
    return  (QUAD_UTYPE) (a!=0);
}

INLINE(QUAD_ITYPE,zneyqi) (QUAD_ITYPE a)
{
    return  (QUAD_ITYPE) (a!=0);
}

INLINE(QUAD_ITYPE,zneyqf) (QUAD_FTYPE a)
{
    return  (QUAD_ITYPE) (a!=0);
}

#endif

#define     DBU_ZNEY(A) vshr_n_u8(vtst_u8(A,vdup_n_u8(UINT8_MAX)),7)
#define     DBI_ZNEY(A) \
vreinterpret_s8_u8(vand_u8(vtst_s8(A,vdup_n_s8(-1)),vdup_n_u8(1)))

#if CHAR_MIN
#   define  DBC_ZNEY    DBI_ZNEY
#else
#   define  DBC_ZNEY    DBU_ZNEY
#endif

#define     DHU_ZNEY(A)  \
vand_u16(vtst_u16(A,vdup_n_u16(UINT16_MAX)),vdup_n_u16(1))

#define     DHI_ZNEY(A) \
vreinterpret_s16_u16(vand_u16(vtst_s16(A,vdup_n_s16(-1)),vdup_n_u16(1)))

#define     DHF_ZNEY(A)                         \
vreinterpret_s16_u16(                           \
   vand_u16(                                    \
        vorr_u16(                               \
            vtst_u16(                           \
                vreinterpret_u16_f16(A),        \
                vdup_n_u16(UINT16_C(0x7fff))    \
            ),                                  \
            vceq_u16(                           \
                vreinterpret_u16_f16(A),        \
                vdup_n_u16(0x8000)              \
            )                                   \
        ),                                      \
        vdup_n_u16(1)                           \
    )                                           \
)


#define     DWU_ZNEY(A) \
vand_u32(vtst_u32(A,vdup_n_u32(UINT32_MAX)),vdup_n_u32(1))

#define     DWI_ZNEY(A) \
vreinterpret_s32_u32(vand_u32(vtst_s32(A,vdup_n_s32(-1)),vdup_n_u32(1)))

#define     DWF_ZNEY(A) \
vreinterpret_s32_u32(vand_u32(vmvn_u32(vceqz_f32(A)),vdup_n_u32(1)))

#define     DDU_ZNEY(A) \
vand_u64(vtst_u64(A,vdup_n_u64(UINT64_MAX)),vdup_n_u64(1))

#define     DDI_ZNEY(A) \
vreinterpret_s64_u64(vand_u64(vtst_s64(A,vdup_n_s64(-1)),vdup_n_u64(1)))

#define     DDF_ZNEY(A) \
vreinterpret_s64_u8(\
    vmvn_u8(vreinterpret_u8_u64(vand_u64(vceqz_f64(A),vdup_n_u64(1))))   \
)


#define     QBU_ZNEY(A) \
vandq_u8(vtstq_u8(A,vdupq_n_u8(UINT8_MAX)),vdupq_n_u8(1))

#define     QBI_ZNEY(A) \
vreinterpretq_s8_u8(vandq_u8(vtstq_s8(A,vdupq_n_s8(-1)),vdupq_n_u8(1)))

#if CHAR_MIN
#   define  QBC_ZNEY    QBI_ZNEY
#else
#   define  QBC_ZNEY    QBU_ZNEY
#endif

#define     QHU_ZNEY(A) \
vandq_u16(vtstq_u16(A,vdupq_n_u16(UINT16_MAX)),vdupq_n_u16(1))

#define     QHI_ZNEY(A) \
vreinterpretq_s16_u16(vandq_u16(vtstq_s16(A,vdupq_n_s16(-1)),vdupq_n_u16(1)))

#define     QHF_ZNEY(A)                         \
vreinterpretq_s16_u16(                          \
   vshrq_n_u16(                                 \
        vorrq_u16(                              \
            vtstq_u16(                          \
                vreinterpretq_u16_f16(A),       \
                vdupq_n_u16(UINT16_C(0x7fff))   \
            ),                                  \
            vceqq_u16(                          \
                vreinterpretq_u16_f16(A),       \
                vdupq_n_u16(0x8000)             \
            )                                   \
        ),                                      \
        15                                      \
    )                                           \
)

#define     QWU_ZNEY(A) \
vandq_u32(vtstq_u32(A,vdupq_n_u32(UINT32_MAX)),vdupq_n_u32(1))

#define     QWI_ZNEY(A) \
vreinterpretq_s32_u32(vandq_u32(vtstq_s32(A,vdupq_n_s32(-1)),vdupq_n_u32(1)))

#define     QWF_ZNEY(A) \
vreinterpretq_s32_u32(vandq_u32(vmvnq_u32(vceqzq_f32(A)),vdupq_n_u32(1)))


#define     QDU_ZNEY(A) vshrq_n_u64(vtstq_u64(A,vdupq_n_u64(UINT64_MAX)),63)
#define     QDI_ZNEY(A) \
vreinterpretq_s64_u64(vshrq_n_u64(vtstq_s64(A,vdupq_n_s64(-1)),63))

#define     QDF_ZNEY(A) \
vreinterpretq_s64_u8(\
    vmvnq_u8(vreinterpretq_u8_u64(vandq_u64(vceqzq_f64(A),vdupq_n_u64(1))))   \
)

INLINE(Vwyu,VWYU_ZNEY) (Vwyu a) {return a;}

INLINE(Vwbu,VWBU_ZNEY) (Vwbu a)
{
    float       f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vtst_u8(v, vdup_n_u8(UINT8_MAX));
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_ZNEY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vtst_u8(v, vdup_n_u8(UINT8_MAX));
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZNEY) (Vwbc a)
{
    return  VWBU_ASBC(VWBU_ZNEY(VWBC_ASBU(a)));
}


INLINE(Vwhu,VWHU_ZNEY) (Vwhu a)
{
    float       f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vtst_u16(v, vdup_n_u16(UINT16_MAX));
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_ZNEY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vtst_u16(v, vdup_n_u16(UINT16_MAX));
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZNEY) (Vwhf a)
{
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vorr_u16(
        vceqz_u16(v),
        vceq_u16(v, vdup_n_u16(UINT16_C(0x8000)))
    );
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_ZNEY) (Vwwu a)
{
    float       f = VWWU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vtst_u32(v, vdup_n_u32(UINT32_MAX));
    v = vand_u32(v, vdup_n_u32(1));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_ZNEY) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vtst_u32(v, vdup_n_u32(UINT32_MAX));
    v = vand_u32(v, vdup_n_u32(1));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZNEY) (Vwwf a)
{
    return VWWF_ASTM(a) ? WWI_ASTV(0) : WWI_ASTV(1);
}



INLINE(Vdyu,VDYU_ZNEY) (Vdyu a) {return a;}
INLINE(Vdbu,VDBU_ZNEY) (Vdbu a) {return DBU_ZNEY(a);}
INLINE(Vdbi,VDBI_ZNEY) (Vdbi a) {return DBI_ZNEY(a);}
INLINE(Vdbc,VDBC_ZNEY) (Vdbc a) {return DBC_ASTV(DBC_ZNEY(VDBC_ASTM(a)));}
INLINE(Vdhu,VDHU_ZNEY) (Vdhu a) {return DHU_ZNEY(a);}
INLINE(Vdhi,VDHI_ZNEY) (Vdhi a) {return DHI_ZNEY(a);}
INLINE(Vdhi,VDHF_ZNEY) (Vdhf a) {return DHF_ZNEY(a);}
INLINE(Vdwu,VDWU_ZNEY) (Vdwu a) {return DWU_ZNEY(a);}
INLINE(Vdwi,VDWI_ZNEY) (Vdwi a) {return DWI_ZNEY(a);}
INLINE(Vdwi,VDWF_ZNEY) (Vdwf a) {return DWF_ZNEY(a);}
INLINE(Vddu,VDDU_ZNEY) (Vddu a) {return DDU_ZNEY(a);}
INLINE(Vddi,VDDI_ZNEY) (Vddi a) {return DDI_ZNEY(a);}
INLINE(Vddi,VDDF_ZNEY) (Vddf a) {return DDF_ZNEY(a);}

INLINE(Vqbu,VQBU_ZNEY) (Vqbu a) {return QBU_ZNEY(a);}
INLINE(Vqbi,VQBI_ZNEY) (Vqbi a) {return QBI_ZNEY(a);}
INLINE(Vqbc,VQBC_ZNEY) (Vqbc a) {return QBC_ASTV(QBC_ZNEY(VQBC_ASTM(a)));}
INLINE(Vqhu,VQHU_ZNEY) (Vqhu a) {return QHU_ZNEY(a);}
INLINE(Vqhi,VQHI_ZNEY) (Vqhi a) {return QHI_ZNEY(a);}
INLINE(Vqhi,VQHF_ZNEY) (Vqhf a) {return QHF_ZNEY(a);}
INLINE(Vqwu,VQWU_ZNEY) (Vqwu a) {return QWU_ZNEY(a);}
INLINE(Vqwi,VQWI_ZNEY) (Vqwi a) {return QWI_ZNEY(a);}
INLINE(Vqwi,VQWF_ZNEY) (Vqwf a) {return QWF_ZNEY(a);}
INLINE(Vqdu,VQDU_ZNEY) (Vqdu a) {return QDU_ZNEY(a);}
INLINE(Vqdi,VQDI_ZNEY) (Vqdi a) {return QDI_ZNEY(a);}
INLINE(Vqdi,VQDF_ZNEY) (Vqdf a) {return QDF_ZNEY(a);}

#if _LEAVE_ARM_ZNEY
}
#endif

#if _ENTER_ARM_ZNES
{
#endif

INLINE(ptrdiff_t, ADDR_ZNES) (void volatile const *a)
{
    return  PTRDIFF_C(0)-(a!=NULL);
}

INLINE(  uchar, UCHAR_ZNES)   (uchar a) {return -(a!=0);}
INLINE(  schar, SCHAR_ZNES)   (schar a) {return -(a!=0);}
INLINE(   char,  CHAR_ZNES)    (char a) {return -(a!=0);}
INLINE( ushort, USHRT_ZNES)  (ushort a) {return -(a!=0);}
INLINE(  short,  SHRT_ZNES)   (short a) {return -(a!=0);}
INLINE(   uint,  UINT_ZNES)    (uint a) {return -(a!=0u);}
INLINE(    int,   INT_ZNES)     (int a) {return -(a!=0);}
INLINE(  ulong, ULONG_ZNES)   (ulong a) {return -(a!=0ul);}
INLINE(   long,  LONG_ZNES)    (long a) {return -(a!=0l);}
INLINE( ullong,ULLONG_ZNES)  (ullong a) {return -(a!=0ull);}
INLINE(  llong, LLONG_ZNES)   (llong a) {return -(a!=0ll);}
INLINE(int16_t, FLT16_ZNES) (flt16_t a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  -vceqzh_f16(a);
#else
    return  -(a!=0.0f16);
#endif
}

INLINE(int32_t,   FLT_ZNES)   (float a) {return -(a!=0.0f);}
INLINE(int64_t,   DBL_ZNES)  (double a) {return -(a!=0.0);}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,znesqu) (QUAD_UTYPE a)
{
    return  (QUAD_UTYPE) 0-(a!=0);
}

INLINE(QUAD_ITYPE,znesqi) (QUAD_ITYPE a)
{
    return  (QUAD_ITYPE) 0-(a!=0);
}

INLINE(QUAD_ITYPE,znesqf) (QUAD_FTYPE a)
{
    return  (QUAD_ITYPE) 0-(a!=0);
}

#endif


INLINE(Vwbu,VWBU_ZNES) (Vwbu a)
{
    float       f = VWBU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vtst_u8(v, vdup_n_u8(UINT8_MAX));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBU_ASTV(f);
}

INLINE(Vwbi,VWBI_ZNES) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vtst_u8(v, vdup_n_u8(UINT8_MAX));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZNES) (Vwbc a)
{
    return  VWBU_ASBC(VWBU_ZNES(VWBC_ASBU(a)));
}


INLINE(Vwhu,VWHU_ZNES) (Vwhu a)
{
    float       f = VWHU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vtst_u16(v, vdup_n_u16(UINT16_MAX));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHU_ASTV(f);
}

INLINE(Vwhi,VWHI_ZNES) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vtst_u16(v, vdup_n_u16(UINT16_MAX));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZNES) (Vwhf a)
{
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint16x4_t  v = vreinterpret_u16_f32(m);
    v = vorr_u16(
        vceqz_u16(v),
        vceq_u16(v, vdup_n_u16(UINT16_C(0x8000)))
    );
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}


INLINE(Vwwu,VWWU_ZNES) (Vwwu a)
{
    float       f = VWWU_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vtst_u32(v, vdup_n_u32(UINT32_MAX));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWU_ASTV(f);
}

INLINE(Vwwi,VWWI_ZNES) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint32x2_t  v = vreinterpret_u32_f32(m);
    v = vtst_u32(v, vdup_n_u32(UINT32_MAX));
    m = vreinterpret_f32_u32(v);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZNES) (Vwwf a)
{
    return VWWF_ASTM(a) ? WWI_ASTV(0) : WWI_ASTV(-1);
}


#define     DBU_ZNES(A)                    vtst_u8(A,vdup_n_u8(UINT8_MAX))
#define     DBI_ZNES(A) vreinterpret_s8_u8(vtst_s8(A,vdup_n_s8(-1)))
#if CHAR_MIN
#   define  DBC_ZNES    DBI_ZNES
#else
#   define  DBC_ZNES    DBU_ZNES
#endif

#define     DHU_ZNES(A)                      vtst_u16(A,vdup_n_u16(UINT16_MAX))
#define     DHI_ZNES(A) vreinterpret_s16_u16(vtst_s16(A,vdup_n_s16(-1)))
#define     DHF_ZNES(A)                     \
vreinterpret_s16_u16(                       \
    vorr_u16(                               \
        vtst_u16(                           \
            vreinterpret_u16_f16(A),        \
            vdup_n_u16(UINT16_C(0x7fff))    \
        ),                                  \
        vceq_u16(                           \
            vreinterpret_u16_f16(A),        \
            vdup_n_u16(0x8000)              \
        )                                   \
    )                                       \
)


#define     DWU_ZNES(A)                      vtst_u32(A,vdup_n_u32(UINT32_MAX))
#define     DWI_ZNES(A) vreinterpret_s32_u32(vtst_s32(A,vdup_n_s32(-1)))
#define     DWF_ZNES(A) vmvn_s32(vreinterpret_s32_u32(vceqz_f32(A)))

#define     DDU_ZNES(A)                      vtst_u64(A,vdup_n_u64(UINT64_MAX))
#define     DDI_ZNES(A) vreinterpret_s64_u64(vtst_s64(A,vdup_n_s64(-1)))
#define     DDF_ZNES(A) \
vreinterpret_f64_u8(vmvn_u8(vreinterpret_u8_u64(vceqz_f64(A))))

#define     QBU_ZNES(A) vtstq_u8(A,vdupq_n_u8(UINT8_MAX))
#define     QBI_ZNES(A) vreinterpretq_s8_u8(vtstq_s8(A,vdupq_n_s8(-1)))
#if CHAR_MIN
#   define  QBC_ZNES    QBI_ZNES
#else
#   define  QBC_ZNES    QBU_ZNES
#endif

#define     QHU_ZNES(A) vtstq_u16(A,vdupq_n_u16(UINT16_MAX))
#define     QHI_ZNES(A) vreinterpretq_s16_u16(vtstq_s16(A,vdupq_n_s16(-1)))
#define     QHF_ZNES(A)                     \
vreinterpretq_s16_u16(                      \
    vorrq_u16(                              \
        vtstq_u16(                          \
            vreinterpretq_u16_f16(A),       \
            vdupq_n_u16(UINT16_C(0x7fff))   \
        ),                                  \
        vceqq_u16(                          \
            vreinterpretq_u16_f16(A),       \
            vdupq_n_u16(0x8000)             \
        )                                   \
    )                                       \
)


#define     QWU_ZNES(A) vtstq_u32(A,vdupq_n_u32(UINT32_MAX))
#define     QWI_ZNES(A) vreinterpretq_s32_u32(vtstq_s32(A,vdupq_n_s32(-1)))
#define     QWF_ZNES(A) vmvnq_s32(vreinterpretq_s32_u32(vceqzq_f32(A)))

#define     QDU_ZNES(A) vtstq_u64(A,vdupq_n_u64(UINT64_MAX))
#define     QDI_ZNES(A) vreinterpretq_s64_u64(vtstq_s64(A,vdupq_n_s64(-1)))
#define     QDF_ZNES(A) \
vreinterpretq_f64_u8(vmvnq_u8(vreinterpretq_u8_u64(vceqzq_f64(A))))


INLINE(Vdbu,VDBU_ZNES) (Vdbu a) {return DBU_ZNES(a);}
INLINE(Vdbi,VDBI_ZNES) (Vdbi a) {return DBI_ZNES(a);}
INLINE(Vdbc,VDBC_ZNES) (Vdbc a) {return DBC_ASTV(DBC_ZNES(VDBC_ASTM(a)));}
INLINE(Vdhu,VDHU_ZNES) (Vdhu a) {return DHU_ZNES(a);}
INLINE(Vdhi,VDHI_ZNES) (Vdhi a) {return DHI_ZNES(a);}
INLINE(Vdhi,VDHF_ZNES) (Vdhf a) {return DHF_ZNES(a);}
INLINE(Vdwu,VDWU_ZNES) (Vdwu a) {return DWU_ZNES(a);}
INLINE(Vdwi,VDWI_ZNES) (Vdwi a) {return DWI_ZNES(a);}
INLINE(Vdwi,VDWF_ZNES) (Vdwf a) {return DWF_ZNES(a);}
INLINE(Vddu,VDDU_ZNES) (Vddu a) {return DDU_ZNES(a);}
INLINE(Vddi,VDDI_ZNES) (Vddi a) {return DDI_ZNES(a);}
INLINE(Vddi,VDDF_ZNES) (Vddf a) {return DDF_ZNES(a);}

INLINE(Vqbu,VQBU_ZNES) (Vqbu a) {return QBU_ZNES(a);}
INLINE(Vqbi,VQBI_ZNES) (Vqbi a) {return QBI_ZNES(a);}
INLINE(Vqbc,VQBC_ZNES) (Vqbc a) {return QBC_ASTV(QBC_ZNES(VQBC_ASTM(a)));}
INLINE(Vqhu,VQHU_ZNES) (Vqhu a) {return QHU_ZNES(a);}
INLINE(Vqhi,VQHI_ZNES) (Vqhi a) {return QHI_ZNES(a);}
INLINE(Vqhi,VQHF_ZNES) (Vqhf a) {return QHF_ZNES(a);}
INLINE(Vqwu,VQWU_ZNES) (Vqwu a) {return QWU_ZNES(a);}
INLINE(Vqwi,VQWI_ZNES) (Vqwi a) {return QWI_ZNES(a);}
INLINE(Vqwi,VQWF_ZNES) (Vqwf a) {return QWF_ZNES(a);}
INLINE(Vqdu,VQDU_ZNES) (Vqdu a) {return QDU_ZNES(a);}
INLINE(Vqdi,VQDI_ZNES) (Vqdi a) {return QDI_ZNES(a);}
INLINE(Vqdi,VQDF_ZNES) (Vqdf a) {return QDF_ZNES(a);}

#if _LEAVE_ARM_ZNES
}
#endif








#if _ENTER_ARM_ZLTY
{
#endif

INLINE(  _Bool,  BOOL_ZLTY)   (_Bool a) {return 0 < a;}
INLINE(  uchar, UCHAR_ZLTY)   (uchar a) {return 0 < a;}
INLINE(  schar, SCHAR_ZLTY)   (schar a) {return 0 < a;}
INLINE(   char,  CHAR_ZLTY)    (char a) {return 0 < a;}
INLINE( ushort, USHRT_ZLTY)  (ushort a) {return 0 < a;}
INLINE(  short,  SHRT_ZLTY)   (short a) {return 0 < a;}
INLINE(   uint,  UINT_ZLTY)    (uint a) {return 0 < a;}
INLINE(    int,   INT_ZLTY)     (int a) {return 0 < a;}
INLINE(  ulong, ULONG_ZLTY)   (ulong a) {return 0 < a;}
INLINE(   long,  LONG_ZLTY)    (long a) {return 0 < a;}
INLINE( ullong,ULLONG_ZLTY)  (ullong a) {return 0 < a;}
INLINE(  llong, LLONG_ZLTY)   (llong a) {return 0 < a;}
INLINE(int16_t, FLT16_ZLTY) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  1&vcgtzh_f16(a);
#else
    return  0.0f16 < a;
#endif
}

INLINE(int32_t,   FLT_ZLTY)   (float a) {return vcgtzs_f32(a)&1;}
INLINE(int64_t,   DBL_ZLTY)  (double a) {return vcgtzd_f64(a)&1;}

#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE,zltyqi) (QUAD_ITYPE a) {return 0 < a;}
INLINE(QUAD_ITYPE,zltyqf) (QUAD_FTYPE a) {return 0 < a;}

#endif

INLINE(Vwbu,VWBU_ZLTY) (Vwbu a)
{
    float       m = VWBU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint8x8_t   z = vreinterpret_u8_f32(f);
    z = vclt_u8(vdup_n_u8(0), z);
    z = vand_u8(vdup_n_u8(1), z);
    f = vreinterpret_f32_u8(z);
    m = vget_lane_f32(f, 0);
    return  WBU_ASTV(m);
}


INLINE(Vwbi,VWBI_ZLTY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcgtz_s8(v);
    u = vand_u8(u, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZLTY) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZLTY(VWBC_ASBI(a)));
#else
    return  VWBU_ASBC(VWBU_ZLTY(VWBC_ASBU(a)));
#endif
}


INLINE(Vwhu,VWHU_ZLTY) (Vwhu a)
{
    float       m = VWHU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint16x4_t  z = vreinterpret_u16_f32(f);
    z = vclt_u16(vdup_n_u16(0), z);
    z = vand_u16(vdup_n_u16(1), z);
    f = vreinterpret_f32_u16(z);
    m = vget_lane_f32(f, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_ZLTY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcgtz_s16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZLTY) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcgtz_f16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcgtzq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}


INLINE(Vwwu,VWWU_ZLTY) (Vwwu a)
{
    return  UINT32_ASTV((0u < VWWU_ASTV(a)));
}

INLINE(Vwwi,VWWI_ZLTY) (Vwwi a)
{
    return  INT32_ASTV((0 < VWWI_ASTV(a)));
}

INLINE(Vwwi,VWWF_ZLTY) (Vwwf a)
{
    return  INT32_ASTV((0.0f < VWWF_ASTV(a)));
}


INLINE(Vdbu,VDBU_ZLTY) (Vdbu a) 
{
    return  vand_u8(
        vclt_u8(vdup_n_u8(0), a), 
        vdup_n_u8(1)
    );
}

INLINE(Vdbi,VDBI_ZLTY) (Vdbi a) 
{
    return  vreinterpret_s8_u8(
        vand_u8(vdup_n_u8(1), vcgtz_s8(a))
    );
}

INLINE(Vdbc,VDBC_ZLTY) (Vdbc a)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_ZLTY(VDBC_ASBI(a)));
#else
    return  VDBU_ASBC(VDBU_ZLTY(VDBC_ASBU(a)));
#endif
}


INLINE(Vdhu,VDHU_ZLTY) (Vdhu a) 
{
    return  vand_u16(
        vclt_u16(vdup_n_u16(0), a), 
        vdup_n_u16(1)
    );
}

INLINE(Vdhi,VDHI_ZLTY) (Vdhi a) 
{
    return  vreinterpret_s16_u16(
        vand_u16(vdup_n_u16(1), vcgtz_s16(a))
    );
}

INLINE(Vdhi,VDHF_ZLTY) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vand_u16(vcgtz_f16(a),vdup_n_u16(1)));
#else
    uint32x4_t q = vcgtzq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vand_u16(vmovn_u32(q), vdup_n_u16(1)));
#endif
}


INLINE(Vdwu,VDWU_ZLTY) (Vdwu a) 
{
    return  vand_u32(
        vclt_u32(vdup_n_u32(0), a), 
        vdup_n_u32(1)
    );
}

INLINE(Vdwi,VDWI_ZLTY) (Vdwi a) 
{
    return vreinterpret_s32_u32(vand_u32(vcgtz_s32(a), vdup_n_u32(1)));
}

INLINE(Vdwi,VDWF_ZLTY) (Vdwf a) 
{
    return vreinterpret_s32_u32(vand_u32(vcgtz_f32(a), vdup_n_u32(1)));
}



INLINE(Vddu,VDDU_ZLTY) (Vddu a) 
{
    return  vand_u64(
        vclt_u64(vdup_n_u64(0), a), 
        vdup_n_u64(1)
    );
}

INLINE(Vddi,VDDI_ZLTY) (Vddi a) 
{
    return vreinterpret_s64_u64(vand_u64(vcgtz_s64(a), vdup_n_u64(1)));
}

INLINE(Vddi,VDDF_ZLTY) (Vddf a) 
{
    return vreinterpret_s64_u64(vand_u64(vcgtz_f64(a), vdup_n_u64(1)));
}


INLINE(Vqbu,VQBU_ZLTY) (Vqbu a) 
{
    return  vandq_u8(
        vcltq_u8(vdupq_n_u8(0), a), 
        vdupq_n_u8(1)
    );
}

INLINE(Vqbi,VQBI_ZLTY) (Vqbi a) 
{
    return vreinterpretq_s8_u8(vandq_u8(vcgtzq_s8(a), vdupq_n_u8(1)));
}

INLINE(Vqbc,VQBC_ZLTY) (Vqbc a)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_ZLTY(VQBC_ASBI(a)));
#else
    return  VQBU_ASBC(VQBU_ZLTY(VQBC_ASBU(a)));
#endif
}


INLINE(Vqhu,VQHU_ZLTY) (Vqhu a) 
{
    return  vandq_u16(
        vcltq_u16(vdupq_n_u16(0), a), 
        vdupq_n_u16(1)
    );
}


INLINE(Vqhi,VQHI_ZLTY) (Vqhi a) 
{
    return vreinterpretq_s16_u16(vandq_u16(vcgtzq_s16(a), vdupq_n_u16(1)));
}

INLINE(Vqhi,VQHF_ZLTY) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vandq_u16(vcgtzq_f16(a), vdupq_n_u16(1)));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcgtzq_f32(wl);
    uint32x4_t  ur = vcgtzq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    uint16x8_t  qq = vcombine_u16(vl, vr);
    qq = vandq_u16(qq, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(qq);
#endif
}


INLINE(Vqwu,VQWU_ZLTY) (Vqwu a) 
{
    return  vandq_u32(
        vcltq_u32(vdupq_n_u32(0), a), 
        vdupq_n_u32(1)
    );
}

INLINE(Vqwi,VQWI_ZLTY) (Vqwi a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcgtzq_s32(a), vdupq_n_u32(1)));
}

INLINE(Vqwi,VQWF_ZLTY) (Vqwf a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcgtzq_f32(a), vdupq_n_u32(1)));
}


INLINE(Vqdu,VQDU_ZLTY) (Vqdu a) 
{
    return  vandq_u64(
        vcltq_u64(vdupq_n_u64(0), a), 
        vdupq_n_u64(1)
    );
}

INLINE(Vqdi,VQDI_ZLTY) (Vqdi a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcgtzq_s64(a), vdupq_n_u64(1)));
}

INLINE(Vqdi,VQDF_ZLTY) (Vqdf a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcgtzq_f64(a), vdupq_n_u64(1)));
}

#if _LEAVE_ARM_ZLTY
}
#endif

#if _ENTER_ARM_ZLTS
{
#endif

INLINE(  _Bool,  BOOL_ZLTS)   (_Bool a) {return 0 < a ? -1 : 0;}
INLINE(  uchar, UCHAR_ZLTS)   (schar a) {return 0 < a ? -1 : 0;}
INLINE(  schar, SCHAR_ZLTS)   (schar a) {return 0 < a ? -1 : 0;}
INLINE(   char,  CHAR_ZLTS)    (char a) {return 0 < a ? -1 : 0;}
INLINE( ushort, USHRT_ZLTS)  (ushort a) {return 0 < a ? -1 : 0;}
INLINE(  short,  SHRT_ZLTS)   (short a) {return 0 < a ? -1 : 0;}
INLINE(   uint,  UINT_ZLTS)    (uint a) {return 0 < a ? -1 : 0;}
INLINE(    int,   INT_ZLTS)     (int a) {return 0 < a ? -1 : 0;}
INLINE(  ulong, ULONG_ZLTS)   (ulong a) {return 0 < a ? -1 : 0;}
INLINE(   long,  LONG_ZLTS)    (long a) {return 0 < a ? -1 : 0;}
INLINE( ullong,ULLONG_ZLTS)  (ullong a) {return 0 < a ? -1 : 0;}
INLINE(  llong, LLONG_ZLTS)   (llong a) {return 0 < a ? -1 : 0;}

INLINE(int16_t, FLT16_ZLTS) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  vcgtzh_f16(a);
#else
    return  0.0f16 < a ? -1 : 0;
#endif
}

INLINE(int32_t,   FLT_ZLTS)   (float a) {return vcgtzs_f32(a);}
INLINE(int64_t,   DBL_ZLTS)  (double a) {return vcgtzd_f64(a);}

#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE,zltsqi) (QUAD_ITYPE a) {return 0 < a ? -1 : 0;}
INLINE(QUAD_ITYPE,zltsqf) (QUAD_FTYPE a) {return 0 < a ? -1 : 0;}

#endif

INLINE(Vwbu,VWBU_ZLTS) (Vwbu a)
{
    float       m = VWBU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint8x8_t   z = vreinterpret_u8_f32(f);
    z = vclt_u8(vdup_n_u8(0), z);
    f = vreinterpret_f32_u8(z);
    m = vget_lane_f32(f, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbi,VWBI_ZLTS) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcgtz_s8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZLTS) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZLTS(VWBC_ASBI(a)));
#else
    return  VWBU_ASBC(VWBU_ZLTS(VWBC_ASBU(a)));
#endif
}


INLINE(Vwhu,VWHU_ZLTS) (Vwhu a)
{
    float       m = VWHU_ASTM(a);
    float32x2_t f = vdup_n_f32(m);
    uint16x4_t  z = vreinterpret_u16_f32(f);
    z = vclt_u16(vdup_n_u16(0), z);
    f = vreinterpret_f32_u16(z);
    m = vget_lane_f32(f, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_ZLTS) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcgtz_s16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZLTS) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcgtz_f16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcgtzq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}


INLINE(Vwwu,VWWU_ZLTS) (Vwwu a)
{
    return  UINT32_ASTV((0u < VWWU_ASTV(a) ? -1 : 0));
}

INLINE(Vwwi,VWWI_ZLTS) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vcgtz_s32(v);
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZLTS) (Vwwf a)
{
    return  WWI_ASTV(0.0f < VWWF_ASTM(a) ? -1 : 0);
}


INLINE(Vdbu,VDBU_ZLTS) (Vdbu a) {return  vclt_u8(vdup_n_u8(0), a);}
INLINE(Vdbi,VDBI_ZLTS) (Vdbi a) {return vreinterpret_s8_u8(vcgtz_s8(a));}
INLINE(Vdbc,VDBC_ZLTS) (Vdbc a)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_ZLTS(VDBC_ASBI(a)));
#else
    return  VDBU_ASBC(VDBU_ZLTS(VDBC_ASBU(a)));
#endif
}

INLINE(Vdhu,VDHU_ZLTS) (Vdhu a) {return  vclt_u16(vdup_n_u16(0), a);}
INLINE(Vdhi,VDHI_ZLTS) (Vdhi a) {return vreinterpret_s16_u16(vcgtz_s16(a));}
INLINE(Vdhi,VDHF_ZLTS) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vcgtz_f16(a));
#else
    uint32x4_t q = vcgtzq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vmovn_u32(q));
#endif
}

INLINE(Vdwu,VDWU_ZLTS) (Vdwu a) {return  vclt_u32(vdup_n_u32(0), a);}
INLINE(Vdwi,VDWI_ZLTS) (Vdwi a) {return vreinterpret_s32_u32(vcgtz_s32(a));}            
INLINE(Vdwi,VDWF_ZLTS) (Vdwf a) {return vreinterpret_s32_u32(vcgtz_f32(a));}

INLINE(Vddu,VDDU_ZLTS) (Vddu a) {return  vclt_u64(vdup_n_u64(0), a);}
INLINE(Vddi,VDDI_ZLTS) (Vddi a) {return vreinterpret_s64_u64(vcgtz_s64(a));}            
INLINE(Vddi,VDDF_ZLTS) (Vddf a) {return vreinterpret_s64_u64(vcgtz_f64(a));}

INLINE(Vqbu,VQBU_ZLTS) (Vqbu a) {return  vcltq_u8(vdupq_n_u8(0), a);}
INLINE(Vqbi,VQBI_ZLTS) (Vqbi a) {return vreinterpretq_u8_s8(vcgtzq_s8(a));}
INLINE(Vqbc,VQBC_ZLTS) (Vqbc a)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_ZLTS(VQBC_ASBI(a)));
#else
    return  VQBU_ASBC(VQBU_ZLTS(VQBC_ASBU(a)));
#endif
}

INLINE(Vqhu,VQHU_ZLTS) (Vqhu a) {return  vcltq_u16(vdupq_n_u16(0), a);}
INLINE(Vqhi,VQHI_ZLTS) (Vqhi a) {return vreinterpretq_s16_u16(vcgtzq_s16(a));}
INLINE(Vqhi,VQHF_ZLTS) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vcgtzq_f16(a));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcgtzq_f32(wl);
    uint32x4_t  ur = vcgtzq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    return  vreinterpretq_s16_u16(vcombine_u16(vl, vr));
#endif
}

INLINE(Vqwu,VQWU_ZLTS) (Vqwu a) {return  vcltq_u32(vdupq_n_u32(0), a);}
INLINE(Vqwi,VQWI_ZLTS) (Vqwi a) {return vreinterpretq_s32_u32(vcgtzq_s32(a));}            
INLINE(Vqwi,VQWF_ZLTS) (Vqwf a) {return vreinterpretq_s32_u32(vcgtzq_f32(a));}

INLINE(Vqdu,VQDU_ZLTS) (Vqdu a) {return  vcltq_u64(vdupq_n_u64(0), a);}
INLINE(Vqdi,VQDI_ZLTS) (Vqdi a) {return vreinterpretq_s64_u64(vcgtzq_s64(a));}            
INLINE(Vqdi,VQDF_ZLTS) (Vqdf a) {return vreinterpretq_s64_u64(vcgtzq_f64(a));}

#if _LEAVE_ARM_ZLTS
}
#endif


#if _ENTER_ARM_ZLEY
{
#endif

INLINE(  schar, SCHAR_ZLEY)   (schar a) {return 0 <= a;}
INLINE(   char,  CHAR_ZLEY)    (char a)
{
#if CHAR_MIN
    return 0 <= a;
#else
    return 1;
#endif
}

INLINE(  short,  SHRT_ZLEY)   (short a) {return 0 <= a;}
INLINE(    int,   INT_ZLEY)     (int a) {return 0 <= a;}
INLINE(   long,  LONG_ZLEY)    (long a) {return 0 <= a;}
INLINE(  llong, LLONG_ZLEY)   (llong a) {return 0 <= a;}

INLINE(int16_t, FLT16_ZLEY) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  1&vcgezh_f16(a);
#else
    return  0.0f16 <= a;
#endif
}

INLINE(int32_t,   FLT_ZLEY)   (float a) {return 1&vcgezs_f32(a);}
INLINE(int64_t,   DBL_ZLEY)  (double a) {return 1&vcgezd_f64(a);}

#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE,zleyqi) (QUAD_ITYPE a) {return 0 <= a;}
INLINE(QUAD_ITYPE,zleyqf) (QUAD_FTYPE a) {return 0 <= a;}

#endif

INLINE(Vwbi,VWBI_ZLEY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcgez_s8(v);
    u = vand_u8(u, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZLEY) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZLEY(VWBC_ASBI(a)));
#else
    uint8x8_t v = vdup_n_u8(1);
    float32x2_t m = vreinterpret_f32_u8(v);
    float f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
#endif
}

INLINE(Vwhi,VWHI_ZLEY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcgez_s16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZLEY) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcgez_f16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcgezq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}

INLINE(Vwwi,VWWI_ZLEY) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vcgez_s32(v);
    r = vand_u32(r, vdup_n_u32(1));
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZLEY) (Vwwf a)
{
    return  WWI_ASTV(0.0f <= VWWF_ASTM(a));
}

INLINE(Vdbi,VDBI_ZLEY) (Vdbi a) 
{
    return vreinterpret_s8_u8(vand_u8(vcgez_s8(a), vdup_n_u8(1)));
}

INLINE(Vdbc,VDBC_ZLEY) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vand_u8(vcgez_s8(VDBC_ASBI(a)), vdup_n_u8(1)));
#else
    return  VDBU_ASBC(vdup_n_u8(1));
#endif
}


INLINE(Vdhi,VDHI_ZLEY) (Vdhi a) 
{
    return vreinterpret_s16_u16(vand_u16(vcgez_s16(a), vdup_n_u16(1)));
}

INLINE(Vdhi,VDHF_ZLEY) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vand_u16(vcgez_f16(a),vdup_n_u16(1)));
#else
    uint32x4_t q = vcgezq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vand_u16(vmovn_u32(q), vdup_n_u16(1)));
#endif
}


INLINE(Vdwi,VDWI_ZLEY) (Vdwi a) 
{
    return vreinterpret_s32_u32(vand_u32(vcgez_s32(a), vdup_n_u32(1)));
}

INLINE(Vdwi,VDWF_ZLEY) (Vdwf a) 
{
    return vreinterpret_s32_u32(vand_u32(vcgez_f32(a), vdup_n_u32(1)));
}


INLINE(Vddi,VDDI_ZLEY) (Vddi a) 
{
    return vreinterpret_s64_u64(vand_u64(vcgez_s64(a), vdup_n_u64(1)));
}

INLINE(Vddi,VDDF_ZLEY) (Vddf a) 
{
    return vreinterpret_s64_u64(vand_u64(vcgez_f64(a), vdup_n_u64(1)));
}


INLINE(Vqbi,VQBI_ZLEY) (Vqbi a) 
{
    return vreinterpretq_s8_u8(vandq_u8(vcgezq_s8(a), vdupq_n_u8(1)));
}

INLINE(Vqbc,VQBC_ZLEY) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vandq_u8(vcgezq_s8(VQBC_ASBI(a)), vdupq_n_u8(1)));
#else
    return  VQBU_ASBC(vdupq_n_u8(1));
#endif
}


INLINE(Vqhi,VQHI_ZLEY) (Vqhi a) 
{
    return vreinterpretq_s16_u16(vandq_u16(vcgezq_s16(a), vdupq_n_u16(1)));
}

INLINE(Vqhi,VQHF_ZLEY) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vandq_u16(vcgezq_f16(a), vdupq_n_u16(1)));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcgezq_f32(wl);
    uint32x4_t  ur = vcgezq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    uint16x8_t  qq = vcombine_u16(vl, vr);
    qq = vandq_u16(qq, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(qq);
#endif
}


INLINE(Vqwi,VQWI_ZLEY) (Vqwi a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcgezq_s32(a), vdupq_n_u32(1)));
}

INLINE(Vqwi,VQWF_ZLEY) (Vqwf a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcgezq_f32(a), vdupq_n_u32(1)));
}


INLINE(Vqdi,VQDI_ZLEY) (Vqdi a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcgezq_s64(a), vdupq_n_u64(1)));
}

INLINE(Vqdi,VQDF_ZLEY) (Vqdf a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcgezq_f64(a), vdupq_n_u64(1)));
}


#if _LEAVE_ARM_ZLEY
}
#endif

#if _ENTER_ARM_ZLES
{
#endif

INLINE(  schar, SCHAR_ZLES)   (schar a) {return 0 <= a ? -1 : 0;}
INLINE(   char,  CHAR_ZLES)    (char a)
{
#if CHAR_MIN
    return 0 < a ? -1 : 0;
#else
    return -1;
#endif
}

INLINE(  short,  SHRT_ZLES)   (short a) {return 0 <= a ? -1 : 0;}
INLINE(    int,   INT_ZLES)     (int a) {return 0 <= a ? -1 : 0;}
INLINE(   long,  LONG_ZLES)    (long a) {return 0 <= a ? -1 : 0;}
INLINE(  llong, LLONG_ZLES)   (llong a) {return 0 <= a ? -1 : 0;}

INLINE(int16_t, FLT16_ZLES) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  vcgezh_f16(a);
#else
    return  0.0f16 <= a ? -1 : 0;
#endif
}

INLINE(int32_t,   FLT_ZLES)   (float a) {return vcgezs_f32(a);}
INLINE(int64_t,   DBL_ZLES)  (double a) {return vcgezd_f64(a);}

#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE,zlesqi) (QUAD_ITYPE a) {return 0 <= a ? -1 : 0;}
INLINE(QUAD_ITYPE,zlesqf) (QUAD_FTYPE a) {return 0 <= a ? -1 : 0;}

#endif

INLINE(Vwbi,VWBI_ZLES) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcgez_s8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZLES) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZLES(VWBC_ASBI(a)));
#else
    uint8x8_t   r = vdup_n_u8(UINT8_MAX);
    float32x2_t m = vreinterpret_f32_u8(r);
    float       f = vget_lane_f32(m, 0);
    return WBC_ASTV(f);
#endif
}

INLINE(Vwhi,VWHI_ZLES) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcgez_s16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZLES) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcgez_f16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcgezq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}

INLINE(Vwwi,VWWI_ZLES) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vcgez_s32(v);
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZLES) (Vwwf a)
{
    return  WWI_ASTV(0.0f <= VWWF_ASTM(a) ? -1 : 0);
}

INLINE(Vdbi,VDBI_ZLES) (Vdbi a) {return vreinterpret_s8_u8(vcgez_s8(a));}
INLINE(Vdbc,VDBC_ZLES) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vcgez_s8(VDBC_ASBI(a)));
#else
    return  VDBU_ASBC(vdup_n_u8(UINT8_MAX));
#endif
}

INLINE(Vdhi,VDHI_ZLES) (Vdhi a) {return vreinterpret_s16_u16(vcgez_s16(a));}
INLINE(Vdhi,VDHF_ZLES) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vcgez_f16(a));
#else
    uint32x4_t q = vcgezq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vmovn_u32(q));
#endif
}

INLINE(Vdwi,VDWI_ZLES) (Vdwi a) {return vreinterpret_s32_u32(vcgez_s32(a));}            
INLINE(Vdwi,VDWF_ZLES) (Vdwf a) {return vreinterpret_s32_u32(vcgez_f32(a));}

INLINE(Vddi,VDDI_ZLES) (Vddi a) {return vreinterpret_s64_u64(vcgez_s64(a));}            
INLINE(Vddi,VDDF_ZLES) (Vddf a) {return vreinterpret_s64_u64(vcgez_f64(a));}

INLINE(Vqbi,VQBI_ZLES) (Vqbi a) {return vreinterpretq_u8_s8(vcgezq_s8(a));}
INLINE(Vqbc,VQBC_ZLES) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vcgezq_s8(VQBC_ASBI(a)));
#else
    return  VQBU_ASBC(vdupq_n_u8(UINT8_MAX));
#endif
}

INLINE(Vqhi,VQHI_ZLES) (Vqhi a) {return vreinterpretq_s16_u16(vcgezq_s16(a));}
INLINE(Vqhi,VQHF_ZLES) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vcgezq_f16(a));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcgezq_f32(wl);
    uint32x4_t  ur = vcgezq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    return  vreinterpretq_s16_u16(vcombine_u16(vl, vr));
#endif
}

INLINE(Vqwi,VQWI_ZLES) (Vqwi a) {return vreinterpretq_s32_u32(vcgezq_s32(a));}            
INLINE(Vqwi,VQWF_ZLES) (Vqwf a) {return vreinterpretq_s32_u32(vcgezq_f32(a));}

INLINE(Vqdi,VQDI_ZLES) (Vqdi a) {return vreinterpretq_s64_u64(vcgezq_s64(a));}            
INLINE(Vqdi,VQDF_ZLES) (Vqdf a) {return vreinterpretq_s64_u64(vcgezq_f64(a));}


#if _LEAVE_ARM_ZLES
}
#endif

#if _ENTER_ARM_ZGTY
{
#endif

INLINE(  schar, SCHAR_ZGTY)   (schar a) {return 0 > a;}
INLINE(   char,  CHAR_ZGTY)    (char a)
{
#if CHAR_MIN
    return 0 > a;
#else
    return 0;
#endif
}

INLINE(  short,  SHRT_ZGTY)   (short a) {return 0 > a;}
INLINE(    int,   INT_ZGTY)     (int a) {return 0 > a;}
INLINE(   long,  LONG_ZGTY)    (long a) {return 0 > a;}
INLINE(  llong, LLONG_ZGTY)   (llong a) {return 0 > a;}

INLINE(int16_t, FLT16_ZGTY) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  1&vcltzh_f16(a);
#else
    return  -0.0f16 >= a;
#endif
}

INLINE(int32_t,   FLT_ZGTY)   (float a) {return 1&vcltzs_f32(a);}
INLINE(int64_t,   DBL_ZGTY)  (double a) {return 1&vcltzd_f64(a);}

#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE,zgtyqi) (QUAD_ITYPE a) {return 0 > a;}
INLINE(QUAD_ITYPE,zgtyqf) (QUAD_FTYPE a) {return 0 > a;}

#endif

INLINE(Vwbi,VWBI_ZGTY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcltz_s8(v);
    u = vand_u8(u, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZGTY) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZGTY(VWBC_ASBI(a)));
#else
    return  WBC_ASTV(0);
#endif
}

INLINE(Vwhi,VWHI_ZGTY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcltz_s16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZGTY) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcltz_f16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcltzq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}

INLINE(Vwwi,VWWI_ZGTY) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vcltz_s32(v);
    r = vand_u32(r, vdup_n_u32(1));
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZGTY) (Vwwf a)
{
    return  WWI_ASTV(0.0f <= VWWF_ASTM(a));
}

INLINE(Vdbi,VDBI_ZGTY) (Vdbi a) 
{
    return vreinterpret_s8_u8(vand_u8(vcltz_s8(a), vdup_n_u8(1)));
}

INLINE(Vdbc,VDBC_ZGTY) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vand_u8(vcltz_s8(VDBC_ASBI(a)), vdup_n_u8(1)));
#else
    return  VDBU_ASBC(vdup_n_u8(1));
#endif
}


INLINE(Vdhi,VDHI_ZGTY) (Vdhi a) 
{
    return vreinterpret_s16_u16(vand_u16(vcltz_s16(a), vdup_n_u16(1)));
}

INLINE(Vdhi,VDHF_ZGTY) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vand_u16(vcltz_f16(a),vdup_n_u16(1)));
#else
    uint32x4_t q = vcltzq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vand_u16(vmovn_u32(q), vdup_n_u16(1)));
#endif
}


INLINE(Vdwi,VDWI_ZGTY) (Vdwi a) 
{
    return vreinterpret_s32_u32(vand_u32(vcltz_s32(a), vdup_n_u32(1)));
}

INLINE(Vdwi,VDWF_ZGTY) (Vdwf a) 
{
    return vreinterpret_s32_u32(vand_u32(vcltz_f32(a), vdup_n_u32(1)));
}


INLINE(Vddi,VDDI_ZGTY) (Vddi a) 
{
    return vreinterpret_s64_u64(vand_u64(vcltz_s64(a), vdup_n_u64(1)));
}

INLINE(Vddi,VDDF_ZGTY) (Vddf a) 
{
    return vreinterpret_s64_u64(vand_u64(vcltz_f64(a), vdup_n_u64(1)));
}


INLINE(Vqbi,VQBI_ZGTY) (Vqbi a) 
{
    return vreinterpretq_s8_u8(vandq_u8(vcltzq_s8(a), vdupq_n_u8(1)));
}

INLINE(Vqbc,VQBC_ZGTY) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vandq_u8(vcltzq_s8(VQBC_ASBI(a)), vdupq_n_u8(1)));
#else
    return  VQBU_ASBC(vdupq_n_u8(1));
#endif
}


INLINE(Vqhi,VQHI_ZGTY) (Vqhi a) 
{
    return vreinterpretq_s16_u16(vandq_u16(vcltzq_s16(a), vdupq_n_u16(1)));
}

INLINE(Vqhi,VQHF_ZGTY) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vandq_u16(vcltzq_f16(a), vdupq_n_u16(1)));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcltzq_f32(wl);
    uint32x4_t  ur = vcltzq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    uint16x8_t  qq = vcombine_u16(vl, vr);
    qq = vandq_u16(qq, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(qq);
#endif
}


INLINE(Vqwi,VQWI_ZGTY) (Vqwi a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcltzq_s32(a), vdupq_n_u32(1)));
}

INLINE(Vqwi,VQWF_ZGTY) (Vqwf a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vcltzq_f32(a), vdupq_n_u32(1)));
}


INLINE(Vqdi,VQDI_ZGTY) (Vqdi a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcltzq_s64(a), vdupq_n_u64(1)));
}

INLINE(Vqdi,VQDF_ZGTY) (Vqdf a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vcltzq_f64(a), vdupq_n_u64(1)));
}


#if _LEAVE_ARM_ZGTY
}
#endif

#if _ENTER_ARM_ZGTS
{
#endif

INLINE(  schar, SCHAR_ZGTS)   (schar a) {return 0 > a ? -1 : 0;}
INLINE(   char,  CHAR_ZGTS)    (char a)
{
#if CHAR_MIN
    return 0 < a ? -1 : 0;
#else
    return 0;
#endif
}

INLINE(  short,  SHRT_ZGTS)   (short a) {return 0 > a ? -1 : 0;}
INLINE(    int,   INT_ZGTS)     (int a) {return 0 > a ? -1 : 0;}
INLINE(   long,  LONG_ZGTS)    (long a) {return 0 > a ? -1 : 0;}
INLINE(  llong, LLONG_ZGTS)   (llong a) {return 0 > a ? -1 : 0;}

INLINE(int16_t, FLT16_ZGTS) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  vcltzh_f16(a);
#else
    return  0.0f16 > a ? -1 : 0;
#endif
}

INLINE(int32_t,   FLT_ZGTS)   (float a) {return vcltzs_f32(a);}
INLINE(int64_t,   DBL_ZGTS)  (double a) {return vcltzd_f64(a);}

#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE,zgtsqi) (QUAD_ITYPE a) {return 0 > a ? -1 : 0;}
INLINE(QUAD_ITYPE,zgtsqf) (QUAD_FTYPE a) {return 0 > a ? -1 : 0;}

#endif

INLINE(Vwbi,VWBI_ZGTS) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vcltz_s8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZGTS) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZGTS(VWBC_ASBI(a)));
#else
    return  WBC_ASTV(0.0f);
#endif
}

INLINE(Vwhi,VWHI_ZGTS) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vcltz_s16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZGTS) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vcltz_f16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vcltzq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}

INLINE(Vwwi,VWWI_ZGTS) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vcltz_s32(v);
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZGTS) (Vwwf a)
{
    return  WWI_ASTV(0.0f > VWWF_ASTM(a) ? -1 : 0);
}

INLINE(Vdbi,VDBI_ZGTS) (Vdbi a) {return vreinterpret_s8_u8(vcltz_s8(a));}
INLINE(Vdbc,VDBC_ZGTS) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vcltz_s8(VDBC_ASBI(a)));
#else
    return  VDBU_ASBC(vdup_n_u8(0));
#endif
}

INLINE(Vdhi,VDHI_ZGTS) (Vdhi a) {return vreinterpret_s16_u16(vcltz_s16(a));}
INLINE(Vdhi,VDHF_ZGTS) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vcltz_f16(a));
#else
    uint32x4_t q = vcltzq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vmovn_u32(q));
#endif
}

INLINE(Vdwi,VDWI_ZGTS) (Vdwi a) {return vreinterpret_s32_u32(vcltz_s32(a));}            
INLINE(Vdwi,VDWF_ZGTS) (Vdwf a) {return vreinterpret_s32_u32(vcltz_f32(a));}

INLINE(Vddi,VDDI_ZGTS) (Vddi a) {return vreinterpret_s64_u64(vcltz_s64(a));}            
INLINE(Vddi,VDDF_ZGTS) (Vddf a) {return vreinterpret_s64_u64(vcltz_f64(a));}

INLINE(Vqbi,VQBI_ZGTS) (Vqbi a) {return vreinterpretq_u8_s8(vcltzq_s8(a));}
INLINE(Vqbc,VQBC_ZGTS) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vcltzq_s8(VQBC_ASBI(a)));
#else
    return  VQBU_ASBC(vdupq_n_u8(0));
#endif
}

INLINE(Vqhi,VQHI_ZGTS) (Vqhi a) {return vreinterpretq_s16_u16(vcltzq_s16(a));}
INLINE(Vqhi,VQHF_ZGTS) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vcltzq_f16(a));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vcltzq_f32(wl);
    uint32x4_t  ur = vcltzq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    return  vreinterpretq_s16_u16(vcombine_u16(vl, vr));
#endif
}

INLINE(Vqwi,VQWI_ZGTS) (Vqwi a) {return vreinterpretq_s32_u32(vcltzq_s32(a));}            
INLINE(Vqwi,VQWF_ZGTS) (Vqwf a) {return vreinterpretq_s32_u32(vcltzq_f32(a));}

INLINE(Vqdi,VQDI_ZGTS) (Vqdi a) {return vreinterpretq_s64_u64(vcltzq_s64(a));}            
INLINE(Vqdi,VQDF_ZGTS) (Vqdf a) {return vreinterpretq_s64_u64(vcltzq_f64(a));}


#if _LEAVE_ARM_ZGTS
}
#endif

#if _ENTER_ARM_ZGEY
{
#endif

INLINE(  schar, SCHAR_ZGEY)   (schar a) {return 0 >= a;}
INLINE(   char,  CHAR_ZGEY)    (char a)
{
#if CHAR_MIN
    return 0 >= a;
#else
    return 0 == a;
#endif
}

INLINE(  short,  SHRT_ZGEY)   (short a) {return 0 >= a;}
INLINE(    int,   INT_ZGEY)     (int a) {return 0 >= a;}
INLINE(   long,  LONG_ZGEY)    (long a) {return 0 >= a;}
INLINE(  llong, LLONG_ZGEY)   (llong a) {return 0 >= a;}

INLINE(int16_t, FLT16_ZGEY) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  1&vclezh_f16(a);
#else
    return  -0.0f16 >= a;
#endif
}

INLINE(int32_t,   FLT_ZGEY)   (float a) {return 1&vclezs_f32(a);}
INLINE(int64_t,   DBL_ZGEY)  (double a) {return 1&vclezd_f64(a);}

#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE,zgeyqi) (QUAD_ITYPE a) {return 0 >= a;}
INLINE(QUAD_ITYPE,zgeyqf) (QUAD_FTYPE a) {return 0 >= a;}

#endif

INLINE(Vwbi,VWBI_ZGEY) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vclez_s8(v);
    u = vand_u8(u, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZGEY) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZGEY(VWBC_ASBI(a)));
#else
    float       f = VWBC_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vceqz_u8(v);
    v = vand_u8(v, vdup_n_u8(1));
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return WBC_ASTV(f);
#endif
}


INLINE(Vwhi,VWHI_ZGEY) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vclez_s16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZGEY) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vclez_f16(v);
    r = vand_u16(r, vdup_n_u16(1));
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vclezq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    v = vand_u16(v, vdup_n_u16(1));
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}

INLINE(Vwwi,VWWI_ZGEY) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vclez_s32(v);
    r = vand_u32(r, vdup_n_u32(1));
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZGEY) (Vwwf a)
{
    return  WWI_ASTV(0.0f <= VWWF_ASTM(a));
}


INLINE(Vdbi,VDBI_ZGEY) (Vdbi a) 
{
    return vreinterpret_s8_u8(vand_u8(vclez_s8(a), vdup_n_u8(1)));
}

INLINE(Vdbc,VDBC_ZGEY) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vand_u8(vclez_s8(VDBC_ASBI(a)), vdup_n_u8(1)));
#else
    uint8x8_t   m = VDBC_ASBU(a);
    m = vceqz_u8(m);
    m = vand_u8(m, vdup_n_u8(1));
    return  VDBU_ASBC(m);
#endif
}


INLINE(Vdhi,VDHI_ZGEY) (Vdhi a) 
{
    return vreinterpret_s16_u16(vand_u16(vclez_s16(a), vdup_n_u16(1)));
}

INLINE(Vdhi,VDHF_ZGEY) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vand_u16(vclez_f16(a),vdup_n_u16(1)));
#else
    uint32x4_t q = vclezq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vand_u16(vmovn_u32(q), vdup_n_u16(1)));
#endif
}


INLINE(Vdwi,VDWI_ZGEY) (Vdwi a) 
{
    return vreinterpret_s32_u32(vand_u32(vclez_s32(a), vdup_n_u32(1)));
}

INLINE(Vdwi,VDWF_ZGEY) (Vdwf a) 
{
    return vreinterpret_s32_u32(vand_u32(vclez_f32(a), vdup_n_u32(1)));
}


INLINE(Vddi,VDDI_ZGEY) (Vddi a) 
{
    return vreinterpret_s64_u64(vand_u64(vclez_s64(a), vdup_n_u64(1)));
}

INLINE(Vddi,VDDF_ZGEY) (Vddf a) 
{
    return vreinterpret_s64_u64(vand_u64(vclez_f64(a), vdup_n_u64(1)));
}


INLINE(Vqbi,VQBI_ZGEY) (Vqbi a) 
{
    return vreinterpretq_s8_u8(vandq_u8(vclezq_s8(a), vdupq_n_u8(1)));
}

INLINE(Vqbc,VQBC_ZGEY) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vandq_u8(vclezq_s8(VQBC_ASBI(a)), vdupq_n_u8(1)));
#else
    uint8x16_t  m = VQBC_ASBU(a);
    m = vceqzq_u8(m);
    m = vandq_u8(m, vdupq_n_u8(1));
    return  VQBU_ASBC(m);
#endif
}


INLINE(Vqhi,VQHI_ZGEY) (Vqhi a) 
{
    return vreinterpretq_s16_u16(vandq_u16(vclezq_s16(a), vdupq_n_u16(1)));
}

INLINE(Vqhi,VQHF_ZGEY) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vandq_u16(vclezq_f16(a), vdupq_n_u16(1)));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vclezq_f32(wl);
    uint32x4_t  ur = vclezq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    uint16x8_t  qq = vcombine_u16(vl, vr);
    qq = vandq_u16(qq, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(qq);
#endif
}


INLINE(Vqwi,VQWI_ZGEY) (Vqwi a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vclezq_s32(a), vdupq_n_u32(1)));
}

INLINE(Vqwi,VQWF_ZGEY) (Vqwf a) 
{
    return vreinterpretq_s32_u32(vandq_u32(vclezq_f32(a), vdupq_n_u32(1)));
}


INLINE(Vqdi,VQDI_ZGEY) (Vqdi a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vclezq_s64(a), vdupq_n_u64(1)));
}

INLINE(Vqdi,VQDF_ZGEY) (Vqdf a) 
{
    return vreinterpretq_s64_u64(vandq_u64(vclezq_f64(a), vdupq_n_u64(1)));
}


#if _LEAVE_ARM_ZGEY
}
#endif

#if _ENTER_ARM_ZGES
{
#endif

INLINE(  schar, SCHAR_ZGES)   (schar a) {return 0 >= a ? -1 : 0;}
INLINE(   char,  CHAR_ZGES)    (char a)
{
#if CHAR_MIN
    return 0 >= a ? -1 : 0;
#else
    return 0 == a;
#endif
}

INLINE(  short,  SHRT_ZGES)   (short a) {return 0 >= a ? -1 : 0;}
INLINE(    int,   INT_ZGES)     (int a) {return 0 >= a ? -1 : 0;}
INLINE(   long,  LONG_ZGES)    (long a) {return 0 >= a ? -1 : 0;}
INLINE(  llong, LLONG_ZGES)   (llong a) {return 0 >= a ? -1 : 0;}

INLINE(int16_t, FLT16_ZGES) (flt16_t a) 
{
#if defined(SPC_ARM_FP16)
    return  vclezh_f16(a);
#else
    return  0.0f16 >= a ? -1 : 0;
#endif
}

INLINE(int32_t,   FLT_ZGES)   (float a) {return vclezs_f32(a);}
INLINE(int64_t,   DBL_ZGES)  (double a) {return vclezd_f64(a);}

#if QUAD_NLLONG == 2

INLINE(QUAD_ITYPE,zgesqi) (QUAD_ITYPE a) {return 0 >= a ? -1 : 0;}
INLINE(QUAD_ITYPE,zgesqf) (QUAD_FTYPE a) {return 0 >= a ? -1 : 0;}

#endif

INLINE(Vwbi,VWBI_ZGES) (Vwbi a)
{
    float       f = VWBI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   u = vreinterpret_u8_f32(m);
    int8x8_t    v = vreinterpret_s8_u8(u);
    u = vclez_s8(v);
    m = vreinterpret_f32_u8(v);
    f = vget_lane_f32(m, 0);
    return  WBI_ASTV(f);
}

INLINE(Vwbc,VWBC_ZGES) (Vwbc a)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_ZGES(VWBC_ASBI(a)));
#else

    float       f = VWBC_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    uint8x8_t   r = vreinterpret_f32_u8(m);
    r = vceqz_u8(r);
    m = vreinterpret_u8_f32(r);
    f = vget_lane_f32(m, 0);
    return  WBC_ASTV(f);
#endif
}


INLINE(Vwhi,VWHI_ZGES) (Vwhi a)
{
    float       f = VWHI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int16x4_t   v = vreinterpret_s16_f32(m);
    uint16x4_t  r = vclez_s16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
}

INLINE(Vwhi,VWHF_ZGES) (Vwhf a)
{
#if defined(SPC_ARM_FP16)
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t v = vreinterpret_f16_f32(m);
    uint16x4_t  r = vclez_f16(v);
    m = vreinterpret_f32_u16(r);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);

#else
    float       f = VWHF_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    float16x4_t d = vreinterpret_f16_f32(m);
    float32x4_t q = vcvt_f32_f16(d);
    uint32x4_t  r = vclezq_f32(q);
    uint16x4_t  v = vmovn_u16(r);
    m = vreinterpret_f32_u16(v);
    f = vget_lane_f32(m, 0);
    return  WHI_ASTV(f);
#endif
}


INLINE(Vwwi,VWWI_ZGES) (Vwwi a)
{
    float       f = VWWI_ASTM(a);
    float32x2_t m = vdup_n_f32(f);
    int32x2_t   v = vreinterpret_s32_f32(m);
    uint32x2_t  r = vclez_s32(v);
    m = vreinterpret_f32_u32(r);
    f = vget_lane_f32(m, 0);
    return  WWI_ASTV(f);
}

INLINE(Vwwi,VWWF_ZGES) (Vwwf a)
{
    return  WWI_ASTV(0.0f >= VWWF_ASTM(a) ? -1 : 0);
}


INLINE(Vdbi,VDBI_ZGES) (Vdbi a) {return vreinterpret_s8_u8(vclez_s8(a));}
INLINE(Vdbc,VDBC_ZGES) (Vdbc a)
{
#if CHAR_MIN
    return  VDBU_ASBC(vclez_s8(VDBC_ASBI(a)));
#else
    return  VDBU_ASBC(vceqz_u8(VDBC_ASBU(a)));
#endif
}

INLINE(Vdhi,VDHI_ZGES) (Vdhi a) {return vreinterpret_s16_u16(vclez_s16(a));}
INLINE(Vdhi,VDHF_ZGES) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpret_s16_u16(vclez_f16(a));
#else
    uint32x4_t q = vclezq_f32(vcvt_f32_f16(a));
    return  vreinterpret_s16_u16(vmovn_u32(q));
#endif
}

INLINE(Vdwi,VDWI_ZGES) (Vdwi a) {return vreinterpret_s32_u32(vclez_s32(a));}            
INLINE(Vdwi,VDWF_ZGES) (Vdwf a) {return vreinterpret_s32_u32(vclez_f32(a));}

INLINE(Vddi,VDDI_ZGES) (Vddi a) {return vreinterpret_s64_u64(vclez_s64(a));}            
INLINE(Vddi,VDDF_ZGES) (Vddf a) {return vreinterpret_s64_u64(vclez_f64(a));}

INLINE(Vqbi,VQBI_ZGES) (Vqbi a) {return vreinterpretq_u8_s8(vclezq_s8(a));}
INLINE(Vqbc,VQBC_ZGES) (Vqbc a)
{
#if CHAR_MIN
    return  VQBU_ASBC(vclezq_s8(VQBC_ASBI(a)));
#else
    return  VQBU_ASBC(vceqzq_u8(VQBC_ASBU(a)));
#endif
}

INLINE(Vqhi,VQHI_ZGES) (Vqhi a) {return vreinterpretq_s16_u16(vclezq_s16(a));}
INLINE(Vqhi,VQHF_ZGES) (Vqhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vreinterpretq_s16_u16(vclezq_f16(a));
#else
    float16x4_t hl = vget_low_f16(a);
    float16x4_t hr = vget_high_f16(a);
    float32x4_t wl = vcvt_f32_f16(hl);
    float32x4_t wr = vcvt_f32_f16(hr);
    uint32x4_t  ul = vclezq_f32(wl);
    uint32x4_t  ur = vclezq_f32(wr);
    uint16x4_t  vl = vmovn_u32(ul);
    uint16x4_t  vr = vmovn_u32(ur);
    return  vreinterpretq_s16_u16(vcombine_u16(vl, vr));
#endif
}

INLINE(Vqwi,VQWI_ZGES) (Vqwi a) {return vreinterpretq_s32_u32(vclezq_s32(a));}            
INLINE(Vqwi,VQWF_ZGES) (Vqwf a) {return vreinterpretq_s32_u32(vclezq_f32(a));}

INLINE(Vqdi,VQDI_ZGES) (Vqdi a) {return vreinterpretq_s64_u64(vclezq_s64(a));}            
INLINE(Vqdi,VQDF_ZGES) (Vqdf a) {return vreinterpretq_s64_u64(vclezq_f64(a));}


#if _LEAVE_ARM_ZGES
}
#endif

#if _ENTER_ARM_VEQY
{
#endif

INLINE(_Bool,VDYU_VEQY) (Vdyu a, _Bool b)
{
    uint64x1_t  l = VDYU_ASDU(a);
    int64x1_t   m = vdup_n_u64(b);
    m = vneg_s64(m);
    uint64x1_t  r = vreinterpret_u64_s64(m);
    r = vtst_u64(l, r);
    return vget_lane_u64(r, 0);
}

INLINE(_Bool,VDBU_VEQY) (Vdbu a, uint8_t b)
{
    uint8x8_t   r = vceq_u8(a, vdup_n_u8(b));
    uint64x1_t  v = vreinterpret_u64_u8(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(v, 0);
}

INLINE(_Bool,VDBI_VEQY) (Vdbi a, int8_t b)
{
    uint8x8_t   r = vceq_s8(a, vdup_n_s8(b));
    uint64x1_t  v = vreinterpret_u64_u8(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(v, 0);
}

INLINE(_Bool,VDBC_VEQY) (Vdbc a, char b)
{
#if CHAR_MIN
    return  VDBI_VEQY(VDBC_ASBI(a), b);
#else
    return  VDBU_VEQY(VDBC_ASBU(a), b);
#endif
}


INLINE(_Bool,VDHU_VEQY) (Vdhu a, uint16_t b)
{
    uint16x4_t  r = vceq_u16(a, vdup_n_u16(b));
    uint64x1_t  v = vreinterpret_u64_u16(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(v, 0);
}

INLINE(_Bool,VDHI_VEQY) (Vdhi a, int16_t b)
{
    uint16x4_t  r = vceq_s16(a, vdup_n_s16(b));
    uint64x1_t  v = vreinterpret_u64_u16(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(v, 0);
}

INLINE(_Bool,VDHF_VEQY) (Vdhf a, flt16_t b)
{
#if defined(SPC_ARM_FP16)
    uint16x4_t  r = vceq_f16(a, vdup_n_f16(b));
    uint64x1_t  v = vreinterpret_u64_u16(r);
#else
    float32x4_t t = vcvt_f32_f16(a);
    uint32x4_t  r = vceqq_f32(t, vdupq_n_f32(b));
    uint64x1_t  v = vreinterpret_u64_u16(vmovn_u32(r));
#endif
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(v, 0);
}



INLINE(_Bool,VDWU_VEQY) (Vdwu a, uint32_t b)
{
    uint32x2_t  r = vceq_u32(a, vdup_n_u32(b));
    uint64x1_t  v = vreinterpret_u64_u32(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(v, 0);
}

INLINE(_Bool,VDWI_VEQY) (Vdwi a, int32_t b)
{
    uint32x2_t  r = vceq_s32(a, vdup_n_s32(b));
    uint64x1_t  v = vreinterpret_u64_u32(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(v, 0);
}

INLINE(_Bool,VDWF_VEQY) (Vdwf a, float b)
{
    uint32x2_t  r = vceq_f32(a, vdup_n_f32(b));
    uint64x1_t  v = vreinterpret_u64_u32(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(v, 0);
}


INLINE(uint64_t, QYU_VEQYQ) (uint64x2_t q)
{
    uint64x1_t  d = vorr_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    d = vtst_u64(d, vdup_n_u64(UINT64_MAX));
    return  vget_lane_u64(d, 0);
}


INLINE(_Bool,VQYU_VEQY) (Vqyu a,    _Bool b)
{
    int64x1_t   i = vdup_n_s64(b);
    uint64x1_t  m = vreinterpret_u64_s64(vneg_s64(i));
    uint64x2_t  q = VQYU_ASDU(a);
    uint64x1_t  d = vorr_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    d = vtst_u64(d, m);
    return  vget_lane_u64(d, 0);
}


INLINE(_Bool,VQBU_VEQY) (Vqbu a,  uint8_t b)
{
    uint8x16_t  r = vceqq_u8(a, vdupq_n_u8(b));
    uint64x2_t  v = vreinterpretq_u64_u8(r);
    return  QYU_VEQYQ(v);
}

INLINE(_Bool,VQBI_VEQY) (Vqbi a,   int8_t b)
{
    uint8x16_t  r = vceqq_s8(a, vdupq_n_s8(b));
    uint64x2_t  v = vreinterpretq_u64_s8(r);
    return  QYU_VEQYQ(v);
}

INLINE(_Bool,VQBC_VEQY) (Vqbc a,     char b)
{
#if CHAR_MIN
    return  VQBI_VEQY(VQBC_ASBI(a), b);
#else
    return  VQBU_VEQY(VQBC_ASBU(a), b);
#endif
}


INLINE(_Bool,VQHU_VEQY) (Vqhu a, uint16_t b)
{
    uint16x8_t  r = vceqq_u16(a, vdupq_n_u16(b));
    uint64x2_t  v = vreinterpretq_u64_u16(r);
    return  QYU_VEQYQ(v);
}

INLINE(_Bool,VQHI_VEQY) (Vqhi a,  int16_t b)
{
    uint16x8_t  r = vceqq_s16(a, vdupq_n_s16(b));
    uint64x2_t  v = vreinterpretq_u64_u16(r);
    return  QYU_VEQYQ(v);
}

INLINE(_Bool,VQHF_VEQY) (Vqhf a,  flt16_t b)
{
#if defined(SPC_ARM_FP16)
    uint16x8_t  r = vceqq_f16(a, vdupq_n_f16(b));
#else
    float16x4_t f;
    uint16x4_t  u;
    uint32x4_t  q;
    float32x4_t t;
    float32x4_t c = vdupq_n_f32(b);

    f = vget_low_f16(a);
    t = vcvt_f32_f16(f);
    q = vceqq_f32(t, c);

    u = vmovn_u32(q);

    f = vget_high_f16(a);
    t = vcvt_f32_f16(f);
    q = vceqq_f32(t, c);
    uint16x8_t r = vcombine_u16(u, vmovn_u32(q));
#endif

    uint64x2_t  v = vreinterpretq_u64_u16(r);
    return  QYU_VEQYQ(v);
}


INLINE(_Bool,VQWU_VEQY) (Vqwu a, uint32_t b)
{
    uint32x4_t  r = vceqq_u32(a, vdupq_n_u32(b));
    uint64x2_t  v = vreinterpretq_u64_u32(r);
    return  QYU_VEQYQ(v);
}

INLINE(_Bool,VQWI_VEQY) (Vqwi a,  int32_t b)
{
    uint32x4_t  r = vceqq_s32(a, vdupq_n_s32(b));
    uint64x2_t  v = vreinterpretq_u64_u32(r);
    return  QYU_VEQYQ(v);
}

INLINE(_Bool,VQWF_VEQY) (Vqwf a,    float b)
{
    uint32x4_t  r = vceqq_s32(a, vdupq_n_f32(b));
    uint64x2_t  v = vreinterpretq_u64_u32(r);
    return  QYU_VEQYQ(v);
}


INLINE(_Bool,VQDU_VEQY) (Vqdu a, uint64_t b)
{
    uint64x2_t  r = vceqq_u64(a, vdupq_n_u64(b));
    return  QYU_VEQYQ(r);
}

INLINE(_Bool,VQDI_VEQY) (Vqdi a,  int64_t b)
{
    uint64x2_t  r = vceqq_s64(a, vdupq_n_s64(b));
    return  QYU_VEQYQ(r);
}

INLINE(_Bool,VQDF_VEQY) (Vqdf a,   double b)
{
    uint64x2_t  r = vceqq_f64(a, vdupq_n_f64(b));
    return  QYU_VEQYQ(r);
}

#if _LEAVE_ARM_VEQY
}
#endif

#if _ENTER_ARM_VEQS
{
#endif

INLINE(Vdyu,VDYU_VEQS) (Vdyu a, _Bool b)
{
    uint64x1_t l = VDYU_ASDU(a);
    l = vtst_u64(l, vdup_n_u64(UINT64_MAX));
    uint64x1_t  r = vsub_u64(vdup_n_u64(0), vdup_n_u64(b));
    r = vand_u64(r, l);
    return  VDDU_ASYU(r);
}

INLINE(Vdbu,VDBU_VEQS) (Vdbu a, uint8_t b)
{
    uint8x8_t   r = vceq_u8(a, vdup_n_u8(b));
    uint64x1_t  v = vreinterpret_u64_u8(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vreinterpret_u8_u64(v);
}

INLINE(Vdbi,VDBI_VEQS) (Vdbi a, int8_t b)
{
    uint8x8_t   r = vceq_s8(a, vdup_n_s8(b));
    uint64x1_t  v = vreinterpret_u64_u8(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vreinterpret_s8_u64(v);
}

INLINE(Vdbc,VDBC_VEQS) (Vdbc a, char b)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_VEQS(VDBC_ASBI(a), b));
#else
    return  VDBU_ASBC(VDBU_VEQS(VDBC_ASBU(a), b));
#endif
}


INLINE(Vdhu,VDHU_VEQS) (Vdhu a, uint16_t b)
{
    uint16x4_t  r = vceq_u16(a, vdup_n_u16(b));
    uint64x1_t  v = vreinterpret_u64_u16(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vreinterpret_u16_u64(v);
}

INLINE(Vdhi,VDHI_VEQS) (Vdhi a, int16_t b)
{
    uint16x4_t  r = vceq_s16(a, vdup_n_s16(b));
    uint64x1_t  v = vreinterpret_u64_u16(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vreinterpret_s16_u64(v);
}

INLINE(Vdhi,VDHF_VEQS) (Vdhf a, flt16_t b)
{
#if defined(SPC_ARM_FP16)
    uint16x4_t  r = vceq_f16(a, vdup_n_f16(b));
    uint64x1_t  v = vreinterpret_u64_u16(r);
#else
    float32x4_t t = vcvt_f32_f16(a);
    uint32x4_t  r = vceqq_f32(t, vdupq_n_f32(b));
    uint64x1_t  v = vreinterpret_u64_u16(vmovn_u32(r));
#endif
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vreinterpret_s16_u64(v);
}



INLINE(Vdwu,VDWU_VEQS) (Vdwu a, uint32_t b)
{
    uint32x2_t  r = vceq_u32(a, vdup_n_u32(b));
    uint64x1_t  v = vreinterpret_u64_u32(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vreinterpret_u32_u64(v);
}

INLINE(Vdwi,VDWI_VEQS) (Vdwi a, int32_t b)
{
    uint32x2_t  r = vceq_s32(a, vdup_n_s32(b));
    uint64x1_t  v = vreinterpret_u64_u32(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vreinterpret_s32_u64(v);
}

INLINE(Vdwi,VDWF_VEQS) (Vdwf a, float b)
{
    uint32x2_t  r = vceq_f32(a, vdup_n_f32(b));
    uint64x1_t  v = vreinterpret_u64_u32(r);
    v = vtst_u64(v, vdup_n_u64(UINT64_MAX));
    return  vreinterpret_s32_u64(v);
}


INLINE(uint64x2_t, QYU_VEQSQ) (uint64x2_t q)
{
    uint64x1_t  d = vorr_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    d = vtst_u64(d, vdup_n_u64(UINT64_MAX));
    return  vcombine_u64(d, d);
}


INLINE(Vqyu,VQYU_VEQS) (Vqyu a,    _Bool b)
{
    int64x1_t   i = vdup_n_s64(b);
    uint64x1_t  m = vreinterpret_u64_s64(vneg_s64(i));
    uint64x2_t  q = VQYU_ASDU(a);
    uint64x1_t  d = vorr_u64(
        vget_low_u64(q),
        vget_high_u64(q)
    );
    d = vtst_u64(d, m);
    q = vcombine_u64(d, d);
    return  VQDU_ASYU(q);
}


INLINE(Vqbu,VQBU_VEQS) (Vqbu a,  uint8_t b)
{
    uint8x16_t  r = vceqq_u8(a, vdupq_n_u8(b));
    uint64x2_t  v = vreinterpretq_u64_u8(r);
    v = QYU_VEQSQ(v);
    return  vreinterpretq_u8_u64(v);
}

INLINE(Vqbi,VQBI_VEQS) (Vqbi a,   int8_t b)
{
    uint8x16_t  r = vceqq_s8(a, vdupq_n_s8(b));
    uint64x2_t  v = vreinterpretq_u64_s8(r);
    v = QYU_VEQSQ(v);
    return  vreinterpretq_s8_u64(v);
}

INLINE(Vqbc,VQBC_VEQS) (Vqbc a,     char b)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_VEQS(VQBC_ASBI(a), b));
#else
    return  VQBU_ASBC(VQBU_VEQS(VQBC_ASBU(a), b));
#endif
}


INLINE(Vqhu,VQHU_VEQS) (Vqhu a, uint16_t b)
{
    uint16x8_t  r = vceqq_u16(a, vdupq_n_u16(b));
    uint64x2_t  v = vreinterpretq_u64_u16(r);
    v = QYU_VEQSQ(v);
    return  vreinterpretq_u16_u64(v);
}

INLINE(Vqhi,VQHI_VEQS) (Vqhi a,  int16_t b)
{
    uint16x8_t  r = vceqq_s16(a, vdupq_n_s16(b));
    uint64x2_t  v = vreinterpretq_u64_u16(r);
    v = QYU_VEQSQ(v);
    return  vreinterpretq_s16_u64(v);
}

INLINE(Vqhi,VQHF_VEQS) (Vqhf a,  flt16_t b)
{
#if defined(SPC_ARM_FP16)
    uint16x8_t  r = vceqq_f16(a, vdupq_n_f16(b));
#else
    float16x4_t f;
    uint16x4_t  u;
    uint32x4_t  q;
    float32x4_t t;
    float32x4_t c = vdupq_n_f32(b);

    f = vget_low_f16(a);
    t = vcvt_f32_f16(f);
    q = vceqq_f32(t, c);

    u = vmovn_u32(q);

    f = vget_high_f16(a);
    t = vcvt_f32_f16(f);
    q = vceqq_f32(t, c);
    uint16x8_t r = vcombine_u16(u, vmovn_u32(q));
#endif

    uint64x2_t  v = vreinterpretq_u64_u16(r);
    v = QYU_VEQSQ(v);
    return  vreinterpretq_s16_u64(v);
}


INLINE(Vqwu,VQWU_VEQS) (Vqwu a, uint32_t b)
{
    uint32x4_t  r = vceqq_u32(a, vdupq_n_u32(b));
    uint64x2_t  v = vreinterpretq_u64_u32(r);
    v = QYU_VEQSQ(v);
    return  vreinterpretq_u32_u64(v);
}

INLINE(Vqwi,VQWI_VEQS) (Vqwi a,  int32_t b)
{
    uint32x4_t  r = vceqq_s32(a, vdupq_n_s32(b));
    uint64x2_t  v = vreinterpretq_u64_u32(r);
    v = QYU_VEQSQ(v);
    return  vreinterpretq_s32_u64(v);
}

INLINE(Vqwi,VQWF_VEQS) (Vqwf a,    float b)
{
    uint32x4_t  r = vceqq_s32(a, vdupq_n_f32(b));
    uint64x2_t  v = vreinterpretq_u64_u32(r);
    v = QYU_VEQSQ(v);
    return  vreinterpretq_s32_u64(v);
}


INLINE(Vqdu,VQDU_VEQS) (Vqdu a, uint64_t b)
{
    uint64x2_t  r = vceqq_u64(a, vdupq_n_u64(b));
    uint64x1_t  d = vorr_u64(
        vget_low_u64(r),
        vget_high_u64(r)
    );
    return  vcombine_u64(d, d);
}

INLINE(Vqdi,VQDI_VEQS) (Vqdi a,  int64_t b)
{
    uint64x2_t  r = vceqq_s64(a, vdupq_n_s64(b));
    uint64x1_t  d = vorr_u64(
        vget_low_u64(r),
        vget_high_u64(r)
    );
    r = vcombine_u64(d, d);
    return  vreinterpretq_s64_u64(r);
}

INLINE(Vqdi,VQDF_VEQS) (Vqdf a,   double b)
{
    uint64x2_t  r = vceqq_f64(a, vdupq_n_f64(b));
    uint64x1_t  d = vorr_u64(
        vget_low_u64(r),
        vget_high_u64(r)
    );
    r = vcombine_u64(d, d);
    return  vreinterpretq_s64_u64(r);
}

#if _LEAVE_ARM_VEQS
}
#endif

#if _ENTER_ARM_CBNS
{
#endif
/*  TODO: fix parameter names ([0]=a, [1]=l, [2]=r)
*/
INLINE(ptrdiff_t, ADDR_CBNS)
(
    void volatile const *a,
    void volatile const *l,
    void volatile const *r
)
{
    return -((l <= a) && (a <= r));
}


INLINE(  uchar,  UCHAR_CBNS)   (uchar a,   uchar l,   uchar r)
{
    return  ((l <= a) && (a <= r)) ? UCHAR_MAX : 0;
}

INLINE(  schar,  SCHAR_CBNS)   (schar a,   schar l,   schar r)
{
    return  -((l <= a) && (a <= r));
}

INLINE(   char,   CHAR_CBNS)    (char a,    char l,    char r)
{
    return  ((l <= a) && (a <= r)) ? '\xff' : '\x00';
}


INLINE( ushort,  USHRT_CBNS)  (ushort a,  ushort l,  ushort r)
{
    return  ((l <= a) && (a <= r)) ? USHRT_MAX : 0;
}

INLINE(  short,   SHRT_CBNS)   (short a,   short l,   short r) 
{
    return  -((l <= a) && (a <= r));
}


INLINE(   uint,   UINT_CBNS)    (uint a,    uint l,    uint r)
{
    return  ((l <= a) && (a <= r)) ? UINT_MAX : 0u;
}

INLINE(    int,    INT_CBNS)     (int a,     int l,     int r)
{
    return  0-((l <= a) && (a <= r));
}


INLINE(  ulong,  ULONG_CBNS)   (ulong a,   ulong l,   ulong r)
{
    return  ((l <= a) && (a <= r)) ? ULONG_MAX : 0ul;
}

INLINE(   long,   LONG_CBNS)    (long a,    long l,    long r)
{
    return  0ll-((l <= a) && (a <= r));
}


INLINE( ullong, ULLONG_CBNS)  (ullong a,  ullong l,  ullong r)
{
    return  ((l <= a) && (a <= r)) ? ULLONG_MAX : 0ull;
}

INLINE(  llong,  LLONG_CBNS)   (llong a,   llong l,   llong r) 
{
    return  0ll-((l <= a) && (a <= r));
}


INLINE(int16_t,  FLT16_CBNS) (flt16_t a, flt16_t l, flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcleh_f16(l, a)&vcleh_f16(a, r);
#else
    return  vcles_f32(l, a)&vcles_f32(a, r);
#endif
}

INLINE(int32_t,    FLT_CBNS)   (float a,   float l,   float r) 
{
    return  vcles_f32(l, a)&vcles_f32(a, r);
}

INLINE(int64_t,    DBL_CBNS)  (double a,  double l, double r) 
{
    return  vcltd_f64(l, a)&vcltd_f64(a, r);
}


#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cbnsqu) (QUAD_UTYPE a, QUAD_UTYPE l, QUAD_UTYPE r)
{
    return  -((l <= a) || (a <= r));
}

INLINE(QUAD_ITYPE,cbnsqi) (QUAD_ITYPE a, QUAD_ITYPE l, QUAD_ITYPE r)
{
    return  -((l <= a) || (a <= r));
}

INLINE(QUAD_ITYPE,cbnsqf) (QUAD_FTYPE a, QUAD_FTYPE l, QUAD_FTYPE r)
{
    return  -((l <= a) || (a <= r));
}

#endif


INLINE(Vwbu,VWBU_CBNS) (Vwbu a,  uint8_t b,  uint8_t c)
{
    float       m = VWBU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   x = vreinterpret_u8_f32(v);
    uint8x8_t   l = vcge_u8(x, vdup_n_u8(b));
    uint8x8_t   r = vcle_u8(x, vdup_n_u8(c));
    x = vand_u8(l, r);
    v = vreinterpret_f32_u8(x);
    m = vget_lane_f32(v, 0);
    return WBU_ASTV(m);
}

INLINE(Vwbi,VWBI_CBNS) (Vwbi a,   int8_t b,   int8_t c)
{
    float       m = VWBI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int8x8_t    x = vreinterpret_s8_f32(v);
    uint8x8_t   l = vcge_s8(x, vdup_n_s8(b));
    uint8x8_t   r = vcle_s8(x, vdup_n_s8(c));
    x = vand_s8(
        vreinterpret_s8_u8(l),
        vreinterpret_s8_u8(r)
    );
    v = vreinterpret_f32_s8(x);
    m = vget_lane_f32(v, 0);
    return  WBI_ASTV(m);
}

INLINE(Vwbc,VWBC_CBNS) (Vwbc a,     char b,     char c)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_CBNS(VWBC_ASBI(a), b, c));
#else
    return  VWBU_ASBC(VWBU_CBNS(VWBC_ASBU(a), b, c));
#endif
}


INLINE(Vwhu,VWHU_CBNS) (Vwhu a, uint16_t b, uint16_t c)
{
    float       m = VWHU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint16x4_t  x = vreinterpret_u16_f32(v);
    uint16x4_t  l = vcge_u16(x, vdup_n_u16(b));
    uint16x4_t  r = vcle_u16(x, vdup_n_u16(c));
    x = vand_u16(l, r);
    v = vreinterpret_f32_u16(x);
    m = vget_lane_f32(v, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_CBNS) (Vwhi a,  int16_t b,  int16_t c)
{
    float       m = VWHI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int16x4_t   x = vreinterpret_s16_f32(v);
    uint16x4_t  l = vcge_s16(x, vdup_n_s16(b));
    uint16x4_t  r = vcle_s16(x, vdup_n_s16(c));
    x = vand_s16(
        vreinterpret_s16_u16(l),
        vreinterpret_s16_u16(r)
    );
    v = vreinterpret_f32_s16(x);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vwhi,VWHF_CBNS) (Vwhf a,  flt16_t b,  flt16_t c)
{
#if defined(SPC_ARM_FP16_SIMD)
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t x = vreinterpret_f16_f32(v);
    uint16x4_t  l = vcge_f16(x, vdup_n_s16(b));
    uint16x4_t  r = vcle_f16(x, vdup_n_s16(c));
    l = vand_u16(
        vreinterpret_s16_u16(l),
        vreinterpret_s16_u16(r)
    );
    v = vreinterpret_f32_u16(l);
#else
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t t = vreinterpret_f16_f32(v);
    float32x4_t x = vcvt_f32_f16(t);
    uint32x4_t  l = vcgeq_f32(x, vdupq_n_f32(b));
    uint32x4_t  r = vcgeq_f32(x, vdupq_n_f32(c));
    l = vorrq_u32(l, r);
    uint16x4_t  n = vmovn_u32(l);
    v = vreinterpret_f32_u16(n);
#endif
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}


INLINE(Vwwu,VWWU_CBNS) (Vwwu a, uint32_t b, uint32_t c)
{
    float       m = VWWU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint32x2_t  x = vreinterpret_u32_f32(v);
    uint32x2_t  l = vcge_u32(x, vdup_n_u32(b));
    uint32x2_t  r = vcle_u32(x, vdup_n_u32(c));
    x = vand_u32(l, r);
    v = vreinterpret_f32_u32(x);
    m = vget_lane_f32(v, 0);
    return  WWU_ASTV(m);
}

INLINE(Vwwi,VWWI_CBNS) (Vwwi a,  int32_t b,  int32_t c)
{
    float       m = VWWI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int32x2_t   x = vreinterpret_s32_f32(v);
    uint32x2_t  l = vcge_s32(x, vdup_n_s32(b));
    uint32x2_t  r = vcle_s32(x, vdup_n_s32(c));
    x = vand_s16(
        vreinterpret_s32_u32(l),
        vreinterpret_s32_u32(r)
    );
    v = vreinterpret_f32_s32(x);
    m = vget_lane_f32(v, 0);
    return  WWI_ASTV(m);
}

INLINE(Vwwi,VWWF_CBNS) (Vwwf a,    float l,    float r)
{
    float       m = VWWF_ASTM(a);
    uint32_t    c = vcles_f32(l, m)&vcles_f32(m, r);
    m = UINT_ASTF(c);
    return  WWI_ASTV(m);
}


INLINE(Vdbu,VDBU_CBNS) (Vdbu a,  uint8_t l,  uint8_t r)
{
    return  vand_u8(
        vcge_u8(a, vdup_n_u8(l)),
        vcle_u8(a, vdup_n_u8(r))
    );
}

INLINE(Vdbi,VDBI_CBNS) (Vdbi a,   int8_t l,   int8_t r)
{
    uint8x8_t p = vcge_s8(a, vdup_n_s8(l));
    uint8x8_t q = vcle_s8(a, vdup_n_s8(r));
    return  vand_s8(
        vreinterpret_s8_u8(p),
        vreinterpret_s8_u8(q)
    );
}

INLINE(Vdbc,VDBC_CBNS) (Vdbc a,     char l,     char r)
{
#if CHAR_MIN
    Vdbi m = VDBC_ASBI(a);
    m = VDBI_CBNS(m, l, r);
    return  VDBI_ASBC(m);
#else
    Vdbu m = VDBC_ASBU(a);
    m = VDBU_CBNS(m, l, r);
    return  VDBU_ASBC(m);
#endif

}


INLINE(Vdhu,VDHU_CBNS) (Vdhu a, uint16_t l, uint16_t r) 
{
    return  vand_u16(
        vcge_u16(a, vdup_n_u16(l)),
        vcle_u16(a, vdup_n_u16(r))
    );
}

INLINE(Vdhi,VDHI_CBNS) (Vdhi a,  int16_t l,  int16_t r)
{
    uint16x4_t p = vcge_s16(a, vdup_n_s16(l));
    uint16x4_t q = vcle_s16(a, vdup_n_s16(r));
    return  vand_s16(
        vreinterpret_s16_u16(p),
        vreinterpret_s16_u16(q)
    );
}

INLINE(Vdhi,VDHF_CBNS) (Vdhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x4_t p = vcge_f16(a, vdup_n_f16(l));
    uint16x4_t q = vcle_f16(a, vdup_n_f16(r));
    return  vand_s16(
        vreinterpret_s16_u16(p),
        vreinterpret_s16_u16(q)
    );
#else
    float32x4_t v = vcvt_f32_f16(a);
    uint32x4_t  p = vcgeq_f32(v, vdupq_n_f32(l));
    uint32x4_t  q = vcleq_f32(v, vdupq_n_f32(r));
    p = vorrq_u32(p, q);
    uint16x4_t  d = vmovn_u32(p);
    return  vreinterpret_s16_u16(d);
#endif
}


INLINE(Vdwu,VDWU_CBNS) (Vdwu a, uint32_t l, uint32_t r)
{
    return  vand_u32(
        vcge_u32(a, vdup_n_u32(l)),
        vcle_u32(a, vdup_n_u32(r))
    );
}

INLINE(Vdwi,VDWI_CBNS) (Vdwi a,  int32_t l,  int32_t r)
{
    uint32x2_t p = vcge_s32(a, vdup_n_s32(l));
    uint32x2_t q = vcle_s32(a, vdup_n_s32(r));
    return  vand_s32(
        vreinterpret_s32_u32(p),
        vreinterpret_s32_u32(q)
    );
}

INLINE(Vdwi,VDWF_CBNS) (Vdwf a,    float l,    float r)
{
    uint32x2_t p = vcge_f32(a, vdup_n_f32(l));
    uint32x2_t q = vcle_f32(a, vdup_n_f32(r));
    return  vand_s32(
        vreinterpret_s32_u32(p),
        vreinterpret_s32_u32(q)
    );
}


INLINE(Vddu,VDDU_CBNS) (Vddu a, uint64_t l, uint64_t r)
{
    return  vand_u64(
        vcge_u64(a, vdup_n_u64(l)),
        vcle_u64(a, vdup_n_u64(r))
    );
}

INLINE(Vddi,VDDI_CBNS) (Vddi a,  int64_t l,  int64_t r)
{
    uint64x1_t p = vcge_s64(a, vdup_n_s64(l));
    uint64x1_t q = vcle_s64(a, vdup_n_s64(r));
    return  vand_s64(
        vreinterpret_s64_u64(p),
        vreinterpret_s64_u64(q)
    );
}

INLINE(Vddi,VDDF_CBNS) (Vddf a,   double l,   double r)
{
    uint64x1_t p = vcge_f64(a, vdup_n_f64(l));
    uint64x1_t q = vcle_f64(a, vdup_n_f64(r));
    return  vand_s64(
        vreinterpret_s64_u64(p),
        vreinterpret_s64_u64(q)
    );
}


INLINE(Vqbu,VQBU_CBNS) (Vqbu a,  uint8_t l,  uint8_t r)
{
    return vandq_u8(
        vcgeq_u8(a, vdupq_n_u8(l)),
        vcleq_u8(a, vdupq_n_u8(r))
    );
}

INLINE(Vqbi,VQBI_CBNS) (Vqbi a,   int8_t l,   int8_t r)
{
    uint8x16_t  p = vcgeq_s8(a, vdupq_n_s8(l));
    uint8x16_t  q = vcleq_s8(a, vdupq_n_s8(r));
    return  vandq_s8(
        vreinterpretq_s8_u8(p),
        vreinterpretq_s8_u8(q)
    );
}

INLINE(Vqbc,VQBC_CBNS) (Vqbc a,     char l,     char r)
{
#if CHAR_MIN
    Vqbi m = VQBC_ASBI(a);
    m = VQBI_CBNS(m, l, r);
    return  VQBI_ASBC(m);
#else
    Vqbu m = VQBC_ASBU(a);
    m = VQBU_CBNS(m, l, r);
    return  VQBU_ASBC(m);
#endif

}


INLINE(Vqhu,VQHU_CBNS) (Vqhu a, uint16_t l, uint16_t r) 
{
    return  vandq_u16(
        vcgeq_u16(a, vdupq_n_u16(l)),
        vcleq_u16(a, vdupq_n_u16(r))
    );
}

INLINE(Vqhi,VQHI_CBNS) (Vqhi a,  int16_t l,  int16_t r)
{
    uint16x8_t p = vcgeq_s16(a, vdupq_n_s16(l));
    uint16x8_t q = vcleq_s16(a, vdupq_n_s16(r));
    return  vandq_s16(
        vreinterpretq_s16_u16(p),
        vreinterpretq_s16_u16(q)
    );
}

INLINE(Vqhi,VQHF_CBNS) (Vqhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x8_t p = vcgeq_f16(a, vdupq_n_f16(l));
    uint16x8_t q = vcleq_f16(a, vdupq_n_f16(r));
    return  vandq_s16(
        vreinterpretq_s16_u16(p),
        vreinterpretq_s16_u16(q)
    );
#else
    float16x4_t p = vget_low_f16(a);
    float16x4_t q = vget_high_f16(a);
    return vcombine_s16(
        VDHF_CBNS(p, l, r),
        VDHF_CBNS(q, l, r)
    );
#endif
}


INLINE(Vqwu,VQWU_CBNS) (Vqwu a, uint32_t l, uint32_t r)
{
    return  vandq_u32(
        vcgeq_u32(a, vdupq_n_u32(l)),
        vcleq_u32(a, vdupq_n_u32(r))
    );
}

INLINE(Vqwi,VQWI_CBNS) (Vqwi a,  int32_t l,  int32_t r)
{
    uint32x4_t p = vcgeq_s32(a, vdupq_n_s32(l));
    uint32x4_t q = vcleq_s32(a, vdupq_n_s32(r));
    return  vandq_s32(
        vreinterpretq_s32_u32(p),
        vreinterpretq_s32_u32(q)
    );
}

INLINE(Vqwi,VQWF_CBNS) (Vqwf a,    float l,    float r)
{
    uint32x4_t p = vcgeq_f32(a, vdupq_n_f32(l));
    uint32x4_t q = vcleq_f32(a, vdupq_n_f32(r));
    return  vandq_s32(
        vreinterpretq_s32_u32(p),
        vreinterpretq_s32_u32(q)
    );
}


INLINE(Vqdu,VQDU_CBNS) (Vqdu a, uint64_t l, uint64_t r)
{
    return  vandq_u64(
        vcgeq_u64(a, vdupq_n_u64(l)),
        vcleq_u64(a, vdupq_n_u64(r))
    );
}

INLINE(Vqdi,VQDI_CBNS) (Vqdi a,  int64_t l,  int64_t r)
{
    uint64x2_t p = vcgeq_s64(a, vdupq_n_s64(l));
    uint64x2_t q = vcleq_s64(a, vdupq_n_s64(r));
    return  vandq_s64(
        vreinterpretq_s64_u64(p),
        vreinterpretq_s64_u64(q)
    );
}

INLINE(Vqdi,VQDF_CBNS) (Vqdf a,   double l,   double r)
{
    uint64x2_t p = vcgeq_f64(a, vdupq_n_f64(l));
    uint64x2_t q = vcleq_f64(a, vdupq_n_f64(r));
    return  vandq_s64(
        vreinterpretq_s64_u64(p),
        vreinterpretq_s64_u64(q)
    );
}


#if _LEAVE_ARM_CBNS
}
#endif

#if _ENTER_ARM_CBNY
{
#endif

INLINE(ptrdiff_t, ADDR_CBNY)
(
    void volatile const *a,
    void volatile const *l,
    void volatile const *r
)
{
    return  (l <= a) && (a <= r);
}


INLINE(  uchar,  UCHAR_CBNY)   (uchar a,   uchar l,   uchar r)
{
    return  (l <= a) && (a <= r);
}

INLINE(  schar,  SCHAR_CBNY)   (schar a,   schar l,   schar r)
{
    return  (l <= a) && (a <= r);
}

INLINE(   char,   CHAR_CBNY)    (char a,    char l,    char r)
{
    return  (l <= a) && (a <= r);
}


INLINE( ushort,  USHRT_CBNY)  (ushort a,  ushort l,  ushort r)
{
    return  (l <= a) && (a <= r);
}

INLINE(  short,   SHRT_CBNY)   (short a,   short l,   short r) 
{
    return  (l <= a) && (a <= r);
}


INLINE(   uint,   UINT_CBNY)    (uint a,    uint l,    uint r)
{
    return  (l <= a) && (a <= r);
}

INLINE(    int,    INT_CBNY)     (int a,     int l,     int r)
{
    return  (l <= a) && (a <= r);
}


INLINE(  ulong,  ULONG_CBNY)   (ulong a,   ulong l,   ulong r)
{
    return  (l <= a) && (a <= r);
}

INLINE(   long,   LONG_CBNY)    (long a,    long l,    long r)
{
    return  (l <= a) && (a <= r);
}


INLINE( ullong, ULLONG_CBNY)  (ullong a,  ullong l,  ullong r)
{
    return  (l <= a) && (a <= r);
}

INLINE(  llong,  LLONG_CBNY)   (llong a,   llong l,   llong r) 
{
    return  (l <= a) && (a <= r);
}


INLINE(int16_t,  FLT16_CBNY) (flt16_t a, flt16_t l, flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  1&(vcleh_f16(l, a)|vcleh_f16(a, r));
#else
    return  1&(vcles_f32(l, a)|vcles_f32(a, r));
#endif
}

INLINE(int32_t,    FLT_CBNY)   (float a,   float l,   float r) 
{
    return  1&(vcles_f32(l, a)|vcles_f32(a, r));
}

INLINE(int64_t,    DBL_CBNY)  (double a,  double l,  double r) 
{
    return  1&(vcled_f64(l, a)|vcled_f64(a, r));
}


#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cbnyqu) (QUAD_UTYPE a, QUAD_UTYPE l, QUAD_UTYPE r)
{
    return  (l <= a) && (a <= r);
}

INLINE(QUAD_ITYPE,cbnyqi) (QUAD_ITYPE a, QUAD_ITYPE l, QUAD_ITYPE r)
{
    return  (l <= a) && (a <= r);
}

INLINE(QUAD_ITYPE,cbnyqf) (QUAD_FTYPE a, QUAD_FTYPE l, QUAD_FTYPE r)
{
    return  (l <= a) && (a <= r);
}

#endif


INLINE(Vwbu,VWBU_CBNY) (Vwbu a,  uint8_t l,  uint8_t r)
{
    float       m = VWBU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   x = vreinterpret_u8_f32(v);
    uint8x8_t   b = vcle_u8(vdup_n_u8(l), x);
    uint8x8_t   c = vcge_u8(vdup_n_u8(r), x);
    c = vand_u8(c, b);
    c = vand_u8(c, vdup_n_u8(1));
    v = vreinterpret_f32_u8(c);
    m = vget_lane_f32(v, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbi,VWBI_CBNY) (Vwbi a,   int8_t l,   int8_t r)
{
    float       m = VWBI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int8x8_t    x = vreinterpret_s8_f32(v);
    uint8x8_t   b = vcle_s8(vdup_n_s8(l), x);
    uint8x8_t   c = vcge_s8(vdup_n_s8(r), x);
    c = vand_u8(c, b);
    c = vand_u8(c, vdup_n_u8(1));
    v = vreinterpret_f32_u8(c);
    m = vget_lane_f32(v, 0);
    return  WBI_ASTV(m);
}

INLINE(Vwbc,VWBC_CBNY) (Vwbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_CBNY(VWBC_ASBI(a), l, r));
#else
    return  VWBU_ASBC(VWBU_CBNY(VWBC_ASBU(a), l, r));
#endif
}


INLINE(Vwhu,VWHU_CBNY) (Vwhu a, uint16_t l, uint16_t r)
{
    float       m = VWHU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint16x4_t  x = vreinterpret_u16_f32(v);
    uint16x4_t  b = vcle_u16(vdup_n_u16(l), x);
    uint16x4_t  c = vcge_u16(vdup_n_u16(r), x);
    c = vand_u16(c, b);
    c = vand_u16(c, vdup_n_u16(1));
    v = vreinterpret_f32_u16(c);
    m = vget_lane_f32(v, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_CBNY) (Vwhi a,  int16_t l,  int16_t r)
{
    float       m = VWHI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int16x4_t   x = vreinterpret_s16_f32(v);
    uint16x4_t  b = vcle_s16(vdup_n_s16(l), x);
    uint16x4_t  c = vcge_s16(vdup_n_s16(r), x);
    c = vand_u16(c, b);
    c = vand_u16(c, vdup_n_u16(1));
    v = vreinterpret_f32_u16(c);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vwhi,VWHF_CBNY) (Vwhf a,  flt16_t l,  flt16_t r)
{
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);

#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t x = vreinterpret_f16_f32(v);
    uint16x4_t  b = vcle_f16(vdup_n_f16(l), x);
    uint16x4_t  c = vcge_s16(vdup_n_f16(r), x);
    c = vand_u16(c, b);
    c = vand_u16(c, vdup_n_u16(1));
    v = vreinterpret_f32_u16(c);
#else
    float16x4_t t = vreinterpret_f16_f32(v);
    float32x4_t x = vcvt_f32_f16(t);
    uint32x4_t  b = vcleq_f32(vdupq_n_f32(l), x);
    uint32x4_t  c = vcgeq_f32(vdupq_n_f32(r), x);
    c = vandq_u32(c, b);
    uint16x4_t  n = vmovn_u32(c);
    n = vand_u16(n, vdup_n_u16(1));
    v = vreinterpret_f32_u16(n);
#endif

    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}


INLINE(Vwwu,VWWU_CBNY) (Vwwu a, uint32_t l, uint32_t r)
{
    float       m = VWWU_ASTM(a);
    uint32_t    c =  FLT_ASTU(m);
    c = (l <= c) && (c <= r);
    m = UINT32_ASTF(c);
    return  WWU_ASTV(m);
}

INLINE(Vwwi,VWWI_CBNY) (Vwwi a,  int32_t l,  int32_t r)
{
    float       m = VWWI_ASTM(a);
    int32_t     c =  FLT_ASTI(m);
    c = (l <= c) && (c <= r);
    m = INT32_ASTF(c);
    return  WWI_ASTV(m);
}

INLINE(Vwwi,VWWF_CBNY) (Vwwf a,    float l,    float r)
{
    float       m = VWWF_ASTM(a);
    uint32_t    c = 1&(vcles_f32(l, m)|vcges_f32(m, r));
    m = UINT32_ASTF(c);
    return  WWI_ASTV(m);
}


INLINE(Vdbu,VDBU_CBNY) (Vdbu a,  uint8_t l,  uint8_t r)
{
    a = vand_u8(
        vcge_u8(a, vdup_n_u8(l)),
        vcle_u8(a, vdup_n_u8(r))
    );
    return  vand_u8(a, vdup_n_u8(1));
}

INLINE(Vdbi,VDBI_CBNY) (Vdbi a,   int8_t l,   int8_t r)
{
    uint8x8_t c = vand_u8(
        vcge_s8(a, vdup_n_s8(l)),
        vcle_s8(a, vdup_n_s8(r))
    );
    c = vand_u8(a, vdup_n_u8(1));
    return  vreinterpret_s8_u8(c);
}

INLINE(Vdbc,VDBC_CBNY) (Vdbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_CBNY(VDBC_ASBI(a), l, r));
#else
    return  VDBI_ASBC(VDBU_CBNY(VDBC_ASBU(a), l, r));
#endif

}


INLINE(Vdhu,VDHU_CBNY) (Vdhu a, uint16_t l, uint16_t r) 
{
    a =  vand_u16(
        vcge_u16(a, vdup_n_u16(l)),
        vcle_u16(a, vdup_n_u16(r))
    );
    return vand_u16(a, vdup_n_u16(1));
}

INLINE(Vdhi,VDHI_CBNY) (Vdhi a,  int16_t l,  int16_t r)
{
    uint16x4_t c = vand_u16(
        vcge_s16(a, vdup_n_s16(l)),
        vcle_s16(a, vdup_n_s16(r))
    );
    c = vand_u16(c, vdup_n_u16(1));
    return  vreinterpret_s16_u16(c);
}

INLINE(Vdhi,VDHF_CBNY) (Vdhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x4_t c = vand_u16(
        vcge_f16(a, vdup_n_f16(l)),
        vcle_f16(a, vdup_n_f16(r))
    );
#else
    float32x4_t q = vcvt_f32_f16(a);
    uint32x4_t  v = vandq_u32(
        vcltq_f32(q, vdupq_n_f32(l)),
        vcgtq_f32(q, vdupq_n_f32(r))
    );
    uint16x4_t  c = vmovn_u32(v);
#endif
    c = vand_u16(c, vdup_n_u16(1));
    return  vreinterpret_s16_u16(c);
}


INLINE(Vdwu,VDWU_CBNY) (Vdwu a, uint32_t l, uint32_t r)
{
    a =  vand_u32(
        vcge_u32(a, vdup_n_u32(l)),
        vcle_u32(a, vdup_n_u32(r))
    );
    return vand_u32(a, vdup_n_u32(1));
}

INLINE(Vdwi,VDWI_CBNY) (Vdwi a,  int32_t l,  int32_t r)
{
    uint32x2_t c = vand_u32(
        vcge_s32(a, vdup_n_s32(l)),
        vcle_s32(a, vdup_n_s32(r))
    );
    c = vand_u32(c, vdup_n_u32(1));
    return  vreinterpret_s32_u32(c);
}

INLINE(Vdwi,VDWF_CBNY) (Vdwf a,    float l,    float r)
{
    uint32x2_t c = vand_u32(
        vcge_f32(a, vdup_n_f32(l)),
        vcle_f32(a, vdup_n_f32(r))
    );
    c = vand_u32(c, vdup_n_u32(1));
    return  vreinterpret_s32_u32(c);
}


INLINE(Vddu,VDDU_CBNY) (Vddu a, uint64_t l, uint64_t r)
{
    a =  vand_u64(
        vcge_u64(a, vdup_n_u64(l)),
        vcle_u64(a, vdup_n_u64(r))
    );
    a = vand_u64(a, vdup_n_u64(1));
    return  a;
}

INLINE(Vddi,VDDI_CBNY) (Vddi a,  int64_t l,  int64_t r)
{
    uint64x1_t c = vand_u64(
        vcge_s64(a, vdup_n_s64(l)),
        vcle_s64(a, vdup_n_s64(r))
    );
    c = vand_u64(c, vdup_n_u64(1));
    return  vreinterpret_s64_u64(c);
}

INLINE(Vddi,VDDF_CBNY) (Vddf a,   double l,   double r)
{
    uint64x1_t c = vand_u64(
        vcge_f64(a, vdup_n_f64(l)),
        vcle_f64(a, vdup_n_f64(r))
    );
    c = vand_u64(c, vdup_n_u64(1));
    return  vreinterpret_s64_u64(c);
}



INLINE(Vqbu,VQBU_CBNY) (Vqbu a,  uint8_t l,  uint8_t r)
{
    a = vandq_u8(
        vcgeq_u8(a, vdupq_n_u8(l)),
        vcleq_u8(a, vdupq_n_u8(r))
    );
    return  vandq_u8(a, vdupq_n_u8(1));
}

INLINE(Vqbi,VQBI_CBNY) (Vqbi a,   int8_t l,   int8_t r)
{
    uint8x16_t c = vandq_u8(
        vcgeq_s8(a, vdupq_n_s8(l)),
        vcleq_s8(a, vdupq_n_s8(r))
    );
    c = vandq_u8(a, vdupq_n_u8(1));
    return  vreinterpretq_s8_u8(c);
}

INLINE(Vqbc,VQBC_CBNY) (Vqbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_CBNY(VQBC_ASBI(a), l, r));
#else
    return  VQBI_ASBC(VQBU_CBNY(VQBC_ASBU(a), l, r));
#endif

}


INLINE(Vqhu,VQHU_CBNY) (Vqhu a, uint16_t l, uint16_t r) 
{
    a =  vandq_u16(
        vcgeq_u16(a, vdupq_n_u16(l)),
        vcleq_u16(a, vdupq_n_u16(r))
    );
    return vandq_u16(a, vdupq_n_u16(1));
}

INLINE(Vqhi,VQHI_CBNY) (Vqhi a,  int16_t l,  int16_t r)
{
    uint16x8_t c = vandq_u16(
        vcgeq_s16(a, vdupq_n_s16(l)),
        vcleq_s16(a, vdupq_n_s16(r))
    );
    c = vandq_u16(c, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(c);
}

INLINE(Vqhi,VQHF_CBNY) (Vqhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x8_t c = vandq_u16(
        vcgeq_f16(a, vdupq_n_f16(l)),
        vcleq_f16(a, vdupq_n_f16(r))
    );
    c = vandq_u16(c, vdupq_n_u16(1));
    return  vreinterpretq_s16_u16(c);
#else
    return vcombine_s16(
        VDHF_CBNY(vget_low_f16(a), l, r),
        VDHF_CBNY(vget_high_f16(a),l, r)
    );
#endif
}


INLINE(Vqwu,VQWU_CBNY) (Vqwu a, uint32_t l, uint32_t r)
{
    a =  vandq_u32(
        vcgeq_u32(a, vdupq_n_u32(l)),
        vcleq_u32(a, vdupq_n_u32(r))
    );
    return  vandq_u32(a, vdupq_n_u32(1));
}

INLINE(Vqwi,VQWI_CBNY) (Vqwi a,  int32_t l,  int32_t r)
{
    uint32x4_t c = vandq_u32(
        vcgeq_s32(a, vdupq_n_s32(l)),
        vcleq_s32(a, vdupq_n_s32(r))
    );
    c = vandq_u32(c, vdupq_n_u32(1));
    return  vreinterpretq_s32_u32(c);
}

INLINE(Vqwi,VQWF_CBNY) (Vqwf a,    float l,    float r)
{
    uint32x4_t c = vandq_u32(
        vcgeq_f32(a, vdupq_n_f32(l)),
        vcleq_f32(a, vdupq_n_f32(r))
    );
    c = vandq_u32(c, vdupq_n_u32(1));
    return  vreinterpretq_s32_u32(c);
}


INLINE(Vqdu,VQDU_CBNY) (Vqdu a, uint64_t l, uint64_t r)
{
    a =  vandq_u64(
        vcgeq_u64(a, vdupq_n_u64(l)),
        vcleq_u64(a, vdupq_n_u64(r))
    );
    a = vandq_u64(a, vdupq_n_u64(1));
    return  a;
}

INLINE(Vqdi,VQDI_CBNY) (Vqdi a,  int64_t l,  int64_t r)
{
    uint64x2_t c = vandq_u64(
        vcgeq_s64(a, vdupq_n_s64(l)),
        vcleq_s64(a, vdupq_n_s64(r))
    );
    c = vandq_u64(c, vdupq_n_u64(1));
    return  vreinterpretq_s64_u64(c);
}

INLINE(Vqdi,VQDF_CBNY) (Vqdf a,   double l,   double r)
{
    uint64x2_t c = vandq_u64(
        vcgeq_f64(a, vdupq_n_f64(l)),
        vcleq_f64(a, vdupq_n_f64(r))
    );
    c = vandq_u64(c, vdupq_n_u64(1));
    return  vreinterpretq_s64_u64(c);
}


#if _LEAVE_ARM_CBNY
}
#endif

#if _ENTER_ARM_CNBS
{
#endif

INLINE(ptrdiff_t, ADDR_CNBS)
(
    void volatile const *a,
    void volatile const *l,
    void volatile const *r
)
{
    return -((a < l) || (r < a));
}


INLINE(  uchar,  UCHAR_CNBS)   (uchar a,   uchar l,   uchar r)
{
    return  ((a < l) || (r < a)) ? UCHAR_MAX : 0;
}

INLINE(  schar,  SCHAR_CNBS)   (schar a,   schar l,   schar r)
{
    return -((a < l) || (r < a));
}

INLINE(   char,   CHAR_CNBS)    (char a,    char l,    char r)
{
    return  ((l <= a) || (a <= r)) ? '\x00' : '\xff';
}


INLINE( ushort,  USHRT_CNBS)  (ushort a,  ushort l,  ushort r)
{
    return  ((a < l) || (r < a)) ? USHRT_MAX : 0;
}

INLINE(  short,   SHRT_CNBS)   (short a,   short l,   short r) 
{
    return  -((a < l) || (r < a));
}


INLINE(   uint,   UINT_CNBS)    (uint a,    uint l,    uint r)
{
    return  ((a < l) || (r < a)) ? UINT_MAX : 0u;
}

INLINE(    int,    INT_CNBS)     (int a,     int l,     int r)
{
    return  -((a < l) || (r < a));
}


INLINE(  ulong,  ULONG_CNBS)   (ulong a,   ulong l,   ulong r)
{
    return  ((l <= a) && (a <= r)) ? 0ul : ULONG_MAX;
}

INLINE(   long,   LONG_CNBS)    (long a,    long l,    long r)
{
    return  -((a < l) || (r < a));
}


INLINE( ullong, ULLONG_CNBS)  (ullong a,  ullong l,  ullong r)
{
    return  ((l <= a) && (a <= r)) ? 0ull : ULLONG_MAX;
}

INLINE(  llong,  LLONG_CNBS)   (llong a,   llong l,   llong r) 
{
    return  -((a < l) || (r < a));
}


INLINE(int16_t,  FLT16_CNBS) (flt16_t a, flt16_t l, flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vclth_f16(a, l)|vclth_f16(r, a);
#else
    return  vclts_f32(a, l)|vclts_f32(r, a);
#endif
}

INLINE(int32_t,    FLT_CNBS)   (float a,   float l,   float r) 
{
    return  vclts_f32(a, l)|vclts_f32(r, a);
}

INLINE(int64_t,    DBL_CNBS)  (double a,  double l,  double r) 
{
    return  vcltd_f64(a, l)|vcltd_f64(r, a);
}


#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cnbsqu) (QUAD_UTYPE a, QUAD_UTYPE l, QUAD_UTYPE r)
{
    return  -((a < l) || (r < a));
}

INLINE(QUAD_ITYPE,cnbsqi) (QUAD_ITYPE a, QUAD_ITYPE l, QUAD_ITYPE r)
{
    return  -((a < l) || (r < a));
}

INLINE(QUAD_ITYPE,cnbsqf) (QUAD_FTYPE a, QUAD_FTYPE l, QUAD_FTYPE r)
{
    return  -((a < l) || (r < a));
}

#endif


INLINE(Vwbu,VWBU_CNBS) (Vwbu a,  uint8_t l,  uint8_t r)
{
    float       m = VWBU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   x = vreinterpret_u8_f32(v);
    uint8x8_t   b = vclt_u8(x, vdup_n_u8(l));
    uint8x8_t   c = vcgt_u8(x, vdup_n_u8(r));
    c = vorr_u8(b, c);
    v = vreinterpret_f32_u8(c);
    m = vget_lane_f32(v, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbi,VWBI_CNBS) (Vwbi a,   int8_t l,   int8_t r)
{
    float       m = VWBI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int8x8_t    x = vreinterpret_s8_f32(v);
    uint8x8_t   b = vclt_s8(x, vdup_n_s8(l));
    uint8x8_t   c = vcgt_s8(x, vdup_n_s8(r));
    c = vorr_u8(b, c);
    v = vreinterpret_f32_u8(c);
    m = vget_lane_f32(v, 0);
    return  WBI_ASTV(m);
}

INLINE(Vwbc,VWBC_CNBS) (Vwbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_CNBS(VWBC_ASBI(a), l, r));
#else
    return  VWBU_ASBC(VWBU_CNBS(VWBC_ASBU(a), l, r));
#endif
}


INLINE(Vwhu,VWHU_CNBS) (Vwhu a, uint16_t l, uint16_t r)
{
    float       m = VWHU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint16x4_t  x = vreinterpret_u16_f32(v);
    uint16x4_t  b = vclt_u16(x, vdup_n_u16(l));
    uint16x4_t  c = vcgt_u16(x, vdup_n_u16(r));
    c = vorr_u16(b, c);
    v = vreinterpret_f32_u16(c);
    m = vget_lane_f32(v, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_CNBS) (Vwhi a,  int16_t l,  int16_t r)
{
    float       m = VWHI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int16x4_t   x = vreinterpret_s16_f32(v);
    uint16x4_t  b = vclt_s16(x, vdup_n_s16(l));
    uint16x4_t  c = vcgt_s16(x, vdup_n_s16(r));
    c = vorr_u16(b, c);
    v = vreinterpret_f32_u16(c);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vwhi,VWHF_CNBS) (Vwhf a,  flt16_t l,  flt16_t r)
{
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);

#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t x = vreinterpret_s16_f32(v);
    uint16x4_t  b = vclt_f16(x, vdup_n_f16(l));
    uint16x4_t  c = vcgt_s16(x, vdup_n_f16(r));
    c = vorr_u16(b, c);
    v = vreinterpret_f32_u16(c);
#else
    float16x4_t t = vreinterpret_s16_f32(v);
    float32x4_t x = vcvt_f32_f16(t);
    uint32x4_t  b = vcltq_f32(x, vdupq_n_f32(l));
    uint32x4_t  c = vcgtq_f32(x, vdupq_n_f32(r));
    c = vorrq_u32(b, c);
    uint16x4_t  n = vmovn_u32(c);
    v = vreinterpret_f32_u16(n);
#endif
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);

}


INLINE(Vwwu,VWWU_CNBS) (Vwwu a, uint32_t l, uint32_t r)
{
    float       m = VWWU_ASTM(a);
    uint32_t    c =  FLT_ASTU(m);
    c = ((c < l) || (r < c)) ? UINT32_MAX : 0;
    m = UINT32_ASTF(c);
    return  WWU_ASTV(m);
}

INLINE(Vwwi,VWWI_CNBS) (Vwwi a,  int32_t l,  int32_t r)
{
    float       m = VWWI_ASTM(a);
    int32_t     c =  FLT_ASTI(m);
    c = ((c < l) || (r < c)) ? -1 : 0;
    m = INT32_ASTF(c);
    return  WWI_ASTV(m);
}

INLINE(Vwwi,VWWF_CNBS) (Vwwf a,    float l,    float r)
{
    float       m = VWWF_ASTM(a);
    uint32_t    c = vclts_f32(m, l)|vclts_f32(r, m);
    m = UINT32_ASTF(c);
    return  WWI_ASTV(m);
}


INLINE(Vdbu,VDBU_CNBS) (Vdbu a,  uint8_t l,  uint8_t r)
{
    return  vorr_u8(
        vclt_u8(a, vdup_n_u8(l)),
        vcgt_u8(a, vdup_n_u8(r))
    );
}

INLINE(Vdbi,VDBI_CNBS) (Vdbi a,   int8_t l,   int8_t r)
{
    uint8x8_t c = vorr_u8(
        vclt_s8(a, vdup_n_s8(l)),
        vcgt_s8(a, vdup_n_s8(r))
    );
    return  vreinterpret_s8_u8(c);
}

INLINE(Vdbc,VDBC_CNBS) (Vdbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_CNBS(VDBC_ASBI(a), l, r));
#else
    return  VDBI_ASBC(VDBU_CNBS(VDBC_ASBU(a), l, r));
#endif

}


INLINE(Vdhu,VDHU_CNBS) (Vdhu a, uint16_t l, uint16_t r) 
{
    return  vorr_u16(
        vclt_u16(a, vdup_n_u16(l)),
        vcgt_u16(a, vdup_n_u16(r))
    );
}

INLINE(Vdhi,VDHI_CNBS) (Vdhi a,  int16_t l,  int16_t r)
{
    uint16x4_t c = vorr_u16(
        vclt_s16(a, vdup_n_s16(l)),
        vcgt_s16(a, vdup_n_s16(r))
    );
    return  vreinterpret_s16_u16(c);
}

INLINE(Vdhi,VDHF_CNBS) (Vdhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x4_t c = vorr_u16(
        vclt_f16(a, vdup_n_f16(l)),
        vcgt_f16(a, vdup_n_f16(r))
    );
#else
    float32x4_t q = vcvt_f32_f16(a);
    uint32x4_t  v = vorrq_u32(
        vcltq_f32(q, vdupq_n_f32(l)),
        vcgtq_f32(q, vdupq_n_f32(r))
    );
    uint16x4_t  c = vmovn_u32(v);
#endif
    return  vreinterpret_s16_u16(c);
}


INLINE(Vdwu,VDWU_CNBS) (Vdwu a, uint32_t l, uint32_t r)
{
    return  vorr_u32(
        vclt_u32(a, vdup_n_u32(l)),
        vcgt_u32(a, vdup_n_u32(r))
    );
}

INLINE(Vdwi,VDWI_CNBS) (Vdwi a,  int32_t l,  int32_t r)
{
    uint32x2_t c = vorr_u32(
        vclt_s32(a, vdup_n_s32(l)),
        vcgt_s32(a, vdup_n_s32(r))
    );
    return  vreinterpret_s32_u32(c);
}

INLINE(Vdwi,VDWF_CNBS) (Vdwf a,    float l,    float r)
{
    uint32x2_t c = vorr_u32(
        vclt_f32(a, vdup_n_f32(l)),
        vcgt_f32(a, vdup_n_f32(r))
    );
    return  vreinterpret_s32_u32(c);
}


INLINE(Vddu,VDDU_CNBS) (Vddu a, uint64_t l, uint64_t r)
{
    return  vorr_u64(
        vclt_u64(a, vdup_n_u64(l)),
        vcgt_u64(a, vdup_n_u64(r))
    );
}

INLINE(Vddi,VDDI_CNBS) (Vddi a,  int64_t l,  int64_t r)
{
    uint64x1_t c = vorr_u64(
        vclt_s64(a, vdup_n_s64(l)),
        vcgt_s64(a, vdup_n_s64(r))
    );
    return  vreinterpret_s64_u64(c);
}

INLINE(Vddi,VDDF_CNBS) (Vddf a,   double l,   double r)
{
    uint64x1_t c = vorr_u64(
        vclt_f64(a, vdup_n_f64(l)),
        vcgt_f64(a, vdup_n_f64(r))
    );
    return  vreinterpret_s64_u64(c);
}



INLINE(Vqbu,VQBU_CNBS) (Vqbu a,  uint8_t l,  uint8_t r)
{
    return  vorrq_u8(
        vcltq_u8(a, vdupq_n_u8(l)),
        vcgtq_u8(a, vdupq_n_u8(r))
    );
}

INLINE(Vqbi,VQBI_CNBS) (Vqbi a,   int8_t l,   int8_t r)
{
    uint8x16_t c = vorrq_u8(
        vcltq_s8(a, vdupq_n_s8(l)),
        vcgtq_s8(a, vdupq_n_s8(r))
    );
    return  vreinterpretq_s8_u8(c);
}

INLINE(Vqbc,VQBC_CNBS) (Vqbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_CNBS(VQBC_ASBI(a), l, r));
#else
    return  VQBI_ASBC(VQBU_CNBS(VQBC_ASBU(a), l, r));
#endif

}


INLINE(Vqhu,VQHU_CNBS) (Vqhu a, uint16_t l, uint16_t r) 
{
    return  vorrq_u16(
        vcltq_u16(a, vdupq_n_u16(l)),
        vcgtq_u16(a, vdupq_n_u16(r))
    );
}

INLINE(Vqhi,VQHI_CNBS) (Vqhi a,  int16_t l,  int16_t r)
{
    uint16x8_t c = vorrq_u16(
        vcltq_s16(a, vdupq_n_s16(l)),
        vcgtq_s16(a, vdupq_n_s16(r))
    );
    return  vreinterpretq_s16_u16(c);
}

INLINE(Vqhi,VQHF_CNBS) (Vqhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x8_t c = vorrq_u16(
        vcltq_f16(a, vdupq_n_f16(l)),
        vcgtq_f16(a, vdupq_n_f16(r))
    );
    return  vreinterpretq_s16_u16(c);
#else
    return  vcombine_s16(
        VDHF_CNBS(vget_low_f16(a), l, r),
        VDHF_CNBS(vget_high_f16(a),l, r)
    );
#endif
}


INLINE(Vqwu,VQWU_CNBS) (Vqwu a, uint32_t l, uint32_t r)
{
    return  vorrq_u32(
        vcltq_u32(a, vdupq_n_u32(l)),
        vcgtq_u32(a, vdupq_n_u32(r))
    );
}

INLINE(Vqwi,VQWI_CNBS) (Vqwi a,  int32_t l,  int32_t r)
{
    uint32x4_t c = vorrq_u32(
        vcltq_s32(a, vdupq_n_s32(l)),
        vcgtq_s32(a, vdupq_n_s32(r))
    );
    return  vreinterpretq_s32_u32(c);
}

INLINE(Vqwi,VQWF_CNBS) (Vqwf a,    float l,    float r)
{
    uint32x4_t c = vorrq_u32(
        vcltq_f32(a, vdupq_n_f32(l)),
        vcgtq_f32(a, vdupq_n_f32(r))
    );
    return  vreinterpretq_s32_u32(c);
}


INLINE(Vqdu,VQDU_CNBS) (Vqdu a, uint64_t l, uint64_t r)
{
    return  vorrq_u64(
        vcltq_u64(a, vdupq_n_u64(l)),
        vcgtq_u64(a, vdupq_n_u64(r))
    );
}

INLINE(Vqdi,VQDI_CNBS) (Vqdi a,  int64_t l,  int64_t r)
{
    uint64x2_t c = vorrq_u64(
        vcltq_s64(a, vdupq_n_s64(l)),
        vcgtq_s64(a, vdupq_n_s64(r))
    );
    return  vreinterpretq_s64_u64(c);
}

INLINE(Vqdi,VQDF_CNBS) (Vqdf a,   double l,   double r)
{
    uint64x2_t c = vorrq_u64(
        vcltq_f64(a, vdupq_n_f64(l)),
        vcgtq_f64(a, vdupq_n_f64(r))
    );
    return  vreinterpretq_s64_u64(c);
}


#if _LEAVE_ARM_CNBS
}
#endif

#if _ENTER_ARM_CNBY
{
#endif

INLINE(ptrdiff_t, ADDR_CNBY)
(
    void volatile const *a,
    void volatile const *l,
    void volatile const *r
)
{
    return (a < l) || (r < a);
}


INLINE(  uchar,  UCHAR_CNBY)   (uchar a,   uchar l,   uchar r)
{
    return  (a < l) || (r < a);
}

INLINE(  schar,  SCHAR_CNBY)   (schar a,   schar l,   schar r)
{
    return  (a < l) || (r < a);
}

INLINE(   char,   CHAR_CNBY)    (char a,    char l,    char r)
{
    return  (a < l) || (r < a);
}


INLINE( ushort,  USHRT_CNBY)  (ushort a,  ushort l,  ushort r)
{
    return  (a < l) || (r < a);
}

INLINE(  short,   SHRT_CNBY)   (short a,   short l,   short r) 
{
    return  (a < l) || (r < a);
}


INLINE(   uint,   UINT_CNBY)    (uint a,    uint l,    uint r)
{
    return  (a < l) || (r < a);
}

INLINE(    int,    INT_CNBY)     (int a,     int l,     int r)
{
    return  (a < l) || (r < a);
}


INLINE(  ulong,  ULONG_CNBY)   (ulong a,   ulong l,   ulong r)
{
    return  (a < l) || (r < a);
}

INLINE(   long,   LONG_CNBY)    (long a,    long l,    long r)
{
    return  (a < l) || (r < a);
}


INLINE( ullong, ULLONG_CNBY)  (ullong a,  ullong l,  ullong r)
{
    return  (a < l) || (r < a);
}

INLINE(  llong,  LLONG_CNBY)   (llong a,   llong l,   llong r) 
{
    return  (a < l) || (r < a);
}


INLINE(int16_t,  FLT16_CNBY) (flt16_t a, flt16_t l, flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  (vclth_f16(a, l)|vclth_f16(r, a))&1;
#else
    return  (vclts_f32(a, l)|vclts_f32(r, a))&1;
#endif
}

INLINE(int32_t,    FLT_CNBY)   (float a,   float l,   float r) 
{
    return  (vclts_f32(a, l)|vclts_f32(r, a))&1;
}

INLINE(int64_t,    DBL_CNBY)  (double a,  double l,  double r) 
{
    return  (vcltd_f64(a, l)|vcltd_f64(r, a))&1;
}


#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cnbyqu) (QUAD_UTYPE a, QUAD_UTYPE l, QUAD_UTYPE r)
{
    return  (a < l) || (r < a);
}

INLINE(QUAD_ITYPE,cnbyqi) (QUAD_ITYPE a, QUAD_ITYPE l, QUAD_ITYPE r)
{
    return  (a < l) || (r < a);
}

INLINE(QUAD_ITYPE,cnbyqf) (QUAD_FTYPE a, QUAD_FTYPE l, QUAD_FTYPE r)
{
    return  (a < l) || (r < a);
}

#endif


INLINE(Vwbu,VWBU_CNBY) (Vwbu a,  uint8_t l,  uint8_t r)
{
    float       m = VWBU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   x = vreinterpret_u8_f32(v);
    uint8x8_t   b = vclt_u8(x, vdup_n_u8(l));
    uint8x8_t   c = vcgt_u8(x, vdup_n_u8(r));
    c = vorr_u8(b, c);
    c = vshr_n_u8(c, 7);
    v = vreinterpret_f32_u8(c);
    m = vget_lane_f32(v, 0);
    return  WBU_ASTV(m);
}

INLINE(Vwbi,VWBI_CNBY) (Vwbi a,   int8_t l,   int8_t r)
{
    float       m = VWBI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int8x8_t    x = vreinterpret_s8_f32(v);
    uint8x8_t   b = vclt_s8(x, vdup_n_s8(l));
    uint8x8_t   c = vcgt_s8(x, vdup_n_s8(r));
    c = vorr_u8(b, c);
    c = vshr_n_u8(c, 7);
    v = vreinterpret_f32_u8(c);
    m = vget_lane_f32(v, 0);
    return  WBI_ASTV(m);
}

INLINE(Vwbc,VWBC_CNBY) (Vwbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VWBI_ASBC(VWBI_CNBY(VWBC_ASBI(a), l, r));
#else
    return  VWBU_ASBC(VWBU_CNBY(VWBC_ASBU(a), l, r));
#endif
}


INLINE(Vwhu,VWHU_CNBY) (Vwhu a, uint16_t l, uint16_t r)
{
    float       m = VWHU_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    uint16x4_t  x = vreinterpret_u16_f32(v);
    uint16x4_t  b = vclt_u16(x, vdup_n_u16(l));
    uint16x4_t  c = vcgt_u16(x, vdup_n_u16(r));
    c = vorr_u16(b, c);
    c = vshr_n_u16(c, 15);
    v = vreinterpret_f32_u16(c);
    m = vget_lane_f32(v, 0);
    return  WHU_ASTV(m);
}

INLINE(Vwhi,VWHI_CNBY) (Vwhi a,  int16_t l,  int16_t r)
{
    float       m = VWHI_ASTM(a);
    float32x2_t v = vdup_n_f32(m);
    int16x4_t   x = vreinterpret_s16_f32(v);
    uint16x4_t  b = vclt_s16(x, vdup_n_s16(l));
    uint16x4_t  c = vcgt_s16(x, vdup_n_s16(r));
    c = vorr_u16(b, c);
    c = vshr_n_u16(c, 15);
    v = vreinterpret_f32_u16(c);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vwhi,VWHF_CNBY) (Vwhf a,  flt16_t l,  flt16_t r)
{
    float       m = VWHF_ASTM(a);
    float32x2_t v = vdup_n_f32(m);

#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t x = vreinterpret_s16_f32(v);
    uint16x4_t  b = vclt_f16(x, vdup_n_f16(l));
    uint16x4_t  c = vcgt_s16(x, vdup_n_f16(r));
    c = vorr_u16(b, c);
    c = vshr_n_u16(c, 15);
    v = vreinterpret_f32_u16(c);
#else
    float16x4_t t = vreinterpret_s16_f32(v);
    float32x4_t x = vcvt_f32_f16(t);
    uint32x4_t  b = vcltq_f32(x, vdupq_n_f32(l));
    uint32x4_t  c = vcgtq_f32(x, vdupq_n_f32(r));
    c = vorrq_u32(b, c);
    uint16x4_t  n = vmovn_u32(c);
    n = vshr_n_u16(n, 15);
    v = vreinterpret_f32_u16(n);
#endif

    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}


INLINE(Vwwu,VWWU_CNBY) (Vwwu a, uint32_t l, uint32_t r)
{
    float       m = VWWU_ASTM(a);
    uint32_t    c =  FLT_ASTU(m);
    c = (c < l) || (r < c);
    m = UINT32_ASTF(c);
    return  WWU_ASTV(m);
}

INLINE(Vwwi,VWWI_CNBY) (Vwwi a,  int32_t l,  int32_t r)
{
    float       m = VWWI_ASTM(a);
    int32_t     c =  FLT_ASTI(m);
    c = (c < l) || (r < c);
    m = INT32_ASTF(c);
    return  WWI_ASTV(m);
}

INLINE(Vwwi,VWWF_CNBY) (Vwwf a,    float l,    float r)
{
    float       m = VWWF_ASTM(a);
    uint32_t    c = 1&(vclts_f32(m, l)|vclts_f32(r, m));
    m = UINT32_ASTF(c);
    return  WWI_ASTV(m);
}


INLINE(Vdbu,VDBU_CNBY) (Vdbu a,  uint8_t l,  uint8_t r)
{
    a = vorr_u8(
        vclt_u8(a, vdup_n_u8(l)),
        vcgt_u8(a, vdup_n_u8(r))
    );
    return  vshr_n_u8(a, 7);
}

INLINE(Vdbi,VDBI_CNBY) (Vdbi a,   int8_t l,   int8_t r)
{
    uint8x8_t c = vorr_u8(
        vclt_s8(a, vdup_n_s8(l)),
        vcgt_s8(a, vdup_n_s8(r))
    );
    c = vshr_n_u8(c, 7);
    return  vreinterpret_s8_u8(c);
}

INLINE(Vdbc,VDBC_CNBY) (Vdbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_CNBY(VDBC_ASBI(a), l, r));
#else
    return  VDBI_ASBC(VDBU_CNBY(VDBC_ASBU(a), l, r));
#endif

}


INLINE(Vdhu,VDHU_CNBY) (Vdhu a, uint16_t l, uint16_t r) 
{
    a =  vorr_u16(
        vclt_u16(a, vdup_n_u16(l)),
        vcgt_u16(a, vdup_n_u16(r))
    );
    return vshr_n_u16(a, 15);
}

INLINE(Vdhi,VDHI_CNBY) (Vdhi a,  int16_t l,  int16_t r)
{
    uint16x4_t c = vorr_u16(
        vclt_s16(a, vdup_n_s16(l)),
        vcgt_s16(a, vdup_n_s16(r))
    );
    c = vshr_n_u16(c, 15);
    return  vreinterpret_s16_u16(c);
}

INLINE(Vdhi,VDHF_CNBY) (Vdhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x4_t c = vorr_u16(
        vclt_f16(a, vdup_n_f16(l)),
        vcgt_f16(a, vdup_n_f16(r))
    );
#else
    float32x4_t q = vcvt_f32_f16(a);
    uint32x4_t  v = vorrq_u32(
        vcltq_f32(q, vdupq_n_f32(l)),
        vcgtq_f32(q, vdupq_n_f32(r))
    );
    uint16x4_t  c = vmovn_u32(v);
#endif
    c = vshr_n_u16(c, 15);
    return  vreinterpret_s16_u16(c);
}


INLINE(Vdwu,VDWU_CNBY) (Vdwu a, uint32_t l, uint32_t r)
{
    a =  vorr_u32(
        vclt_u32(a, vdup_n_u32(l)),
        vcgt_u32(a, vdup_n_u32(r))
    );
    return vshr_n_u32(a, 31);
}

INLINE(Vdwi,VDWI_CNBY) (Vdwi a,  int32_t l,  int32_t r)
{
    uint32x2_t c = vorr_u32(
        vclt_s32(a, vdup_n_s32(l)),
        vcgt_s32(a, vdup_n_s32(r))
    );
    c = vshr_n_u32(c, 31);
    return  vreinterpret_s32_u32(c);
}

INLINE(Vdwi,VDWF_CNBY) (Vdwf a,    float l,    float r)
{
    uint32x2_t c = vorr_u32(
        vclt_f32(a, vdup_n_f32(l)),
        vcgt_f32(a, vdup_n_f32(r))
    );
    c = vshr_n_u32(c, 31);
    return  vreinterpret_s32_u32(c);
}


INLINE(Vddu,VDDU_CNBY) (Vddu a, uint64_t l, uint64_t r)
{
    a =  vorr_u64(
        vclt_u64(a, vdup_n_u64(l)),
        vcgt_u64(a, vdup_n_u64(r))
    );
    a = vshr_n_u64(a, 63);
    return  a;
}

INLINE(Vddi,VDDI_CNBY) (Vddi a,  int64_t l,  int64_t r)
{
    uint64x1_t c = vorr_u64(
        vclt_s64(a, vdup_n_s64(l)),
        vcgt_s64(a, vdup_n_s64(r))
    );
    c = vshr_n_u64(c, 63);
    return  vreinterpret_s64_u64(c);
}

INLINE(Vddi,VDDF_CNBY) (Vddf a,   double l,   double r)
{
    uint64x1_t c = vorr_u64(
        vclt_f64(a, vdup_n_f64(l)),
        vcgt_f64(a, vdup_n_f64(r))
    );
    c = vshr_n_u64(c, 63);
    return  vreinterpret_s64_u64(c);
}



INLINE(Vqbu,VQBU_CNBY) (Vqbu a,  uint8_t l,  uint8_t r)
{
    a =  vorrq_u8(
        vcltq_u8(a, vdupq_n_u8(l)),
        vcgtq_u8(a, vdupq_n_u8(r))
    );
    return vshrq_n_u8(a, 7);
}

INLINE(Vqbi,VQBI_CNBY) (Vqbi a,   int8_t l,   int8_t r)
{
    uint8x16_t c = vorrq_u8(
        vcltq_s8(a, vdupq_n_s8(l)),
        vcgtq_s8(a, vdupq_n_s8(r))
    );
    c = vshrq_n_u8(c, 7);
    return  vreinterpretq_s8_u8(c);
}

INLINE(Vqbc,VQBC_CNBY) (Vqbc a,     char l,     char r)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_CNBY(VQBC_ASBI(a), l, r));
#else
    return  VQBI_ASBC(VQBU_CNBY(VQBC_ASBU(a), l, r));
#endif

}


INLINE(Vqhu,VQHU_CNBY) (Vqhu a, uint16_t l, uint16_t r) 
{
    a = vorrq_u16(
        vcltq_u16(a, vdupq_n_u16(l)),
        vcgtq_u16(a, vdupq_n_u16(r))
    );
    return vshrq_n_u16(a,  15);
}

INLINE(Vqhi,VQHI_CNBY) (Vqhi a,  int16_t l,  int16_t r)
{
    uint16x8_t c = vorrq_u16(
        vcltq_s16(a, vdupq_n_s16(l)),
        vcgtq_s16(a, vdupq_n_s16(r))
    );
    c = vshrq_n_u16(c, 15);
    return  vreinterpretq_s16_u16(c);
}

INLINE(Vqhi,VQHF_CNBY) (Vqhf a,  flt16_t l,  flt16_t r)
{
#if defined(SPC_ARM_FP16_SIMD)
    uint16x8_t c = vorrq_u16(
        vcltq_f16(a, vdupq_n_f16(l)),
        vcgtq_f16(a, vdupq_n_f16(r))
    );
    c = vshrq_n_u16(c, 15);
    return  vreinterpretq_s16_u16(c);
#else
    return  vcombine_s16(
        VDHF_CNBY(vget_low_f16(a), l, r),
        VDHF_CNBY(vget_high_f16(a),l, r)
    );
#endif
}


INLINE(Vqwu,VQWU_CNBY) (Vqwu a, uint32_t l, uint32_t r)
{
    a = vorrq_u32(
        vcltq_u32(a, vdupq_n_u32(l)),
        vcgtq_u32(a, vdupq_n_u32(r))
    );
    return vshrq_n_u32(a, 31);
}

INLINE(Vqwi,VQWI_CNBY) (Vqwi a,  int32_t l,  int32_t r)
{
    uint32x4_t c = vorrq_u32(
        vcltq_s32(a, vdupq_n_s32(l)),
        vcgtq_s32(a, vdupq_n_s32(r))
    );
    c = vshrq_n_u32(c, 31);
    return  vreinterpretq_s32_u32(c);
}

INLINE(Vqwi,VQWF_CNBY) (Vqwf a,    float l,    float r)
{
    uint32x4_t c = vorrq_u32(
        vcltq_f32(a, vdupq_n_f32(l)),
        vcgtq_f32(a, vdupq_n_f32(r))
    );
    c = vshrq_n_u32(c, 31);
    return  vreinterpretq_s32_u32(c);
}


INLINE(Vqdu,VQDU_CNBY) (Vqdu a, uint64_t l, uint64_t r)
{
    a =  vorrq_u64(
        vcltq_u64(a, vdupq_n_u64(l)),
        vcgtq_u64(a, vdupq_n_u64(r))
    );
    return vshrq_n_u64(a, 63);
}

INLINE(Vqdi,VQDI_CNBY) (Vqdi a,  int64_t l,  int64_t r)
{
    uint64x2_t c = vorrq_u64(
        vcltq_s64(a, vdupq_n_s64(l)),
        vcgtq_s64(a, vdupq_n_s64(r))
    );
    c = vshrq_n_u64(c, 63);
    return  vreinterpretq_s64_u64(c);
}

INLINE(Vqdi,VQDF_CNBY) (Vqdf a,   double l,   double r)
{
    uint64x2_t c = vorrq_u64(
        vcltq_f64(a, vdupq_n_f64(l)),
        vcgtq_f64(a, vdupq_n_f64(r))
    );
    c = vshrq_n_u64(c, 63);
    return  vreinterpretq_s64_u64(c);
}


#if _LEAVE_ARM_CNBY
}
#endif

#if _ENTER_ARM_RAZB
{
#endif

/*  Round each floating point element in the operand to
    the nearest integer then convert the result to int8_t.
    Ties are rounded away from zero, i.e. towards ±∞.

    If rounded values are greater than INT8_MAX or less
    than INT8_MIN, the result is implementation defined.

*/

INLINE(int8_t,FLT16_RAZB) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtah_s16_f16(x);
#else
    return  vcvtas_s32_f32(x);
#endif
}

INLINE(int8_t,  FLT_RAZB)   (float x) {return vcvtas_s32_f32(x);}
INLINE(int8_t,  DBL_RAZB)  (double x) {return vcvtad_s64_f64(x);}
INLINE(int8_t,razbqf)  (QUAD_FTYPE x) {return vcvtad_s64_f64(x);}

INLINE(Vwbi,VDHF_RAZB) (Vdhf x)
{
// TODO
    return  VWBI_VOID;
}

INLINE(Vdbi,VQHF_RAZB) (Vqhf x)
{
// TODO
    return  VDBI_VOID;
}

INLINE(Vwbi,VQWF_RAZB) (Vqwf x)
{
    return  VQWI_CVBI(vcvtaq_s32_f32(x));
}

#if _LEAVE_ARM_RAZB
}
#endif

#if _ENTER_ARM_RAZH
{
#endif

/*  Round each floating point element in the operand to
    the nearest integer then convert the result to int16_t.
    Ties round away from zero, i.e. towards ±∞.

    If rounded values are greater than INT16_MAX or less
    than INT16_MIN, the result is implementation defined.

*/

INLINE(int16_t,FLT16_RAZH) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtah_s16_f16(x);
#else
    return  vcvtas_s32_f32(x);
#endif
}

INLINE(int16_t,  FLT_RAZH)   (float x) {return vcvtas_s32_f32(x);}
INLINE(int16_t,  DBL_RAZH)  (double x) {return vcvtad_s64_f64(x);}
INLINE(int16_t,razhqf)  (QUAD_FTYPE x) {return vcvtad_s64_f64(x);}

INLINE(Vwhi,VWHF_RAZH) (Vwhf x) {return VWHI_VOID;}
INLINE(Vdhi,VDHF_RAZH) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvta_s16_f16(x);
#else
    return  vmovn_s32(vcvtaq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vwhi,VDWF_RAZH) (Vdwf x)
{
    return  VDWI_CVHI(vcvta_s32_f32(x));
}

INLINE(Vwhi,VQDF_RAZH) (Vqdf x)
{
    return  VQDI_CVHI(vcvtaq_s64_f64(x));
}

#if _LEAVE_ARM_RAZH
}
#endif

#if _ENTER_ARM_RAZW
{
#endif

/*  Round each floating point element in the operand to
    the nearest integer then convert the result to int32_t.

    Ties are rounded away from zero, i.e. towards ±∞.

    If rounded values are greater than INT32_MAX or less
    than INT32_MIN, the result is implementation defined.

*/


INLINE(int32_t,FLT16_RAZW) (flt16_t x) {return x;}

INLINE(int32_t,  FLT_RAZW)   (float x) {return vcvtas_s32_f32(x);}
INLINE(int32_t,  DBL_RAZW)  (double x) {return vcvtad_s64_f64(x);}
INLINE(int32_t,razwqf)  (QUAD_FTYPE x) {return vcvtad_s64_f64(x);}

// TODO
INLINE(Vdwi,VWHF_RAZW) (Vwhf x) {return VDWI_VOID;}
INLINE(Vwwi,VWWF_RAZW) (Vwwf x)
{
    return  INT_ASTV(vcvtas_s32_f32(VWWF_ASTM(x)));
}

INLINE(Vdwi,VDWF_RAZW) (Vdwf x) {return vcvta_s32_f32(x);}
INLINE(Vwwi,VDDF_RAZW) (Vddf x)
{
    return  INT_ASTV(vcvtad_s64_f64(vget_lane_f64(x, 0)));
}

INLINE(Vqwi,VQWF_RAZW) (Vqwf x) {return vcvtaq_s32_f32(x);}
INLINE(Vdwi,VQDF_RAZW) (Vqdf x) {return vmovn_s64(vcvtaq_s64_f64(x));}

#if _LEAVE_ARM_RAZW
}
#endif

#if _ENTER_ARM_RAZD
{
#endif

/*  Round each floating point element in the operand to
    the nearest integer then convert the result to int64_t.
    Ties round away from zero, i.e. towards ±infinity.

    If rounded values are greater than INT64_MAX or less
    than INT64_MIN, the result is implementation defined.

*/

INLINE(int64_t,FLT16_RAZD) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtah_s64_f16(x);
#else
    return  vcvtas_s32_f32(x);
#endif
}

INLINE(int64_t,  FLT_RAZD)   (float x) {return vcvtas_s32_f32(x);}
INLINE(int64_t,  DBL_RAZD)  (double x) {return vcvtad_s64_f64(x);}
INLINE(int64_t,razdqf)  (QUAD_FTYPE x) {return vcvtad_s64_f64(x);}

// TODO
INLINE(Vqdi,VWHF_RAZD) (Vwhf x) {return VQDI_VOID;}
INLINE(Vddi,VWWF_RAZD) (Vwwf x)
{
    return  vdup_n_s64(vcvtas_s32_f32(VWWF_ASTM(x)));
}

INLINE(Vqdi,VDWF_RAZD) (Vdwf x)
{
    return  vcvtaq_s64_f64(vcvt_f64_f32(x));
}
INLINE(Vddi,VDDF_RAZD) (Vddf x)
{
    return  vcvta_s64_f64(x);
}

INLINE(Vqdi,VQDF_RAZD) (Vqdf x) {return vcvtaq_s64_f64(x);}

#if _LEAVE_ARM_RAZD
}
#endif

#if _ENTER_ARM_RAZF
{
#endif

INLINE(double,DBL_RAZF) (double x)
{
#if defined(SPC_ARM_FRINT)
    float64x1_t v = vdup_n_f63(x); // nsta vrndad_f64?...
    v = vrnda_f64(v);
    return  vget_lane_f64(v, 0);
#else
    union {
        double Value;
        struct {
            uint64_t
                Mant:   52,
                Expo:   11,
                Sign:   1,
                :       0;
        };
    } f = {x};

    if (f.Expo > 1074)
        return x;
    if (f.Expo <= 1022)
        return  (f.Expo == 1022)
        ?   (f.Sign ? -1.0 : 1.0)
        :   (f.Sign ? -0.0 : 0.0);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    uint64_t r = 52-(f.Expo-1022);
    uint64_t h = f.Mant>>r;
    uint64_t m = f.Mant-(h<<r);
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5) : (x+0.5))
        :   x;
    }
    if (h&1)
        h+=1;
    m = h<<r;
    if (m <= 0xfffffffffffffULL)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(float,FLT_RAZF) (float x)
{
#if defined(SPC_ARM_FRINT)
    float32x2_t v = vdup_n_f32(x); // nsta vrndas_f32?...
    v = vrnda_f32(v);
    return  vget_lane_f32(v, V2_K0);
#else
    union {
        float Value;
        struct {
            uint32_t
                Mant:   23,
                Expo:   8,
                Sign:   1,
                :       0;
        };
    } f = {x};

    if (f.Expo > 149)
        return x;
    if (f.Expo <= 126)
        return  (f.Expo == 126)
        ?   (f.Sign ? -1.0f : 1.0f)
        :   (f.Sign ? -0.0f : 0.0f);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    unsigned r = 23-(f.Expo-126);
    unsigned h = f.Mant>>r;
    unsigned m = f.Mant-(h<<r);
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5f) : (x+0.5f))
        :   x;
    }
    if (h&1)
        h+=1;
    m = h<<r;
    if (m <= 0x7fffff)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(flt16_t,FLT16_RAZF) (flt16_t x)
{
#if defined(SPC_ARM_FRINT)
    return  vrndah_f16(x);
#else
    return  FLT_RAZF(x);
#endif
}

INLINE(Vwhf,VWHF_RAZF) (Vwhf x)
{
    return  VWHF_VOID; // TODO
}

INLINE(Vwwf,VWWF_RAZF) (Vwwf x)
{
    return  WWF_ASTV(FLT_RAZF(VWWF_ASTM(x)));
}

INLINE(Vdhf,VDHF_RAZF) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f16(x);
#else
    x = vset_lane_f16(FLT16_RAZF(vget_lane_f16(x, V4_K0)), x, V4_K0);
    x = vset_lane_f16(FLT16_RAZF(vget_lane_f16(x, V4_K1)), x, V4_K1);
    x = vset_lane_f16(FLT16_RAZF(vget_lane_f16(x, V4_K2)), x, V4_K2);
    x = vset_lane_f16(FLT16_RAZF(vget_lane_f16(x, V4_K3)), x, V4_K3);
    return  x;
#endif
}

INLINE(Vdwf,VDWF_RAZF) (Vdwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f32(x);
#else
    x = vset_lane_f32(FLT_RAZF(vget_lane_f32(x, V2_K0)), x, V2_K0);
    x = vset_lane_f32(FLT_RAZF(vget_lane_f32(x, V2_K1)), x, V2_K1);
    return  x;
#endif
}

INLINE(Vddf,VDDF_RAZF) (Vddf x)
{
    return  DBL_ASTV(DBL_RAZF(vget_lane_f64(x, 0)));
}


INLINE(Vqhf,VQHF_RAZF) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f16(x);
#else
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, V8_K0)), x, V8_K0);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, V8_K1)), x, V8_K1);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, V8_K2)), x, V8_K2);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, V8_K3)), x, V8_K3);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, V8_K4)), x, V8_K4);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, V8_K5)), x, V8_K5);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, V8_K6)), x, V8_K6);
    x = vsetq_lane_f16(FLT16_RAZF(vgetq_lane_f16(x, V8_K7)), x, V8_K7);
    return  x;
#endif
}

INLINE(Vqwf,VQWF_RAZF) (Vqwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f32(x);
#else
    x = vsetq_lane_f32(FLT_RAZF(vgetq_lane_f32(x, V4_K0)), x, V4_K0);
    x = vsetq_lane_f32(FLT_RAZF(vgetq_lane_f32(x, V4_K1)), x, V4_K1);
    x = vsetq_lane_f32(FLT_RAZF(vgetq_lane_f32(x, V4_K2)), x, V4_K2);
    x = vsetq_lane_f32(FLT_RAZF(vgetq_lane_f32(x, V4_K3)), x, V4_K3);
    return  x;
#endif
}

INLINE(Vqdf,VQDF_RAZF) (Vqdf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f64(x);
#else
    x = vsetq_lane_f64(DBL_RAZF(vgetq_lane_f64(x, V2_K0)), x, V2_K0);
    x = vsetq_lane_f64(DBL_RAZF(vgetq_lane_f64(x, V2_K1)), x, V2_K1);
    return  x;
#endif
}

#if _LEAVE_ARM_RAZF
}
#endif


#if _ENTER_ARM_RTNB
{
#endif

INLINE(int8_t,FLT16_RTNB) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtmh_s16_f16(x);
#else
    return  vcvtms_s32_f32(x);
#endif
}

INLINE(int8_t,  FLT_RTNB)   (float x) {return vcvtms_s32_f32(x);}
INLINE(int8_t,  DBL_RTNB)  (double x) {return vcvtmd_s64_f64(x);}

INLINE(Vwbi,VDHF_RTNB) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  VDHI_CVBI(vcvtm_s16_f16(x));
#else
    return  VQWI_CVBI(vcvtmq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vdbi,VQHF_RTNB) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmovn_s16(vcvtmq_s16_f16(x));
#else
    return  VDBI_VOID;
/*
    float32x4_t r = vcvt_f32_f16(vget_high_f16(x));
    return  vmovn_s16(
        vcombine_s16(
            vmovn_s32(vcvtmq_s32_f32(l)),
            vmovn_s32(vcvtmq_s32_f32(r)),
        )
    );
*/    float32x4_t l = vcvt_f32_f16(vget_low_f16(x));
#endif
}

INLINE(Vwbi,VQWF_RTNB) (Vqwf x)
{
    return  VQWI_CVBI(vcvtmq_s32_f32(x));
}

#if _LEAVE_ARM_RTNB
}
#endif

#if _ENTER_ARM_RTNH
{
#endif

INLINE(int16_t,FLT16_RTNH) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtmh_s16_f16(x);
#else
    return  vcvtms_s32_f32(x);
#endif
}

INLINE(int16_t,  FLT_RTNH)   (float x) {return vcvtms_s32_f32(x);}
INLINE(int16_t,  DBL_RTNH)  (double x) {return vcvtmd_s64_f64(x);}

INLINE(Vwhi,VWHF_RTNH) (Vwhf x) 
{
    float       m = VWHF_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    int16x4_t   z = vcvtm_s16_f16(f);
#else
    float32x4_t q = vcvt_f32_f16(f);
    int16x4_t   z = vmovn_s32(vcvtmq_s32_f32(q));
#endif
    v = vreinterpret_f32_s16(z);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vdhi,VDHF_RTNH) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtm_s16_f16(x);
#else
    return  vmovn_s32(vcvtmq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vwhi,VDWF_RTNH) (Vdwf x)
{
    return  VDWI_CVHI(vcvtm_s32_f32(x));
}

INLINE(Vwhi,VQDF_RTNH) (Vqdf x)
{
    return  VQDI_CVHI(vcvtmq_s64_f64(x));
}

#if _LEAVE_ARM_RTNH
}
#endif

#if _ENTER_ARM_RTNW
{
#endif

INLINE(int32_t,FLT16_RTNW) (flt16_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtmh_s16_f16(x);
#else
    return  vcvtms_s32_f32(x);
#endif
}

INLINE(int32_t,  FLT_RTNW)   (float x) {return vcvtms_s32_f32(x);}
INLINE(int32_t,  DBL_RTNW)  (double x) {return vcvtmd_s64_f64(x);}

INLINE(Vdwi,VWHF_RTNW) (Vwhf x) 
{
    float       m = VWHF_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    int16x4_t   z = vcvtm_s16_f16(f);
    return  vget_low_s32(vmovl_s16(z));
#else
    float32x4_t q = vcvt_f32_f16(f);
    int32x4_t   z = vcvtmq_s32_f32(q);
    return vget_low_s32(z);
#endif
}

INLINE(Vwwi,VWWF_RTNW) (Vwwf x)
{
    return  INT_ASTV(vcvtms_s32_f32(VWWF_ASTM(x)));
}

INLINE(Vdwi,VDWF_RTNW) (Vdwf x) {return vcvtm_s32_f32(x);}
INLINE(Vwwi,VDDF_RTNW) (Vddf x)
{
    return  INT_ASTV(vcvtmd_s64_f64(vget_lane_f64(x, 0)));
}

INLINE(Vqwi,VQWF_RTNW) (Vqwf x) {return vcvtmq_s32_f32(x);}
INLINE(Vdwi,VQDF_RTNW) (Vqdf x) {return vmovn_s64(vcvtmq_s64_f64(x));}

#if _LEAVE_ARM_RTNW
}
#endif

#if _ENTER_ARM_RTND
{
#endif

INLINE(int64_t,FLT16_RTND) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtmh_s64_f16(x);
#else
    return  vcvtms_s32_f32(x);
#endif
}

INLINE(int64_t,  FLT_RTND)   (float x) {return vcvtms_s32_f32(x);}
INLINE(int64_t,  DBL_RTND)  (double x) {return vcvtmd_s64_f64(x);}

INLINE(Vqdi,VWHF_RTND) (Vwhf x) 
{
    return  vcvtmq_s64_f64(VWHF_CVDF(x));
}

INLINE(Vddi,VWWF_RTND) (Vwwf x)
{
    return  vcvtm_s64_f64(vdup_n_f64(VWWF_ASTV(x)));
}

INLINE(Vqdi,VDWF_RTND) (Vdwf x)
{
    return  vcvtmq_s64_f64(vcvt_f64_f32(x));
}

INLINE(Vddi,VDDF_RTND) (Vddf x)
{
    return  vcvtm_s64_f64(x);
}

INLINE(Vqdi,VQDF_RTND) (Vqdf x) {return vcvtmq_s64_f64(x);}

#if _LEAVE_ARM_RTND
}
#endif

#if _ENTER_ARM_RTNF
{
#endif
// TODO: fix this

INLINE(double,DBL_RTNF) (double x)
{
#if defined(SPC_ARM_FRINT)
    float64x1_t v = vdup_n_f64(x); // nsta vrndad_f64?...
    v = vrnda_f64(v);
    return  vget_lane_f64(v, 0);
#else
    union {
        double Value;
        struct {
            uint64_t
                Mant:   52,
                Expo:   11,
                Sign:   1,
                :       0;
        };
    } f = {x};
    // 
    if (f.Expo > 1074)
        return x;

    if (f.Expo <= 1022)
        return  (f.Expo == 1022)
        ?   (f.Sign ? -1.0 : 1.0)
        :   (f.Sign ? -0.0 : 0.0);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    uint64_t r = 52-(f.Expo-1022);
    uint64_t h = f.Mant>>r;
    uint64_t m = f.Mant-(h<<r);
    printf(
        "r=%" UINT64_DFMT ", "
        "h=%" UINT64_DFMT ", "
        "m=%" UINT64_DFMT "\n",
        r,h,m
    );
    // x=+1.99: r=51, h=1, m=2206763817411543
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5) : (x+0.5))
        :   x;
    }
    if (h&1)
        h-=1;
    m = h<<r;
    if (m <= 0xfffffffffffffULL)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(float,FLT_RTNF) (float x)
{
#if defined(SPC_ARM_FRINT)
    float32x2_t v = vdup_n_f32(x); // nsta vrndas_f32?...
    v = vrnda_f32(v);
    return  vget_lane_f32(v, V2_K0);
#else
    union {
        float Value;
        struct {
            uint32_t
                Mant:   23,
                Expo:   8,
                Sign:   1,
                :       0;
        };
    } f = {x};

    if (f.Expo > 149)
        return x;
    if (f.Expo <= 126)
        return  (f.Expo == 126)
        ?   (f.Sign ? -1.0f : 1.0f)
        :   (f.Sign ? -0.0f : 0.0f);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    unsigned r = 23-(f.Expo-126);
    unsigned h = f.Mant>>r;
    unsigned m = f.Mant-(h<<r);
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5f) : (x+0.5f))
        :   x;
    }
    if (h&1)
        h+=1;
    m = h<<r;
    if (m <= 0x7fffff)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(flt16_t,FLT16_RTNF) (flt16_t x)
{
#if defined(SPC_ARM_FRINT)
    return  vrndah_f16(x);
#else
    return  FLT_RTNF(x);
#endif
}

INLINE(Vwhf,VWHF_RTNF) (Vwhf x)
{
    return  VWHF_VOID; // TODO
}

INLINE(Vwwf,VWWF_RTNF) (Vwwf x)
{
    return  WWF_ASTV(FLT_RTNF(VWWF_ASTM(x)));
}

INLINE(Vdhf,VDHF_RTNF) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f16(x);
#else
    x = vset_lane_f16(FLT16_RTNF(vget_lane_f16(x, 0)), x, 0);
    x = vset_lane_f16(FLT16_RTNF(vget_lane_f16(x, 1)), x, 1);
    x = vset_lane_f16(FLT16_RTNF(vget_lane_f16(x, 2)), x, 2);
    x = vset_lane_f16(FLT16_RTNF(vget_lane_f16(x, 3)), x, 3);
    return  x;
#endif
}

INLINE(Vdwf,VDWF_RTNF) (Vdwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f32(x);
#else
    x = vset_lane_f32(FLT_RTNF(vget_lane_f32(x, 0)), x, 0);
    x = vset_lane_f32(FLT_RTNF(vget_lane_f32(x, 1)), x, 1);
    return  x;
#endif
}

INLINE(Vddf,VDDF_RTNF) (Vddf x)
{
    return  DBL_ASTV(DBL_RTNF(vget_lane_f64(x, 0)));
}


INLINE(Vqhf,VQHF_RTNF) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f16(x);
#else
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 0)), x, 0);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 1)), x, 1);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 2)), x, 2);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 3)), x, 3);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 4)), x, 4);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 5)), x, 5);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 6)), x, 6);
    x = vsetq_lane_f16(FLT16_RTNF(vgetq_lane_f16(x, 7)), x, 7);
    return  x;
#endif
}

INLINE(Vqwf,VQWF_RTNF) (Vqwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f32(x);
#else
    x = vsetq_lane_f32(FLT_RTNF(vgetq_lane_f32(x, 0)), x, 0);
    x = vsetq_lane_f32(FLT_RTNF(vgetq_lane_f32(x, 1)), x, 1);
    x = vsetq_lane_f32(FLT_RTNF(vgetq_lane_f32(x, 2)), x, 2);
    x = vsetq_lane_f32(FLT_RTNF(vgetq_lane_f32(x, 3)), x, 3);
    return  x;
#endif
}

INLINE(Vqdf,VQDF_RTNF) (Vqdf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f64(x);
#else
    x = vsetq_lane_f64(DBL_RTNF(vgetq_lane_f64(x, 0)), x, 0);
    x = vsetq_lane_f64(DBL_RTNF(vgetq_lane_f64(x, 1)), x, 1);
    return  x;
#endif
}

#if _LEAVE_ARM_RTNF
}
#endif


#if _ENTER_ARM_RTPB
{
#endif

INLINE(int8_t,FLT16_RTPB) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtph_s16_f16(x);
#else
    return  vcvtps_s32_f32(x);
#endif
}

INLINE(int8_t,  FLT_RTPB)   (float x) {return vcvtps_s32_f32(x);}
INLINE(int8_t,  DBL_RTPB)  (double x) {return vcvtpd_s64_f64(x);}

INLINE(Vwbi,VDHF_RTPB) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  VDHI_CVBI(vcvtp_s16_f16(x));
#else
    return  VQWI_CVBI(vcvtpq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vdbi,VQHF_RTPB) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmovn_s16(vcvtpq_s16_f16(x));
#else
    return  VDBI_VOID;
/*
    float32x4_t r = vcvt_f32_f16(vget_high_f16(x));
    return  vmovn_s16(
        vcombine_s16(
            vmovn_s32(vcvtpq_s32_f32(l)),
            vmovn_s32(vcvtpq_s32_f32(r)),
        )
    );
*/    float32x4_t l = vcvt_f32_f16(vget_low_f16(x));
#endif
}

INLINE(Vwbi,VQWF_RTPB) (Vqwf x)
{
    return  VQWI_CVBI(vcvtpq_s32_f32(x));
}

#if _LEAVE_ARM_RTPB
}
#endif

#if _ENTER_ARM_RTPH
{
#endif

INLINE(int16_t,FLT16_RTPH) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtph_s16_f16(x);
#else
    return  vcvtps_s32_f32(x);
#endif
}

INLINE(int16_t,  FLT_RTPH)   (float x) {return vcvtps_s32_f32(x);}
INLINE(int16_t,  DBL_RTPH)  (double x) {return vcvtpd_s64_f64(x);}

INLINE(Vwhi,VWHF_RTPH) (Vwhf x) 
{
    float       m = VWHF_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    int16x4_t   z = vcvtp_s16_f16(f);
#else
    float32x4_t q = vcvt_f32_f16(f);
    int16x4_t   z = vmovn_s32(vcvtpq_s32_f32(q));
#endif
    v = vreinterpret_f32_s16(z);
    m = vget_lane_f32(v, 0);
    return  WHI_ASTV(m);
}

INLINE(Vdhi,VDHF_RTPH) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtp_s16_f16(x);
#else
    return  vmovn_s32(vcvtpq_s32_f32(vcvt_f32_f16(x)));
#endif
}

INLINE(Vwhi,VDWF_RTPH) (Vdwf x)
{
    return  VDWI_CVHI(vcvtp_s32_f32(x));
}

INLINE(Vwhi,VQDF_RTPH) (Vqdf x)
{
    return  VQDI_CVHI(vcvtpq_s64_f64(x));
}

#if _LEAVE_ARM_RTPH
}
#endif

#if _ENTER_ARM_RTPW
{
#endif

INLINE(int32_t,FLT16_RTPW) (flt16_t x) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtph_s16_f16(x);
#else
    return  vcvtps_s32_f32(x);
#endif
}

INLINE(int32_t,  FLT_RTPW)   (float x) {return vcvtps_s32_f32(x);}
INLINE(int32_t,  DBL_RTPW)  (double x) {return vcvtpd_s64_f64(x);}

INLINE(Vdwi,VWHF_RTPW) (Vwhf x) 
{
    float       m = VWHF_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    float16x4_t f = vreinterpret_f16_f32(v);
#if defined(SPC_ARM_FP16_SIMD)
    int16x4_t   z = vcvtp_s16_f16(f);
    return  vget_low_s32(vmovl_s16(z));
#else
    float32x4_t q = vcvt_f32_f16(f);
    int32x4_t   z = vcvtpq_s32_f32(q);
    return vget_low_s32(z);
#endif
}

INLINE(Vwwi,VWWF_RTPW) (Vwwf x)
{
    return  INT_ASTV(vcvtps_s32_f32(VWWF_ASTM(x)));
}

INLINE(Vdwi,VDWF_RTPW) (Vdwf x) {return vcvtp_s32_f32(x);}
INLINE(Vwwi,VDDF_RTPW) (Vddf x)
{
    return  INT_ASTV(vcvtpd_s64_f64(vget_lane_f64(x, 0)));
}

INLINE(Vqwi,VQWF_RTPW) (Vqwf x) {return vcvtpq_s32_f32(x);}
INLINE(Vdwi,VQDF_RTPW) (Vqdf x) {return vmovn_s64(vcvtpq_s64_f64(x));}

#if _LEAVE_ARM_RTPW
}
#endif

#if _ENTER_ARM_RTPD
{
#endif

INLINE(int64_t,FLT16_RTPD) (flt16_t x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vcvtph_s64_f16(x);
#else
    return  vcvtps_s32_f32(x);
#endif
}

INLINE(int64_t,  FLT_RTPD)   (float x) {return vcvtps_s32_f32(x);}
INLINE(int64_t,  DBL_RTPD)  (double x) {return vcvtpd_s64_f64(x);}

INLINE(Vqdi,VWHF_RTPD) (Vwhf x) 
{
    return  vcvtpq_s64_f64(VWHF_CVDF(x));
}

INLINE(Vddi,VWWF_RTPD) (Vwwf x)
{
    return  vcvtp_s64_f64(vdup_n_f64(VWWF_ASTV(x)));
}

INLINE(Vqdi,VDWF_RTPD) (Vdwf x)
{
    return  vcvtpq_s64_f64(vcvt_f64_f32(x));
}

INLINE(Vddi,VDDF_RTPD) (Vddf x)
{
    return  vcvtp_s64_f64(x);
}

INLINE(Vqdi,VQDF_RTPD) (Vqdf x) {return vcvtpq_s64_f64(x);}

#if _LEAVE_ARM_RTPD
}
#endif

#if _ENTER_ARM_RTPF
{
#endif
// TODO: fix this

INLINE(double,DBL_RTPF) (double x)
{
#if defined(SPC_ARM_FRINT)
    float64x1_t v = vdup_n_f64(x); // nsta vrndad_f64?...
    v = vrnda_f64(v);
    return  vget_lane_f64(v, 0);
#else
    union {
        double Value;
        struct {
            uint64_t
                Mant:   52,
                Expo:   11,
                Sign:   1,
                :       0;
        };
    } f = {x};
    // 
    if (f.Expo > 1074)
        return x;

    if (f.Expo <= 1022)
        return  (f.Expo == 1022)
        ?   (f.Sign ? -1.0 : 1.0)
        :   (f.Sign ? -0.0 : 0.0);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    uint64_t r = 52-(f.Expo-1022);
    uint64_t h = f.Mant>>r;
    uint64_t m = f.Mant-(h<<r);
    printf(
        "r=%" UINT64_DFMT ", "
        "h=%" UINT64_DFMT ", "
        "m=%" UINT64_DFMT "\n",
        r,h,m
    );
    // x=+1.99: r=51, h=1, m=2206763817411543
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5) : (x+0.5))
        :   x;
    }
    if (h&1)
        h-=1;
    m = h<<r;
    if (m <= 0xfffffffffffffULL)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(float,FLT_RTPF) (float x)
{
#if defined(SPC_ARM_FRINT)
    float32x2_t v = vdup_n_f32(x); // nsta vrndas_f32?...
    v = vrnda_f32(v);
    return  vget_lane_f32(v, V2_K0);
#else
    union {
        float Value;
        struct {
            uint32_t
                Mant:   23,
                Expo:   8,
                Sign:   1,
                :       0;
        };
    } f = {x};

    if (f.Expo > 149)
        return x;
    if (f.Expo <= 126)
        return  (f.Expo == 126)
        ?   (f.Sign ? -1.0f : 1.0f)
        :   (f.Sign ? -0.0f : 0.0f);

    // tis a power of 2
    if (f.Mant == 0)
        return x;

    unsigned r = 23-(f.Expo-126);
    unsigned h = f.Mant>>r;
    unsigned m = f.Mant-(h<<r);
    if (!m)
    {
        return (h&1)
        ?   (f.Sign ? (x-0.5f) : (x+0.5f))
        :   x;
    }
    if (h&1)
        h+=1;
    m = h<<r;
    if (m <= 0x7fffff)
    {
        f.Mant = m;
    }
    else
    {
        f.Expo += 1;
        f.Mant = 0;
    }
    return  f.Value;
#endif
}

INLINE(flt16_t,FLT16_RTPF) (flt16_t x)
{
#if defined(SPC_ARM_FRINT)
    return  vrndah_f16(x);
#else
    return  FLT_RTPF(x);
#endif
}

INLINE(Vwhf,VWHF_RTPF) (Vwhf x)
{
    return  VWHF_VOID; // TODO
}

INLINE(Vwwf,VWWF_RTPF) (Vwwf x)
{
    return  WWF_ASTV(FLT_RTPF(VWWF_ASTM(x)));
}

INLINE(Vdhf,VDHF_RTPF) (Vdhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f16(x);
#else
    x = vset_lane_f16(FLT16_RTPF(vget_lane_f16(x, 0)), x, 0);
    x = vset_lane_f16(FLT16_RTPF(vget_lane_f16(x, 1)), x, 1);
    x = vset_lane_f16(FLT16_RTPF(vget_lane_f16(x, 2)), x, 2);
    x = vset_lane_f16(FLT16_RTPF(vget_lane_f16(x, 3)), x, 3);
    return  x;
#endif
}

INLINE(Vdwf,VDWF_RTPF) (Vdwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrnda_f32(x);
#else
    x = vset_lane_f32(FLT_RTPF(vget_lane_f32(x, 0)), x, 0);
    x = vset_lane_f32(FLT_RTPF(vget_lane_f32(x, 1)), x, 1);
    return  x;
#endif
}

INLINE(Vddf,VDDF_RTPF) (Vddf x)
{
    return  DBL_ASTV(DBL_RTPF(vget_lane_f64(x, 0)));
}


INLINE(Vqhf,VQHF_RTPF) (Vqhf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f16(x);
#else
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 0)), x, 0);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 1)), x, 1);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 2)), x, 2);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 3)), x, 3);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 4)), x, 4);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 5)), x, 5);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 6)), x, 6);
    x = vsetq_lane_f16(FLT16_RTPF(vgetq_lane_f16(x, 7)), x, 7);
    return  x;
#endif
}

INLINE(Vqwf,VQWF_RTPF) (Vqwf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f32(x);
#else
    x = vsetq_lane_f32(FLT_RTPF(vgetq_lane_f32(x, 0)), x, 0);
    x = vsetq_lane_f32(FLT_RTPF(vgetq_lane_f32(x, 1)), x, 1);
    x = vsetq_lane_f32(FLT_RTPF(vgetq_lane_f32(x, 2)), x, 2);
    x = vsetq_lane_f32(FLT_RTPF(vgetq_lane_f32(x, 3)), x, 3);
    return  x;
#endif
}

INLINE(Vqdf,VQDF_RTPF) (Vqdf x)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vrndaq_f64(x);
#else
    x = vsetq_lane_f64(DBL_RTPF(vgetq_lane_f64(x, 0)), x, 0);
    x = vsetq_lane_f64(DBL_RTPF(vgetq_lane_f64(x, 1)), x, 1);
    return  x;
#endif
}

#if _LEAVE_ARM_RTPF
}
#endif


#if _ENTER_ARM_MAXL
{
#endif

INLINE(void *,  ADDR_MAXL) 
(
    void const *a,
    void const *b
)
{
    return  (void *)(a < b ? b : a);
}

INLINE( _Bool,  BOOL_MAXL)  (_Bool a,  _Bool b) {return a&b;}
INLINE( uchar, UCHAR_MAXL)  (uchar a,  uchar b) {return a < b ? b : a;}
INLINE( schar, SCHAR_MAXL)  (schar a,  schar b) {return a < b ? b : a;}
INLINE(  char,  CHAR_MAXL)   (char a,   char b) {return a < b ? b : a;}
INLINE(ushort, USHRT_MAXL) (ushort a, ushort b) {return a < b ? b : a;}
INLINE( short,  SHRT_MAXL)  (short a,  short b) {return a < b ? b : a;}
INLINE(  uint,  UINT_MAXL)   (uint a,   uint b) {return a < b ? b : a;}
INLINE(   int,   INT_MAXL)    (int a,    int b) {return a < b ? b : a;}
INLINE( ulong, ULONG_MAXL)  (ulong a,  ulong b) {return a < b ? b : a;}
INLINE(  long,  LONG_MAXL)   (long a,   long b) {return a < b ? b : a;}
INLINE(ullong,ULLONG_MAXL) (ullong a, ullong b) {return a < b ? b : a;}
INLINE( llong, LLONG_MAXL)  (llong a,  llong b) {return a < b ? b : a;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,maxlqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return (a <= b ? b : a);
}

INLINE(QUAD_ITYPE,maxlqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return (a <= b ? b : a);
}

INLINE(QUAD_FTYPE,maxlqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return (a <= b ? b : a);
}

#endif

INLINE(Vdyu,VDYU_MAXL) (Vdyu a, Vdyu b)
{
#define     VDYU_MAXL(A, B) DYU_ASTV(vorr_u64(VDYU_ASTM(A), VDYU_ASTM(B)))
    return  VDYU_MAXL(a, b);
}

INLINE(Vdbu,VDBU_MAXL) (Vdbu a, Vdbu b) {return vmax_u8(a, b);}
INLINE(Vdbi,VDBI_MAXL) (Vdbi a, Vdbi b) {return vmax_s8(a, b);}
INLINE(Vdbc,VDBC_MAXL) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_MAXL(A, B) VDBI_ASBC(vmax_s8(VDBC_ASBI(A),VDBC_ASBI(B)))
#else
#   define  VDBC_MAXL(A, B) VDBU_ASBC(vmax_u8(VDBC_ASBU(A),VDBC_ASBU(B)))
#endif
    return  VDBC_MAXL(a, b);
}

INLINE(Vdhu,VDHU_MAXL) (Vdhu a, Vdhu b) {return vmax_u16(a, b);}
INLINE(Vdhi,VDHI_MAXL) (Vdhi a, Vdhi b) {return vmax_s16(a, b);}
INLINE(Vdhf,VDHF_MAXL) (Vdhf a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vmax_f16(a, b);
#else
    return  vcvt_f16_f32(
        vmaxq_f32(
            vcvt_f32_f16(a),
            vcvt_f32_f16(b)
        )
    );
#endif
}

INLINE(Vdwu,VDWU_MAXL) (Vdwu a, Vdwu b) {return vmax_u32(a, b);}
INLINE(Vdwi,VDWI_MAXL) (Vdwi a, Vdwi b) {return vmax_s32(a, b);}
INLINE(Vdwf,VDWF_MAXL) (Vdwf a, Vdwf b) {return vmax_f32(a, b);}

INLINE(Vddu,VDDU_MAXL) (Vddu a, Vddu b)
{
    b = vand_u64(
        veor_u64(a, b),
        vclt_u64(a, b)
    );
    return  veor_u64(a, b);
}

INLINE(Vddi,VDDI_MAXL) (Vddi a, Vddi b)
{
    b = vand_s64(
        veor_s64(a, b),
        vclt_s64(a, b)
    );
    return  veor_s64(a, b);
}

INLINE(Vddf,VDDF_MAXL) (Vddf a, Vddf b) {return vmax_f64(a, b);}


INLINE(Vqyu,VQYU_MAXL) (Vqyu a, Vqyu b)
{
#define     VQYU_MAXL(A, B) QYU_ASTV(vandq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_MAXL(a, b);
}
INLINE(Vqbu,VQBU_MAXL) (Vqbu a, Vqbu b) {return vmaxq_u8(a, b);}
INLINE(Vqbi,VQBI_MAXL) (Vqbi a, Vqbi b) {return vmaxq_s8(a, b);}
INLINE(Vqbc,VQBC_MAXL) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_MAXL(A, B) VQBI_ASBC(vmaxq_s8(VQBC_ASBI(A),VQBC_ASBI(B)))
#else
#   define  VQBC_MAXL(A, B) VQBU_ASBC(vmaxq_u8(VQBC_ASBU(A),VQBC_ASBU(B)))
#endif
    return  VQBC_MAXL(a, b);
}

INLINE(Vqhu,VQHU_MAXL) (Vqhu a, Vqhu b) {return vmaxq_u16(a, b);}
INLINE(Vqhi,VQHI_MAXL) (Vqhi a, Vqhi b) {return vmaxq_s16(a, b);}
INLINE(Vqhf,VQHF_MAXL) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vmaxq_f16(a, b);
#else
    float32x4_t al = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t bl = vcvt_f32_f16(vget_low_f16(b));
    float32x4_t ar = vcvt_f32_f16(vget_high_f16(a));
    float32x4_t br = vcvt_f32_f16(vget_high_f16(b));
    return  vcombine_f16(
        vcvt_f16_f32(vmaxq_f32(al, bl)),
        vcvt_f16_f32(vmaxq_f32(ar, br))
    );
#endif
}

INLINE(Vqwu,VQWU_MAXL) (Vqwu a, Vqwu b) {return vmaxq_u32(a, b);}
INLINE(Vqwi,VQWI_MAXL) (Vqwi a, Vqwi b) {return vmaxq_s32(a, b);}
INLINE(Vqwf,VQWF_MAXL) (Vqwf a, Vqwf b) {return vmaxq_f32(a, b);}

INLINE(Vqdu,VQDU_MAXL) (Vqdu a, Vqdu b)
{
/*  TODO: verify that this is superior to simply performing
    two VDDU_MAXL ops
*/
    b = vandq_u64(
        veorq_u64(a, b),
        vcltq_u64(a, b)
    );
    return veorq_u64(a, b);
}

INLINE(Vqdi,VQDI_MAXL) (Vqdi a, Vqdi b)
{
// use a = vandq_s64.. for MINL
    b = vandq_s64(
        veorq_s64(a, b),
        vcltq_s64(a, b)
    );
    return  veorq_s64(a, b);
}

INLINE(Vqdf,VQDF_MAXL) (Vqdf a, Vqdf b) {return vmaxq_f64(a, b);}

#if _LEAVE_ARM_MAXL
}
#endif

#if _ENTER_ARM_MINL
{
#endif

INLINE(void *,  ADDR_MINL) 
(
    void const *a,
    void const *b
)
{
    return  (void *)(a > b ? b : a);
}

INLINE( _Bool,  BOOL_MINL)  (_Bool a,  _Bool b) {return a&b;}
INLINE( uchar, UCHAR_MINL)  (uchar a,  uchar b) {return a > b ? b : a;}
INLINE( schar, SCHAR_MINL)  (schar a,  schar b) {return a > b ? b : a;}
INLINE(  char,  CHAR_MINL)   (char a,   char b) {return a > b ? b : a;}
INLINE(ushort, USHRT_MINL) (ushort a, ushort b) {return a > b ? b : a;}
INLINE( short,  SHRT_MINL)  (short a,  short b) {return a > b ? b : a;}
INLINE(  uint,  UINT_MINL)   (uint a,   uint b) {return a > b ? b : a;}
INLINE(   int,   INT_MINL)    (int a,    int b) {return a > b ? b : a;}
INLINE( ulong, ULONG_MINL)  (ulong a,  ulong b) {return a > b ? b : a;}
INLINE(  long,  LONG_MINL)   (long a,   long b) {return a > b ? b : a;}
INLINE(ullong,ULLONG_MINL) (ullong a, ullong b) {return a > b ? b : a;}
INLINE( llong, LLONG_MINL)  (llong a,  llong b) {return a > b ? b : a;}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,minlqu) (QUAD_UTYPE a, QUAD_UTYPE b)
{
    return (a > b) ? b : a;
}

INLINE(QUAD_ITYPE,minlqi) (QUAD_ITYPE a, QUAD_ITYPE b)
{
    return (a > b ? b : a);
}

INLINE(QUAD_FTYPE,minlqf) (QUAD_FTYPE a, QUAD_FTYPE b)
{
    return (a > b ? b : a);
}

#endif

INLINE(Vdyu,VDYU_MINL) (Vdyu a, Vdyu b)
{
#define     VDYU_MINL(A, B) DYU_ASTV(vand_u64(VDYU_ASTM(A), VDYU_ASTM(B)))
    return  VDYU_MINL(a, b);
}

INLINE(Vdbu,VDBU_MINL) (Vdbu a, Vdbu b) {return vmin_u8(a, b);}
INLINE(Vdbi,VDBI_MINL) (Vdbi a, Vdbi b) {return vmin_s8(a, b);}
INLINE(Vdbc,VDBC_MINL) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
#   define  VDBC_MINL(A, B) VDBI_ASBC(vmin_s8(VDBC_ASBI(A),VDBC_ASBI(B)))
#else
#   define  VDBC_MINL(A, B) VDBU_ASBC(vmin_u8(VDBC_ASBU(A),VDBC_ASBU(B)))
#endif
    return  VDBC_MINL(a, b);
}

INLINE(Vdhu,VDHU_MINL) (Vdhu a, Vdhu b) {return vmin_u16(a, b);}
INLINE(Vdhi,VDHI_MINL) (Vdhi a, Vdhi b) {return vmin_s16(a, b);}
INLINE(Vdhf,VDHF_MINL) (Vdhf a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return vmin_f16(a, b);
#else
    return  vcvt_f16_f32(
        vminq_f32(
            vcvt_f32_f16(a),
            vcvt_f32_f16(b)
        )
    );
#endif
}

INLINE(Vdwu,VDWU_MINL) (Vdwu a, Vdwu b) {return vmin_u32(a, b);}
INLINE(Vdwi,VDWI_MINL) (Vdwi a, Vdwi b) {return vmin_s32(a, b);}
INLINE(Vdwf,VDWF_MINL) (Vdwf a, Vdwf b) {return vmin_f32(a, b);}

INLINE(Vddu,VDDU_MINL) (Vddu a, Vddu b)
{
    a = vand_u64(
        veor_u64(a, b),
        vclt_u64(a, b)
    );
    return  veor_u64(a, b);
}

INLINE(Vddi,VDDI_MINL) (Vddi a, Vddi b)
{
    a = vand_s64(
        veor_s64(a, b),
        vclt_s64(a, b)
    );
    return  veor_s64(a, b);
}

INLINE(Vddf,VDDF_MINL) (Vddf a, Vddf b) {return vmin_f64(a, b);}


INLINE(Vqyu,VQYU_MINL) (Vqyu a, Vqyu b)
{
#define     VQYU_MINL(A, B) QYU_ASTV(vandq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_MINL(a, b);
}
INLINE(Vqbu,VQBU_MINL) (Vqbu a, Vqbu b) {return vminq_u8(a, b);}
INLINE(Vqbi,VQBI_MINL) (Vqbi a, Vqbi b) {return vminq_s8(a, b);}
INLINE(Vqbc,VQBC_MINL) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
#   define  VQBC_MINL(A, B) VQBI_ASBC(vminq_s8(VQBC_ASBI(A),VQBC_ASBI(B)))
#else
#   define  VQBC_MINL(A, B) VQBU_ASBC(vminq_u8(VQBC_ASBU(A),VQBC_ASBU(B)))
#endif
    return  VQBC_MINL(a, b);
}

INLINE(Vqhu,VQHU_MINL) (Vqhu a, Vqhu b) {return vminq_u16(a, b);}
INLINE(Vqhi,VQHI_MINL) (Vqhi a, Vqhi b) {return vminq_s16(a, b);}
INLINE(Vqhf,VQHF_MINL) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vminq_f16(a, b);
#else
    float32x4_t al = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t bl = vcvt_f32_f16(vget_low_f16(b));
    float32x4_t ar = vcvt_f32_f16(vget_high_f16(a));
    float32x4_t br = vcvt_f32_f16(vget_high_f16(b));
    return  vcombine_f16(
        vcvt_f16_f32(vminq_f32(al, bl)),
        vcvt_f16_f32(vminq_f32(ar, br))
    );
#endif
}

INLINE(Vqwu,VQWU_MINL) (Vqwu a, Vqwu b) {return vminq_u32(a, b);}
INLINE(Vqwi,VQWI_MINL) (Vqwi a, Vqwi b) {return vminq_s32(a, b);}
INLINE(Vqwf,VQWF_MINL) (Vqwf a, Vqwf b) {return vminq_f32(a, b);}

INLINE(Vqdu,VQDU_MINL) (Vqdu a, Vqdu b)
{
/*  TODO: verify that this is superior to simply performing
    two VDDU_MINL ops
*/
    a = vandq_u64(
        veorq_u64(a, b),
        vcltq_u64(a, b)
    );
    return veorq_u64(a, b);
}

INLINE(Vqdi,VQDI_MINL) (Vqdi a, Vqdi b)
{
// use b = vandq_s64.. for MAXL
    a = vandq_s64(
        veorq_s64(a, b),
        vcltq_s64(a, b)
    );
    return  veorq_s64(a, b);
}

INLINE(Vqdf,VQDF_MINL) (Vqdf a, Vqdf b) {return vminq_f64(a, b);}

#if _LEAVE_ARM_MINL
}
#endif

#if _ENTER_ARM_MAXV
{
#endif

INLINE(   _Bool,VDYU_MAXV) (Vdyu a)
{
#define     VDYU_MAXV(A)        \
(                               \
    (_Bool)                     \
    vtstd_u64(                  \
        VDDU_ASTV(VDYU_ASDU(A)),\
        vdup_n_u64(UINT64_MAX)  \
    )                           \
)
    return  VDYU_MAXV(a);
}

INLINE( uint8_t,VDBU_MAXV) (Vdbu a) {return vmaxv_u8(a);}
INLINE(  int8_t,VDBI_MAXV) (Vdbi a) {return vmaxv_s8(a);}
INLINE(    char,VDBC_MAXV) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_MAXV(A) vmaxv_s8(VDBC_ASBI(A))
#else
#   define  VDBC_MAXV(A) vmaxv_u8(VDBC_ASBU(A))
#endif
    return  VDBC_MAXV(a);
}
INLINE( uint16_t,VDHU_MAXV) (Vdhu a) {return vmaxv_u16(a);}
INLINE(  int16_t,VDHI_MAXV) (Vdhi a) {return vmaxv_s16(a);}
INLINE(  flt16_t,VDHF_MAXV) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
// no such thing as vmaxv_f16
#endif
    return  vmaxvq_f32(vcvt_f32_f16(a));
}

INLINE(uint32_t,VDWU_MAXV) (Vdwu a) {return vmaxv_u32(a);}
INLINE( int32_t,VDWI_MAXV) (Vdwi a) {return vmaxv_s32(a);}
INLINE(   float,VDWF_MAXV) (Vdwf a) {return vmaxv_f32(a);}

INLINE(   _Bool,VQYU_MAXV) (Vqyu a)
{
    uint64x2_t  m = VQYU_ASTM(a);
    m = vtstq_u64(m, vdupq_n_u64(UINT64_MAX));
    return  (vgetq_lane_u64(m, V2_K0)|vgetq_lane_u64(m, V2_K1))>>63;
}

INLINE( uint8_t,VQBU_MAXV) (Vqbu a) {return vmaxvq_u8(a);}
INLINE(  int8_t,VQBI_MAXV) (Vqbi a) {return vmaxvq_s8(a);}
INLINE(    char,VQBC_MAXV) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_MAXV(A, B) vmaxvq_s8(VQBC_ASBI(A))
#else
#   define  VQBC_MAXV(A, B) vmaxvq_u8(VQBC_ASBU(A))
#endif
    return  VQBC_MAXV(a, b);
}

INLINE(uint16_t,VQHU_MAXV) (Vqhu a) {return vmaxvq_u16(a);}
INLINE( int16_t,VQHI_MAXV) (Vqhi a) {return vmaxvq_s16(a);}
INLINE( flt16_t,VQHF_MAXV) (Vqhf a)
{

    float32x4_t lo = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t hi = vcvt_f32_f16(vget_high_f16(a));
    lo = vsetq_lane_f32(
        vmaxvq_f32(lo),
        lo,
        V4_K0
    );
    lo = vsetq_lane_f32(
        vmaxvq_f32(hi),
        lo,
        V4_K0
    );
    return  vmaxv_f32(vget_low_f32(lo));
}

INLINE(uint32_t,VQWU_MAXV) (Vqwu a) {return vmaxvq_u32(a);}
INLINE( int32_t,VQWI_MAXV) (Vqwi a) {return vmaxvq_s32(a);}
INLINE(   float,VQWF_MAXV) (Vqwf a) {return vmaxvq_f32(a);}
INLINE(uint64_t,VQDU_MAXV) (Vqdu a)
{
    return  vget_lane_u64(
        VDDU_MAXL(
            vget_low_u64(a),
            vget_high_u64(a)
        ),
        0
    );
}

INLINE( int64_t,VQDI_MAXV) (Vqdi a)
{
    return  vget_lane_s64(
        VDDI_MAXL(
            vget_low_s64(a),
            vget_high_s64(a)
        ),
        0
    );
}

INLINE(  double,VQDF_MAXV) (Vqdf a) {return vmaxvq_f64(a);}

#if _LEAVE_ARM_MAXV
}
#endif

#if _ENTER_ARM_MINV
{
#endif

INLINE(   _Bool,VDYU_MINV) (Vdyu a)
{
#define     VDYU_MINV(A) ((_Bool)(UINT64_MAX==vget_lane_u64(VDYU_ASDU(A),0)))
    return  VDYU_MINV(a);
}

INLINE( uint8_t,VDBU_MINV) (Vdbu a) {return vminv_u8(a);}
INLINE(  int8_t,VDBI_MINV) (Vdbi a) {return vminv_s8(a);}
INLINE(    char,VDBC_MINV) (Vdbc a)
{
#if CHAR_MIN
#   define  VDBC_MINV(A) vminv_s8(VDBC_ASBI(A))
#else
#   define  VDBC_MINV(A) vminv_u8(VDBC_ASBU(A))
#endif
    return  VDBC_MINV(a);
}

INLINE( uint16_t,VDHU_MINV) (Vdhu a) {return vminv_u16(a);}
INLINE(  int16_t,VDHI_MINV) (Vdhi a) {return vminv_s16(a);}
INLINE(  flt16_t,VDHF_MINV) (Vdhf a)
{
#if defined(SPC_ARM_FP16_SIMD)
// no such thing as vminv_f16
#endif
    return  vminvq_f32(vcvt_f32_f16(a));
}

INLINE(uint32_t,VDWU_MINV) (Vdwu a) {return vminv_u32(a);}
INLINE( int32_t,VDWI_MINV) (Vdwi a) {return vminv_s32(a);}
INLINE(   float,VDWF_MINV) (Vdwf a) {return vminv_f32(a);}

INLINE(   _Bool,VQYU_MINV) (Vqyu a)
{
#define     VQYU_MINV(A) ((_Bool)(128==vaddvq_u8(vcntq_u8(VQYU_ASBU(A)))))
    return  VQYU_MINV(a);
}

INLINE( uint8_t,VQBU_MINV) (Vqbu a) {return vminvq_u8(a);}
INLINE(  int8_t,VQBI_MINV) (Vqbi a) {return vminvq_s8(a);}
INLINE(    char,VQBC_MINV) (Vqbc a)
{
#if CHAR_MIN
#   define  VQBC_MINV(A, B) vminvq_s8(VQBC_ASBI(A))
#else
#   define  VQBC_MINV(A, B) vminvq_u8(VQBC_ASBU(A))
#endif
    return  VQBC_MINV(a, b);
}

INLINE(uint16_t,VQHU_MINV) (Vqhu a) {return vminvq_u16(a);}
INLINE( int16_t,VQHI_MINV) (Vqhi a) {return vminvq_s16(a);}
INLINE( flt16_t,VQHF_MINV) (Vqhf a)
{

    float32x4_t lo = vcvt_f32_f16(vget_low_f16(a));
    float32x4_t hi = vcvt_f32_f16(vget_high_f16(a));
    lo = vsetq_lane_f32(
        vminvq_f32(lo),
        lo,
        V4_K0
    );
    lo = vsetq_lane_f32(
        vminvq_f32(hi),
        lo,
        V4_K0
    );
    return  vminv_f32(vget_low_f32(lo));
}

INLINE(uint32_t,VQWU_MINV) (Vqwu a) {return vminvq_u32(a);}
INLINE( int32_t,VQWI_MINV) (Vqwi a) {return vminvq_s32(a);}
INLINE(   float,VQWF_MINV) (Vqwf a) {return vminvq_f32(a);}
INLINE(uint64_t,VQDU_MINV) (Vqdu a)
{
    return  vget_lane_u64(
        VDDU_MINL(
            vget_low_u64(a),
            vget_high_u64(a)
        ),
        0
    );
}

INLINE( int64_t,VQDI_MINV) (Vqdi a)
{
    return  vget_lane_s64(
        VDDI_MINL(
            vget_low_s64(a),
            vget_high_s64(a)
        ),
        0
    );
}

INLINE(  double,VQDF_MINV) (Vqdf a) {return vminvq_f64(a);}

#if _LEAVE_ARM_MINV
}
#endif

#if _ENTER_ARM_CNT1
{
#endif

INLINE( UCHAR_STG(UTYPE), UCHAR_CNT1)  (uchar x) {return __builtin_popcount(x);}
INLINE( SCHAR_STG(ITYPE), SCHAR_CNT1)  (schar x) {return __builtin_popcount(x);}
INLINE(             char,  CHAR_CNT1)   (char x) {return __builtin_popcount(x);}
INLINE( USHRT_STG(UTYPE), USHRT_CNT1) (ushort x) {return __builtin_popcount(x);}
INLINE(  SHRT_STG(ITYPE),  SHRT_CNT1)  (short x) {return __builtin_popcount(x);}
INLINE(  UINT_STG(UTYPE),  UINT_CNT1)   (uint x) {return __builtin_popcount(x);}
INLINE(   INT_STG(ITYPE),   INT_CNT1)    (int x) {return __builtin_popcount(x);}
INLINE( ULONG_STG(UTYPE), ULONG_CNT1)  (ulong x) {return __builtin_popcountl(x);}
INLINE(  LONG_STG(UTYPE),  LONG_CNT1)   (long x) {return __builtin_popcountl(x);}
INLINE(ULLONG_STG(UTYPE),ULLONG_CNT1) (ullong x) {return __builtin_popcountll(x);}
INLINE( LLONG_STG(UTYPE), LLONG_CNT1)  (llong x) {return __builtin_popcountll(x);}

INLINE(Vwbu,VWBU_CNT1) (Vwbu x)
{
    float       m = VWBU_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   n = vreinterpret_u8_f32(v);
    n = vcnt_u8(n);
    v = vreinterpret_f32_u8(n);
    m = vget_lane_f32(v, V2_K0);
    return  WBU_ASTV(m);
}

INLINE(Vwbi,VWBI_CNT1) (Vwbi x)
{
    float       m = VWBI_ASTM(x);
    float32x2_t v = vdup_n_f32(m);
    uint8x8_t   n = vreinterpret_u8_f32(v);
    n = vcnt_u8(n);
    v = vreinterpret_f32_u8(n);
    m = vget_lane_f32(v, V2_K0);
    return  WBI_ASTV(m);
}

INLINE(Vwbc,VWBC_CNT1) (Vwbc x)
{
    return  VWBU_ASBC(VWBU_CNT1(VWBC_ASBU(x)));
}

INLINE(Vwhu,VWHU_CNT1) (Vwhu x)
{
    float32x2_t m = vdup_n_f32(VWHU_ASTM(x));
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vcnt_u8(v);
    uint16x4_t  c = vpaddl_u8(v);
    m = vreinterpret_f32_u16(c);
    return  WHU_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vwhi,VWHI_CNT1) (Vwhi x)
{
    float32x2_t m = vdup_n_f32(VWHI_ASTM(x));
    uint8x8_t   v = vreinterpret_u8_f32(m);
    v = vcnt_u8(v);
    uint16x4_t  c = vpaddl_u8(v);
    m = vreinterpret_f32_u16(c);
    return  WHI_ASTV(vget_lane_f32(m, V2_K0));
}

INLINE(Vwwu,VWWU_CNT1) (Vwwu x)
{
    return UINT_ASTV(
        __builtin_popcount(FLT_ASWU(VWWU_ASTM(x)))
    );
}

INLINE(Vwwi,VWWI_CNT1) (Vwwi x)
{
    return INT_ASTV(
        __builtin_popcount(FLT_ASWU(VWWI_ASTM(x)))
    );
}

INLINE(Vdbu,VDBU_CNT1) (Vdbu x) {return vcnt_u8(x);}
INLINE(Vdbi,VDBI_CNT1) (Vdbi x) {return vcnt_s8(x);}
INLINE(Vdbc,VDBC_CNT1) (Vdbc x) {return VDBU_ASBC(vcnt_u8(VDBC_ASBU(x)));}

INLINE(Vdhu,VDHU_CNT1) (Vdhu x)
{
    return  vpaddl_u8(vcnt_u8(vreinterpret_u8_u16(x)));
}

INLINE(Vdhi,VDHI_CNT1) (Vdhi x)
{
    return  vpaddl_s8(vcnt_s8(vreinterpret_s8_s16(x)));
}


INLINE(Vdwu,VDWU_CNT1) (Vdwu x)
{
    return  vpaddl_u16(vpaddl_u8(vcnt_u8(x)));
}

INLINE(Vdwi,VDWI_CNT1) (Vdwi x)
{
    return  vpaddl_s16(vpaddl_s8(vcnt_s8(x)));
}


INLINE(Vddu,VDDU_CNT1) (Vddu x)
{
    return  vdup_n_u64(vaddv_u8(vcnt_u8(vreinterpret_u8_u64(x))));
}

INLINE(Vddi,VDDI_CNT1) (Vddi x)
{
    return  vdup_n_u64(vaddv_u8(vcnt_u8(vreinterpret_u8_u64(x))));
}

INLINE(Vqbu,VQBU_CNT1) (Vqbu x) {return vcntq_u8(x);}
INLINE(Vqbi,VQBI_CNT1) (Vqbi x) {return vcntq_s8(x);}
INLINE(Vqbc,VQBC_CNT1) (Vqbc x) {return VQBU_ASBC(vcntq_u8(VQBC_ASBU(x)));}

INLINE(Vqhu,VQHU_CNT1) (Vqhu x)
{
    return vpaddlq_u8(vcntq_u8(vreinterpretq_u8_u16(x)));
}

INLINE(Vqhi,VQHI_CNT1) (Vqhi x)
{
    return vpaddlq_s8(vcntq_s8(vreinterpretq_s8_s16(x)));
}


INLINE(Vqwu,VQWU_CNT1) (Vqwu x)
{
    return vpaddlq_u16(vpaddlq_u8(vcntq_u8(vreinterpretq_u8_u32(x))));
}

INLINE(Vqwi,VQWI_CNT1) (Vqwi x)
{
    return vpaddlq_s16(vpaddlq_s8(vcntq_s8(vreinterpretq_s8_u32(x))));
}

/*  TODO: check if arm's cnt1qdu/cnt1qdi actually use
    different sequences and if so, make both use the more
    efficient method.
*/

INLINE(Vqdu,VQDU_CNT1) (Vqdu x)
{
    return vpaddlq_u32(
        vpaddlq_u16(
            vpaddlq_u8(
                vcntq_u8(vreinterpretq_u8_u32(x))
            )
        )
    );
}

INLINE(Vqdi,VQDI_CNT1) (Vqdi x)
{
    int8x16_t n = vcntq_s8(vreinterpretq_s8_s64(x));
    return vcombine_s64(
        vdup_n_s64(vaddv_s8(vget_low_s8(n))),
        vdup_n_s64(vaddv_s8(vget_high_s8(n)))
    );
}

#if _LEAVE_ARM_CNT1
}
#endif

#if _ENTER_ARM_CNTS
{
#endif

INLINE( uchar, UCHAR_CNTS)  (uchar x)
{
#define     UCHAR_CNTS(X)       \
vget_lane_u8(                   \
    vreinterpret_u8_s8(         \
        vcls_u8(vdup_n_u8(X))   \
    ),                          \
    0                           \
)
    return  UCHAR_CNTS(x);
}

INLINE( schar, SCHAR_CNTS)  (schar x)
{
#define     SCHAR_CNTS(X)   \
vget_lane_s8(               \
    vcls_s8(vdup_n_s8(X)),  \
    0                       \
)
    return  SCHAR_CNTS(x);
}

INLINE(  char,  CHAR_CNTS)   (char x)
{
#if CHAR_MIN
    return  SCHAR_CNTS(x);
#else
    return  UCHAR_CNTS(x);
#endif
}

INLINE(ushort, USHRT_CNTS) (ushort x)
{
#define     USHRT_CNTS(X)       \
vget_lane_u16(                  \
    vreinterpret_u16_s16(       \
        vcls_u16(vdup_n_u16(X)) \
    ),                          \
    0                           \
)
    return  USHRT_CNTS(x);
}

INLINE(short, SHRT_CNTS) (short x)
{
#define     SHRT_CNTS(X)    \
vget_lane_s16(              \
    vcls_s16(vdup_n_s16(X)),\
    0                       \
)
    return  SHRT_CNTS(x);
}

INLINE(  uint,  UINT_CNTS)   (uint x) {return __cls(x);}
INLINE(   int,   INT_CNTS)    (int x) {return __cls(x);}
INLINE( ulong, ULONG_CNTS)  (ulong x) {return __clsl(x);}
INLINE(  long,  LONG_CNTS)   (long x) {return __clsl(x);}
INLINE(ullong,ULLONG_CNTS) (ullong x) {return __clsll(x);}
INLINE( llong, LLONG_CNTS)  (llong x) {return __clsll(x);}

INLINE(Vwbu,VWBU_CNTS) (Vwbu x)
{
    float32x2_t v = vdup_n_f32(VWBU_ASTM(x));
    int8x8_t    n = vcls_u8(vreinterpret_u8_f32(v));
    v = vreinterpret_f32_s8(n);
    return  WBU_ASTV(vget_lane_f32(v, V2_K0));
}

INLINE(Vwbi,VWBI_CNTS) (Vwbi x)
{
    float32x2_t v = vdup_n_f32(VWBI_ASTM(x));
    int8x8_t    n = vcls_s8(vreinterpret_s8_f32(v));
    v = vreinterpret_f32_s8(n);
    return  WBI_ASTV(vget_lane_f32(v, V2_K0));
}

INLINE(Vwbc,VWBC_CNTS) (Vwbc x)
{
#if CHAR_MIN
#   define  VWBC_CNTS(X)    VWBI_ASBC(VWBI_CNTS(VWBC_ASBI(X)))
#else
#   define  VWBC_CNTS(X)    VWBU_ASBC(VWBU_CNTS(VWBC_ASBU(X)))
#endif
    return  VWBC_CNTS(x);
}


INLINE(Vwhu,VWHU_CNTS) (Vwhu x)
{
    float32x2_t v = vdup_n_f32(VWHU_ASTM(x));
    int16x4_t   n = vcls_u16(vreinterpret_u16_f32(v));
    v = vreinterpret_f32_s16(n);
    return  WHU_ASTV(vget_lane_f32(v, V2_K0));
}

INLINE(Vwhi,VWHI_CNTS) (Vwhi x)
{
    float32x2_t v = vdup_n_f32(VWHI_ASTM(x));
    int16x4_t   n = vcls_s16(vreinterpret_s16_f32(v));
    v = vreinterpret_f32_s16(n);
    return  WHI_ASTV(vget_lane_f32(v, V2_K0));
}


INLINE(Vwwu,VWWU_CNTS) (Vwwu x)
{
#define     VWWU_CNTS(X)    UINT_ASTV(__cls(VWWU_ASTV(X)))
    return  VWWU_CNTS(x);
}

INLINE(Vwwi,VWWI_CNTS) (Vwwi x)
{
#define     VWWI_CNTS(X)    INT_ASTV(__cls(VWWI_ASTV(X)))
    return  VWWI_CNTS(x);
}


INLINE(Vdbu,VDBU_CNTS) (Vdbu x) {return vreinterpret_u8_s8(vcls_u8(x));}
INLINE(Vdbi,VDBI_CNTS) (Vdbi x) {return vcls_s8(x);}
INLINE(Vdbc,VDBC_CNTS) (Vdbc x)
{
#if CHAR_MIN
#   define  VDBC_CNTS(X)    VDBI_ASBC(VDBI_CNTS(VDBC_ASBI(X)))
#else
#   define  VDBC_CNTS(X)    VDBU_ASBC(VDBU_CNTS(VDBC_ASBU(X)))
#endif
    return  VDBC_CNTS(x);
}

INLINE(Vdhu,VDHU_CNTS) (Vdhu x)
{
    return  vreinterpret_u16_s16(vcls_u16(x));
}

INLINE(Vdhi,VDHI_CNTS) (Vdhi x)
{
    return  vcls_s16(x);
}


INLINE(Vdwu,VDWU_CNTS) (Vdwu x)
{
    return  vreinterpret_u32_s32(vcls_u32(x));
}

INLINE(Vdwi,VDWI_CNTS) (Vdwi x)
{
    return  vcls_s32(x);
}



INLINE(Vddu,VDDU_CNTS) (Vddu x)
{
    return  vdup_n_u64(__clsll(vget_lane_u64(x, 0)));
}

INLINE(Vddi,VDDI_CNTS) (Vddi x)
{
    return  vdup_n_s64(__clsll(vget_lane_s64(x, 0)));
}

INLINE(Vqbu,VQBU_CNTS) (Vqbu x) {return vreinterpretq_u8_s8(vclsq_u8(x));}
INLINE(Vqbi,VQBI_CNTS) (Vqbi x) {return vclsq_s8(x);}
INLINE(Vqbc,VQBC_CNTS) (Vqbc x)
{
#if CHAR_MIN
#   define  VQBC_CNTS(X)    VQBI_ASBC(VQBI_CNTS(VQBC_ASBI(X)))
#else
#   define  VQBC_CNTS(X)    VQBU_ASBC(VQBU_CNTS(VQBC_ASBU(X)))
#endif
    return  VQBC_CNTS(x);
}

INLINE(Vqhu,VQHU_CNTS) (Vqhu x)
{
    return  vreinterpretq_u16_s16(vclsq_u16(x));
}

INLINE(Vqhi,VQHI_CNTS) (Vqhi x)
{
    return  vclsq_s16(x);
}


INLINE(Vqwu,VQWU_CNTS) (Vqwu x)
{
    return  vreinterpretq_u32_s32(vclsq_u32(x));
}

INLINE(Vqwi,VQWI_CNTS) (Vqwi x)
{
    return  vclsq_s32(x);
}

INLINE(Vqdu,VQDU_CNTS) (Vqdu x)
{
    return vcombine_u64(
        VDDU_CNTS(vget_low_u64(x)),
        VDDU_CNTS(vget_high_u64(x))
    );
}

INLINE(Vqdi,VQDI_CNTS) (Vqdi x)
{
    return vcombine_s64(
        VDDU_CNTS(vget_low_u64(x)),
        VDDU_CNTS(vget_high_u64(x))
    );
}

#if _LEAVE_ARM_CNTS
}
#endif

#if _ENTER_ARM_CSZL
{
#endif

INLINE( UCHAR_STG(UTYPE), UCHAR_CSZL)  (uchar x) {return __builtin_ctz(x)-24;}
INLINE( SCHAR_STG(ITYPE), SCHAR_CSZL)  (schar x) {return UCHAR_CSZL(x&0xff);}
INLINE(             char,  CHAR_CSZL)   (char x) {return UCHAR_CSZL(x);}
INLINE( USHRT_STG(UTYPE), USHRT_CSZL) (ushort x) {return __builtin_ctz(x)-16;}
INLINE(  SHRT_STG(ITYPE),  SHRT_CSZL)  (short x) {return USHRT_CSZL(x&0xffff);}
INLINE(  UINT_STG(UTYPE),  UINT_CSZL)   (uint x) {return __builtin_ctz(x);}
INLINE(   INT_STG(ITYPE),   INT_CSZL)    (int x) {return __builtin_ctz(x);}
INLINE( ULONG_STG(UTYPE), ULONG_CSZL)  (ulong x) {return __builtin_ctzl(x);}
INLINE(  LONG_STG(ITYPE),  LONG_CSZL)   (long x) {return __builtin_ctzl(x);}
INLINE(ULLONG_STG(UTYPE),ULLONG_CSZL) (ullong x) {return __builtin_ctzll(x);}
INLINE( LLONG_STG(UTYPE), LLONG_CSZL)  (llong x) {return __builtin_ctzll(x);}

#if QUAD_NLLONG == 1
#   define  UINT128_CSZL    ULLONG_CSZL
#   define  INT128_CSZL     LLONG_CSZL
#else

INLINE(unsigned __int128,UINT128_CSZL) (unsigned __int128 x)
{
    Quad r;
    Quad v = {.U=x};
    if (v.Hi.U)
        r.Lo.U = __builtin_ctzll(v.Lo.U);
    else
        r.Lo.U = __builtin_ctzll(v.Hi.U)+64;
    return r.U;
}

INLINE(  signed __int128, INT128_CSZL)   (signed __int128 x)
{
    return  UINT128_CSZL(x);
}
#endif

INLINE(float,WBU_CSZL) (float x)
{
    uint8x8_t m = vreinterpret_u8_f32(vdup_n_f32(x));
    m = VDBU_REVY(m);
    m = vclz_u8(m);
    return vget_lane_f32(vreinterpret_f32_u8(m), V2_K0);
}

INLINE(float,WHU_CSZL) (float x)
{
    uint16x4_t m = vreinterpret_u16_f32(vdup_n_f32(x));
    m = VDHU_REVY(m);
    m = vclz_u16(m);
    return vget_lane_f32(vreinterpret_f32_u16(m), V2_K0);
}

INLINE(float,WWU_CSZL) (float x)
{
    uint32x2_t m = vreinterpret_u32_f32(vdup_n_f32(x));
    m = VDWU_REVY(m);
    m = vclz_u32(m);
    return vget_lane_f32(vreinterpret_f32_u32(m), V2_K0);
}

INLINE(Vwbu,VWBU_CSZL) (Vwbu x)
{
    return WBU_ASTV(WBU_CSZL(VWBU_ASTM(x)));
}

INLINE(Vwbi,VWBI_CSZL) (Vwbi x)
{
    return WBI_ASTV(WBU_CSZL(VWBI_ASTM(x)));
}

INLINE(Vwbc,VWBC_CSZL) (Vwbc x)
{
    return WBC_ASTV(WBU_CSZL(VWBC_ASTM(x)));
}

INLINE(Vwhu,VWHU_CSZL) (Vwhu x)
{
    return WHU_ASTV(WHU_CSZL(VWHU_ASTM(x)));
}

INLINE(Vwhi,VWHI_CSZL) (Vwhi x)
{
    return WHI_ASTV(WHU_CSZL(VWHI_ASTM(x)));
}


INLINE(Vwwu,VWWU_CSZL) (Vwwu x)
{
    return WWU_ASTV(WWU_CSZL(VWWU_ASTM(x)));
}

INLINE(Vwwi,VWWI_CSZL) (Vwwi x)
{
    return WWI_ASTV(WWU_CSZL(VWWI_ASTM(x)));
}

INLINE(Vdbu,VDBU_CSZL) (Vdbu x) {return vclz_u8(VDBU_REVY(x));}
INLINE(Vdbi,VDBI_CSZL) (Vdbu x) {return vclz_s8(VDBI_REVY(x));}
INLINE(Vdbc,VDBC_CSZL) (Vdbc x)
{
    return VDBU_ASBC(vclz_u8(VDBU_REVY(VDBC_ASBU(x))));
}
INLINE(Vdhu,VDHU_CSZL) (Vdhu x) {return vclz_u16(VDHU_REVY(x));}
INLINE(Vdhi,VDHI_CSZL) (Vdhi x) {return vclz_s16(VDHI_REVY(x));}
INLINE(Vdwu,VDWU_CSZL) (Vdwu x) {return vclz_u32(VDWU_REVY(x));}
INLINE(Vdwi,VDWI_CSZL) (Vdwi x) {return vclz_s32(VDWI_REVY(x));}
INLINE(Vddu,VDDU_CSZL) (Vddu x)
{
    return  vdup_n_u64(__builtin_ctzll(vget_lane_u64(x, 0)));
}
INLINE(Vddi,VDDI_CSZL) (Vddi x)
{
    return  vdup_n_s64(__builtin_ctzll(vget_lane_s64(x, 0)));
}
INLINE(Vqbu,VQBU_CSZL) (Vqbu x) {return vclzq_u8(VQBU_REVY(x));}
INLINE(Vqbi,VQBI_CSZL) (Vqbu x) {return vclzq_s8(VQBI_REVY(x));}
INLINE(Vqbc,VQBC_CSZL) (Vqbc x)
{
    return VQBU_ASBC(VQBU_CSZL(VQBC_ASBU(x)));
}
INLINE(Vqhu,VQHU_CSZL) (Vqhu x) {return vclzq_u16(VQHU_REVY(x));}
INLINE(Vqhi,VQHI_CSZL) (Vqhi x) {return vclzq_s16(VQHI_REVY(x));}
INLINE(Vqwu,VQWU_CSZL) (Vqwu x) {return vclzq_u32(VQWU_REVY(x));}
INLINE(Vqwi,VQWI_CSZL) (Vqwi x) {return vclzq_s32(VQWI_REVY(x));}
INLINE(Vqdu,VQDU_CSZL) (Vqdu x)
{
    return vcombine_u64(
        VDDU_CSZL(vget_low_u64(x)),
        VDDU_CSZL(vget_high_u64(x))
    );
}

INLINE(Vqdi,VQDI_CSZL) (Vqdi x)
{
    return vcombine_s64(
        VDDI_CSZL(vget_low_s64(x)),
        VDDI_CSZL(vget_high_s64(x))
    );
}

#if _LEAVE_ARM_CSZL
}
#endif

#if _ENTER_ARM_CSZR
{
#endif

INLINE( UCHAR_STG(UTYPE), UCHAR_CSZR)  (uchar x) {return __builtin_clz(x)-24;}
INLINE( SCHAR_STG(ITYPE), SCHAR_CSZR)  (schar x) {return UCHAR_CSZR(x&0xff);}
INLINE(             char,  CHAR_CSZR)   (char x) {return UCHAR_CSZR(x);}
INLINE( USHRT_STG(UTYPE), USHRT_CSZR) (ushort x) {return __builtin_clz(x)-16;}
INLINE(  SHRT_STG(ITYPE),  SHRT_CSZR)  (short x) {return USHRT_CSZR(x&0xffff);}
INLINE(  UINT_STG(UTYPE),  UINT_CSZR)   (uint x) {return __builtin_clz(x);}
INLINE(   INT_STG(ITYPE),   INT_CSZR)    (int x) {return __builtin_clz(x);}
INLINE( ULONG_STG(UTYPE), ULONG_CSZR)  (ulong x) {return __builtin_clzl(x);}
INLINE(  LONG_STG(ITYPE),  LONG_CSZR)   (long x) {return __builtin_clzl(x);}
INLINE(ULLONG_STG(UTYPE),ULLONG_CSZR) (ullong x) {return __builtin_clzll(x);}
INLINE( LLONG_STG(UTYPE), LLONG_CSZR)  (llong x) {return __builtin_clzll(x);}

#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,cszrqu) (QUAD_UTYPE x)
{
    QUAD_TYPE v = {.U=x};
    return
        v.Hi.U
        ?   __builtin_clzll(v.Hi.U)
        :   __builtin_clzll(v.Lo.U)+64;
}

INLINE(QUAD_ITYPE,cszrqi) (QUAD_ITYPE x)
{
    QUAD_TYPE r;
    QUAD_TYPE v = {.I=x};
    if (v.Hi.I)
        r.Lo.I = __builtin_clzll(v.Lo.I);
    else
        r.Lo.I = __builtin_clzll(v.Hi.I)+64;
    return r.I;
}

#endif

INLINE(float,WBU_CSZR) (float x)
{
    uint8x8_t m = vreinterpret_u8_f32(vdup_n_f32(x));
    m = vclz_u8(m);
    return vget_lane_f32(vreinterpret_f32_u8(m), 0);
}

INLINE(float,WHU_CSZR) (float x)
{
    uint16x4_t m = vreinterpret_u16_f32(vdup_n_f32(x));
    m = vclz_u16(m);
    return vget_lane_f32(vreinterpret_f32_u16(m), 0);
}

INLINE(float,WWU_CSZR) (float x)
{
    uint32x2_t m = vreinterpret_u32_f32(vdup_n_f32(x));
    m = vclz_u32(m);
    return vget_lane_f32(vreinterpret_f32_u32(m), 0);
}

INLINE(Vwbu,VWBU_CSZR) (Vwbu x)
{
    return WBU_ASTV(WBU_CSZR(VWBU_ASTM(x)));
}

INLINE(Vwbi,VWBI_CSZR) (Vwbi x)
{
    return WBI_ASTV(WBU_CSZR(VWBI_ASTM(x)));
}

INLINE(Vwbc,VWBC_CSZR) (Vwbc x)
{
    return WBC_ASTV(WBU_CSZR(VWBC_ASTM(x)));
}

INLINE(Vwhu,VWHU_CSZR) (Vwhu x)
{
    return WHU_ASTV(WHU_CSZR(VWHU_ASTM(x)));
}

INLINE(Vwhi,VWHI_CSZR) (Vwhi x)
{
    return WHI_ASTV(WHU_CSZR(VWHI_ASTM(x)));
}


INLINE(Vwwu,VWWU_CSZR) (Vwwu x)
{
    return WWU_ASTV(WWU_CSZR(VWWU_ASTM(x)));
}

INLINE(Vwwi,VWWI_CSZR) (Vwwi x)
{
    return WWI_ASTV(WWU_CSZR(VWWI_ASTM(x)));
}

INLINE(Vdbu,VDBU_CSZR) (Vdbu x) {return vclz_u8(x);}
INLINE(Vdbi,VDBI_CSZR) (Vdbu x) {return vclz_s8(x);}
INLINE(Vdbc,VDBC_CSZR) (Vdbc x)
{
    return VDBU_ASBC(vclz_u8(VDBC_ASBU(x)));
}
INLINE(Vdhu,VDHU_CSZR) (Vdhu x) {return vclz_u16(x);}
INLINE(Vdhi,VDHI_CSZR) (Vdhi x) {return vclz_s16(x);}
INLINE(Vdwu,VDWU_CSZR) (Vdwu x) {return vclz_u32(x);}
INLINE(Vdwi,VDWI_CSZR) (Vdwi x) {return vclz_s32(x);}
INLINE(Vddu,VDDU_CSZR) (Vddu x)
{
    return  vdup_n_u64(__builtin_clzll(vget_lane_u64(x, 0)));
}
INLINE(Vddi,VDDI_CSZR) (Vddi x)
{
    return  vdup_n_s64(__builtin_clzll(vget_lane_s64(x, 0)));
}
INLINE(Vqbu,VQBU_CSZR) (Vqbu x) {return vclzq_u8(x);}
INLINE(Vqbi,VQBI_CSZR) (Vqbu x) {return vclzq_s8(x);}
INLINE(Vqbc,VQBC_CSZR) (Vqbc x)
{
    return VQBU_ASBC(vclzq_u8(VQBC_ASBU(x)));
}
INLINE(Vqhu,VQHU_CSZR) (Vqhu x) {return vclzq_u16(x);}
INLINE(Vqhi,VQHI_CSZR) (Vqhi x) {return vclzq_s16(x);}
INLINE(Vqwu,VQWU_CSZR) (Vqwu x) {return vclzq_u32(x);}
INLINE(Vqwi,VQWI_CSZR) (Vqwi x) {return vclzq_s32(x);}
INLINE(Vqdu,VQDU_CSZR) (Vqdu x)
{
    return vcombine_u64(
        VDDU_CSZR(vget_low_u64(x)),
        VDDU_CSZR(vget_high_u64(x))
    );
}

INLINE(Vqdi,VQDI_CSZR) (Vqdi x)
{
    return vcombine_s64(
        VDDI_CSZR(vget_low_s64(x)),
        VDDI_CSZR(vget_high_s64(x))
    );
}

#if _LEAVE_ARM_CSZR
}
#endif


#if _ENTER_ARM_DIVL
{
#endif

INLINE( _Bool,  BOOL_DIVL)  (_Bool a,  _Bool b) {return a/b;}

INLINE( uchar, UCHAR_DIVL)  (uchar a,  uchar b) {return a/b;}
INLINE( schar, SCHAR_DIVL)  (schar a,  schar b) {return a/b;}
INLINE(  char,  CHAR_DIVL)   (char a,   char b) {return a/b;}

INLINE(ushort, USHRT_DIVL) (ushort a, ushort b) {return a/b;}
INLINE( short,  SHRT_DIVL)  (short a,  short b) {return a/b;}

INLINE(  uint,  UINT_DIVL)   (uint a,   uint b) {return a/b;}
INLINE(   int,   INT_DIVL)    (int a,    int b) {return a/b;}

INLINE( ulong, ULONG_DIVL)  (ulong a,  ulong b) {return a/b;}
INLINE(  long,  LONG_DIVL)   (long a,   long b) {return a/b;}

INLINE(ullong,ULLONG_DIVL) (ullong a, ullong b) {return a/b;}
INLINE( llong, LLONG_DIVL)  (llong a,  llong b) {return a/b;}

#if QUAD_NLLONG == 2
INLINE(QUAD_UTYPE,divloqu) (QUAD_UTYPE a, QUAD_UTYPE b) {return a/b;}
INLINE(QUAD_ITYPE,divloqi) (QUAD_ITYPE a, QUAD_UTYPE b) {return a/b;}
#endif


INLINE(Vwyu,VWYU_DIVL) (Vwyu a, Vwyu b)
{
    float p = VWYU_ASTM(a);
    float q = VWYU_ASTM(b);
    float32x2_t l = vdup_n_f32(p);
    float32x2_t r = vdup_n_f32(q);
    uint32x2_t  x = vreinterpret_u32_f32(l);
    uint32x2_t  y = vreinterpret_u32_f32(r);
    x = vand_u32(x, y);
    l = vreinterpret_f32_u32(x);
    p = vget_lane_f32(l, 0);
    return  WYU_ASTV(p);
}


INLINE(Vwbu,VWBU_DIVL) (Vwbu a, Vwbu b)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t l = VWBU_CVHF(a);
    float16x4_t r = VWBU_CVHF(b);
    l = vdiv_f16(l, r);
    return  VDHF_CVBU(l);
#else
    float32x4_t l = VWBU_CVWF(a);
    float32x4_t r = VWBU_CVWF(b);
    l = vdivq_f32(l, r);
    return  VQWF_CVBU(l);
#endif
}

INLINE(Vwbi,VWBI_DIVL) (Vwbi a, Vwbi b)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t l = VWBI_CVHF(a);
    float16x4_t r = VWBI_CVHF(b);
    l = vdiv_f16(l, r);
    return  VDHF_CVBI(l);
#else
    float32x4_t l = VWBI_CVWF(a);
    float32x4_t r = VWBI_CVWF(b);
    l = vdivq_f32(l, r);
    return  VQWF_CVBI(l);
#endif
}

INLINE(Vwbc,VWBC_DIVL) (Vwbc a, Vwbc b)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x4_t l = VWBC_CVHF(a);
    float16x4_t r = VWBC_CVHF(b);
    l = vdiv_f16(l, r);
    return  VDHF_CVBC(l);
#else
    float32x4_t l = VWBC_CVWF(a);
    float32x4_t r = VWBC_CVWF(b);
    l = vdivq_f32(l, r);
    return  VQWF_CVBC(l);
#endif
}


INLINE(Vwhu,VWHU_DIVL) (Vwhu a, Vwhu b)
{
    float32x2_t l = VWHU_CVWF(a);
    float32x2_t r = VWHU_CVWF(b);
    l = vdiv_f32(l, r);
    return  VDWF_CVHU(l);
}

INLINE(Vwhi,VWHI_DIVL) (Vwhi a, Vwhi b)
{
    float32x2_t l = VWHI_CVWF(a);
    float32x2_t r = VWHI_CVWF(b);
    l = vdiv_f32(l, r);
    return  VDWF_CVHI(l);
}

    
INLINE(Vwwu,VWWU_DIVL) (Vwwu a, Vwwu b)
{
    return  UINT_ASTV( (VWWU_ASTV(a)/VWWU_ASTV(b)) );
}

INLINE(Vwwi,VWWI_DIVL) (Vwwi a, Vwwi b)
{
    return  INT_ASTV( (VWWI_ASTV(a)/VWWI_ASTV(b)) );
}


INLINE(Vdyu,VDYU_DIVL) (Vdyu a, Vdyu b)
{
    return  VDDU_ASYU(vand_u64(VDYU_ASDU(a), VDYU_ASDU(b)));
}


INLINE(Vdbu,VDBU_DIVL) (Vdbu a, Vdbu b)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t l = VDBU_CVHF(a);
    float16x8_t r = VDBU_CVHF(b);
    r = vdivq_f16(l, r);
    return  VQHF_CVBU(r);
#else
    uint8x8_t   c = {0};
    c = vset_lane_u8(vget_lane_u8(a, 0)/vget_lane_u8(b, 0), c, 0);
    c = vset_lane_u8(vget_lane_u8(a, 1)/vget_lane_u8(b, 1), c, 1);
    c = vset_lane_u8(vget_lane_u8(a, 2)/vget_lane_u8(b, 2), c, 2);
    c = vset_lane_u8(vget_lane_u8(a, 3)/vget_lane_u8(b, 3), c, 3);
    c = vset_lane_u8(vget_lane_u8(a, 4)/vget_lane_u8(b, 4), c, 4);
    c = vset_lane_u8(vget_lane_u8(a, 5)/vget_lane_u8(b, 5), c, 5);
    c = vset_lane_u8(vget_lane_u8(a, 6)/vget_lane_u8(b, 6), c, 6);
    c = vset_lane_u8(vget_lane_u8(a, 7)/vget_lane_u8(b, 7), c, 7);
    return c;
#endif
}

INLINE(Vdbi,VDBI_DIVL) (Vdbi a, Vdbi b)
{
#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t l = VDBI_CVHF(a);
    float16x8_t r = VDBI_CVHF(b);
    r = vdivq_f16(l, r);
    return  VQHF_CVBI(r);
#else
    int8x8_t    c = {0};
    c = vset_lane_s8(vget_lane_s8(a, 0)/vget_lane_s8(b, 0), c, 0);
    c = vset_lane_s8(vget_lane_s8(a, 1)/vget_lane_s8(b, 1), c, 1);
    c = vset_lane_s8(vget_lane_s8(a, 2)/vget_lane_s8(b, 2), c, 2);
    c = vset_lane_s8(vget_lane_s8(a, 3)/vget_lane_s8(b, 3), c, 3);
    c = vset_lane_s8(vget_lane_s8(a, 4)/vget_lane_s8(b, 4), c, 4);
    c = vset_lane_s8(vget_lane_s8(a, 5)/vget_lane_s8(b, 5), c, 5);
    c = vset_lane_s8(vget_lane_s8(a, 6)/vget_lane_s8(b, 6), c, 6);
    c = vset_lane_s8(vget_lane_s8(a, 7)/vget_lane_s8(b, 7), c, 7);
    return c;
#endif
}

INLINE(Vdbc,VDBC_DIVL) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
    return  VDBI_ASBC(VDBI_DIVL(VDBC_ASBI(a), VDBC_ASBI(b)));
#else
    return  VDBU_ASBC(VDBU_DIVL(VDBC_ASBU(a), VDBC_ASBU(b)));
#endif
}


INLINE(Vdhu,VDHU_DIVL) (Vdhu a, Vdhu b)
{
#if 0
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(a));
    float32x4_t r = vcvtq_f32_u32(vmovl_u16(b));
    r = vdivq_f32(l, r);
    return  vmovn_u32(vcvtq_u32_f32(r));
#else
/*  
Will the compiler be able to keep itself from needlessly
adding an and #0xffff to each operand?
*/
    uint16x4_t  c = {0};
    c = vset_lane_u16(vget_lane_u16(a, 0)/vget_lane_u16(b, 0), c, 0);
    c = vset_lane_u16(vget_lane_u16(a, 1)/vget_lane_u16(b, 1), c, 1);
    c = vset_lane_u16(vget_lane_u16(a, 2)/vget_lane_u16(b, 2), c, 2);
    c = vset_lane_u16(vget_lane_u16(a, 3)/vget_lane_u16(b, 3), c, 3);
    return  c;
#endif
}

INLINE(Vdhi,VDHI_DIVL) (Vdhi a, Vdhi b)
{
#if 0
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(a));
    float32x4_t r = vcvtq_f32_s32(vmovl_s16(b));
    r = vdivq_f32(l, r);
    return  vmovn_s32(vcvtq_s32_f32(r));
#else
    int16x4_t  c = {0};
    c = vset_lane_s16(vget_lane_s16(a, 0)/vget_lane_s16(b, 0), c, 0);
    c = vset_lane_s16(vget_lane_s16(a, 1)/vget_lane_s16(b, 1), c, 1);
    c = vset_lane_s16(vget_lane_s16(a, 2)/vget_lane_s16(b, 2), c, 2);
    c = vset_lane_s16(vget_lane_s16(a, 3)/vget_lane_s16(b, 3), c, 3);
    return  c;
#endif
}


INLINE(Vdwu,VDWU_DIVL) (Vdwu a, Vdwu b)
{
    uint32x2_t c = {0};
    c = vset_lane_u32(vget_lane_u32(a, 0)/vget_lane_u32(b, 0), c, 0);
    c = vset_lane_u32(vget_lane_u32(a, 1)/vget_lane_u32(b, 1), c, 1);
    return  c;
}

INLINE(Vdwi,VDWI_DIVL) (Vdwi a, Vdwi b)
{
    int32x2_t c = {0};
    c = vset_lane_s32(vget_lane_s32(a, 0)/vget_lane_s32(b, 0), c, 0);
    c = vset_lane_s32(vget_lane_s32(a, 1)/vget_lane_s32(b, 1), c, 1);
    return  c;
}


INLINE(Vddu,VDDU_DIVL) (Vddu a, Vddu b)
{
    return  vdup_n_u64(vget_lane_u64(a, 0)/vget_lane_u64(b, 0));
}

INLINE(Vddi,VDDI_DIVL) (Vddi a, Vddi b)
{
    return  vdup_n_s64(vget_lane_s64(a, 0)/vget_lane_s64(b, 0));
}


INLINE(Vqyu,VQYU_DIVL) (Vqyu a, Vqyu b)
{
    return  VQDU_ASYU(vandq_u64(VQYU_ASDU(a), VQYU_ASDU(b)));
}


INLINE(Vqbu,VQBU_DIVL) (Vqbu a, Vqbu b)
{
    return  vcombine_u8(
        VDBU_DIVL(vget_low_u8(a),  vget_low_u8(b)),
        VDBU_DIVL(vget_high_u8(a), vget_high_u8(b))
    );
}

INLINE(Vqbi,VQBI_DIVL) (Vqbi a, Vqbi b)
{
    return  vcombine_s8(
        VDBI_DIVL(vget_low_s8(a),  vget_low_s8(b)),
        VDBI_DIVL(vget_high_s8(a), vget_high_s8(b))
    );
}

INLINE(Vqbc,VQBC_DIVL) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
    return  VQBI_ASBC(VQBI_DIVL(VQBC_ASBI(a), VQBC_ASBI(b)));
#else
    return  VQBU_ASBC(VQBU_DIVL(VQBC_ASBU(a), VQBC_ASBU(b)));
#endif
}


INLINE(Vqhu,VQHU_DIVL) (Vqhu a, Vqhu b)
{
    return  vcombine_u16(
        VDHU_DIVL(vget_low_u16(a),  vget_low_u16(b)),
        VDHU_DIVL(vget_high_u16(a), vget_high_u16(b))
    );
}

INLINE(Vqhi,VQHI_DIVL) (Vqhi a, Vqhi b)
{
    return  vcombine_s16(
        VDHI_DIVL(vget_low_s16(a),  vget_low_s16(b)),
        VDHI_DIVL(vget_high_s16(a), vget_high_s16(b))
    );
}


INLINE(Vqwu,VQWU_DIVL) (Vqwu a, Vqwu b)
{
    return  vcombine_u32(
        VDWU_DIVL(vget_low_u32(a),  vget_low_u32(b)),
        VDWU_DIVL(vget_high_u32(a), vget_high_u32(b))
    );
}

INLINE(Vqwi,VQWI_DIVL) (Vqwi a, Vqwi b)
{
    return  vcombine_s32(
        VDWI_DIVL(vget_low_s32(a),  vget_low_s32(b)),
        VDWI_DIVL(vget_high_s32(a), vget_high_s32(b))
    );
}


INLINE(Vqdu,VQDU_DIVL) (Vqdu a, Vqdu b)
{
    return  vcombine_u64(
        vdup_n_u64(vgetq_lane_u64(a, 0)/vgetq_lane_u64(b, 0)),
        vdup_n_u64(vgetq_lane_u64(a, 1)/vgetq_lane_u64(b, 1))
    );
}

INLINE(Vqdi,VQDI_DIVL) (Vqdi a, Vqdi b)
{
    return  vcombine_s64(
        vdup_n_s64(vgetq_lane_s64(a, 0)/vgetq_lane_s64(b, 0)),
        vdup_n_s64(vgetq_lane_s64(a, 1)/vgetq_lane_s64(b, 1))
    );
}

#if _LEAVE_ARM_DIVL
}
#endif


#if _ENTER_ARM_DIV2
{
#endif


INLINE( uint8_t, USHRT_DIV2) (ushort a,  uint8_t b)
{
    return  a/b;
}

INLINE(  int8_t,  SHRT_DIV2)  (short a,   int8_t b)
{
    return  a/b;
}


INLINE(uint16_t,  UINT_DIV2)   (uint a, uint16_t b)
{
    return  a/b;
}

INLINE( int16_t,   INT_DIV2)    (int a,  int16_t b)
{
    return  a/b;
}

#if DWRD_NLONG == 2

INLINE(uint16_t, ULONG_DIV2)  (ulong a, uint16_t b)
{
    return  a/b;
}

INLINE( int16_t,  LONG_DIV2)   (long a,  int16_t b)
{
    return  a/b;
}

#else

INLINE(uint32_t, ULONG_DIV2)  (ulong a, uint32_t b)
{
    return  a/b;
}

INLINE(int32_t,   LONG_DIV2)   (long a,  int32_t b)
{
    return  a/b;
}

#endif


#if QUAD_NLLONG == 2

INLINE(uint32_t,ULLONG_DIV2) (ullong a, uint32_t b)
{
    return  a/b;
}

INLINE( int32_t, LLONG_DIV2)  (llong a,  int32_t b)
{
    return  a/b;
}


INLINE( uint64_t,div2qu)  (QUAD_UTYPE a,   uint64_t b)
{
    return  a/b;
}

INLINE(  int64_t,div2qi) (QUAD_ITYPE a,    int64_t b)
{
    return  a/b;
}


#else

INLINE(uint64_t,ULLONG_DIV2) (ullong a, uint64_t b)
{
    return  a/b;
}

INLINE( int64_t, LLONG_DIV2)  (llong a,  int64_t b)
{
    return  a/b;
}

#endif

INLINE(Vwbu,VDHU_DIV2) (Vdhu a, Vwbu b)
{
    return  VWBU_NEWL(
        (vget_lane_u16(a, 0)/VWBU_GET1(b, 0)),
        (vget_lane_u16(a, 1)/VWBU_GET1(b, 1)),
        (vget_lane_u16(a, 2)/VWBU_GET1(b, 2)),
        (vget_lane_u16(a, 3)/VWBU_GET1(b, 3))
    );
}

INLINE(Vwbi,VDHI_DIV2) (Vdhi a, Vwbi b)
{
    return  VWBI_NEWL(
        (vget_lane_s16(a, 0)/VWBI_GET1(b, 0)),
        (vget_lane_s16(a, 1)/VWBI_GET1(b, 1)),
        (vget_lane_s16(a, 2)/VWBI_GET1(b, 2)),
        (vget_lane_s16(a, 3)/VWBI_GET1(b, 3))
    );
}


INLINE(Vwhu,VDWU_DIV2) (Vdwu a, Vwhu b)
{
    return  VWHU_NEWL(
        (vget_lane_u32(a, 0)/VWHU_GET1(b, 0)),
        (vget_lane_u32(a, 1)/VWHU_GET1(b, 1))
    );
}

INLINE(Vwhi,VDWI_DIV2) (Vdwi a, Vwhi b)
{
    return  VWHI_NEWL(
        (vget_lane_s32(a, 0)/VWHI_GET1(b, 0)),
        (vget_lane_s32(a, 1)/VWHI_GET1(b, 1))
    );
}


INLINE(Vwwu,VDDU_DIV2) (Vddu a, Vwwu b)
{
    return  UINT32_ASTV((vget_lane_u64(a, 0)/VWWU_ASTV(b)));
}

INLINE(Vwwi,VDDI_DIV2) (Vddi a, Vwwi b)
{
    return  INT32_ASTV((vget_lane_s64(a, 0)/VWWI_ASTV(b)));
}


INLINE(Vdbu,VQHU_DIV2) (Vqhu a, Vdbu b)
{
    return  VDBU_NEWL(
        (vgetq_lane_u16(a, 0)/vget_lane_u8(b, 0)),
        (vgetq_lane_u16(a, 1)/vget_lane_u8(b, 1)),
        (vgetq_lane_u16(a, 2)/vget_lane_u8(b, 2)),
        (vgetq_lane_u16(a, 3)/vget_lane_u8(b, 3)),
        (vgetq_lane_u16(a, 4)/vget_lane_u8(b, 4)),
        (vgetq_lane_u16(a, 5)/vget_lane_u8(b, 5)),
        (vgetq_lane_u16(a, 6)/vget_lane_u8(b, 6)),
        (vgetq_lane_u16(a, 7)/vget_lane_u8(b, 7))
    );
}

INLINE(Vdbi,VQHI_DIV2) (Vqhi a, Vdbi b)
{
    return  VDBU_NEWL(
        (vgetq_lane_s16(a, 0)/vget_lane_s8(b, 0)),
        (vgetq_lane_s16(a, 1)/vget_lane_s8(b, 1)),
        (vgetq_lane_s16(a, 2)/vget_lane_s8(b, 2)),
        (vgetq_lane_s16(a, 3)/vget_lane_s8(b, 3)),
        (vgetq_lane_s16(a, 4)/vget_lane_s8(b, 4)),
        (vgetq_lane_s16(a, 5)/vget_lane_s8(b, 5)),
        (vgetq_lane_s16(a, 6)/vget_lane_s8(b, 6)),
        (vgetq_lane_s16(a, 7)/vget_lane_s8(b, 7))
    );
}


INLINE(Vdhu,VQWU_DIV2) (Vqwu a, Vdhu b)
{
    return  VDHU_NEWL(
        (vgetq_lane_u32(a, 0)/vget_lane_u16(b, 0)),
        (vgetq_lane_u32(a, 1)/vget_lane_u16(b, 1)),
        (vgetq_lane_u32(a, 2)/vget_lane_u16(b, 2)),
        (vgetq_lane_u32(a, 3)/vget_lane_u16(b, 3))
    );
}

INLINE(Vdhi,VQWI_DIV2) (Vqwi a, Vdhi b)
{
    return  VDHI_NEWL(
        (vgetq_lane_s32(a, 0)/vget_lane_s16(b, 0)),
        (vgetq_lane_s32(a, 1)/vget_lane_s16(b, 1)),
        (vgetq_lane_s32(a, 2)/vget_lane_s16(b, 2)),
        (vgetq_lane_s32(a, 3)/vget_lane_s16(b, 3))
    );
}


INLINE(Vdwu,VQDU_DIV2) (Vqdu a, Vdwu b)
{
    return  VDWU_NEWL(
        (vgetq_lane_u64(a, 0)/vget_lane_u32(b, 0)),
        (vgetq_lane_u64(a, 1)/vget_lane_u32(b, 1))
    );
}

INLINE(Vdwi,VQDI_DIV2) (Vqdi a, Vdwi b)
{
    return  VDWI_NEWL(
        (vgetq_lane_s64(a, 0)/vget_lane_s32(b, 0)),
        (vgetq_lane_s64(a, 1)/vget_lane_s32(b, 1))
    );
}

#if _LEAVE_ARM_DIV2
}
#endif

#if _ENTER_ARM_DIVH
{
#endif
/*
There's a vdivh_f16 but...
*/
INLINE(flt16_t,  BOOL_DIVH)   (_Bool a, flt16_t b) {return a/b;}
INLINE(flt16_t, UCHAR_DIVH)   (uchar a, flt16_t b) {return a/b;}
INLINE(flt16_t, SCHAR_DIVH)   (schar a, flt16_t b) {return a/b;}
INLINE(flt16_t,  CHAR_DIVH)    (char a, flt16_t b) {return a/b;}
INLINE(flt16_t, USHRT_DIVH)  (ushort a, flt16_t b) {return a/b;}
INLINE(flt16_t,  SHRT_DIVH)   (short a, flt16_t b) {return a/b;}
INLINE(flt16_t,  UINT_DIVH)    (uint a, flt16_t b) {return a/b;}
INLINE(flt16_t,   INT_DIVH)     (int a, flt16_t b) {return a/b;}
INLINE(flt16_t, ULONG_DIVH)   (ulong a, flt16_t b) {return a/b;}
INLINE(flt16_t,  LONG_DIVH)    (long a, flt16_t b) {return a/b;}
INLINE(flt16_t,ULLONG_DIVH)  (ullong a, flt16_t b) {return a/b;}
INLINE(flt16_t, LLONG_DIVH)   (llong a, flt16_t b) {return a/b;}

INLINE(flt16_t, FLT16_DIVH) (flt16_t a, flt16_t b) {return a/b;}
INLINE(float,     FLT_DIVH)   (float a, flt16_t b) {return a/b;}
INLINE(double,    DBL_DIVH)  (double a, flt16_t b) {return a/b;}

#if QUAD_NLLONG == 2
INLINE(flt16_t,divhqu) (QUAD_UTYPE a, flt16_t b) {return a/b;}
INLINE(flt16_t,divhqi) (QUAD_ITYPE a, flt16_t b) {return a/b;}
INLINE(QUAD_FTYPE,divhqf) (QUAD_FTYPE a, flt16_t b) {return a/b;}
#endif


INLINE(Vdhf,VWBU_DIVH) (Vwbu a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdiv_f16(VWBU_CVHF(a), b);
#else
    float32x4_t p = VWBU_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vdivq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBI_DIVH) (Vwbi a, Vdhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdiv_f16(VWBI_CVHF(a), b);
#else
    float32x4_t p = VWBI_CVWF(a);
    float32x4_t q = VDHF_CVWF(b);
    p = vdivq_f32(p, q);
    return  vcvt_f16_f32(p);
#endif
}

INLINE(Vdhf,VWBC_DIVH) (Vwbc a, Vdhf b)
{
#if CHAR_MIN
    return VWBI_DIVH(VWBC_ASBI(a), b);
#else
    return VWBU_DIVH(VWBC_ASBU(a), b);
#endif
}


INLINE(float,WHF_DIVH) (float am, float bm)
{
#if defined(SPC_ARM_FP16_SIMD)
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    ah = vdiv_f16(ah, bh);
#else
    float32x2_t aw = vdup_n_f32(am);
    float16x4_t ah = vreinterpret_f16_f32(aw);
    float32x4_t av = vcvt_f32_f16(ah);
    float32x2_t bw = vdup_n_f32(bm);
    float16x4_t bh = vreinterpret_f16_f32(bw);
    float32x4_t bv = vcvt_f32_f16(bh);
    av = vdivq_f32(av, bv);
    ah = vcvt_f16_f32(av);
#endif
    aw = vreinterpret_f32_f16(ah);
    return  vget_lane_f32(aw, 0);
    
}


INLINE(Vwhf,VWHU_DIVH) (Vwhu a, Vwhf b)
{
    Vwhf    v = VWHU_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_DIVH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHI_DIVH) (Vwhi a, Vwhf b)
{
    Vwhf    v = VWHI_CVHF(a);
    float   m = VWHF_ASTM(v);
    m = WHF_DIVH(m, VWHF_ASTM(b));
    return  WHF_ASTV(m);
}

INLINE(Vwhf,VWHF_DIVH) (Vwhf a, Vwhf b)
{
    float   m = WHF_DIVH(VWHF_ASTM(a), VWHF_ASTM(b));
    return WHF_ASTV(m);
}


INLINE(Vqhf,VDBU_DIVH) (Vdbu a, Vqhf b) 
{
    uint16x8_t  c = vmovl_u8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_u16(c);
    return  vdivq_f16(f, b);
#else

    uint32x4_t  zl = vmovl_u16(vget_low_u16(c));
    float32x4_t fl = vcvtq_f32_u32(zl);
    uint32x4_t  zr = vmovl_u16(vget_high_u16(c));
    float32x4_t fr = vcvtq_f32_u32(zr);
    fl = vdivq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vdivq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBI_DIVH) (Vdbi a, Vqhf b) 
{
    int16x8_t   c = vmovl_s8(a);

#if defined(SPC_ARM_FP16_SIMD)
    float16x8_t f = vcvtq_f16_s16(c);
    return  vdivq_f16(f, b);
#else
    int32x4_t   zl = vmovl_s16(vget_low_s16(c));
    float32x4_t fl = vcvtq_f32_s32(zl);
    int32x4_t   zr = vmovl_s16(vget_high_s16(c));
    float32x4_t fr = vcvtq_f32_s32(zr);
    fl = vdivq_f32(fl, vcvt_f32_f16(vget_low_f16(b)));
    fr = vdivq_f32(fr, vcvt_f32_f16(vget_high_f16(b)));
    return  vcombine_f16(
        vcvt_f16_f32(fl),
        vcvt_f16_f32(fr)
    );
#endif
}

INLINE(Vqhf,VDBC_DIVH) (Vdbc a, Vqhf b)
{
#if CHAR_MIN
    return  VDBI_DIVH(VDBC_ASBI(a), b);
#else
    return  VDBU_DIVH(VDBC_ASBU(a), b);
#endif
}


INLINE(Vdhf,VDHU_DIVH) (Vdhu a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdiv_f16(vcvt_f16_u16(a), b);
#else
    float32x4_t l = vcvtq_f32_u32(vmovl_u16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vdivq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHI_DIVH) (Vdhi a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdiv_f16(vcvt_f16_s16(a), b);
#else
    float32x4_t l = vcvtq_f32_s32(vmovl_s16(a));
    float32x4_t r = vcvt_f32_f16(b);
    l = vdivq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}

INLINE(Vdhf,VDHF_DIVH) (Vdhf a, Vdhf b) 
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdiv_f16(a, b);
#else
    float32x4_t l = vcvt_f32_f16(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vdivq_f32(l, r);
    return  vcvt_f16_f32(l);
#endif
}


INLINE(Vwhf,VDWU_DIVH) (Vdwu a, Vwhf b)
{
    return  VWHF_DIVH(VDWU_CVHF(a), b);
}

INLINE(Vwhf,VDWI_DIVH) (Vdwi a, Vwhf b)
{
    return  VWHF_DIVH(VDWI_CVHF(a), b);
}

INLINE(Vdwf,VDWF_DIVH) (Vdwf a, Vwhf b)
{
    return vdiv_f32(a, VWHF_CVWF(b));
}


INLINE(Vqhf,VQHU_DIVH) (Vqhu a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdivq_f16(vcvtq_f16_u16(a), b);
#else
    return vcombine_f16(
        VDHU_DIVH(vget_low_u16(a),  vget_low_f16(b)),
        VDHU_DIVH(vget_high_u16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHI_DIVH) (Vqhi a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdivq_f16(vcvtq_f16_s16(a), b);
#else
    return vcombine_f16(
        VDHI_DIVH(vget_low_s16(a),  vget_low_f16(b)),
        VDHI_DIVH(vget_high_s16(a), vget_high_f16(b))
    );
#endif

}

INLINE(Vqhf,VQHF_DIVH) (Vqhf a, Vqhf b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vdivq_f16(a, b);
#else
    return vcombine_f16(
        VDHF_DIVH(vget_low_f16(a),  vget_low_f16(b)),
        VDHF_DIVH(vget_high_f16(a), vget_high_f16(b))
    );
#endif

}


INLINE(Vdhf,VQWU_DIVH) (Vqwu a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_u32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vdivq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vdhf,VQWI_DIVH) (Vqwi a, Vdhf b)
{
    float32x4_t l = vcvtq_f32_s32(a);
    float32x4_t r = vcvt_f32_f16(b);
    l = vdivq_f32(l, r);
    return  vcvt_f16_f32(l);
}

INLINE(Vqwf,VQWF_DIVH) (Vqwf a, Vdhf b)
{
    return  vdivq_f32(a, vcvt_f32_f16(b));
}

INLINE(Vwhf,VQDU_DIVH) (Vqdu a, Vwhf b)
{
    return  VWHF_DIVH(VQDU_CVHF(a), b);
}

INLINE(Vwhf,VQDI_DIVH) (Vqdi a, Vwhf b)
{
    return  VWHF_DIVH(VQDI_CVHF(a), b);
}

INLINE(Vqdf,VQDF_DIVH) (Vqdf a, Vwhf b)
{
    return  vdivq_f64(a, VWHF_CVDF(b));
}

#if _LEAVE_ARM_DIVH
}
#endif

#if _ENTER_ARM_DIVW
{
#endif

INLINE(float,  BOOL_DIVW)   (_Bool a, float b) {return a/b;}

INLINE(float, UCHAR_DIVW)   (uchar a, float b) {return a/b;}
INLINE(float, SCHAR_DIVW)   (schar a, float b) {return a/b;}
INLINE(float,  CHAR_DIVW)    (char a, float b) {return a/b;}
INLINE(float, USHRT_DIVW)  (ushort a, float b) {return a/b;}
INLINE(float,  SHRT_DIVW)   (short a, float b) {return a/b;}
INLINE(float,  UINT_DIVW)    (uint a, float b) {return a/b;}
INLINE(float,   INT_DIVW)     (int a, float b) {return a/b;}
INLINE(float, ULONG_DIVW)   (ulong a, float b) {return a/b;}
INLINE(float,  LONG_DIVW)    (long a, float b) {return a/b;}
INLINE(float,ULLONG_DIVW)  (ullong a, float b) {return a/b;}
INLINE(float, LLONG_DIVW)   (llong a, float b) {return a/b;}

INLINE(float, FLT16_DIVW) (flt16_t a, float b) {return a/b;}
INLINE(float,   FLT_DIVW)   (float a, float b) {return a/b;}
INLINE(double,  DBL_DIVW)  (double a, float b) {return a/b;}

#if QUAD_NLLONG == 2
INLINE(float,divwqu)   (QUAD_UTYPE a, float b) {return a/b;}
INLINE(float,divwqi)   (QUAD_ITYPE a, float b) {return a/b;}
INLINE(QUAD_FTYPE,divwqf) (QUAD_FTYPE a, float b) {return a/b;}
#endif


INLINE(Vqwf,VWBU_DIVW) (Vwbu a, Vqwf b)
{
    return  vdivq_f32(VWBU_CVWF(a), b);
}

INLINE(Vqwf,VWBI_DIVW) (Vwbi a, Vqwf b)
{
    return  vdivq_f32(VWBI_CVWF(a), b);
}

INLINE(Vqwf,VWBC_DIVW) (Vwbc a, Vqwf b)
{
#if CHAR_MIN
    return  VWBI_DIVW(VWBC_ASBI(a), b);
#else
    return  VWBU_DIVW(VWBC_ASBU(a), b);
#endif
}



INLINE(Vdwf,VWHU_DIVW) (Vwhu a, Vdwf b)
{
    return  vdiv_f32(VWHU_CVWF(a), b);
}

INLINE(Vdwf,VWHI_DIVW) (Vwhi a, Vdwf b)
{
    return  vdiv_f32(VWHI_CVWF(a), b);
}

INLINE(Vdwf,VWHF_DIVW) (Vwhf a, Vdwf b)
{
    return  vdiv_f32(VWHF_CVWF(a), b);
}


INLINE(Vwwf,VWWU_DIVW) (Vwwu a, Vwwf b)
{
    return WWF_ASTV((VWWU_ASTV(a)/VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWI_DIVW) (Vwwi a, Vwwf b)
{
    return WWF_ASTV((VWWI_ASTV(a)/VWWF_ASTM(b)));
}

INLINE(Vwwf,VWWF_DIVW) (Vwwf a, Vwwf b)
{
    return WWF_ASTV((VWWF_ASTM(a)/VWWF_ASTM(b)));
}


INLINE(Vqwf,VDHU_DIVW) (Vdhu a, Vqwf b) 
{
    return  vdivq_f32(VDHU_CVWF(a), b);
}

INLINE(Vqwf,VDHI_DIVW) (Vdhi a, Vqwf b) 
{
    return  vdivq_f32(VDHI_CVWF(a), b);
}

INLINE(Vqwf,VDHF_DIVW) (Vdhf a, Vqwf b) 
{
    return  vdivq_f32(VDHF_CVWF(a), b);
}


INLINE(Vdwf,VDWU_DIVW) (Vdwu a, Vdwf b)
{
    return vdiv_f32(vcvt_f32_u32(a), b);
}

INLINE(Vdwf,VDWI_DIVW) (Vdwi a, Vdwf b)
{
    return vdiv_f32(vcvt_f32_s32(a), b);
}

INLINE(Vdwf,VDWF_DIVW) (Vdwf a, Vdwf b)
{
    return vdiv_f32(a, b);
}


INLINE(Vwwf,VDDU_DIVW) (Vddu a, Vwwf b)
{
    return WWF_ASTV((VDDU_ASTV(a)/VWWF_ASTM(b)));
}

INLINE(Vwwf,VDDI_DIVW) (Vddi a, Vwwf b)
{
    return WWF_ASTV((VDDI_ASTV(a)/VWWF_ASTM(b)));
}

INLINE(Vddf,VDDF_DIVW) (Vddf a, Vwwf b)
{
    return vdiv_f64(a, vdup_n_f64(VWWF_ASTM(b)));
}


INLINE(Vqwf,VQWU_DIVW) (Vqwu a, Vqwf b)
{
    return  vdivq_f32(vcvtq_f32_u32(a), b);
}

INLINE(Vqwf,VQWI_DIVW) (Vqwi a, Vqwf b)
{
    return  vdivq_f32(vcvtq_f32_s32(a), b);
}

INLINE(Vqwf,VQWF_DIVW) (Vqwf a, Vqwf b)
{
    return  vdivq_f32(a, b);
}


INLINE(Vdwf,VQDU_DIVW) (Vqdu a, Vdwf b)
{
    return  vdiv_f32(vcvt_f32_f64(vcvtq_f64_u64(a)), b);
}

INLINE(Vdwf,VQDI_DIVW) (Vqdi a, Vdwf b)
{
    return  vdiv_f32(vcvt_f32_f64(vcvtq_f64_s64(a)), b);
}

INLINE(Vqdf,VQDF_DIVW) (Vqdf a, Vdwf b)
{
    return  vdivq_f64(a, vcvt_f64_f32(b));
}

#if _LEAVE_ARM_DIVW
}
#endif

#if _ENTER_ARM_DIVD
{
#endif

INLINE(double,  BOOL_DIVD)   (_Bool a, double b) {return a/b;}

INLINE(double, UCHAR_DIVD)   (uchar a, double b) {return a/b;}
INLINE(double, SCHAR_DIVD)   (schar a, double b) {return a/b;}
INLINE(double,  CHAR_DIVD)    (char a, double b) {return a/b;}
INLINE(double, USHRT_DIVD)  (ushort a, double b) {return a/b;}
INLINE(double,  SHRT_DIVD)   (short a, double b) {return a/b;}
INLINE(double,  UINT_DIVD)    (uint a, double b) {return a/b;}
INLINE(double,   INT_DIVD)     (int a, double b) {return a/b;}
INLINE(double, ULONG_DIVD)   (ulong a, double b) {return a/b;}
INLINE(double,  LONG_DIVD)    (long a, double b) {return a/b;}
INLINE(double,ULLONG_DIVD)  (ullong a, double b) {return a/b;}
INLINE(double, LLONG_DIVD)   (llong a, double b) {return a/b;}

INLINE(double, FLT16_DIVD) (flt16_t a, double b) {return a/b;}
INLINE(double,   FLT_DIVD)   (float a, double b) {return a/b;}
INLINE(double,   DBL_DIVD)  (double a, double b) {return a/b;}

#if QUAD_NLLONG == 2
INLINE(double,divdqu)   (QUAD_UTYPE a, double b) {return a/b;}
INLINE(double,divdqi)   (QUAD_ITYPE a, double b) {return a/b;}
INLINE(QUAD_FTYPE,divdqf) (QUAD_FTYPE a, double b) {return a/b;}
#endif


INLINE(Vqdf,VWHU_DIVD) (Vwhu a, Vqdf b)
{
    return  vdivq_f64(VWHU_CVDF(a), b);
}

INLINE(Vqdf,VWHI_DIVD) (Vwhi a, Vqdf b)
{
    return  vdivq_f64(VWHI_CVDF(a), b);
}

INLINE(Vqdf,VWHF_DIVD) (Vwhf a, Vqdf b)
{
    return  vdivq_f64(VWHF_CVDF(a), b);
}


INLINE(Vddf,VWWU_DIVD) (Vwwu a, Vddf b)
{
    return  vdiv_f64(VWWU_CVDF(a), b);
}

INLINE(Vddf,VWWI_DIVD) (Vwwi a, Vddf b)
{
    return  vdiv_f64(VWWI_CVDF(a), b);
}

INLINE(Vddf,VWWF_DIVD) (Vwwf a, Vddf b)
{
    return  vdiv_f64(VWWF_CVDF(a), b);
}


INLINE(Vqdf,VDWU_DIVD) (Vdwu a, Vqdf b)
{
    return  vdivq_f64(vcvtq_f64_u64(vmovl_u32(a)), b);
}

INLINE(Vqdf,VDWI_DIVD) (Vdwi a, Vqdf b)
{
    return  vdivq_f64(vcvtq_f64_s64(vmovl_s32(a)), b);
}

INLINE(Vqdf,VDWF_DIVD) (Vdwf a, Vqdf b)
{
    return  vdivq_f64(vcvt_f64_f32(a), b);
}


INLINE(Vddf,VDDU_DIVD) (Vddu a, Vddf b)
{
    return  vdiv_f64(vcvt_f64_u64(a), b);
}

INLINE(Vddf,VDDI_DIVD) (Vddi a, Vddf b)
{
    return  vdiv_f64(vcvt_f64_s64(a), b);
}

INLINE(Vddf,VDDF_DIVD) (Vddf a, Vddf b)
{
    return  vdiv_f64(a, b);
}


INLINE(Vqdf,VQDU_DIVD) (Vqdu a, Vqdf b)
{
    return  vdivq_f64(vcvtq_f64_u64(a), b);
}

INLINE(Vqdf,VQDI_DIVD) (Vqdi a, Vqdf b)
{
    return  vdivq_f64(vcvtq_f64_s64(a), b);
}

INLINE(Vqdf,VQDF_DIVD) (Vqdf a, Vqdf b)
{
    return  vdivq_f64(a, b);
}

#if _LEAVE_ARM_DIVD
}
#endif

#if _ENTER_ARM_DIFU
{
#endif

INLINE( _Bool,  BOOL_DIFU)  (_Bool a,  _Bool b) {return a^b;}


INLINE( uchar, UCHAR_DIFU)  (uchar a,  uchar b) 
{
    return  vget_lane_s32(vabd_s32(vdup_n_s32(a), vdup_n_s32(b)), 0);
}

INLINE( uchar, SCHAR_DIFU)  (schar a,  schar b) 
{
    return  vget_lane_s32(vabd_s32(vdup_n_s32(a), vdup_n_s32(b)), 0);
}

INLINE( uchar,  CHAR_DIFU)   (char a,   char b) 
{
#if CHAR_MIN
    return  SCHAR_DIFU(a, b);
#else
    return  UCHAR_DIFU(a, b);
#endif
}


INLINE(ushort, USHRT_DIFU) (ushort a, ushort b) 
{
    return  vget_lane_s32(vabd_s32(vdup_n_s32(a), vdup_n_s32(b)), 0);
}

INLINE(ushort,  SHRT_DIFU)  (short a,  short b)
{
    return  vget_lane_s32(vabd_s32(vdup_n_s32(a), vdup_n_s32(b)), 0);
}


INLINE(  uint,  UINT_DIFU)   (uint a,   uint b) 
{
    return  vget_lane_u32(vabd_u32(vdup_n_u32(a), vdup_n_u32(b)), 0);
}

INLINE(   int,   INT_DIFU)    (int a,    int b) 
{
    return  vget_lane_s32(vabd_s32(vdup_n_s32(a), vdup_n_s32(b)), 0);
}


INLINE( ulong, ULONG_DIFU)  (ulong a,  ulong b) 
{
#if DWRD_NLONG == 2
    return  UINT_DIFU(a, b);
#else
    return  (a <= b) ? (b-a) : (a-b);
#endif
}

INLINE( ulong,  LONG_DIFU)   (long a,   long b) 
{
#if DWRD_NLONG == 2
    return  INT_DIFU(a, b);
#else
    return  (a <= b)
    ?   (ulong) b-(ulong) a
    :   (ulong) a-(ulong) b;
#endif
}


INLINE(ullong,ULLONG_DIFU) (ullong a, ullong b) 
{
    return  (a <= b) ? (b-a) : (a-b);
}

INLINE(ullong, LLONG_DIFU)  (llong a,  llong b) 
{
    return  (a <= b)
    ?   (ullong) b-(ullong) a
    :   (ullong) a-(ullong) b;
}


INLINE(uint16_t,FLT16_DIFU) (flt16_t a, flt16_t b)
{
#if defined(SPC_ARM_FP16_SIMD)
    return  vabdh_f16(a, b);
#else
    return  vabds_f32(a, b);
#endif
}

INLINE(uint32_t,FLT_DIFU)  (float a,  float b) {return  vabds_f32(a, b);}
INLINE(uint64_t,DBL_DIFU) (double a, double b) {return  vabdd_f64(a, b);}


#if QUAD_NLLONG == 2

INLINE(QUAD_UTYPE,difuqu) (QUAD_UTYPE a, QUAD_UTYPE b) 
{
    return  (a <= b) ? (b-a) : (a-b);
}

INLINE(QUAD_UTYPE,difuqi) (QUAD_ITYPE a, QUAD_ITYPE b) 
{
    return  (a <= b)
    ?   (QUAD_UTYPE) b-(QUAD_UTYPE) a
    :   (QUAD_UTYPE) a-(QUAD_UTYPE) b;
}

INLINE(QUAD_UTYPE,difuqf) (QUAD_FTYPE a, QUAD_FTYPE b) 
{
    return  (a <= b) ? (b-a) : (a-b);
}

#endif

INLINE(Vwyu,VWYU_DIFU) (Vwyu a, Vwyu b)
{
    float l = VWYU_ASTM(a);
    float r = VWYU_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    uint32x2_t  x = vreinterpret_u32_f32(p);
    uint32x2_t  y = vreinterpret_u32_f32(q);
    x = veor_u32(x, y);
    p = vreinterpret_f32_u32(x);
    l = vget_lane_f32(p, 0);
    return  WYU_ASTV(l);
}


INLINE(Vwbu,VWBU_DIFU) (Vwbu a, Vwbu b)
{
    float l = VWBU_ASTM(a);
    float r = VWBU_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    uint8x8_t  x = vreinterpret_u8_f32(p);
    uint8x8_t  y = vreinterpret_u8_f32(q);
    x = vabd_u8(x, y);
    p = vreinterpret_f32_u8(x);
    l = vget_lane_f32(p, 0);
    return  WBU_ASTV(l);
}

INLINE(Vwbu,VWBI_DIFU) (Vwbi a, Vwbi b)
{
    float l = VWBI_ASTM(a);
    float r = VWBI_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    int8x8_t    x = vreinterpret_s8_f32(p);
    int8x8_t    y = vreinterpret_s8_f32(q);
    x = vabd_s8(x, y);
    p = vreinterpret_f32_s8(x);
    l = vget_lane_f32(p, 0);
    return  WBU_ASTV(l);
}

INLINE(Vwbu,VWBC_DIFU) (Vwbc a, Vwbc b)
{
#if CHAR_MIN
    return  VWBI_DIFU(VWBC_ASBI(a), VWBC_ASBI(b));
#else
    return  VWBU_DIFU(VWBC_ASBU(a), VWBC_ASBU(b));
#endif
}


INLINE(Vwhu,VWHU_DIFU) (Vwhu a, Vwhu b)
{
    float l = VWHU_ASTM(a);
    float r = VWHU_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    uint16x4_t  x = vreinterpret_u16_f32(p);
    uint16x4_t  y = vreinterpret_u16_f32(q);
    x = vabd_u16(x, y);
    p = vreinterpret_f32_u16(x);
    l = vget_lane_f32(p, 0);
    return  WHU_ASTV(l);
}

INLINE(Vwhu,VWHI_DIFU) (Vwhi a, Vwhi b)
{
    float l = VWHI_ASTM(a);
    float r = VWHI_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    int16x4_t   x = vreinterpret_s16_f32(p);
    int16x4_t   y = vreinterpret_s16_f32(q);
    x = vabd_s16(x, y);
    p = vreinterpret_f32_s16(x);
    l = vget_lane_f32(p, 0);
    return  WHU_ASTV(l);
}

INLINE(Vwhu,VWHF_DIFU) (Vwhf a, Vwhf b)
{
    float       l = VWHF_ASTM(a);
    float       r = VWHF_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    float16x4_t x = vreinterpret_f16_f32(p);
    float16x4_t y = vreinterpret_f16_f32(q);

#if defined(SPC_ARM_FP16_SIMD)
    x = vabd_f16(x, y);
    uint16x4_t  u = vcvt_u16_f16(x);
#else
    float32x4_t z = vabdq_f32(
        vcvt_f32_f16(x), 
        vcvt_f32_f16(y)
    );
    uint32x4_t  w = vcvtq_u32_f32(z);
    uint16x4_t  u = vqmovn_u32(w);
#endif
    p = vreinterpret_f32_u16(u);
    l = vget_lane_f32(p, 0);
    return  WHU_ASTV(l);
}


INLINE(Vwwu,VWWU_DIFU) (Vwwu a, Vwwu b)
{
    float l = VWWU_ASTM(a);
    float r = VWWU_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    uint32x2_t  x = vreinterpret_u32_f32(p);
    uint32x2_t  y = vreinterpret_u32_f32(q);
    x = vabd_u32(x, y);
    p = vreinterpret_f32_u32(x);
    l = vget_lane_f32(p, 0);
    return  WWU_ASTV(l);
}

INLINE(Vwwu,VWWI_DIFU) (Vwwi a, Vwwi b)
{
    float l = VWWI_ASTM(a);
    float r = VWWI_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    int32x2_t   x = vreinterpret_s32_f32(p);
    int32x2_t   y = vreinterpret_s32_f32(q);
    x = vabd_s32(x, y);
    p = vreinterpret_f32_s32(x);
    l = vget_lane_f32(p, 0);
    return  WWU_ASTV(l);
}

INLINE(Vwwu,VWWF_DIFU) (Vwwf a, Vwwf b)
{
    float l = VWWF_ASTM(a);
    float r = VWWF_ASTM(b);
    float32x2_t p = vdup_n_f32(l);
    float32x2_t q = vdup_n_f32(r);
    p = vabd_f32(p, q);
    uint32x2_t  u = vcvt_u32_f32(p);
    p = vreinterpret_f32_u32(u);
    l = vget_lane_f32(p, 0);
    return  WWU_ASTV(l);
}


INLINE(Vdyu,VDYU_DIFU) (Vdyu a, Vdyu b)
{
#define     VDYU_DIFU(A, B) DYU_ASTV(veor_u64(VDYU_ASTM(A),VDYU_ASTM(B)))
    return  VDYU_DIFU(a, b);
}


INLINE(Vdbu,VDBU_DIFU) (Vdbu a, Vdbu b) {return vabd_u8(a, b);}

INLINE(Vdbu,VDBI_DIFU) (Vdbi a, Vdbi b) 
{
    a = vabd_s8(a, b);
    return  vreinterpret_u8_s8(a);
}

INLINE(Vdbu,VDBC_DIFU) (Vdbc a, Vdbc b)
{
#if CHAR_MIN
    return  VDBI_DIFU(VDBC_ASBI(a), VDBC_ASBI(b));
#else
    return  VDBU_DIFU(VDBC_ASBU(a), VDBC_ASBU(b));
#endif
}


INLINE(Vdhu,VDHU_DIFU) (Vdhu a, Vdhu b) {return vabd_u16(a, b);}

INLINE(Vdhi,VDHI_DIFU) (Vdhi a, Vdhi b) 
{
    a = vabd_s16(a, b);
    return  vreinterpret_u16_s16(a);
}

INLINE(Vdwu,VDWU_DIFU) (Vdwu a, Vdwu b) {return vabd_u32(a, b);}

INLINE(Vdwi,VDWI_DIFU) (Vdwi a, Vdwi b)
{
    a = vabd_s32(a, b);
    return  vreinterpret_u32_s32(a);
}

INLINE(Vddu,VDDU_DIFU) (Vddu a, Vddu b) 
{
    uint64x1_t l = VDDU_MINL(a, b);
    uint64x1_t r = VDDU_MAXL(a, b);
    return  vsub_u64(l, r);
}

INLINE(Vddu,VDDI_DIFU) (Vddi a, Vddi b)
{
    int64x1_t p = VDDI_MINL(a, b);
    int64x1_t q = VDDI_MAXL(a, b);
    return VDDU_DIFU(
        vreinterpret_u64_s64(p),
        vreinterpret_u64_s64(q)
    );
}


INLINE(Vqyu,VQYU_DIFU) (Vqyu a, Vqyu b)
{
#define     VQYU_DIFU(A, B) QYU_ASTV(veorq_u64(VQYU_ASTM(A),VQYU_ASTM(B)))
    return  VQYU_DIFU(a, b);
}


INLINE(Vqbu,VQBU_DIFU) (Vqbu a, Vqbu b) {return vabdq_u8(a, b);}

INLINE(Vqbu,VQBI_DIFU) (Vqbi a, Vqbi b) 
{
    a = vabdq_s8(a, b);
    return  vreinterpretq_u8_s8(a);
}

INLINE(Vqbu,VQBC_DIFU) (Vqbc a, Vqbc b)
{
#if CHAR_MIN
    return  VQBI_DIFU(VQBC_ASBI(a), VQBC_ASBI(b));
#else
    return  VQBU_DIFU(VQBC_ASBU(a), VQBC_ASBU(b));
#endif
}


INLINE(Vqhu,VQHU_DIFU) (Vqhu a, Vqhu b) {return vabdq_u16(a, b);}

INLINE(Vqhi,VQHI_DIFU) (Vqhi a, Vqhi b) 
{
    a = vabdq_s16(a, b);
    return  vreinterpretq_u16_s16(a);
}


INLINE(Vqwu,VQWU_DIFU) (Vqwu a, Vqwu b) {return vabdq_u32(a, b);}

INLINE(Vqwi,VQWI_DIFU) (Vqwi a, Vqwi b)
{
    a = vabdq_s32(a, b);
    return  vreinterpretq_u32_s32(a);
}


INLINE(Vqdu,VQDU_DIFU) (Vqdu a, Vqdu b) 
{
    uint64x2_t l = VQDU_MINL(a, b);
    uint64x2_t r = VQDU_MAXL(a, b);
    return  vsubq_u64(l, r);
}

INLINE(Vqdu,VQDI_DIFU) (Vqdi a, Vqdi b)
{
    int64x2_t p = VQDI_MINL(a, b);
    int64x2_t q = VQDI_MAXL(a, b);
    return  VQDU_DIFU(
        vreinterpretq_u64_s64(p),
        vreinterpretq_u64_s64(q)
    );
}



#if _LEAVE_ARM_DIFU
}
#endif

#endif // #ifdef SPC_ARM_NEON


#if _LEAVE_ARM__
}
#endif
